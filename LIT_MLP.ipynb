{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaviCMaciel/LIT/blob/main/LIT_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfkjR2KgPWMz"
      },
      "source": [
        "Tarefa 4:\n",
        "\n",
        "\n",
        "1.   MLP pra o mesmo problema da tarefa 3.\n",
        "\n",
        "2.   Explorar numero de neurônios na camada escondida (só uma).\n",
        "\n",
        "\n",
        "3. Explorar diferentes funções de ativação.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl-2pLtuNPXY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.feature import local_binary_pattern\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn\n",
        "import cv2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAr_qCjPbgpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0e5dfa-a2c3-49f1-f9e7-dfbe9b1aa664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle opencv-python-headless scikit-learn matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6YkHunjQuEV",
        "outputId": "b2256786-1b9b-41aa-9741-57cf2e6f7cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/saurabhshahane/barkvn50?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 176M/176M [00:03<00:00, 61.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/saurabhshahane/barkvn50/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"saurabhshahane/barkvn50\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "blpz8dKyRHtI",
        "outputId": "1eaba302-fbfa-4a89-dfb6-9ca9883cedfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/saurabhshahane/barkvn50/versions/1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFYnwtcDROnn"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BOxhp-pUabH",
        "outputId": "761c1cd6-99ae-438d-b301-fda724daf89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de imagens: 551\n",
            "Dimensões das imagens: (551, 128, 128)\n",
            "Classes únicas: ['Acacia' 'Cedrus' 'Eucalyptus' 'Musa' 'Wrightia']\n"
          ]
        }
      ],
      "source": [
        "def load_dataset_recursive(path, classes):\n",
        "    data, labels = [], []\n",
        "\n",
        "    # Caminhando pelos diretórios e arquivos\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            # Verificando a extensão da imagem\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "                img_path = os.path.join(root, file)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                if img is not None:\n",
        "                    # Verificar se a classe (diretório) está na lista de classes\n",
        "                    class_name = os.path.basename(root)\n",
        "                    if class_name in classes:\n",
        "                        img = cv2.resize(img, (128, 128))\n",
        "                        data.append(img)\n",
        "                        labels.append(class_name)\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Classes específicas que você deseja incluir\n",
        "classes = ['Acacia', 'Cedrus', 'Eucalyptus', 'Musa', 'Wrightia']\n",
        "\n",
        "# Caminho para o dataset\n",
        "dataset_path = path\n",
        "\n",
        "# Carregar dataset\n",
        "data, labels = load_dataset_recursive(dataset_path, classes)\n",
        "\n",
        "# Exibir os dados carregados e as classes\n",
        "print(f\"Total de imagens: {len(data)}\")\n",
        "print(f\"Dimensões das imagens: {data.shape}\")\n",
        "print(f\"Classes únicas: {np.unique(labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba5KS844Vifc"
      },
      "outputs": [],
      "source": [
        "def LBP(imagens, raio = 1, n_pontos = 8):\n",
        "  features = []\n",
        "  for img in imagens:\n",
        "        lbp = local_binary_pattern(img, n_pontos, raio, method=\"uniform\")\n",
        "        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_pontos + 3), range=(0, n_pontos + 2))\n",
        "        hist = hist.astype(\"float\")\n",
        "        hist /= hist.sum()\n",
        "        features.append(hist)\n",
        "  return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLCrw6-uV8_r"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "X = LBP(data)\n",
        "y = le.fit_transform(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QE2dp5jcZat"
      },
      "outputs": [],
      "source": [
        "neurons = [30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
        "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
        "alphas = [0.001, 0.01, 0.1]\n",
        "melhores_resultados = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6u5CD6zWKco"
      },
      "outputs": [],
      "source": [
        "# Criar o modelo MLP\n",
        "\n",
        "def create_MLP (neurons, activation, alpha):\n",
        "  acc = []\n",
        "  mlp = MLPClassifier(\n",
        "    hidden_layer_sizes= (neurons),    # Uma camada escondida: N neurônios\n",
        "    random_state= 10,                 # Para reprodutibilidade\n",
        "    verbose= True,                    # Mostrar progresso do treinamento\n",
        "    learning_rate_init= alpha,        # Taxa de aprendizado inicial\n",
        "    max_iter= 100,                    # Número máximo de iterações (épocas)\n",
        "    activation= activation            # Função de ativação\n",
        "    )\n",
        "  # Treinamento do modelo\n",
        "  # print(\"Treinando o MLP...\")\n",
        "  # for i in range(1, 31):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state= 42)\n",
        "  mlp.fit(X_train, y_train)\n",
        "  # Predição no conjunto de teste\n",
        "  y_pred = mlp.predict(X_test)\n",
        "  accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "  # acc.append(accuracy)\n",
        "  # accuracy = np.mean(acc)\n",
        "  # print(\"Acurácia média:\", media_acc)\n",
        "\n",
        "  return accuracy * 100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def explorar_MLP ():\n",
        "  resultados = {}\n",
        "  aux = 0.0\n",
        "  for neuron in neurons:\n",
        "    for act in activation:\n",
        "      for alpha in alphas:\n",
        "        acuracia = create_MLP(neuron, act, alpha)\n",
        "        resultados[(neuron, act, alpha)] = acuracia\n",
        "        chave = (neuron, act)\n",
        "        if chave not in melhores_resultados or acuracia > melhores_resultados[chave][\"accuracy\"]:\n",
        "          melhores_resultados[chave] = {\n",
        "                        \"neurons\": neuron,\n",
        "                        \"activation\": act,\n",
        "                        \"alpha\": alpha,\n",
        "                        \"accuracy\": acuracia\n",
        "                    }\n",
        "  return resultados"
      ],
      "metadata": {
        "id": "4vflTZNiJP_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwGoEzNHgM5Z",
        "outputId": "5704762a-cf71-4ea4-a7cb-5ff73d8ba5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.69572185\n",
            "Iteration 2, loss = 1.68647367\n",
            "Iteration 3, loss = 1.67776671\n",
            "Iteration 4, loss = 1.66977566\n",
            "Iteration 5, loss = 1.66126823\n",
            "Iteration 6, loss = 1.65456649\n",
            "Iteration 7, loss = 1.64759528\n",
            "Iteration 8, loss = 1.64182598\n",
            "Iteration 9, loss = 1.63578098\n",
            "Iteration 10, loss = 1.63061859\n",
            "Iteration 11, loss = 1.62588507\n",
            "Iteration 12, loss = 1.62142420\n",
            "Iteration 13, loss = 1.61782476\n",
            "Iteration 14, loss = 1.61404405\n",
            "Iteration 15, loss = 1.61094738\n",
            "Iteration 16, loss = 1.60786873\n",
            "Iteration 17, loss = 1.60534516\n",
            "Iteration 18, loss = 1.60279676\n",
            "Iteration 19, loss = 1.60064986\n",
            "Iteration 20, loss = 1.59851315\n",
            "Iteration 21, loss = 1.59676024\n",
            "Iteration 22, loss = 1.59518767\n",
            "Iteration 23, loss = 1.59368413\n",
            "Iteration 24, loss = 1.59249711\n",
            "Iteration 25, loss = 1.59105510\n",
            "Iteration 26, loss = 1.58999853\n",
            "Iteration 27, loss = 1.58875194\n",
            "Iteration 28, loss = 1.58776963\n",
            "Iteration 29, loss = 1.58676872\n",
            "Iteration 30, loss = 1.58583560\n",
            "Iteration 31, loss = 1.58482912\n",
            "Iteration 32, loss = 1.58388433\n",
            "Iteration 33, loss = 1.58305756\n",
            "Iteration 34, loss = 1.58212812\n",
            "Iteration 35, loss = 1.58117224\n",
            "Iteration 36, loss = 1.58026367\n",
            "Iteration 37, loss = 1.57939938\n",
            "Iteration 38, loss = 1.57847471\n",
            "Iteration 39, loss = 1.57764755\n",
            "Iteration 40, loss = 1.57666622\n",
            "Iteration 41, loss = 1.57574111\n",
            "Iteration 42, loss = 1.57481235\n",
            "Iteration 43, loss = 1.57387846\n",
            "Iteration 44, loss = 1.57292847\n",
            "Iteration 45, loss = 1.57206686\n",
            "Iteration 46, loss = 1.57100615\n",
            "Iteration 47, loss = 1.57016496\n",
            "Iteration 48, loss = 1.56906508\n",
            "Iteration 49, loss = 1.56813200\n",
            "Iteration 50, loss = 1.56710690\n",
            "Iteration 51, loss = 1.56613000\n",
            "Iteration 52, loss = 1.56507825\n",
            "Iteration 53, loss = 1.56412067\n",
            "Iteration 54, loss = 1.56304543\n",
            "Iteration 55, loss = 1.56198064\n",
            "Iteration 56, loss = 1.56102888\n",
            "Iteration 57, loss = 1.55990441\n",
            "Iteration 58, loss = 1.55881903\n",
            "Iteration 59, loss = 1.55776306\n",
            "Iteration 60, loss = 1.55669040\n",
            "Iteration 61, loss = 1.55558747\n",
            "Iteration 62, loss = 1.55445727\n",
            "Iteration 63, loss = 1.55337654\n",
            "Iteration 64, loss = 1.55221676\n",
            "Iteration 65, loss = 1.55111938\n",
            "Iteration 66, loss = 1.54989717\n",
            "Iteration 67, loss = 1.54876241\n",
            "Iteration 68, loss = 1.54756858\n",
            "Iteration 69, loss = 1.54640977\n",
            "Iteration 70, loss = 1.54518605\n",
            "Iteration 71, loss = 1.54396566\n",
            "Iteration 72, loss = 1.54281707\n",
            "Iteration 73, loss = 1.54149359\n",
            "Iteration 74, loss = 1.54024952\n",
            "Iteration 75, loss = 1.53898619\n",
            "Iteration 76, loss = 1.53780328\n",
            "Iteration 77, loss = 1.53639857\n",
            "Iteration 78, loss = 1.53510327\n",
            "Iteration 79, loss = 1.53379489\n",
            "Iteration 80, loss = 1.53248444\n",
            "Iteration 81, loss = 1.53120689\n",
            "Iteration 82, loss = 1.52973207\n",
            "Iteration 83, loss = 1.52844628\n",
            "Iteration 84, loss = 1.52704782\n",
            "Iteration 85, loss = 1.52575686\n",
            "Iteration 86, loss = 1.52423376\n",
            "Iteration 87, loss = 1.52284371\n",
            "Iteration 88, loss = 1.52135957\n",
            "Iteration 89, loss = 1.51996510\n",
            "Iteration 90, loss = 1.51850156\n",
            "Iteration 91, loss = 1.51708213\n",
            "Iteration 92, loss = 1.51555710\n",
            "Iteration 93, loss = 1.51407848\n",
            "Iteration 94, loss = 1.51251167\n",
            "Iteration 95, loss = 1.51111249\n",
            "Iteration 96, loss = 1.50948180\n",
            "Iteration 97, loss = 1.50793968\n",
            "Iteration 98, loss = 1.50636235\n",
            "Iteration 99, loss = 1.50489759\n",
            "Iteration 100, loss = 1.50321124\n",
            "Iteration 1, loss = 1.67486791\n",
            "Iteration 2, loss = 1.61567090\n",
            "Iteration 3, loss = 1.59561624\n",
            "Iteration 4, loss = 1.59415782\n",
            "Iteration 5, loss = 1.59033657\n",
            "Iteration 6, loss = 1.57978989\n",
            "Iteration 7, loss = 1.56825755\n",
            "Iteration 8, loss = 1.55474550"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9, loss = 1.54310575\n",
            "Iteration 10, loss = 1.53417988\n",
            "Iteration 11, loss = 1.52410455\n",
            "Iteration 12, loss = 1.51415783\n",
            "Iteration 13, loss = 1.50138516\n",
            "Iteration 14, loss = 1.48662751\n",
            "Iteration 15, loss = 1.47212253\n",
            "Iteration 16, loss = 1.45672290\n",
            "Iteration 17, loss = 1.44296464\n",
            "Iteration 18, loss = 1.42709183\n",
            "Iteration 19, loss = 1.40976305\n",
            "Iteration 20, loss = 1.39410886\n",
            "Iteration 21, loss = 1.37775307\n",
            "Iteration 22, loss = 1.36050541\n",
            "Iteration 23, loss = 1.34391053\n",
            "Iteration 24, loss = 1.32836805\n",
            "Iteration 25, loss = 1.31371435\n",
            "Iteration 26, loss = 1.29929100\n",
            "Iteration 27, loss = 1.28500765\n",
            "Iteration 28, loss = 1.27242065\n",
            "Iteration 29, loss = 1.25943317\n",
            "Iteration 30, loss = 1.24905838\n",
            "Iteration 31, loss = 1.23853703\n",
            "Iteration 32, loss = 1.22868198\n",
            "Iteration 33, loss = 1.21957492\n",
            "Iteration 34, loss = 1.21163815\n",
            "Iteration 35, loss = 1.20468046\n",
            "Iteration 36, loss = 1.19731736\n",
            "Iteration 37, loss = 1.19076812\n",
            "Iteration 38, loss = 1.18543427\n",
            "Iteration 39, loss = 1.18021449\n",
            "Iteration 40, loss = 1.17461344\n",
            "Iteration 41, loss = 1.17021308\n",
            "Iteration 42, loss = 1.16581820\n",
            "Iteration 43, loss = 1.16198547\n",
            "Iteration 44, loss = 1.15818328\n",
            "Iteration 45, loss = 1.15538683\n",
            "Iteration 46, loss = 1.15113540\n",
            "Iteration 47, loss = 1.14978971\n",
            "Iteration 48, loss = 1.14535837\n",
            "Iteration 49, loss = 1.14303540\n",
            "Iteration 50, loss = 1.14004448\n",
            "Iteration 51, loss = 1.13829303\n",
            "Iteration 52, loss = 1.13534216\n",
            "Iteration 53, loss = 1.13410130\n",
            "Iteration 54, loss = 1.13111279\n",
            "Iteration 55, loss = 1.12882089\n",
            "Iteration 56, loss = 1.12726787\n",
            "Iteration 57, loss = 1.12478659\n",
            "Iteration 58, loss = 1.12314572\n",
            "Iteration 59, loss = 1.12161364\n",
            "Iteration 60, loss = 1.11971252\n",
            "Iteration 61, loss = 1.11997070\n",
            "Iteration 62, loss = 1.11759330\n",
            "Iteration 63, loss = 1.11569665\n",
            "Iteration 64, loss = 1.11388884\n",
            "Iteration 65, loss = 1.11321134\n",
            "Iteration 66, loss = 1.11161979\n",
            "Iteration 67, loss = 1.11137208\n",
            "Iteration 68, loss = 1.10825852\n",
            "Iteration 69, loss = 1.10720248\n",
            "Iteration 70, loss = 1.10644926\n",
            "Iteration 71, loss = 1.10483080\n",
            "Iteration 72, loss = 1.10479501\n",
            "Iteration 73, loss = 1.10306329\n",
            "Iteration 74, loss = 1.10076130\n",
            "Iteration 75, loss = 1.10130361\n",
            "Iteration 76, loss = 1.10060908\n",
            "Iteration 77, loss = 1.09849567\n",
            "Iteration 78, loss = 1.09735365\n",
            "Iteration 79, loss = 1.09583799\n",
            "Iteration 80, loss = 1.09548302\n",
            "Iteration 81, loss = 1.09571558\n",
            "Iteration 82, loss = 1.09325993\n",
            "Iteration 83, loss = 1.09172221\n",
            "Iteration 84, loss = 1.09247000\n",
            "Iteration 85, loss = 1.09106628\n",
            "Iteration 86, loss = 1.08911083\n",
            "Iteration 87, loss = 1.08714284\n",
            "Iteration 88, loss = 1.08590138\n",
            "Iteration 89, loss = 1.08518261\n",
            "Iteration 90, loss = 1.08371123\n",
            "Iteration 91, loss = 1.08324461\n",
            "Iteration 92, loss = 1.08293417\n",
            "Iteration 93, loss = 1.08043580\n",
            "Iteration 94, loss = 1.07934343\n",
            "Iteration 95, loss = 1.08078340\n",
            "Iteration 96, loss = 1.07860780\n",
            "Iteration 97, loss = 1.07713319\n",
            "Iteration 98, loss = 1.07696401\n",
            "Iteration 99, loss = 1.07594194\n",
            "Iteration 100, loss = 1.07265347\n",
            "Iteration 1, loss = 1.73699989\n",
            "Iteration 2, loss = 1.68432460\n",
            "Iteration 3, loss = 1.56246464\n",
            "Iteration 4, loss = 1.49718301\n",
            "Iteration 5, loss = 1.37044447\n",
            "Iteration 6, loss = 1.31789516\n",
            "Iteration 7, loss = 1.24226254\n",
            "Iteration 8, loss = 1.19422146\n",
            "Iteration 9, loss = 1.17745670\n",
            "Iteration 10, loss = 1.17250149\n",
            "Iteration 11, loss = 1.14970530\n",
            "Iteration 12, loss = 1.13870774\n",
            "Iteration 13, loss = 1.13223355\n",
            "Iteration 14, loss = 1.13089569\n",
            "Iteration 15, loss = 1.12288150\n",
            "Iteration 16, loss = 1.11466468\n",
            "Iteration 17, loss = 1.12680164\n",
            "Iteration 18, loss = 1.10697859\n",
            "Iteration 19, loss = 1.10768103\n",
            "Iteration 20, loss = 1.10357461\n",
            "Iteration 21, loss = 1.10567219\n",
            "Iteration 22, loss = 1.09756527\n",
            "Iteration 23, loss = 1.12516590\n",
            "Iteration 24, loss = 1.10209886\n",
            "Iteration 25, loss = 1.10960214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 1.09338492\n",
            "Iteration 27, loss = 1.09472963\n",
            "Iteration 28, loss = 1.08100507\n",
            "Iteration 29, loss = 1.07543833\n",
            "Iteration 30, loss = 1.06990613\n",
            "Iteration 31, loss = 1.06439650\n",
            "Iteration 32, loss = 1.05624260\n",
            "Iteration 33, loss = 1.04610361\n",
            "Iteration 34, loss = 1.04828083\n",
            "Iteration 35, loss = 1.05560475\n",
            "Iteration 36, loss = 1.02867319\n",
            "Iteration 37, loss = 1.02494703\n",
            "Iteration 38, loss = 1.03215523\n",
            "Iteration 39, loss = 1.03135794\n",
            "Iteration 40, loss = 1.02229762\n",
            "Iteration 41, loss = 1.02183518\n",
            "Iteration 42, loss = 1.01631369\n",
            "Iteration 43, loss = 0.99715065\n",
            "Iteration 44, loss = 1.00265802\n",
            "Iteration 45, loss = 0.96066581\n",
            "Iteration 46, loss = 0.96626377\n",
            "Iteration 47, loss = 0.94762613\n",
            "Iteration 48, loss = 0.94968090\n",
            "Iteration 49, loss = 0.95373278\n",
            "Iteration 50, loss = 0.97490765\n",
            "Iteration 51, loss = 0.92135258\n",
            "Iteration 52, loss = 0.91525636\n",
            "Iteration 53, loss = 0.89070012\n",
            "Iteration 54, loss = 1.04206532\n",
            "Iteration 55, loss = 0.96426116\n",
            "Iteration 56, loss = 0.99208073\n",
            "Iteration 57, loss = 1.01516778\n",
            "Iteration 58, loss = 0.98748409\n",
            "Iteration 59, loss = 0.91878173\n",
            "Iteration 60, loss = 0.90444402\n",
            "Iteration 61, loss = 0.90091563\n",
            "Iteration 62, loss = 0.86816419\n",
            "Iteration 63, loss = 0.86237026\n",
            "Iteration 64, loss = 0.87592429\n",
            "Iteration 65, loss = 0.88188773\n",
            "Iteration 66, loss = 0.84508244\n",
            "Iteration 67, loss = 0.84593774\n",
            "Iteration 68, loss = 0.86341823\n",
            "Iteration 69, loss = 0.85485161\n",
            "Iteration 70, loss = 0.85954830\n",
            "Iteration 71, loss = 0.83378241\n",
            "Iteration 72, loss = 0.85181248\n",
            "Iteration 73, loss = 0.85525240\n",
            "Iteration 74, loss = 0.82683605\n",
            "Iteration 75, loss = 0.87474044\n",
            "Iteration 76, loss = 0.82336565\n",
            "Iteration 77, loss = 0.82408272\n",
            "Iteration 78, loss = 0.82397156\n",
            "Iteration 79, loss = 0.84874257\n",
            "Iteration 80, loss = 0.84533936\n",
            "Iteration 81, loss = 0.82917319\n",
            "Iteration 82, loss = 0.83753853\n",
            "Iteration 83, loss = 0.82712397\n",
            "Iteration 84, loss = 0.88374733\n",
            "Iteration 85, loss = 0.99242475\n",
            "Iteration 86, loss = 0.96994237\n",
            "Iteration 87, loss = 0.84888294\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.72477451\n",
            "Iteration 2, loss = 1.71067851\n",
            "Iteration 3, loss = 1.69775511\n",
            "Iteration 4, loss = 1.68640739\n",
            "Iteration 5, loss = 1.67404550\n",
            "Iteration 6, loss = 1.66475443\n",
            "Iteration 7, loss = 1.65542010\n",
            "Iteration 8, loss = 1.64750811\n",
            "Iteration 9, loss = 1.63961137\n",
            "Iteration 10, loss = 1.63312196\n",
            "Iteration 11, loss = 1.62724528\n",
            "Iteration 12, loss = 1.62197963\n",
            "Iteration 13, loss = 1.61777937\n",
            "Iteration 14, loss = 1.61390502\n",
            "Iteration 15, loss = 1.61091142\n",
            "Iteration 16, loss = 1.60795675\n",
            "Iteration 17, loss = 1.60545298\n",
            "Iteration 18, loss = 1.60406185\n",
            "Iteration 19, loss = 1.60264077\n",
            "Iteration 20, loss = 1.60176795\n",
            "Iteration 21, loss = 1.60092592\n",
            "Iteration 22, loss = 1.59990695\n",
            "Iteration 23, loss = 1.59958372\n",
            "Iteration 24, loss = 1.59921838\n",
            "Iteration 25, loss = 1.59865800\n",
            "Iteration 26, loss = 1.59862400\n",
            "Iteration 27, loss = 1.59830994\n",
            "Iteration 28, loss = 1.59824478\n",
            "Iteration 29, loss = 1.59804334\n",
            "Iteration 30, loss = 1.59801250\n",
            "Iteration 31, loss = 1.59783548\n",
            "Iteration 32, loss = 1.59773310\n",
            "Iteration 33, loss = 1.59770895\n",
            "Iteration 34, loss = 1.59760279\n",
            "Iteration 35, loss = 1.59750696\n",
            "Iteration 36, loss = 1.59739723\n",
            "Iteration 37, loss = 1.59732650\n",
            "Iteration 38, loss = 1.59726062\n",
            "Iteration 39, loss = 1.59721669\n",
            "Iteration 40, loss = 1.59710958\n",
            "Iteration 41, loss = 1.59700637\n",
            "Iteration 42, loss = 1.59691250\n",
            "Iteration 43, loss = 1.59684472\n",
            "Iteration 44, loss = 1.59675153\n",
            "Iteration 45, loss = 1.59674229\n",
            "Iteration 46, loss = 1.59658905\n",
            "Iteration 47, loss = 1.59658028\n",
            "Iteration 48, loss = 1.59643960\n",
            "Iteration 49, loss = 1.59639500\n",
            "Iteration 50, loss = 1.59628922\n",
            "Iteration 51, loss = 1.59622089\n",
            "Iteration 52, loss = 1.59613371\n",
            "Iteration 53, loss = 1.59606527\n",
            "Iteration 54, loss = 1.59595740\n",
            "Iteration 55, loss = 1.59589237\n",
            "Iteration 56, loss = 1.59583830\n",
            "Iteration 57, loss = 1.59570028\n",
            "Iteration 58, loss = 1.59562478\n",
            "Iteration 59, loss = 1.59554003\n",
            "Iteration 60, loss = 1.59547509\n",
            "Iteration 61, loss = 1.59543674\n",
            "Iteration 62, loss = 1.59528566\n",
            "Iteration 63, loss = 1.59524077\n",
            "Iteration 64, loss = 1.59512989\n",
            "Iteration 65, loss = 1.59512247\n",
            "Iteration 66, loss = 1.59496732\n",
            "Iteration 67, loss = 1.59491952\n",
            "Iteration 68, loss = 1.59478428\n",
            "Iteration 69, loss = 1.59474455\n",
            "Iteration 70, loss = 1.59463650\n",
            "Iteration 71, loss = 1.59453534\n",
            "Iteration 72, loss = 1.59456176\n",
            "Iteration 73, loss = 1.59438449\n",
            "Iteration 74, loss = 1.59428867\n",
            "Iteration 75, loss = 1.59420562\n",
            "Iteration 76, loss = 1.59420455\n",
            "Iteration 77, loss = 1.59403531\n",
            "Iteration 78, loss = 1.59393280\n",
            "Iteration 79, loss = 1.59384784\n",
            "Iteration 80, loss = 1.59375526\n",
            "Iteration 81, loss = 1.59376387\n",
            "Iteration 82, loss = 1.59354306\n",
            "Iteration 83, loss = 1.59350928\n",
            "Iteration 84, loss = 1.59343578\n",
            "Iteration 85, loss = 1.59343779\n",
            "Iteration 86, loss = 1.59321675\n",
            "Iteration 87, loss = 1.59317261\n",
            "Iteration 88, loss = 1.59304433\n",
            "Iteration 89, loss = 1.59294086\n",
            "Iteration 90, loss = 1.59282861\n",
            "Iteration 91, loss = 1.59277389\n",
            "Iteration 92, loss = 1.59271465\n",
            "Iteration 93, loss = 1.59254059\n",
            "Iteration 94, loss = 1.59244016\n",
            "Iteration 95, loss = 1.59244548\n",
            "Iteration 96, loss = 1.59226173\n",
            "Iteration 97, loss = 1.59215199\n",
            "Iteration 98, loss = 1.59205674\n",
            "Iteration 99, loss = 1.59211448\n",
            "Iteration 100, loss = 1.59184782\n",
            "Iteration 1, loss = 1.69415406\n",
            "Iteration 2, loss = 1.61600213\n",
            "Iteration 3, loss = 1.60346133\n",
            "Iteration 4, loss = 1.62697468\n",
            "Iteration 5, loss = 1.62617693\n",
            "Iteration 6, loss = 1.61157358\n",
            "Iteration 7, loss = 1.60247683\n",
            "Iteration 8, loss = 1.60044170\n",
            "Iteration 9, loss = 1.60103941\n",
            "Iteration 10, loss = 1.60058280\n",
            "Iteration 11, loss = 1.59853357\n",
            "Iteration 12, loss = 1.59764749\n",
            "Iteration 13, loss = 1.59605554\n",
            "Iteration 14, loss = 1.59310325\n",
            "Iteration 15, loss = 1.59171420\n",
            "Iteration 16, loss = 1.59063465\n",
            "Iteration 17, loss = 1.59187047\n",
            "Iteration 18, loss = 1.59072542\n",
            "Iteration 19, loss = 1.58749582\n",
            "Iteration 20, loss = 1.58655890\n",
            "Iteration 21, loss = 1.58614165\n",
            "Iteration 22, loss = 1.58438735\n",
            "Iteration 23, loss = 1.58292285\n",
            "Iteration 24, loss = 1.58129417\n",
            "Iteration 25, loss = 1.57965436\n",
            "Iteration 26, loss = 1.57927510\n",
            "Iteration 27, loss = 1.57670852\n",
            "Iteration 28, loss = 1.57515439\n",
            "Iteration 29, loss = 1.57299060\n",
            "Iteration 30, loss = 1.57087035\n",
            "Iteration 31, loss = 1.56875495\n",
            "Iteration 32, loss = 1.56644267\n",
            "Iteration 33, loss = 1.56403716\n",
            "Iteration 34, loss = 1.56147418\n",
            "Iteration 35, loss = 1.55913491\n",
            "Iteration 36, loss = 1.55547284\n",
            "Iteration 37, loss = 1.55239415\n",
            "Iteration 38, loss = 1.54928108\n",
            "Iteration 39, loss = 1.54605234\n",
            "Iteration 40, loss = 1.54173886\n",
            "Iteration 41, loss = 1.53750161\n",
            "Iteration 42, loss = 1.53303136\n",
            "Iteration 43, loss = 1.52854689\n",
            "Iteration 44, loss = 1.52367014\n",
            "Iteration 45, loss = 1.51950831\n",
            "Iteration 46, loss = 1.51331942"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 47, loss = 1.50926842\n",
            "Iteration 48, loss = 1.50187994\n",
            "Iteration 49, loss = 1.49617375\n",
            "Iteration 50, loss = 1.48928094\n",
            "Iteration 51, loss = 1.48309133\n",
            "Iteration 52, loss = 1.47589925\n",
            "Iteration 53, loss = 1.46929139\n",
            "Iteration 54, loss = 1.46085064\n",
            "Iteration 55, loss = 1.45336203\n",
            "Iteration 56, loss = 1.44627314\n",
            "Iteration 57, loss = 1.43758221\n",
            "Iteration 58, loss = 1.42922394\n",
            "Iteration 59, loss = 1.42100793\n",
            "Iteration 60, loss = 1.41289751\n",
            "Iteration 61, loss = 1.40494431\n",
            "Iteration 62, loss = 1.39604507\n",
            "Iteration 63, loss = 1.38820212\n",
            "Iteration 64, loss = 1.37933479\n",
            "Iteration 65, loss = 1.37138395\n",
            "Iteration 66, loss = 1.36244714\n",
            "Iteration 67, loss = 1.35499034\n",
            "Iteration 68, loss = 1.34625875\n",
            "Iteration 69, loss = 1.33855175\n",
            "Iteration 70, loss = 1.33060992\n",
            "Iteration 71, loss = 1.32320426\n",
            "Iteration 72, loss = 1.31721217\n",
            "Iteration 73, loss = 1.30888217\n",
            "Iteration 74, loss = 1.30152762\n",
            "Iteration 75, loss = 1.29503131\n",
            "Iteration 76, loss = 1.28938524\n",
            "Iteration 77, loss = 1.28291835\n",
            "Iteration 78, loss = 1.27663398\n",
            "Iteration 79, loss = 1.27069333\n",
            "Iteration 80, loss = 1.26558568\n",
            "Iteration 81, loss = 1.26076483\n",
            "Iteration 82, loss = 1.25484568\n",
            "Iteration 83, loss = 1.25036861\n",
            "Iteration 84, loss = 1.24624773\n",
            "Iteration 85, loss = 1.24268216\n",
            "Iteration 86, loss = 1.23740460\n",
            "Iteration 87, loss = 1.23290677\n",
            "Iteration 88, loss = 1.22859307\n",
            "Iteration 89, loss = 1.22570350\n",
            "Iteration 90, loss = 1.22227436\n",
            "Iteration 91, loss = 1.21864182\n",
            "Iteration 92, loss = 1.21605241\n",
            "Iteration 93, loss = 1.21201020\n",
            "Iteration 94, loss = 1.20906332\n",
            "Iteration 95, loss = 1.20777052\n",
            "Iteration 96, loss = 1.20371443\n",
            "Iteration 97, loss = 1.20109765\n",
            "Iteration 98, loss = 1.19841969\n",
            "Iteration 99, loss = 1.19708303\n",
            "Iteration 100, loss = 1.19318222\n",
            "Iteration 1, loss = 1.86457045\n",
            "Iteration 2, loss = 1.79931143\n",
            "Iteration 3, loss = 1.75049179\n",
            "Iteration 4, loss = 1.69023123\n",
            "Iteration 5, loss = 1.60921323\n",
            "Iteration 6, loss = 1.63580676\n",
            "Iteration 7, loss = 1.62993033"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 8, loss = 1.60031320\n",
            "Iteration 9, loss = 1.58903047\n",
            "Iteration 10, loss = 1.58899955\n",
            "Iteration 11, loss = 1.58904915\n",
            "Iteration 12, loss = 1.58261605\n",
            "Iteration 13, loss = 1.57025119\n",
            "Iteration 14, loss = 1.55698071\n",
            "Iteration 15, loss = 1.54690624\n",
            "Iteration 16, loss = 1.53286216\n",
            "Iteration 17, loss = 1.51696923\n",
            "Iteration 18, loss = 1.49592282\n",
            "Iteration 19, loss = 1.46458302\n",
            "Iteration 20, loss = 1.43282942\n",
            "Iteration 21, loss = 1.40123473\n",
            "Iteration 22, loss = 1.36385501\n",
            "Iteration 23, loss = 1.32569577\n",
            "Iteration 24, loss = 1.29380662\n",
            "Iteration 25, loss = 1.26348886\n",
            "Iteration 26, loss = 1.23896210\n",
            "Iteration 27, loss = 1.21536684\n",
            "Iteration 28, loss = 1.19985872\n",
            "Iteration 29, loss = 1.18492083\n",
            "Iteration 30, loss = 1.17393704\n",
            "Iteration 31, loss = 1.16420283\n",
            "Iteration 32, loss = 1.15371292\n",
            "Iteration 33, loss = 1.15136546\n",
            "Iteration 34, loss = 1.14623304\n",
            "Iteration 35, loss = 1.14659321\n",
            "Iteration 36, loss = 1.13597747\n",
            "Iteration 37, loss = 1.13502104\n",
            "Iteration 38, loss = 1.13352648\n",
            "Iteration 39, loss = 1.12770505\n",
            "Iteration 40, loss = 1.12241026\n",
            "Iteration 41, loss = 1.12207995\n",
            "Iteration 42, loss = 1.11968950\n",
            "Iteration 43, loss = 1.11878248\n",
            "Iteration 44, loss = 1.11510441\n",
            "Iteration 45, loss = 1.11880620\n",
            "Iteration 46, loss = 1.11474827\n",
            "Iteration 47, loss = 1.12201689\n",
            "Iteration 48, loss = 1.11277854\n",
            "Iteration 49, loss = 1.11986568\n",
            "Iteration 50, loss = 1.11603808\n",
            "Iteration 51, loss = 1.11103292\n",
            "Iteration 52, loss = 1.11128746\n",
            "Iteration 53, loss = 1.11102714\n",
            "Iteration 54, loss = 1.11367470\n",
            "Iteration 55, loss = 1.11136368\n",
            "Iteration 56, loss = 1.10911732\n",
            "Iteration 57, loss = 1.10922505\n",
            "Iteration 58, loss = 1.10858323\n",
            "Iteration 59, loss = 1.10667305\n",
            "Iteration 60, loss = 1.10773429\n",
            "Iteration 61, loss = 1.10957083\n",
            "Iteration 62, loss = 1.10692863\n",
            "Iteration 63, loss = 1.10627352\n",
            "Iteration 64, loss = 1.10563319\n",
            "Iteration 65, loss = 1.10956142\n",
            "Iteration 66, loss = 1.10433082\n",
            "Iteration 67, loss = 1.10813027\n",
            "Iteration 68, loss = 1.10273643\n",
            "Iteration 69, loss = 1.10864982\n",
            "Iteration 70, loss = 1.10944293\n",
            "Iteration 71, loss = 1.10653402\n",
            "Iteration 72, loss = 1.11211377\n",
            "Iteration 73, loss = 1.10204014\n",
            "Iteration 74, loss = 1.10890178\n",
            "Iteration 75, loss = 1.10766414\n",
            "Iteration 76, loss = 1.11416811\n",
            "Iteration 77, loss = 1.10046648\n",
            "Iteration 78, loss = 1.11302602\n",
            "Iteration 79, loss = 1.09376629\n",
            "Iteration 80, loss = 1.11008649\n",
            "Iteration 81, loss = 1.11127315\n",
            "Iteration 82, loss = 1.10322938\n",
            "Iteration 83, loss = 1.10267705\n",
            "Iteration 84, loss = 1.11148541\n",
            "Iteration 85, loss = 1.11095439\n",
            "Iteration 86, loss = 1.09454092\n",
            "Iteration 87, loss = 1.08835800\n",
            "Iteration 88, loss = 1.08609372\n",
            "Iteration 89, loss = 1.08425370\n",
            "Iteration 90, loss = 1.08326232\n",
            "Iteration 91, loss = 1.08103916\n",
            "Iteration 92, loss = 1.08387261\n",
            "Iteration 93, loss = 1.07327640\n",
            "Iteration 94, loss = 1.07739430\n",
            "Iteration 95, loss = 1.07653559\n",
            "Iteration 96, loss = 1.07197775\n",
            "Iteration 97, loss = 1.06817084\n",
            "Iteration 98, loss = 1.07178292\n",
            "Iteration 99, loss = 1.07484147\n",
            "Iteration 100, loss = 1.06611149\n",
            "Iteration 1, loss = 1.69088385\n",
            "Iteration 2, loss = 1.68222203\n",
            "Iteration 3, loss = 1.67405132\n",
            "Iteration 4, loss = 1.66654540\n",
            "Iteration 5, loss = 1.65850571\n",
            "Iteration 6, loss = 1.65218899\n",
            "Iteration 7, loss = 1.64558551\n",
            "Iteration 8, loss = 1.64012278\n",
            "Iteration 9, loss = 1.63436712\n",
            "Iteration 10, loss = 1.62945416\n",
            "Iteration 11, loss = 1.62493366\n",
            "Iteration 12, loss = 1.62066392\n",
            "Iteration 13, loss = 1.61722543\n",
            "Iteration 14, loss = 1.61359266\n",
            "Iteration 15, loss = 1.61062013\n",
            "Iteration 16, loss = 1.60765631\n",
            "Iteration 17, loss = 1.60522700\n",
            "Iteration 18, loss = 1.60278318\n",
            "Iteration 19, loss = 1.60071113\n",
            "Iteration 20, loss = 1.59865835\n",
            "Iteration 21, loss = 1.59697283\n",
            "Iteration 22, loss = 1.59545545\n",
            "Iteration 23, loss = 1.59401909\n",
            "Iteration 24, loss = 1.59288218\n",
            "Iteration 25, loss = 1.59148988\n",
            "Iteration 26, loss = 1.59049198"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 27, loss = 1.58929295\n",
            "Iteration 28, loss = 1.58835726\n",
            "Iteration 29, loss = 1.58739486\n",
            "Iteration 30, loss = 1.58650823\n",
            "Iteration 31, loss = 1.58554299\n",
            "Iteration 32, loss = 1.58463958\n",
            "Iteration 33, loss = 1.58385428\n",
            "Iteration 34, loss = 1.58296702\n",
            "Iteration 35, loss = 1.58205026\n",
            "Iteration 36, loss = 1.58118784\n",
            "Iteration 37, loss = 1.58036578\n",
            "Iteration 38, loss = 1.57948334\n",
            "Iteration 39, loss = 1.57869848\n",
            "Iteration 40, loss = 1.57776379\n",
            "Iteration 41, loss = 1.57688215\n",
            "Iteration 42, loss = 1.57599819\n",
            "Iteration 43, loss = 1.57511030\n",
            "Iteration 44, loss = 1.57420415\n",
            "Iteration 45, loss = 1.57338353\n",
            "Iteration 46, loss = 1.57237065\n",
            "Iteration 47, loss = 1.57156603\n",
            "Iteration 48, loss = 1.57051743\n",
            "Iteration 49, loss = 1.56962684\n",
            "Iteration 50, loss = 1.56864607\n",
            "Iteration 51, loss = 1.56771159\n",
            "Iteration 52, loss = 1.56670552\n",
            "Iteration 53, loss = 1.56578836\n",
            "Iteration 54, loss = 1.56475834\n",
            "Iteration 55, loss = 1.56373890\n",
            "Iteration 56, loss = 1.56282629\n",
            "Iteration 57, loss = 1.56174724\n",
            "Iteration 58, loss = 1.56070596\n",
            "Iteration 59, loss = 1.55969163\n",
            "Iteration 60, loss = 1.55866172\n",
            "Iteration 61, loss = 1.55760148\n",
            "Iteration 62, loss = 1.55651430\n",
            "Iteration 63, loss = 1.55547561\n",
            "Iteration 64, loss = 1.55435965\n",
            "Iteration 65, loss = 1.55330293\n",
            "Iteration 66, loss = 1.55212580\n",
            "Iteration 67, loss = 1.55103349\n",
            "Iteration 68, loss = 1.54988142\n",
            "Iteration 69, loss = 1.54876290\n",
            "Iteration 70, loss = 1.54758297\n",
            "Iteration 71, loss = 1.54640440\n",
            "Iteration 72, loss = 1.54529360\n",
            "Iteration 73, loss = 1.54401635\n",
            "Iteration 74, loss = 1.54281330\n",
            "Iteration 75, loss = 1.54159172\n",
            "Iteration 76, loss = 1.54044718\n",
            "Iteration 77, loss = 1.53908584\n",
            "Iteration 78, loss = 1.53783142\n",
            "Iteration 79, loss = 1.53656249\n",
            "Iteration 80, loss = 1.53529228\n",
            "Iteration 81, loss = 1.53405356\n",
            "Iteration 82, loss = 1.53262162\n",
            "Iteration 83, loss = 1.53137265\n",
            "Iteration 84, loss = 1.53001297\n",
            "Iteration 85, loss = 1.52875687\n",
            "Iteration 86, loss = 1.52727700\n",
            "Iteration 87, loss = 1.52592423\n",
            "Iteration 88, loss = 1.52447954\n",
            "Iteration 89, loss = 1.52312140\n",
            "Iteration 90, loss = 1.52169454\n",
            "Iteration 91, loss = 1.52031107\n",
            "Iteration 92, loss = 1.51882253\n",
            "Iteration 93, loss = 1.51738072\n",
            "Iteration 94, loss = 1.51585128\n",
            "Iteration 95, loss = 1.51447999\n",
            "Iteration 96, loss = 1.51289065\n",
            "Iteration 97, loss = 1.51138232\n",
            "Iteration 98, loss = 1.50983927\n",
            "Iteration 99, loss = 1.50839874\n",
            "Iteration 100, loss = 1.50675471\n",
            "Iteration 1, loss = 1.67128935\n",
            "Iteration 2, loss = 1.61523527\n",
            "Iteration 3, loss = 1.59562537\n",
            "Iteration 4, loss = 1.59422321\n",
            "Iteration 5, loss = 1.59044611\n",
            "Iteration 6, loss = 1.58051970\n",
            "Iteration 7, loss = 1.56961842\n",
            "Iteration 8, loss = 1.55647765\n",
            "Iteration 9, loss = 1.54504370\n",
            "Iteration 10, loss = 1.53610811\n",
            "Iteration 11, loss = 1.52608185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 1.51629105\n",
            "Iteration 13, loss = 1.50385851\n",
            "Iteration 14, loss = 1.48929731\n",
            "Iteration 15, loss = 1.47489221\n",
            "Iteration 16, loss = 1.45949494\n",
            "Iteration 17, loss = 1.44565278\n",
            "Iteration 18, loss = 1.42974981\n",
            "Iteration 19, loss = 1.41237047\n",
            "Iteration 20, loss = 1.39659255\n",
            "Iteration 21, loss = 1.38015837\n",
            "Iteration 22, loss = 1.36277395\n",
            "Iteration 23, loss = 1.34591860\n",
            "Iteration 24, loss = 1.33014486\n",
            "Iteration 25, loss = 1.31532319\n",
            "Iteration 26, loss = 1.30060463\n",
            "Iteration 27, loss = 1.28621121\n",
            "Iteration 28, loss = 1.27338212\n",
            "Iteration 29, loss = 1.26023688\n",
            "Iteration 30, loss = 1.24963541\n",
            "Iteration 31, loss = 1.23894286\n",
            "Iteration 32, loss = 1.22899798\n",
            "Iteration 33, loss = 1.21982993\n",
            "Iteration 34, loss = 1.21181137\n",
            "Iteration 35, loss = 1.20468993\n",
            "Iteration 36, loss = 1.19731585\n",
            "Iteration 37, loss = 1.19072094\n",
            "Iteration 38, loss = 1.18537971\n",
            "Iteration 39, loss = 1.18009312\n",
            "Iteration 40, loss = 1.17453839\n",
            "Iteration 41, loss = 1.17011044\n",
            "Iteration 42, loss = 1.16571325\n",
            "Iteration 43, loss = 1.16189876\n",
            "Iteration 44, loss = 1.15810701\n",
            "Iteration 45, loss = 1.15516220\n",
            "Iteration 46, loss = 1.15104111\n",
            "Iteration 47, loss = 1.14936396\n",
            "Iteration 48, loss = 1.14520900\n",
            "Iteration 49, loss = 1.14288845\n",
            "Iteration 50, loss = 1.14000092\n",
            "Iteration 51, loss = 1.13811983\n",
            "Iteration 52, loss = 1.13513710\n",
            "Iteration 53, loss = 1.13370089\n",
            "Iteration 54, loss = 1.13108734\n",
            "Iteration 55, loss = 1.12866188\n",
            "Iteration 56, loss = 1.12713930\n",
            "Iteration 57, loss = 1.12476590\n",
            "Iteration 58, loss = 1.12312410\n",
            "Iteration 59, loss = 1.12159274\n",
            "Iteration 60, loss = 1.11970741\n",
            "Iteration 61, loss = 1.11979457\n",
            "Iteration 62, loss = 1.11749864\n",
            "Iteration 63, loss = 1.11562223\n",
            "Iteration 64, loss = 1.11382422\n",
            "Iteration 65, loss = 1.11301198\n",
            "Iteration 66, loss = 1.11139794\n",
            "Iteration 67, loss = 1.11108586\n",
            "Iteration 68, loss = 1.10820445\n",
            "Iteration 69, loss = 1.10702693\n",
            "Iteration 70, loss = 1.10631901\n",
            "Iteration 71, loss = 1.10466422\n",
            "Iteration 72, loss = 1.10453467\n",
            "Iteration 73, loss = 1.10298514\n",
            "Iteration 74, loss = 1.10059657\n",
            "Iteration 75, loss = 1.10082555\n",
            "Iteration 76, loss = 1.10018791\n",
            "Iteration 77, loss = 1.09800112\n",
            "Iteration 78, loss = 1.09711174\n",
            "Iteration 79, loss = 1.09552611\n",
            "Iteration 80, loss = 1.09501576\n",
            "Iteration 81, loss = 1.09523229\n",
            "Iteration 82, loss = 1.09283758\n",
            "Iteration 83, loss = 1.09136252\n",
            "Iteration 84, loss = 1.09178249\n",
            "Iteration 85, loss = 1.09028282\n",
            "Iteration 86, loss = 1.08839558\n",
            "Iteration 87, loss = 1.08654723\n",
            "Iteration 88, loss = 1.08520692\n",
            "Iteration 89, loss = 1.08440413\n",
            "Iteration 90, loss = 1.08300416\n",
            "Iteration 91, loss = 1.08249166\n",
            "Iteration 92, loss = 1.08194588\n",
            "Iteration 93, loss = 1.07945612\n",
            "Iteration 94, loss = 1.07838610\n",
            "Iteration 95, loss = 1.07919553\n",
            "Iteration 96, loss = 1.07721752\n",
            "Iteration 97, loss = 1.07572394\n",
            "Iteration 98, loss = 1.07529961\n",
            "Iteration 99, loss = 1.07400389\n",
            "Iteration 100, loss = 1.07118477\n",
            "Iteration 1, loss = 1.72074133\n",
            "Iteration 2, loss = 1.67294745\n",
            "Iteration 3, loss = 1.55580167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 1.48048617\n",
            "Iteration 5, loss = 1.35247689\n",
            "Iteration 6, loss = 1.28789853\n",
            "Iteration 7, loss = 1.22514093\n",
            "Iteration 8, loss = 1.18594042\n",
            "Iteration 9, loss = 1.16634531\n",
            "Iteration 10, loss = 1.16131895\n",
            "Iteration 11, loss = 1.14537155\n",
            "Iteration 12, loss = 1.13839084\n",
            "Iteration 13, loss = 1.13304726\n",
            "Iteration 14, loss = 1.12840050\n",
            "Iteration 15, loss = 1.12455914\n",
            "Iteration 16, loss = 1.11455974\n",
            "Iteration 17, loss = 1.13120743\n",
            "Iteration 18, loss = 1.10663783\n",
            "Iteration 19, loss = 1.10987884\n",
            "Iteration 20, loss = 1.10379759\n",
            "Iteration 21, loss = 1.10566595\n",
            "Iteration 22, loss = 1.09458758\n",
            "Iteration 23, loss = 1.12650501\n",
            "Iteration 24, loss = 1.09704882\n",
            "Iteration 25, loss = 1.10439319\n",
            "Iteration 26, loss = 1.08734835\n",
            "Iteration 27, loss = 1.08890722\n",
            "Iteration 28, loss = 1.07771362\n",
            "Iteration 29, loss = 1.06940306\n",
            "Iteration 30, loss = 1.06608755\n",
            "Iteration 31, loss = 1.06158776\n",
            "Iteration 32, loss = 1.05159378\n",
            "Iteration 33, loss = 1.04120632\n",
            "Iteration 34, loss = 1.04428582\n",
            "Iteration 35, loss = 1.05111322\n",
            "Iteration 36, loss = 1.02610698\n",
            "Iteration 37, loss = 1.02335508\n",
            "Iteration 38, loss = 1.03186900\n",
            "Iteration 39, loss = 1.03346038\n",
            "Iteration 40, loss = 1.01989072\n",
            "Iteration 41, loss = 1.01300326\n",
            "Iteration 42, loss = 1.01267287\n",
            "Iteration 43, loss = 0.97117823\n",
            "Iteration 44, loss = 0.97332986\n",
            "Iteration 45, loss = 0.94726259\n",
            "Iteration 46, loss = 0.94529778\n",
            "Iteration 47, loss = 0.95300412\n",
            "Iteration 48, loss = 0.95261193\n",
            "Iteration 49, loss = 0.92513679\n",
            "Iteration 50, loss = 0.95693376\n",
            "Iteration 51, loss = 0.89867702\n",
            "Iteration 52, loss = 0.90754842\n",
            "Iteration 53, loss = 0.90414655\n",
            "Iteration 54, loss = 1.00753322\n",
            "Iteration 55, loss = 0.93246676\n",
            "Iteration 56, loss = 0.95813385\n",
            "Iteration 57, loss = 0.97013349\n",
            "Iteration 58, loss = 0.96328354\n",
            "Iteration 59, loss = 0.92050363\n",
            "Iteration 60, loss = 0.91254086\n",
            "Iteration 61, loss = 0.91424637\n",
            "Iteration 62, loss = 0.89818680\n",
            "Iteration 63, loss = 0.88142563\n",
            "Iteration 64, loss = 0.85718867\n",
            "Iteration 65, loss = 0.86099467\n",
            "Iteration 66, loss = 0.87566422\n",
            "Iteration 67, loss = 0.86324503\n",
            "Iteration 68, loss = 0.84289376\n",
            "Iteration 69, loss = 0.83108537\n",
            "Iteration 70, loss = 0.83962085\n",
            "Iteration 71, loss = 0.82391325\n",
            "Iteration 72, loss = 0.84931300\n",
            "Iteration 73, loss = 0.89351948\n",
            "Iteration 74, loss = 0.84818604\n",
            "Iteration 75, loss = 0.90385258\n",
            "Iteration 76, loss = 0.83811123\n",
            "Iteration 77, loss = 0.87467203\n",
            "Iteration 78, loss = 0.87280769\n",
            "Iteration 79, loss = 0.85000151\n",
            "Iteration 80, loss = 0.86460152\n",
            "Iteration 81, loss = 0.87198255\n",
            "Iteration 82, loss = 0.87277495\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.68792064\n",
            "Iteration 2, loss = 1.68400847\n",
            "Iteration 3, loss = 1.68030201\n",
            "Iteration 4, loss = 1.67684919\n",
            "Iteration 5, loss = 1.67305545\n",
            "Iteration 6, loss = 1.66985014\n",
            "Iteration 7, loss = 1.66650631\n",
            "Iteration 8, loss = 1.66340315\n",
            "Iteration 9, loss = 1.66030756\n",
            "Iteration 10, loss = 1.65731352\n",
            "Iteration 11, loss = 1.65452730\n",
            "Iteration 12, loss = 1.65177005\n",
            "Iteration 13, loss = 1.64905986\n",
            "Iteration 14, loss = 1.64651235\n",
            "Iteration 15, loss = 1.64410095\n",
            "Iteration 16, loss = 1.64160570\n",
            "Iteration 17, loss = 1.63923281\n",
            "Iteration 18, loss = 1.63695802\n",
            "Iteration 19, loss = 1.63486650\n",
            "Iteration 20, loss = 1.63275406\n",
            "Iteration 21, loss = 1.63074946\n",
            "Iteration 22, loss = 1.62870353\n",
            "Iteration 23, loss = 1.62688362\n",
            "Iteration 24, loss = 1.62512418\n",
            "Iteration 25, loss = 1.62315586\n",
            "Iteration 26, loss = 1.62153237\n",
            "Iteration 27, loss = 1.61983331\n",
            "Iteration 28, loss = 1.61834163\n",
            "Iteration 29, loss = 1.61670849\n",
            "Iteration 30, loss = 1.61525972\n",
            "Iteration 31, loss = 1.61370831\n",
            "Iteration 32, loss = 1.61223939\n",
            "Iteration 33, loss = 1.61098599\n",
            "Iteration 34, loss = 1.60964048\n",
            "Iteration 35, loss = 1.60811010\n",
            "Iteration 36, loss = 1.60698620\n",
            "Iteration 37, loss = 1.60580964\n",
            "Iteration 38, loss = 1.60456054\n",
            "Iteration 39, loss = 1.60349727\n",
            "Iteration 40, loss = 1.60222703\n",
            "Iteration 41, loss = 1.60123438\n",
            "Iteration 42, loss = 1.60020125\n",
            "Iteration 43, loss = 1.59917576\n",
            "Iteration 44, loss = 1.59811276\n",
            "Iteration 45, loss = 1.59709497\n",
            "Iteration 46, loss = 1.59622492\n",
            "Iteration 47, loss = 1.59541789\n",
            "Iteration 48, loss = 1.59436562\n",
            "Iteration 49, loss = 1.59348749\n",
            "Iteration 50, loss = 1.59265117\n",
            "Iteration 51, loss = 1.59183278\n",
            "Iteration 52, loss = 1.59104364\n",
            "Iteration 53, loss = 1.59028465\n",
            "Iteration 54, loss = 1.58950174\n",
            "Iteration 55, loss = 1.58870148\n",
            "Iteration 56, loss = 1.58800848\n",
            "Iteration 57, loss = 1.58725466\n",
            "Iteration 58, loss = 1.58651515\n",
            "Iteration 59, loss = 1.58585640\n",
            "Iteration 60, loss = 1.58513203\n",
            "Iteration 61, loss = 1.58449230\n",
            "Iteration 62, loss = 1.58377888\n",
            "Iteration 63, loss = 1.58307959\n",
            "Iteration 64, loss = 1.58243104\n",
            "Iteration 65, loss = 1.58183043\n",
            "Iteration 66, loss = 1.58115809\n",
            "Iteration 67, loss = 1.58046423\n",
            "Iteration 68, loss = 1.57984245\n",
            "Iteration 69, loss = 1.57924857\n",
            "Iteration 70, loss = 1.57856794\n",
            "Iteration 71, loss = 1.57787985\n",
            "Iteration 72, loss = 1.57728791\n",
            "Iteration 73, loss = 1.57664775\n",
            "Iteration 74, loss = 1.57601874\n",
            "Iteration 75, loss = 1.57531454\n",
            "Iteration 76, loss = 1.57476992\n",
            "Iteration 77, loss = 1.57404301\n",
            "Iteration 78, loss = 1.57339585\n",
            "Iteration 79, loss = 1.57279708\n",
            "Iteration 80, loss = 1.57210764\n",
            "Iteration 81, loss = 1.57148153\n",
            "Iteration 82, loss = 1.57080011\n",
            "Iteration 83, loss = 1.57019132\n",
            "Iteration 84, loss = 1.56947568\n",
            "Iteration 85, loss = 1.56891993\n",
            "Iteration 86, loss = 1.56816660\n",
            "Iteration 87, loss = 1.56750380\n",
            "Iteration 88, loss = 1.56680035\n",
            "Iteration 89, loss = 1.56612574\n",
            "Iteration 90, loss = 1.56543184\n",
            "Iteration 91, loss = 1.56475102\n",
            "Iteration 92, loss = 1.56402266\n",
            "Iteration 93, loss = 1.56332575\n",
            "Iteration 94, loss = 1.56258126\n",
            "Iteration 95, loss = 1.56189955\n",
            "Iteration 96, loss = 1.56112169\n",
            "Iteration 97, loss = 1.56037522\n",
            "Iteration 98, loss = 1.55961971\n",
            "Iteration 99, loss = 1.55891422\n",
            "Iteration 100, loss = 1.55808342\n",
            "Iteration 1, loss = 1.67868778\n",
            "Iteration 2, loss = 1.64696752\n",
            "Iteration 3, loss = 1.62409021\n",
            "Iteration 4, loss = 1.60949544\n",
            "Iteration 5, loss = 1.59691597\n",
            "Iteration 6, loss = 1.59092849\n",
            "Iteration 7, loss = 1.58546959\n",
            "Iteration 8, loss = 1.58278267\n",
            "Iteration 9, loss = 1.57833155\n",
            "Iteration 10, loss = 1.57455570\n",
            "Iteration 11, loss = 1.56926140\n",
            "Iteration 12, loss = 1.56360014\n",
            "Iteration 13, loss = 1.55747519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14, loss = 1.55043153\n",
            "Iteration 15, loss = 1.54332437\n",
            "Iteration 16, loss = 1.53564970\n",
            "Iteration 17, loss = 1.52799838\n",
            "Iteration 18, loss = 1.51928952\n",
            "Iteration 19, loss = 1.50945756\n",
            "Iteration 20, loss = 1.49952784\n",
            "Iteration 21, loss = 1.48897896\n",
            "Iteration 22, loss = 1.47767289\n",
            "Iteration 23, loss = 1.46571571\n",
            "Iteration 24, loss = 1.45341684\n",
            "Iteration 25, loss = 1.44135252\n",
            "Iteration 26, loss = 1.42812874\n",
            "Iteration 27, loss = 1.41460891\n",
            "Iteration 28, loss = 1.40070513\n",
            "Iteration 29, loss = 1.38683279\n",
            "Iteration 30, loss = 1.37336308\n",
            "Iteration 31, loss = 1.35962868\n",
            "Iteration 32, loss = 1.34625815\n",
            "Iteration 33, loss = 1.33292905\n",
            "Iteration 34, loss = 1.32028787\n",
            "Iteration 35, loss = 1.30829442\n",
            "Iteration 36, loss = 1.29605147\n",
            "Iteration 37, loss = 1.28470463\n",
            "Iteration 38, loss = 1.27420333\n",
            "Iteration 39, loss = 1.26450335\n",
            "Iteration 40, loss = 1.25457629\n",
            "Iteration 41, loss = 1.24588466\n",
            "Iteration 42, loss = 1.23741091\n",
            "Iteration 43, loss = 1.22975389\n",
            "Iteration 44, loss = 1.22264487\n",
            "Iteration 45, loss = 1.21617846\n",
            "Iteration 46, loss = 1.20937197\n",
            "Iteration 47, loss = 1.20410887\n",
            "Iteration 48, loss = 1.19817938\n",
            "Iteration 49, loss = 1.19349227\n",
            "Iteration 50, loss = 1.18866240\n",
            "Iteration 51, loss = 1.18443013\n",
            "Iteration 52, loss = 1.17982436\n",
            "Iteration 53, loss = 1.17616456\n",
            "Iteration 54, loss = 1.17263866\n",
            "Iteration 55, loss = 1.16887742\n",
            "Iteration 56, loss = 1.16603067\n",
            "Iteration 57, loss = 1.16274021\n",
            "Iteration 58, loss = 1.15965200\n",
            "Iteration 59, loss = 1.15693444\n",
            "Iteration 60, loss = 1.15477937\n",
            "Iteration 61, loss = 1.15232394\n",
            "Iteration 62, loss = 1.14984813\n",
            "Iteration 63, loss = 1.14762129\n",
            "Iteration 64, loss = 1.14519345\n",
            "Iteration 65, loss = 1.14359715\n",
            "Iteration 66, loss = 1.14138817\n",
            "Iteration 67, loss = 1.13993246\n",
            "Iteration 68, loss = 1.13753855\n",
            "Iteration 69, loss = 1.13589105\n",
            "Iteration 70, loss = 1.13486543\n",
            "Iteration 71, loss = 1.13264348\n",
            "Iteration 72, loss = 1.13166862\n",
            "Iteration 73, loss = 1.13013677\n",
            "Iteration 74, loss = 1.12807177\n",
            "Iteration 75, loss = 1.12721567\n",
            "Iteration 76, loss = 1.12654533\n",
            "Iteration 77, loss = 1.12434592\n",
            "Iteration 78, loss = 1.12326593\n",
            "Iteration 79, loss = 1.12191724\n",
            "Iteration 80, loss = 1.12140443\n",
            "Iteration 81, loss = 1.12086548\n",
            "Iteration 82, loss = 1.11883568\n",
            "Iteration 83, loss = 1.11754892\n",
            "Iteration 84, loss = 1.11743946\n",
            "Iteration 85, loss = 1.11664236\n",
            "Iteration 86, loss = 1.11467763\n",
            "Iteration 87, loss = 1.11349436\n",
            "Iteration 88, loss = 1.11223210\n",
            "Iteration 89, loss = 1.11156652\n",
            "Iteration 90, loss = 1.11046654\n",
            "Iteration 91, loss = 1.10997104\n",
            "Iteration 92, loss = 1.10929227\n",
            "Iteration 93, loss = 1.10777344\n",
            "Iteration 94, loss = 1.10705682\n",
            "Iteration 95, loss = 1.10701442\n",
            "Iteration 96, loss = 1.10598026\n",
            "Iteration 97, loss = 1.10479962\n",
            "Iteration 98, loss = 1.10480373\n",
            "Iteration 99, loss = 1.10383029\n",
            "Iteration 100, loss = 1.10231079\n",
            "Iteration 1, loss = 1.63537018\n",
            "Iteration 2, loss = 1.60831056\n",
            "Iteration 3, loss = 1.55827172"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4, loss = 1.52651249\n",
            "Iteration 5, loss = 1.47252749\n",
            "Iteration 6, loss = 1.40524419\n",
            "Iteration 7, loss = 1.33924565\n",
            "Iteration 8, loss = 1.27805117\n",
            "Iteration 9, loss = 1.22956187\n",
            "Iteration 10, loss = 1.19338809\n",
            "Iteration 11, loss = 1.17020823\n",
            "Iteration 12, loss = 1.15593571\n",
            "Iteration 13, loss = 1.14494033\n",
            "Iteration 14, loss = 1.12913809\n",
            "Iteration 15, loss = 1.12306946\n",
            "Iteration 16, loss = 1.11767724\n",
            "Iteration 17, loss = 1.12840158\n",
            "Iteration 18, loss = 1.12621393\n",
            "Iteration 19, loss = 1.11656122\n",
            "Iteration 20, loss = 1.11674714\n",
            "Iteration 21, loss = 1.11773212\n",
            "Iteration 22, loss = 1.10227221\n",
            "Iteration 23, loss = 1.12874271\n",
            "Iteration 24, loss = 1.09695964\n",
            "Iteration 25, loss = 1.09487876\n",
            "Iteration 26, loss = 1.09225897\n",
            "Iteration 27, loss = 1.08914297\n",
            "Iteration 28, loss = 1.08907697\n",
            "Iteration 29, loss = 1.08342791\n",
            "Iteration 30, loss = 1.08790991\n",
            "Iteration 31, loss = 1.07714325\n",
            "Iteration 32, loss = 1.07716485\n",
            "Iteration 33, loss = 1.07661913\n",
            "Iteration 34, loss = 1.06182920\n",
            "Iteration 35, loss = 1.07452310\n",
            "Iteration 36, loss = 1.06007999\n",
            "Iteration 37, loss = 1.05753780\n",
            "Iteration 38, loss = 1.07113015\n",
            "Iteration 39, loss = 1.04709476\n",
            "Iteration 40, loss = 1.04552866\n",
            "Iteration 41, loss = 1.03770080\n",
            "Iteration 42, loss = 1.01681970\n",
            "Iteration 43, loss = 1.01881019\n",
            "Iteration 44, loss = 0.99932599\n",
            "Iteration 45, loss = 1.00089549\n",
            "Iteration 46, loss = 0.99390539\n",
            "Iteration 47, loss = 1.00192186\n",
            "Iteration 48, loss = 0.98006149\n",
            "Iteration 49, loss = 0.96058936\n",
            "Iteration 50, loss = 0.96645525\n",
            "Iteration 51, loss = 0.96606078\n",
            "Iteration 52, loss = 0.97429922\n",
            "Iteration 53, loss = 0.99006106\n",
            "Iteration 54, loss = 0.93926723\n",
            "Iteration 55, loss = 0.95816781\n",
            "Iteration 56, loss = 0.97001217\n",
            "Iteration 57, loss = 0.95376111\n",
            "Iteration 58, loss = 0.92055724\n",
            "Iteration 59, loss = 0.91086213\n",
            "Iteration 60, loss = 0.90450560\n",
            "Iteration 61, loss = 0.90909835\n",
            "Iteration 62, loss = 0.90428001\n",
            "Iteration 63, loss = 0.89939095\n",
            "Iteration 64, loss = 0.88680257\n",
            "Iteration 65, loss = 0.88888272\n",
            "Iteration 66, loss = 0.88592899\n",
            "Iteration 67, loss = 0.87669940\n",
            "Iteration 68, loss = 0.86804087\n",
            "Iteration 69, loss = 0.86398539\n",
            "Iteration 70, loss = 0.86469793\n",
            "Iteration 71, loss = 0.85918208\n",
            "Iteration 72, loss = 0.86833219\n",
            "Iteration 73, loss = 0.86468707\n",
            "Iteration 74, loss = 0.85753289\n",
            "Iteration 75, loss = 0.86503896\n",
            "Iteration 76, loss = 0.86678968\n",
            "Iteration 77, loss = 0.89199120\n",
            "Iteration 78, loss = 0.89345593\n",
            "Iteration 79, loss = 0.85065087\n",
            "Iteration 80, loss = 0.84209736\n",
            "Iteration 81, loss = 0.83366807\n",
            "Iteration 82, loss = 0.82609240\n",
            "Iteration 83, loss = 0.82828941\n",
            "Iteration 84, loss = 0.84713001\n",
            "Iteration 85, loss = 0.83758057\n",
            "Iteration 86, loss = 0.82319639\n",
            "Iteration 87, loss = 0.81276776\n",
            "Iteration 88, loss = 0.80498999\n",
            "Iteration 89, loss = 0.80331320\n",
            "Iteration 90, loss = 0.80292303\n",
            "Iteration 91, loss = 0.80180442\n",
            "Iteration 92, loss = 0.80704983\n",
            "Iteration 93, loss = 0.81455110\n",
            "Iteration 94, loss = 0.79261032\n",
            "Iteration 95, loss = 0.79551030\n",
            "Iteration 96, loss = 0.79653678\n",
            "Iteration 97, loss = 0.80181195\n",
            "Iteration 98, loss = 0.79099761\n",
            "Iteration 99, loss = 0.79945179\n",
            "Iteration 100, loss = 0.78136168\n",
            "Iteration 1, loss = 1.67698543\n",
            "Iteration 2, loss = 1.66889605\n",
            "Iteration 3, loss = 1.66135830\n",
            "Iteration 4, loss = 1.65386804\n",
            "Iteration 5, loss = 1.64702274\n",
            "Iteration 6, loss = 1.64109783\n",
            "Iteration 7, loss = 1.63501348\n",
            "Iteration 8, loss = 1.62999284\n",
            "Iteration 9, loss = 1.62491618\n",
            "Iteration 10, loss = 1.62039528\n",
            "Iteration 11, loss = 1.61641597\n",
            "Iteration 12, loss = 1.61247880\n",
            "Iteration 13, loss = 1.60890089\n",
            "Iteration 14, loss = 1.60586359\n",
            "Iteration 15, loss = 1.60318178\n",
            "Iteration 16, loss = 1.60020301\n",
            "Iteration 17, loss = 1.59792317\n",
            "Iteration 18, loss = 1.59582506\n",
            "Iteration 19, loss = 1.59360353\n",
            "Iteration 20, loss = 1.59194140\n",
            "Iteration 21, loss = 1.58987911\n",
            "Iteration 22, loss = 1.58854093\n",
            "Iteration 23, loss = 1.58678079\n",
            "Iteration 24, loss = 1.58547744\n",
            "Iteration 25, loss = 1.58387516\n",
            "Iteration 26, loss = 1.58268497\n",
            "Iteration 27, loss = 1.58132438\n",
            "Iteration 28, loss = 1.58018879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 29, loss = 1.57899286\n",
            "Iteration 30, loss = 1.57777707\n",
            "Iteration 31, loss = 1.57673716\n",
            "Iteration 32, loss = 1.57554367\n",
            "Iteration 33, loss = 1.57441485\n",
            "Iteration 34, loss = 1.57324768\n",
            "Iteration 35, loss = 1.57215888\n",
            "Iteration 36, loss = 1.57104055\n",
            "Iteration 37, loss = 1.56997817\n",
            "Iteration 38, loss = 1.56901802\n",
            "Iteration 39, loss = 1.56778242\n",
            "Iteration 40, loss = 1.56667166\n",
            "Iteration 41, loss = 1.56556876\n",
            "Iteration 42, loss = 1.56443481\n",
            "Iteration 43, loss = 1.56330993\n",
            "Iteration 44, loss = 1.56220345\n",
            "Iteration 45, loss = 1.56101771\n",
            "Iteration 46, loss = 1.55990507\n",
            "Iteration 47, loss = 1.55872961\n",
            "Iteration 48, loss = 1.55757895\n",
            "Iteration 49, loss = 1.55634717\n",
            "Iteration 50, loss = 1.55518632\n",
            "Iteration 51, loss = 1.55395260\n",
            "Iteration 52, loss = 1.55274917\n",
            "Iteration 53, loss = 1.55153546\n",
            "Iteration 54, loss = 1.55030555\n",
            "Iteration 55, loss = 1.54904271\n",
            "Iteration 56, loss = 1.54780022\n",
            "Iteration 57, loss = 1.54649315\n",
            "Iteration 58, loss = 1.54528250\n",
            "Iteration 59, loss = 1.54397348\n",
            "Iteration 60, loss = 1.54265023\n",
            "Iteration 61, loss = 1.54133562\n",
            "Iteration 62, loss = 1.53996300\n",
            "Iteration 63, loss = 1.53863667\n",
            "Iteration 64, loss = 1.53727242\n",
            "Iteration 65, loss = 1.53588466\n",
            "Iteration 66, loss = 1.53453084\n",
            "Iteration 67, loss = 1.53315539\n",
            "Iteration 68, loss = 1.53171329\n",
            "Iteration 69, loss = 1.53034652\n",
            "Iteration 70, loss = 1.52890796\n",
            "Iteration 71, loss = 1.52748626\n",
            "Iteration 72, loss = 1.52592895\n",
            "Iteration 73, loss = 1.52449024\n",
            "Iteration 74, loss = 1.52300857\n",
            "Iteration 75, loss = 1.52150857\n",
            "Iteration 76, loss = 1.51999122\n",
            "Iteration 77, loss = 1.51859393\n",
            "Iteration 78, loss = 1.51695385\n",
            "Iteration 79, loss = 1.51544068\n",
            "Iteration 80, loss = 1.51378964\n",
            "Iteration 81, loss = 1.51223860\n",
            "Iteration 82, loss = 1.51071054\n",
            "Iteration 83, loss = 1.50909062\n",
            "Iteration 84, loss = 1.50742605\n",
            "Iteration 85, loss = 1.50587696\n",
            "Iteration 86, loss = 1.50421279\n",
            "Iteration 87, loss = 1.50259762\n",
            "Iteration 88, loss = 1.50090890\n",
            "Iteration 89, loss = 1.49917151\n",
            "Iteration 90, loss = 1.49755243\n",
            "Iteration 91, loss = 1.49582732\n",
            "Iteration 92, loss = 1.49412716\n",
            "Iteration 93, loss = 1.49238698\n",
            "Iteration 94, loss = 1.49060314\n",
            "Iteration 95, loss = 1.48894019\n",
            "Iteration 96, loss = 1.48710850\n",
            "Iteration 97, loss = 1.48532327\n",
            "Iteration 98, loss = 1.48356725\n",
            "Iteration 99, loss = 1.48175229\n",
            "Iteration 100, loss = 1.47997350\n",
            "Iteration 1, loss = 1.65896453\n",
            "Iteration 2, loss = 1.60822608\n",
            "Iteration 3, loss = 1.59112749\n",
            "Iteration 4, loss = 1.58204790\n",
            "Iteration 5, loss = 1.57673269\n",
            "Iteration 6, loss = 1.56781392\n",
            "Iteration 7, loss = 1.55806973\n",
            "Iteration 8, loss = 1.54408909\n",
            "Iteration 9, loss = 1.53092280\n",
            "Iteration 10, loss = 1.51767134\n",
            "Iteration 11, loss = 1.50486360\n",
            "Iteration 12, loss = 1.49139620\n",
            "Iteration 13, loss = 1.47887200\n",
            "Iteration 14, loss = 1.46348668\n",
            "Iteration 15, loss = 1.44762597\n",
            "Iteration 16, loss = 1.42950103\n",
            "Iteration 17, loss = 1.41241983\n",
            "Iteration 18, loss = 1.39520485\n",
            "Iteration 19, loss = 1.37694162\n",
            "Iteration 20, loss = 1.36019198\n",
            "Iteration 21, loss = 1.34220659\n",
            "Iteration 22, loss = 1.32528153\n",
            "Iteration 23, loss = 1.30838695\n",
            "Iteration 24, loss = 1.29279958\n",
            "Iteration 25, loss = 1.27786308\n",
            "Iteration 26, loss = 1.26404268\n",
            "Iteration 27, loss = 1.25043188\n",
            "Iteration 28, loss = 1.23889009\n",
            "Iteration 29, loss = 1.22823915\n",
            "Iteration 30, loss = 1.21763584\n",
            "Iteration 31, loss = 1.20933256\n",
            "Iteration 32, loss = 1.20136848\n",
            "Iteration 33, loss = 1.19322950\n",
            "Iteration 34, loss = 1.18577280\n",
            "Iteration 35, loss = 1.17949714\n",
            "Iteration 36, loss = 1.17398861\n",
            "Iteration 37, loss = 1.16855092\n",
            "Iteration 38, loss = 1.16449099\n",
            "Iteration 39, loss = 1.15928512\n",
            "Iteration 40, loss = 1.15543211\n",
            "Iteration 41, loss = 1.15219850\n",
            "Iteration 42, loss = 1.14808216\n",
            "Iteration 43, loss = 1.14472839\n",
            "Iteration 44, loss = 1.14291034\n",
            "Iteration 45, loss = 1.13909369\n",
            "Iteration 46, loss = 1.13672374\n",
            "Iteration 47, loss = 1.13460169\n",
            "Iteration 48, loss = 1.13163728\n",
            "Iteration 49, loss = 1.12845375\n",
            "Iteration 50, loss = 1.12769346\n",
            "Iteration 51, loss = 1.12531007\n",
            "Iteration 52, loss = 1.12332117\n",
            "Iteration 53, loss = 1.12151026\n",
            "Iteration 54, loss = 1.11906393\n",
            "Iteration 55, loss = 1.11907079\n",
            "Iteration 56, loss = 1.11672477\n",
            "Iteration 57, loss = 1.11375249"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 58, loss = 1.11240026\n",
            "Iteration 59, loss = 1.11175569\n",
            "Iteration 60, loss = 1.11008757\n",
            "Iteration 61, loss = 1.10887977\n",
            "Iteration 62, loss = 1.10685373\n",
            "Iteration 63, loss = 1.10523523\n",
            "Iteration 64, loss = 1.10401132\n",
            "Iteration 65, loss = 1.10326997\n",
            "Iteration 66, loss = 1.10173165\n",
            "Iteration 67, loss = 1.10083627\n",
            "Iteration 68, loss = 1.09903324\n",
            "Iteration 69, loss = 1.09914964\n",
            "Iteration 70, loss = 1.10042393\n",
            "Iteration 71, loss = 1.09667758\n",
            "Iteration 72, loss = 1.09644303\n",
            "Iteration 73, loss = 1.09435004\n",
            "Iteration 74, loss = 1.09194260\n",
            "Iteration 75, loss = 1.09079332\n",
            "Iteration 76, loss = 1.08944117\n",
            "Iteration 77, loss = 1.08906940\n",
            "Iteration 78, loss = 1.08819821\n",
            "Iteration 79, loss = 1.08761058\n",
            "Iteration 80, loss = 1.08477691\n",
            "Iteration 81, loss = 1.08353402\n",
            "Iteration 82, loss = 1.08360849\n",
            "Iteration 83, loss = 1.08151785\n",
            "Iteration 84, loss = 1.08012053\n",
            "Iteration 85, loss = 1.07914480\n",
            "Iteration 86, loss = 1.07788762\n",
            "Iteration 87, loss = 1.07707776\n",
            "Iteration 88, loss = 1.07624794\n",
            "Iteration 89, loss = 1.07407892\n",
            "Iteration 90, loss = 1.07333041\n",
            "Iteration 91, loss = 1.07179463\n",
            "Iteration 92, loss = 1.07038168\n",
            "Iteration 93, loss = 1.06987856\n",
            "Iteration 94, loss = 1.06847402\n",
            "Iteration 95, loss = 1.06740212\n",
            "Iteration 96, loss = 1.06650395\n",
            "Iteration 97, loss = 1.06389487\n",
            "Iteration 98, loss = 1.06493959\n",
            "Iteration 99, loss = 1.06348924\n",
            "Iteration 100, loss = 1.06197573\n",
            "Iteration 1, loss = 1.65854341\n",
            "Iteration 2, loss = 1.58120170\n",
            "Iteration 3, loss = 1.50598790\n",
            "Iteration 4, loss = 1.39377204\n",
            "Iteration 5, loss = 1.29321878\n",
            "Iteration 6, loss = 1.22716005\n",
            "Iteration 7, loss = 1.18972134\n",
            "Iteration 8, loss = 1.17325730\n",
            "Iteration 9, loss = 1.14532885\n",
            "Iteration 10, loss = 1.12953264\n",
            "Iteration 11, loss = 1.13372235"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 12, loss = 1.12381091\n",
            "Iteration 13, loss = 1.11009110\n",
            "Iteration 14, loss = 1.12216329\n",
            "Iteration 15, loss = 1.11063935\n",
            "Iteration 16, loss = 1.10356064\n",
            "Iteration 17, loss = 1.11626258\n",
            "Iteration 18, loss = 1.10623651\n",
            "Iteration 19, loss = 1.09771642\n",
            "Iteration 20, loss = 1.09908664\n",
            "Iteration 21, loss = 1.09929070\n",
            "Iteration 22, loss = 1.09291621\n",
            "Iteration 23, loss = 1.08341042\n",
            "Iteration 24, loss = 1.08442039\n",
            "Iteration 25, loss = 1.08257748\n",
            "Iteration 26, loss = 1.07446344\n",
            "Iteration 27, loss = 1.07039711\n",
            "Iteration 28, loss = 1.06796521\n",
            "Iteration 29, loss = 1.09627263\n",
            "Iteration 30, loss = 1.10210559\n",
            "Iteration 31, loss = 1.14384982\n",
            "Iteration 32, loss = 1.09343724\n",
            "Iteration 33, loss = 1.07212231\n",
            "Iteration 34, loss = 1.10448795\n",
            "Iteration 35, loss = 1.05721183\n",
            "Iteration 36, loss = 1.07245069\n",
            "Iteration 37, loss = 1.05729477\n",
            "Iteration 38, loss = 1.04751998\n",
            "Iteration 39, loss = 1.04217375\n",
            "Iteration 40, loss = 1.00654020\n",
            "Iteration 41, loss = 1.00947302\n",
            "Iteration 42, loss = 0.99167966\n",
            "Iteration 43, loss = 0.99364359\n",
            "Iteration 44, loss = 0.97218742\n",
            "Iteration 45, loss = 0.98707289\n",
            "Iteration 46, loss = 0.99045603\n",
            "Iteration 47, loss = 1.04399844\n",
            "Iteration 48, loss = 0.97457379\n",
            "Iteration 49, loss = 0.94455258\n",
            "Iteration 50, loss = 0.95893268\n",
            "Iteration 51, loss = 0.95223570\n",
            "Iteration 52, loss = 0.95545567\n",
            "Iteration 53, loss = 0.95514186\n",
            "Iteration 54, loss = 0.93010635\n",
            "Iteration 55, loss = 0.90544020\n",
            "Iteration 56, loss = 0.90790351\n",
            "Iteration 57, loss = 0.90936724\n",
            "Iteration 58, loss = 0.89392885\n",
            "Iteration 59, loss = 0.88204637\n",
            "Iteration 60, loss = 1.01692668\n",
            "Iteration 61, loss = 0.92668399\n",
            "Iteration 62, loss = 0.88822738\n",
            "Iteration 63, loss = 0.88377036\n",
            "Iteration 64, loss = 0.88975722\n",
            "Iteration 65, loss = 0.90692520\n",
            "Iteration 66, loss = 0.91176609\n",
            "Iteration 67, loss = 0.93838595\n",
            "Iteration 68, loss = 0.88115022\n",
            "Iteration 69, loss = 0.83777575\n",
            "Iteration 70, loss = 0.85179249\n",
            "Iteration 71, loss = 0.86702301\n",
            "Iteration 72, loss = 0.86714651\n",
            "Iteration 73, loss = 0.85693630\n",
            "Iteration 74, loss = 0.87282794\n",
            "Iteration 75, loss = 0.86965078\n",
            "Iteration 76, loss = 0.84142144\n",
            "Iteration 77, loss = 0.87210119\n",
            "Iteration 78, loss = 0.87159201\n",
            "Iteration 79, loss = 0.91565459\n",
            "Iteration 80, loss = 0.95142616\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.75362523\n",
            "Iteration 2, loss = 1.73980789\n",
            "Iteration 3, loss = 1.72673324\n",
            "Iteration 4, loss = 1.71411875\n",
            "Iteration 5, loss = 1.70183915\n",
            "Iteration 6, loss = 1.69148279\n",
            "Iteration 7, loss = 1.68120943\n",
            "Iteration 8, loss = 1.67215388\n",
            "Iteration 9, loss = 1.66393574\n",
            "Iteration 10, loss = 1.65631817\n",
            "Iteration 11, loss = 1.64986630\n",
            "Iteration 12, loss = 1.64328019\n",
            "Iteration 13, loss = 1.63766218\n",
            "Iteration 14, loss = 1.63263668\n",
            "Iteration 15, loss = 1.62820494\n",
            "Iteration 16, loss = 1.62456247\n",
            "Iteration 17, loss = 1.62065639\n",
            "Iteration 18, loss = 1.61752400\n",
            "Iteration 19, loss = 1.61474205\n",
            "Iteration 20, loss = 1.61225725\n",
            "Iteration 21, loss = 1.60990529\n",
            "Iteration 22, loss = 1.60810749\n",
            "Iteration 23, loss = 1.60603122\n",
            "Iteration 24, loss = 1.60435892\n",
            "Iteration 25, loss = 1.60336807\n",
            "Iteration 26, loss = 1.60232551\n",
            "Iteration 27, loss = 1.60093011\n",
            "Iteration 28, loss = 1.60021979\n",
            "Iteration 29, loss = 1.59930160\n",
            "Iteration 30, loss = 1.59899318\n",
            "Iteration 31, loss = 1.59826496\n",
            "Iteration 32, loss = 1.59807819\n",
            "Iteration 33, loss = 1.59790587\n",
            "Iteration 34, loss = 1.59740498\n",
            "Iteration 35, loss = 1.59718312\n",
            "Iteration 36, loss = 1.59712356\n",
            "Iteration 37, loss = 1.59681439\n",
            "Iteration 38, loss = 1.59682477\n",
            "Iteration 39, loss = 1.59655007\n",
            "Iteration 40, loss = 1.59637937\n",
            "Iteration 41, loss = 1.59630744\n",
            "Iteration 42, loss = 1.59623427\n",
            "Iteration 43, loss = 1.59608534\n",
            "Iteration 44, loss = 1.59604607\n",
            "Iteration 45, loss = 1.59587251\n",
            "Iteration 46, loss = 1.59588005\n",
            "Iteration 47, loss = 1.59573496\n",
            "Iteration 48, loss = 1.59562836\n",
            "Iteration 49, loss = 1.59547803\n",
            "Iteration 50, loss = 1.59543962\n",
            "Iteration 51, loss = 1.59532410\n",
            "Iteration 52, loss = 1.59523828\n",
            "Iteration 53, loss = 1.59509981\n",
            "Iteration 54, loss = 1.59504731\n",
            "Iteration 55, loss = 1.59494746\n",
            "Iteration 56, loss = 1.59481746\n",
            "Iteration 57, loss = 1.59470049\n",
            "Iteration 58, loss = 1.59462074\n",
            "Iteration 59, loss = 1.59453418\n",
            "Iteration 60, loss = 1.59450444\n",
            "Iteration 61, loss = 1.59433804\n",
            "Iteration 62, loss = 1.59421601\n",
            "Iteration 63, loss = 1.59410434\n",
            "Iteration 64, loss = 1.59399590\n",
            "Iteration 65, loss = 1.59391151\n",
            "Iteration 66, loss = 1.59384470\n",
            "Iteration 67, loss = 1.59375091\n",
            "Iteration 68, loss = 1.59357772\n",
            "Iteration 69, loss = 1.59354133\n",
            "Iteration 70, loss = 1.59343893\n",
            "Iteration 71, loss = 1.59333896\n",
            "Iteration 72, loss = 1.59317549\n",
            "Iteration 73, loss = 1.59313904\n",
            "Iteration 74, loss = 1.59302784\n",
            "Iteration 75, loss = 1.59286872\n",
            "Iteration 76, loss = 1.59275015\n",
            "Iteration 77, loss = 1.59278072\n",
            "Iteration 78, loss = 1.59255474\n",
            "Iteration 79, loss = 1.59250575\n",
            "Iteration 80, loss = 1.59229296\n",
            "Iteration 81, loss = 1.59219137\n",
            "Iteration 82, loss = 1.59215138\n",
            "Iteration 83, loss = 1.59199658\n",
            "Iteration 84, loss = 1.59188607\n",
            "Iteration 85, loss = 1.59178897\n",
            "Iteration 86, loss = 1.59166455\n",
            "Iteration 87, loss = 1.59157105\n",
            "Iteration 88, loss = 1.59144777\n",
            "Iteration 89, loss = 1.59130008\n",
            "Iteration 90, loss = 1.59123350\n",
            "Iteration 91, loss = 1.59108801\n",
            "Iteration 92, loss = 1.59100092\n",
            "Iteration 93, loss = 1.59087142\n",
            "Iteration 94, loss = 1.59074818\n",
            "Iteration 95, loss = 1.59069598\n",
            "Iteration 96, loss = 1.59051842\n",
            "Iteration 97, loss = 1.59038365\n",
            "Iteration 98, loss = 1.59035374\n",
            "Iteration 99, loss = 1.59017174\n",
            "Iteration 100, loss = 1.59013490\n",
            "Iteration 1, loss = 1.72202389\n",
            "Iteration 2, loss = 1.63711076\n",
            "Iteration 3, loss = 1.61250199\n",
            "Iteration 4, loss = 1.60513770\n",
            "Iteration 5, loss = 1.61004701\n",
            "Iteration 6, loss = 1.61705979\n",
            "Iteration 7, loss = 1.61714921\n",
            "Iteration 8, loss = 1.60849716\n",
            "Iteration 9, loss = 1.59881989\n",
            "Iteration 10, loss = 1.59363990\n",
            "Iteration 11, loss = 1.59389132\n",
            "Iteration 12, loss = 1.59484473\n",
            "Iteration 13, loss = 1.59516934\n",
            "Iteration 14, loss = 1.59263631\n",
            "Iteration 15, loss = 1.59017692\n",
            "Iteration 16, loss = 1.58867895\n",
            "Iteration 17, loss = 1.58575984\n",
            "Iteration 18, loss = 1.58463115\n",
            "Iteration 19, loss = 1.58376656\n",
            "Iteration 20, loss = 1.58326133\n",
            "Iteration 21, loss = 1.58175755\n",
            "Iteration 22, loss = 1.57966326\n",
            "Iteration 23, loss = 1.57752919\n",
            "Iteration 24, loss = 1.57622575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 25, loss = 1.57378294\n",
            "Iteration 26, loss = 1.57228544\n",
            "Iteration 27, loss = 1.56970827\n",
            "Iteration 28, loss = 1.56761160\n",
            "Iteration 29, loss = 1.56511215\n",
            "Iteration 30, loss = 1.56252334\n",
            "Iteration 31, loss = 1.55980563\n",
            "Iteration 32, loss = 1.55707285\n",
            "Iteration 33, loss = 1.55420376\n",
            "Iteration 34, loss = 1.55036487\n",
            "Iteration 35, loss = 1.54673969\n",
            "Iteration 36, loss = 1.54358110\n",
            "Iteration 37, loss = 1.53922026\n",
            "Iteration 38, loss = 1.53712004\n",
            "Iteration 39, loss = 1.53117079\n",
            "Iteration 40, loss = 1.52675639\n",
            "Iteration 41, loss = 1.52181409\n",
            "Iteration 42, loss = 1.51771624\n",
            "Iteration 43, loss = 1.51165447\n",
            "Iteration 44, loss = 1.50703994\n",
            "Iteration 45, loss = 1.50058225\n",
            "Iteration 46, loss = 1.49549451\n",
            "Iteration 47, loss = 1.48927507\n",
            "Iteration 48, loss = 1.48260114\n",
            "Iteration 49, loss = 1.47551436\n",
            "Iteration 50, loss = 1.46985264\n",
            "Iteration 51, loss = 1.46270240\n",
            "Iteration 52, loss = 1.45554571\n",
            "Iteration 53, loss = 1.44772407\n",
            "Iteration 54, loss = 1.44127278\n",
            "Iteration 55, loss = 1.43386559\n",
            "Iteration 56, loss = 1.42591529\n",
            "Iteration 57, loss = 1.41771501\n",
            "Iteration 58, loss = 1.41023815\n",
            "Iteration 59, loss = 1.40276945\n",
            "Iteration 60, loss = 1.39521210\n",
            "Iteration 61, loss = 1.38707011\n",
            "Iteration 62, loss = 1.37861820\n",
            "Iteration 63, loss = 1.37122921\n",
            "Iteration 64, loss = 1.36353296\n",
            "Iteration 65, loss = 1.35544652\n",
            "Iteration 66, loss = 1.34872964\n",
            "Iteration 67, loss = 1.34158831\n",
            "Iteration 68, loss = 1.33345881\n",
            "Iteration 69, loss = 1.32757070\n",
            "Iteration 70, loss = 1.32019760\n",
            "Iteration 71, loss = 1.31296042\n",
            "Iteration 72, loss = 1.30618337\n",
            "Iteration 73, loss = 1.30094255\n",
            "Iteration 74, loss = 1.29489410\n",
            "Iteration 75, loss = 1.28829984\n",
            "Iteration 76, loss = 1.28214912\n",
            "Iteration 77, loss = 1.27836140\n",
            "Iteration 78, loss = 1.27228379\n",
            "Iteration 79, loss = 1.26763098\n",
            "Iteration 80, loss = 1.26173477\n",
            "Iteration 81, loss = 1.25701329\n",
            "Iteration 82, loss = 1.25375494\n",
            "Iteration 83, loss = 1.24870335\n",
            "Iteration 84, loss = 1.24421034\n",
            "Iteration 85, loss = 1.24011348\n",
            "Iteration 86, loss = 1.23651651\n",
            "Iteration 87, loss = 1.23282830\n",
            "Iteration 88, loss = 1.22964296\n",
            "Iteration 89, loss = 1.22561730\n",
            "Iteration 90, loss = 1.22276491\n",
            "Iteration 91, loss = 1.21945002\n",
            "Iteration 92, loss = 1.21651755\n",
            "Iteration 93, loss = 1.21372147\n",
            "Iteration 94, loss = 1.21083332\n",
            "Iteration 95, loss = 1.20921057\n",
            "Iteration 96, loss = 1.20580928\n",
            "Iteration 97, loss = 1.20304576\n",
            "Iteration 98, loss = 1.20184046\n",
            "Iteration 99, loss = 1.19859077\n",
            "Iteration 100, loss = 1.19689050\n",
            "Iteration 1, loss = 1.88787553\n",
            "Iteration 2, loss = 1.78456701\n",
            "Iteration 3, loss = 1.75842843\n",
            "Iteration 4, loss = 1.67942678\n",
            "Iteration 5, loss = 1.62671840\n",
            "Iteration 6, loss = 1.63391398\n",
            "Iteration 7, loss = 1.62976933\n",
            "Iteration 8, loss = 1.60888161\n",
            "Iteration 9, loss = 1.59497914\n",
            "Iteration 10, loss = 1.59081387\n",
            "Iteration 11, loss = 1.58637519\n",
            "Iteration 12, loss = 1.58057680\n",
            "Iteration 13, loss = 1.57182987\n",
            "Iteration 14, loss = 1.56360259\n",
            "Iteration 15, loss = 1.55342373\n",
            "Iteration 16, loss = 1.54264542\n",
            "Iteration 17, loss = 1.52676888\n",
            "Iteration 18, loss = 1.50773267\n",
            "Iteration 19, loss = 1.48316214\n",
            "Iteration 20, loss = 1.45321026\n",
            "Iteration 21, loss = 1.41673731\n",
            "Iteration 22, loss = 1.37944879\n",
            "Iteration 23, loss = 1.34014488\n",
            "Iteration 24, loss = 1.30266040\n",
            "Iteration 25, loss = 1.27096904\n",
            "Iteration 26, loss = 1.24511285\n",
            "Iteration 27, loss = 1.21730997\n",
            "Iteration 28, loss = 1.20022072\n",
            "Iteration 29, loss = 1.18387476\n",
            "Iteration 30, loss = 1.17142976\n",
            "Iteration 31, loss = 1.16589430"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 32, loss = 1.15103866\n",
            "Iteration 33, loss = 1.14433845\n",
            "Iteration 34, loss = 1.13731844\n",
            "Iteration 35, loss = 1.13444941\n",
            "Iteration 36, loss = 1.12779586\n",
            "Iteration 37, loss = 1.12837657\n",
            "Iteration 38, loss = 1.12241379\n",
            "Iteration 39, loss = 1.12190584\n",
            "Iteration 40, loss = 1.11661934\n",
            "Iteration 41, loss = 1.11901771\n",
            "Iteration 42, loss = 1.11902954\n",
            "Iteration 43, loss = 1.11172189\n",
            "Iteration 44, loss = 1.11470779\n",
            "Iteration 45, loss = 1.11102861\n",
            "Iteration 46, loss = 1.11412297\n",
            "Iteration 47, loss = 1.11056945\n",
            "Iteration 48, loss = 1.11143339\n",
            "Iteration 49, loss = 1.10834634\n",
            "Iteration 50, loss = 1.10760839\n",
            "Iteration 51, loss = 1.10890140\n",
            "Iteration 52, loss = 1.10469817\n",
            "Iteration 53, loss = 1.10667405\n",
            "Iteration 54, loss = 1.11109874\n",
            "Iteration 55, loss = 1.11170447\n",
            "Iteration 56, loss = 1.11011741\n",
            "Iteration 57, loss = 1.10641602\n",
            "Iteration 58, loss = 1.10882709\n",
            "Iteration 59, loss = 1.10633426\n",
            "Iteration 60, loss = 1.10924305\n",
            "Iteration 61, loss = 1.11363083\n",
            "Iteration 62, loss = 1.10595106\n",
            "Iteration 63, loss = 1.10375642\n",
            "Iteration 64, loss = 1.10886037\n",
            "Iteration 65, loss = 1.10466539\n",
            "Iteration 66, loss = 1.10627255\n",
            "Iteration 67, loss = 1.10175000\n",
            "Iteration 68, loss = 1.10093077\n",
            "Iteration 69, loss = 1.10235808\n",
            "Iteration 70, loss = 1.10857385\n",
            "Iteration 71, loss = 1.09286542\n",
            "Iteration 72, loss = 1.10659104\n",
            "Iteration 73, loss = 1.10760854\n",
            "Iteration 74, loss = 1.10557369\n",
            "Iteration 75, loss = 1.10789235\n",
            "Iteration 76, loss = 1.08984970\n",
            "Iteration 77, loss = 1.10953735\n",
            "Iteration 78, loss = 1.09563850\n",
            "Iteration 79, loss = 1.10242297\n",
            "Iteration 80, loss = 1.08534467\n",
            "Iteration 81, loss = 1.08547465\n",
            "Iteration 82, loss = 1.09152532\n",
            "Iteration 83, loss = 1.08380565\n",
            "Iteration 84, loss = 1.08304696\n",
            "Iteration 85, loss = 1.07596634\n",
            "Iteration 86, loss = 1.07806771\n",
            "Iteration 87, loss = 1.07140373\n",
            "Iteration 88, loss = 1.07652408\n",
            "Iteration 89, loss = 1.06789739\n",
            "Iteration 90, loss = 1.06929555\n",
            "Iteration 91, loss = 1.06212221\n",
            "Iteration 92, loss = 1.06047453\n",
            "Iteration 93, loss = 1.05911009\n",
            "Iteration 94, loss = 1.05224887\n",
            "Iteration 95, loss = 1.05723354\n",
            "Iteration 96, loss = 1.04870637\n",
            "Iteration 97, loss = 1.04231275\n",
            "Iteration 98, loss = 1.04374301\n",
            "Iteration 99, loss = 1.03927506\n",
            "Iteration 100, loss = 1.03985694\n",
            "Iteration 1, loss = 1.67369467\n",
            "Iteration 2, loss = 1.66607903\n",
            "Iteration 3, loss = 1.65897360\n",
            "Iteration 4, loss = 1.65188604\n",
            "Iteration 5, loss = 1.64539946\n",
            "Iteration 6, loss = 1.63977829\n",
            "Iteration 7, loss = 1.63397853\n",
            "Iteration 8, loss = 1.62918838\n",
            "Iteration 9, loss = 1.62432540\n",
            "Iteration 10, loss = 1.61998457\n",
            "Iteration 11, loss = 1.61615475\n",
            "Iteration 12, loss = 1.61235633\n",
            "Iteration 13, loss = 1.60889472\n",
            "Iteration 14, loss = 1.60594841\n",
            "Iteration 15, loss = 1.60334201\n",
            "Iteration 16, loss = 1.60044335\n",
            "Iteration 17, loss = 1.59821875\n",
            "Iteration 18, loss = 1.59616939\n",
            "Iteration 19, loss = 1.59399962\n",
            "Iteration 20, loss = 1.59237339\n",
            "Iteration 21, loss = 1.59035612\n",
            "Iteration 22, loss = 1.58904956\n",
            "Iteration 23, loss = 1.58732468\n",
            "Iteration 24, loss = 1.58605325\n",
            "Iteration 25, loss = 1.58449200\n",
            "Iteration 26, loss = 1.58333389\n",
            "Iteration 27, loss = 1.58200409\n",
            "Iteration 28, loss = 1.58090455\n",
            "Iteration 29, loss = 1.57973766\n",
            "Iteration 30, loss = 1.57856329\n",
            "Iteration 31, loss = 1.57755615\n",
            "Iteration 32, loss = 1.57640340\n",
            "Iteration 33, loss = 1.57531504\n",
            "Iteration 34, loss = 1.57418436\n",
            "Iteration 35, loss = 1.57313559\n",
            "Iteration 36, loss = 1.57205578\n",
            "Iteration 37, loss = 1.57103481\n",
            "Iteration 38, loss = 1.57011513\n",
            "Iteration 39, loss = 1.56892274\n",
            "Iteration 40, loss = 1.56785242\n",
            "Iteration 41, loss = 1.56679207\n",
            "Iteration 42, loss = 1.56570082\n",
            "Iteration 43, loss = 1.56461968\n",
            "Iteration 44, loss = 1.56355821\n",
            "Iteration 45, loss = 1.56241607\n",
            "Iteration 46, loss = 1.56134664\n",
            "Iteration 47, loss = 1.56021490\n",
            "Iteration 48, loss = 1.55910930\n",
            "Iteration 49, loss = 1.55792229\n",
            "Iteration 50, loss = 1.55680383\n",
            "Iteration 51, loss = 1.55561647\n",
            "Iteration 52, loss = 1.55445745\n",
            "Iteration 53, loss = 1.55328692\n",
            "Iteration 54, loss = 1.55210196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 1.55088253\n",
            "Iteration 56, loss = 1.54968407\n",
            "Iteration 57, loss = 1.54842203\n",
            "Iteration 58, loss = 1.54725333\n",
            "Iteration 59, loss = 1.54598816\n",
            "Iteration 60, loss = 1.54471016\n",
            "Iteration 61, loss = 1.54343895\n",
            "Iteration 62, loss = 1.54211101\n",
            "Iteration 63, loss = 1.54082771\n",
            "Iteration 64, loss = 1.53950737\n",
            "Iteration 65, loss = 1.53816356\n",
            "Iteration 66, loss = 1.53685222\n",
            "Iteration 67, loss = 1.53551848\n",
            "Iteration 68, loss = 1.53412113\n",
            "Iteration 69, loss = 1.53279508\n",
            "Iteration 70, loss = 1.53139891\n",
            "Iteration 71, loss = 1.53001877\n",
            "Iteration 72, loss = 1.52850730\n",
            "Iteration 73, loss = 1.52710888\n",
            "Iteration 74, loss = 1.52566904\n",
            "Iteration 75, loss = 1.52421044\n",
            "Iteration 76, loss = 1.52273491\n",
            "Iteration 77, loss = 1.52137355\n",
            "Iteration 78, loss = 1.51977731\n",
            "Iteration 79, loss = 1.51830196\n",
            "Iteration 80, loss = 1.51669482\n",
            "Iteration 81, loss = 1.51518253\n",
            "Iteration 82, loss = 1.51369053\n",
            "Iteration 83, loss = 1.51211079\n",
            "Iteration 84, loss = 1.51048548\n",
            "Iteration 85, loss = 1.50897153\n",
            "Iteration 86, loss = 1.50734573\n",
            "Iteration 87, loss = 1.50576641\n",
            "Iteration 88, loss = 1.50411405\n",
            "Iteration 89, loss = 1.50241548\n",
            "Iteration 90, loss = 1.50082932\n",
            "Iteration 91, loss = 1.49913975\n",
            "Iteration 92, loss = 1.49747375\n",
            "Iteration 93, loss = 1.49576824\n",
            "Iteration 94, loss = 1.49401874\n",
            "Iteration 95, loss = 1.49238479\n",
            "Iteration 96, loss = 1.49058749\n",
            "Iteration 97, loss = 1.48883482\n",
            "Iteration 98, loss = 1.48710711\n",
            "Iteration 99, loss = 1.48532335\n",
            "Iteration 100, loss = 1.48357093\n",
            "Iteration 1, loss = 1.65670461\n",
            "Iteration 2, loss = 1.60827432\n",
            "Iteration 3, loss = 1.59117272\n",
            "Iteration 4, loss = 1.58195070\n",
            "Iteration 5, loss = 1.57689448\n",
            "Iteration 6, loss = 1.56850568\n",
            "Iteration 7, loss = 1.55918681\n",
            "Iteration 8, loss = 1.54570123\n",
            "Iteration 9, loss = 1.53283666\n",
            "Iteration 10, loss = 1.51968963\n",
            "Iteration 11, loss = 1.50690176\n",
            "Iteration 12, loss = 1.49348250\n",
            "Iteration 13, loss = 1.48110347\n",
            "Iteration 14, loss = 1.46584521\n",
            "Iteration 15, loss = 1.45012950\n",
            "Iteration 16, loss = 1.43218842\n",
            "Iteration 17, loss = 1.41509412\n",
            "Iteration 18, loss = 1.39779591\n",
            "Iteration 19, loss = 1.37941688\n",
            "Iteration 20, loss = 1.36254426\n",
            "Iteration 21, loss = 1.34441881\n",
            "Iteration 22, loss = 1.32737975\n",
            "Iteration 23, loss = 1.31035136\n",
            "Iteration 24, loss = 1.29460597\n",
            "Iteration 25, loss = 1.27940669\n",
            "Iteration 26, loss = 1.26532856\n",
            "Iteration 27, loss = 1.25154796\n",
            "Iteration 28, loss = 1.23978003\n",
            "Iteration 29, loss = 1.22891128\n",
            "Iteration 30, loss = 1.21819611\n",
            "Iteration 31, loss = 1.20966923\n",
            "Iteration 32, loss = 1.20162725\n",
            "Iteration 33, loss = 1.19328545\n",
            "Iteration 34, loss = 1.18583136\n",
            "Iteration 35, loss = 1.17956224\n",
            "Iteration 36, loss = 1.17391871\n",
            "Iteration 37, loss = 1.16849354\n",
            "Iteration 38, loss = 1.16432291\n",
            "Iteration 39, loss = 1.15921583\n",
            "Iteration 40, loss = 1.15538070\n",
            "Iteration 41, loss = 1.15213117\n",
            "Iteration 42, loss = 1.14795285"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 43, loss = 1.14464884\n",
            "Iteration 44, loss = 1.14275575\n",
            "Iteration 45, loss = 1.13899443\n",
            "Iteration 46, loss = 1.13660294\n",
            "Iteration 47, loss = 1.13455979\n",
            "Iteration 48, loss = 1.13160463\n",
            "Iteration 49, loss = 1.12845561\n",
            "Iteration 50, loss = 1.12765195\n",
            "Iteration 51, loss = 1.12531406\n",
            "Iteration 52, loss = 1.12329781\n",
            "Iteration 53, loss = 1.12143393\n",
            "Iteration 54, loss = 1.11904569\n",
            "Iteration 55, loss = 1.11890419\n",
            "Iteration 56, loss = 1.11656631\n",
            "Iteration 57, loss = 1.11385306\n",
            "Iteration 58, loss = 1.11231281\n",
            "Iteration 59, loss = 1.11172063\n",
            "Iteration 60, loss = 1.11004312\n",
            "Iteration 61, loss = 1.10874068\n",
            "Iteration 62, loss = 1.10679375\n",
            "Iteration 63, loss = 1.10518675\n",
            "Iteration 64, loss = 1.10382264\n",
            "Iteration 65, loss = 1.10323261\n",
            "Iteration 66, loss = 1.10155813\n",
            "Iteration 67, loss = 1.10056192\n",
            "Iteration 68, loss = 1.09886096\n",
            "Iteration 69, loss = 1.09877007\n",
            "Iteration 70, loss = 1.09992208\n",
            "Iteration 71, loss = 1.09629765\n",
            "Iteration 72, loss = 1.09599370\n",
            "Iteration 73, loss = 1.09364132\n",
            "Iteration 74, loss = 1.09154321\n",
            "Iteration 75, loss = 1.09031458\n",
            "Iteration 76, loss = 1.08886509\n",
            "Iteration 77, loss = 1.08841154\n",
            "Iteration 78, loss = 1.08760321\n",
            "Iteration 79, loss = 1.08667171\n",
            "Iteration 80, loss = 1.08402706\n",
            "Iteration 81, loss = 1.08281717\n",
            "Iteration 82, loss = 1.08253617\n",
            "Iteration 83, loss = 1.08062480\n",
            "Iteration 84, loss = 1.07920034\n",
            "Iteration 85, loss = 1.07811487\n",
            "Iteration 86, loss = 1.07681220\n",
            "Iteration 87, loss = 1.07592628\n",
            "Iteration 88, loss = 1.07500272\n",
            "Iteration 89, loss = 1.07282817\n",
            "Iteration 90, loss = 1.07195009\n",
            "Iteration 91, loss = 1.07042043\n",
            "Iteration 92, loss = 1.06891832\n",
            "Iteration 93, loss = 1.06828270\n",
            "Iteration 94, loss = 1.06684384\n",
            "Iteration 95, loss = 1.06545631\n",
            "Iteration 96, loss = 1.06459281\n",
            "Iteration 97, loss = 1.06200883\n",
            "Iteration 98, loss = 1.06268263\n",
            "Iteration 99, loss = 1.06125941\n",
            "Iteration 100, loss = 1.05965886\n",
            "Iteration 1, loss = 1.65493556\n",
            "Iteration 2, loss = 1.59514078\n",
            "Iteration 3, loss = 1.51480096\n",
            "Iteration 4, loss = 1.39933298\n",
            "Iteration 5, loss = 1.30001062\n",
            "Iteration 6, loss = 1.23001272\n",
            "Iteration 7, loss = 1.18406911\n",
            "Iteration 8, loss = 1.16137453\n",
            "Iteration 9, loss = 1.14033367\n",
            "Iteration 10, loss = 1.11858559\n",
            "Iteration 11, loss = 1.12997303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 1.12123748\n",
            "Iteration 13, loss = 1.11016272\n",
            "Iteration 14, loss = 1.10804549\n",
            "Iteration 15, loss = 1.11414348\n",
            "Iteration 16, loss = 1.11038330\n",
            "Iteration 17, loss = 1.11122229\n",
            "Iteration 18, loss = 1.09729105\n",
            "Iteration 19, loss = 1.09717996\n",
            "Iteration 20, loss = 1.09866348\n",
            "Iteration 21, loss = 1.09742839\n",
            "Iteration 22, loss = 1.08978517\n",
            "Iteration 23, loss = 1.07972674\n",
            "Iteration 24, loss = 1.08283220\n",
            "Iteration 25, loss = 1.07990951\n",
            "Iteration 26, loss = 1.07102121\n",
            "Iteration 27, loss = 1.06334130\n",
            "Iteration 28, loss = 1.06194532\n",
            "Iteration 29, loss = 1.08318132\n",
            "Iteration 30, loss = 1.11122759\n",
            "Iteration 31, loss = 1.15373025\n",
            "Iteration 32, loss = 1.11051608\n",
            "Iteration 33, loss = 1.06636278\n",
            "Iteration 34, loss = 1.12336809\n",
            "Iteration 35, loss = 1.06358281\n",
            "Iteration 36, loss = 1.08028075\n",
            "Iteration 37, loss = 1.05624040\n",
            "Iteration 38, loss = 1.05389814\n",
            "Iteration 39, loss = 1.03961420\n",
            "Iteration 40, loss = 1.00748179\n",
            "Iteration 41, loss = 1.00459984\n",
            "Iteration 42, loss = 0.99363059\n",
            "Iteration 43, loss = 0.99626437\n",
            "Iteration 44, loss = 0.97800802\n",
            "Iteration 45, loss = 0.98154670\n",
            "Iteration 46, loss = 0.98976337\n",
            "Iteration 47, loss = 1.00043931\n",
            "Iteration 48, loss = 0.94964828\n",
            "Iteration 49, loss = 0.93614069\n",
            "Iteration 50, loss = 0.93923366\n",
            "Iteration 51, loss = 0.96830565\n",
            "Iteration 52, loss = 0.97267698\n",
            "Iteration 53, loss = 0.97506048\n",
            "Iteration 54, loss = 0.94888030\n",
            "Iteration 55, loss = 0.91852169\n",
            "Iteration 56, loss = 0.93253853\n",
            "Iteration 57, loss = 0.93268056\n",
            "Iteration 58, loss = 0.92737406\n",
            "Iteration 59, loss = 0.93156865\n",
            "Iteration 60, loss = 1.05687613\n",
            "Iteration 61, loss = 0.94371126\n",
            "Iteration 62, loss = 0.92447573\n",
            "Iteration 63, loss = 0.92811880\n",
            "Iteration 64, loss = 0.90685576\n",
            "Iteration 65, loss = 0.87420461\n",
            "Iteration 66, loss = 0.86884963\n",
            "Iteration 67, loss = 0.85752722\n",
            "Iteration 68, loss = 0.86850762\n",
            "Iteration 69, loss = 0.90192091\n",
            "Iteration 70, loss = 0.87482942\n",
            "Iteration 71, loss = 0.84891583\n",
            "Iteration 72, loss = 0.84795645\n",
            "Iteration 73, loss = 0.83965858\n",
            "Iteration 74, loss = 0.84604461\n",
            "Iteration 75, loss = 0.86447452\n",
            "Iteration 76, loss = 0.88566126\n",
            "Iteration 77, loss = 0.90017270\n",
            "Iteration 78, loss = 0.92663749\n",
            "Iteration 79, loss = 0.96462478\n",
            "Iteration 80, loss = 0.94499558\n",
            "Iteration 81, loss = 0.89201304\n",
            "Iteration 82, loss = 0.90086257\n",
            "Iteration 83, loss = 0.88879555\n",
            "Iteration 84, loss = 0.84742639\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.65925474\n",
            "Iteration 2, loss = 1.65570655\n",
            "Iteration 3, loss = 1.65226609\n",
            "Iteration 4, loss = 1.64861514\n",
            "Iteration 5, loss = 1.64513820\n",
            "Iteration 6, loss = 1.64215400\n",
            "Iteration 7, loss = 1.63884169\n",
            "Iteration 8, loss = 1.63598842\n",
            "Iteration 9, loss = 1.63303965\n",
            "Iteration 10, loss = 1.63027764\n",
            "Iteration 11, loss = 1.62763581\n",
            "Iteration 12, loss = 1.62491415\n",
            "Iteration 13, loss = 1.62240356\n",
            "Iteration 14, loss = 1.61992988\n",
            "Iteration 15, loss = 1.61766751\n",
            "Iteration 16, loss = 1.61535142\n",
            "Iteration 17, loss = 1.61315166\n",
            "Iteration 18, loss = 1.61110817\n",
            "Iteration 19, loss = 1.60901202\n",
            "Iteration 20, loss = 1.60716343\n",
            "Iteration 21, loss = 1.60507773\n",
            "Iteration 22, loss = 1.60345854\n",
            "Iteration 23, loss = 1.60143807\n",
            "Iteration 24, loss = 1.59977211\n",
            "Iteration 25, loss = 1.59808058\n",
            "Iteration 26, loss = 1.59656824\n",
            "Iteration 27, loss = 1.59478942\n",
            "Iteration 28, loss = 1.59340322\n",
            "Iteration 29, loss = 1.59178666\n",
            "Iteration 30, loss = 1.59047760\n",
            "Iteration 31, loss = 1.58907759\n",
            "Iteration 32, loss = 1.58780502\n",
            "Iteration 33, loss = 1.58662228\n",
            "Iteration 34, loss = 1.58521932\n",
            "Iteration 35, loss = 1.58403578\n",
            "Iteration 36, loss = 1.58283344\n",
            "Iteration 37, loss = 1.58170492\n",
            "Iteration 38, loss = 1.58072604\n",
            "Iteration 39, loss = 1.57952047\n",
            "Iteration 40, loss = 1.57835184\n",
            "Iteration 41, loss = 1.57731393\n",
            "Iteration 42, loss = 1.57625643\n",
            "Iteration 43, loss = 1.57522714\n",
            "Iteration 44, loss = 1.57428248\n",
            "Iteration 45, loss = 1.57315460\n",
            "Iteration 46, loss = 1.57224496\n",
            "Iteration 47, loss = 1.57125546\n",
            "Iteration 48, loss = 1.57024631\n",
            "Iteration 49, loss = 1.56926260\n",
            "Iteration 50, loss = 1.56823620\n",
            "Iteration 51, loss = 1.56733956\n",
            "Iteration 52, loss = 1.56639439\n",
            "Iteration 53, loss = 1.56538311\n",
            "Iteration 54, loss = 1.56449661\n",
            "Iteration 55, loss = 1.56341568\n",
            "Iteration 56, loss = 1.56257198\n",
            "Iteration 57, loss = 1.56153160\n",
            "Iteration 58, loss = 1.56065846\n",
            "Iteration 59, loss = 1.55962240\n",
            "Iteration 60, loss = 1.55870240\n",
            "Iteration 61, loss = 1.55771019\n",
            "Iteration 62, loss = 1.55675747\n",
            "Iteration 63, loss = 1.55580403\n",
            "Iteration 64, loss = 1.55477825\n",
            "Iteration 65, loss = 1.55380453\n",
            "Iteration 66, loss = 1.55280254\n",
            "Iteration 67, loss = 1.55185077\n",
            "Iteration 68, loss = 1.55082945\n",
            "Iteration 69, loss = 1.54985243\n",
            "Iteration 70, loss = 1.54883037\n",
            "Iteration 71, loss = 1.54780113\n",
            "Iteration 72, loss = 1.54671065\n",
            "Iteration 73, loss = 1.54568281\n",
            "Iteration 74, loss = 1.54462530\n",
            "Iteration 75, loss = 1.54357133\n",
            "Iteration 76, loss = 1.54250591\n",
            "Iteration 77, loss = 1.54151924\n",
            "Iteration 78, loss = 1.54036255\n",
            "Iteration 79, loss = 1.53928429\n",
            "Iteration 80, loss = 1.53812685\n",
            "Iteration 81, loss = 1.53703219\n",
            "Iteration 82, loss = 1.53593156\n",
            "Iteration 83, loss = 1.53480128\n",
            "Iteration 84, loss = 1.53361465\n",
            "Iteration 85, loss = 1.53250285\n",
            "Iteration 86, loss = 1.53132533\n",
            "Iteration 87, loss = 1.53017340\n",
            "Iteration 88, loss = 1.52894744\n",
            "Iteration 89, loss = 1.52772788\n",
            "Iteration 90, loss = 1.52656066\n",
            "Iteration 91, loss = 1.52532420\n",
            "Iteration 92, loss = 1.52410564\n",
            "Iteration 93, loss = 1.52284783\n",
            "Iteration 94, loss = 1.52156383\n",
            "Iteration 95, loss = 1.52034491\n",
            "Iteration 96, loss = 1.51903338\n",
            "Iteration 97, loss = 1.51774094\n",
            "Iteration 98, loss = 1.51644970\n",
            "Iteration 99, loss = 1.51513213\n",
            "Iteration 100, loss = 1.51382366\n",
            "Iteration 1, loss = 1.65188643\n",
            "Iteration 2, loss = 1.62245044\n",
            "Iteration 3, loss = 1.60182141\n",
            "Iteration 4, loss = 1.58494979\n",
            "Iteration 5, loss = 1.57532827\n",
            "Iteration 6, loss = 1.57048889\n",
            "Iteration 7, loss = 1.56435394\n",
            "Iteration 8, loss = 1.55888933\n",
            "Iteration 9, loss = 1.55150927\n",
            "Iteration 10, loss = 1.54276131\n",
            "Iteration 11, loss = 1.53396667\n",
            "Iteration 12, loss = 1.52359726\n",
            "Iteration 13, loss = 1.51358178\n",
            "Iteration 14, loss = 1.50216701\n",
            "Iteration 15, loss = 1.49081465"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 16, loss = 1.47860889\n",
            "Iteration 17, loss = 1.46592505\n",
            "Iteration 18, loss = 1.45264315\n",
            "Iteration 19, loss = 1.43812764\n",
            "Iteration 20, loss = 1.42399425\n",
            "Iteration 21, loss = 1.40853233\n",
            "Iteration 22, loss = 1.39373219\n",
            "Iteration 23, loss = 1.37811047\n",
            "Iteration 24, loss = 1.36344289\n",
            "Iteration 25, loss = 1.34823293\n",
            "Iteration 26, loss = 1.33385169\n",
            "Iteration 27, loss = 1.31953388\n",
            "Iteration 28, loss = 1.30629760\n",
            "Iteration 29, loss = 1.29355780\n",
            "Iteration 30, loss = 1.28106661\n",
            "Iteration 31, loss = 1.26995276\n",
            "Iteration 32, loss = 1.25961449\n",
            "Iteration 33, loss = 1.24860189\n",
            "Iteration 34, loss = 1.23911774\n",
            "Iteration 35, loss = 1.23047840\n",
            "Iteration 36, loss = 1.22266219\n",
            "Iteration 37, loss = 1.21498249\n",
            "Iteration 38, loss = 1.20901874\n",
            "Iteration 39, loss = 1.20189011\n",
            "Iteration 40, loss = 1.19604695\n",
            "Iteration 41, loss = 1.19127432\n",
            "Iteration 42, loss = 1.18590383\n",
            "Iteration 43, loss = 1.18108317\n",
            "Iteration 44, loss = 1.17790513\n",
            "Iteration 45, loss = 1.17295102\n",
            "Iteration 46, loss = 1.16975948\n",
            "Iteration 47, loss = 1.16676305\n",
            "Iteration 48, loss = 1.16319287\n",
            "Iteration 49, loss = 1.15935197\n",
            "Iteration 50, loss = 1.15766839\n",
            "Iteration 51, loss = 1.15502134\n",
            "Iteration 52, loss = 1.15222235\n",
            "Iteration 53, loss = 1.14990900\n",
            "Iteration 54, loss = 1.14765485\n",
            "Iteration 55, loss = 1.14639015\n",
            "Iteration 56, loss = 1.14381165\n",
            "Iteration 57, loss = 1.14157999\n",
            "Iteration 58, loss = 1.13954960\n",
            "Iteration 59, loss = 1.13846702\n",
            "Iteration 60, loss = 1.13730980\n",
            "Iteration 61, loss = 1.13548839\n",
            "Iteration 62, loss = 1.13346746\n",
            "Iteration 63, loss = 1.13231613\n",
            "Iteration 64, loss = 1.13054018\n",
            "Iteration 65, loss = 1.12995809\n",
            "Iteration 66, loss = 1.12880870\n",
            "Iteration 67, loss = 1.12744230\n",
            "Iteration 68, loss = 1.12572965\n",
            "Iteration 69, loss = 1.12575753\n",
            "Iteration 70, loss = 1.12619259\n",
            "Iteration 71, loss = 1.12324927\n",
            "Iteration 72, loss = 1.12283490\n",
            "Iteration 73, loss = 1.12111156\n",
            "Iteration 74, loss = 1.11991795\n",
            "Iteration 75, loss = 1.11953231\n",
            "Iteration 76, loss = 1.11784344\n",
            "Iteration 77, loss = 1.11764607\n",
            "Iteration 78, loss = 1.11713830\n",
            "Iteration 79, loss = 1.11644596\n",
            "Iteration 80, loss = 1.11487696\n",
            "Iteration 81, loss = 1.11418706\n",
            "Iteration 82, loss = 1.11429252\n",
            "Iteration 83, loss = 1.11303461\n",
            "Iteration 84, loss = 1.11239656\n",
            "Iteration 85, loss = 1.11158280\n",
            "Iteration 86, loss = 1.11077750\n",
            "Iteration 87, loss = 1.11079348\n",
            "Iteration 88, loss = 1.10996503\n",
            "Iteration 89, loss = 1.10847357\n",
            "Iteration 90, loss = 1.10865077\n",
            "Iteration 91, loss = 1.10775648\n",
            "Iteration 92, loss = 1.10714065\n",
            "Iteration 93, loss = 1.10722472\n",
            "Iteration 94, loss = 1.10619489\n",
            "Iteration 95, loss = 1.10604663\n",
            "Iteration 96, loss = 1.10597791\n",
            "Iteration 97, loss = 1.10417913\n",
            "Iteration 98, loss = 1.10615818\n",
            "Iteration 99, loss = 1.10426803\n",
            "Iteration 100, loss = 1.10427651\n",
            "Iteration 1, loss = 1.63033317\n",
            "Iteration 2, loss = 1.59467065\n",
            "Iteration 3, loss = 1.52594047\n",
            "Iteration 4, loss = 1.48122422"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 5, loss = 1.41799520\n",
            "Iteration 6, loss = 1.33926057\n",
            "Iteration 7, loss = 1.28478852\n",
            "Iteration 8, loss = 1.23845763\n",
            "Iteration 9, loss = 1.20000180\n",
            "Iteration 10, loss = 1.17376583\n",
            "Iteration 11, loss = 1.15930562\n",
            "Iteration 12, loss = 1.14554462\n",
            "Iteration 13, loss = 1.13570941\n",
            "Iteration 14, loss = 1.12683019\n",
            "Iteration 15, loss = 1.11680221\n",
            "Iteration 16, loss = 1.12382426\n",
            "Iteration 17, loss = 1.12438594\n",
            "Iteration 18, loss = 1.11020461\n",
            "Iteration 19, loss = 1.12003610\n",
            "Iteration 20, loss = 1.12615826\n",
            "Iteration 21, loss = 1.11286811\n",
            "Iteration 22, loss = 1.11560470\n",
            "Iteration 23, loss = 1.11918379\n",
            "Iteration 24, loss = 1.11014758\n",
            "Iteration 25, loss = 1.11279065\n",
            "Iteration 26, loss = 1.11396553\n",
            "Iteration 27, loss = 1.10506429\n",
            "Iteration 28, loss = 1.10554208\n",
            "Iteration 29, loss = 1.10623124\n",
            "Iteration 30, loss = 1.11160421\n",
            "Iteration 31, loss = 1.11570476\n",
            "Iteration 32, loss = 1.10604054\n",
            "Iteration 33, loss = 1.10097218\n",
            "Iteration 34, loss = 1.09664040\n",
            "Iteration 35, loss = 1.09640159\n",
            "Iteration 36, loss = 1.08932654\n",
            "Iteration 37, loss = 1.09771313\n",
            "Iteration 38, loss = 1.08695969\n",
            "Iteration 39, loss = 1.08577270\n",
            "Iteration 40, loss = 1.08555369\n",
            "Iteration 41, loss = 1.08327005\n",
            "Iteration 42, loss = 1.07615666\n",
            "Iteration 43, loss = 1.06794768\n",
            "Iteration 44, loss = 1.07112035\n",
            "Iteration 45, loss = 1.06339741\n",
            "Iteration 46, loss = 1.07487676\n",
            "Iteration 47, loss = 1.05162586\n",
            "Iteration 48, loss = 1.05839606\n",
            "Iteration 49, loss = 1.05485959\n",
            "Iteration 50, loss = 1.05789143\n",
            "Iteration 51, loss = 1.03844547\n",
            "Iteration 52, loss = 1.05035721\n",
            "Iteration 53, loss = 1.04855014\n",
            "Iteration 54, loss = 1.03215937\n",
            "Iteration 55, loss = 1.03264893\n",
            "Iteration 56, loss = 1.01960687\n",
            "Iteration 57, loss = 1.01278379\n",
            "Iteration 58, loss = 0.99994900\n",
            "Iteration 59, loss = 1.00997410\n",
            "Iteration 60, loss = 0.99725516\n",
            "Iteration 61, loss = 0.98339380\n",
            "Iteration 62, loss = 0.97699670\n",
            "Iteration 63, loss = 0.97108980\n",
            "Iteration 64, loss = 0.96516929\n",
            "Iteration 65, loss = 0.96662531\n",
            "Iteration 66, loss = 0.96028938\n",
            "Iteration 67, loss = 0.96476197\n",
            "Iteration 68, loss = 0.94419763\n",
            "Iteration 69, loss = 0.93139856\n",
            "Iteration 70, loss = 0.94421395\n",
            "Iteration 71, loss = 0.96419920\n",
            "Iteration 72, loss = 0.92688856\n",
            "Iteration 73, loss = 0.91919738\n",
            "Iteration 74, loss = 0.90540104\n",
            "Iteration 75, loss = 0.91869103\n",
            "Iteration 76, loss = 0.91032937\n",
            "Iteration 77, loss = 0.93145065\n",
            "Iteration 78, loss = 0.93637335\n",
            "Iteration 79, loss = 0.97104866\n",
            "Iteration 80, loss = 0.97387264\n",
            "Iteration 81, loss = 0.94287956\n",
            "Iteration 82, loss = 0.95688096\n",
            "Iteration 83, loss = 0.89706918\n",
            "Iteration 84, loss = 0.89449727\n",
            "Iteration 85, loss = 0.90228275\n",
            "Iteration 86, loss = 0.88154669\n",
            "Iteration 87, loss = 0.86280300\n",
            "Iteration 88, loss = 0.86707414\n",
            "Iteration 89, loss = 0.86005819\n",
            "Iteration 90, loss = 0.85480221\n",
            "Iteration 91, loss = 0.84543628\n",
            "Iteration 92, loss = 0.84639709\n",
            "Iteration 93, loss = 0.84754102\n",
            "Iteration 94, loss = 0.83216498\n",
            "Iteration 95, loss = 0.83133542\n",
            "Iteration 96, loss = 0.82524851\n",
            "Iteration 97, loss = 0.82701248\n",
            "Iteration 98, loss = 0.82197519\n",
            "Iteration 99, loss = 0.82812488\n",
            "Iteration 100, loss = 0.83450182\n",
            "Iteration 1, loss = 1.62068592\n",
            "Iteration 2, loss = 1.61568833\n",
            "Iteration 3, loss = 1.61219451\n",
            "Iteration 4, loss = 1.60874842\n",
            "Iteration 5, loss = 1.60566135\n",
            "Iteration 6, loss = 1.60272535\n",
            "Iteration 7, loss = 1.60061833\n",
            "Iteration 8, loss = 1.59870437\n",
            "Iteration 9, loss = 1.59672630\n",
            "Iteration 10, loss = 1.59525606\n",
            "Iteration 11, loss = 1.59432279\n",
            "Iteration 12, loss = 1.59326505\n",
            "Iteration 13, loss = 1.59200650\n",
            "Iteration 14, loss = 1.59103616\n",
            "Iteration 15, loss = 1.59016035\n",
            "Iteration 16, loss = 1.58948670\n",
            "Iteration 17, loss = 1.58880559\n",
            "Iteration 18, loss = 1.58803466\n",
            "Iteration 19, loss = 1.58733663\n",
            "Iteration 20, loss = 1.58666367\n",
            "Iteration 21, loss = 1.58599983\n",
            "Iteration 22, loss = 1.58535294\n",
            "Iteration 23, loss = 1.58460292\n",
            "Iteration 24, loss = 1.58391905\n",
            "Iteration 25, loss = 1.58321274\n",
            "Iteration 26, loss = 1.58260669\n",
            "Iteration 27, loss = 1.58180609\n",
            "Iteration 28, loss = 1.58108658\n",
            "Iteration 29, loss = 1.58038995\n",
            "Iteration 30, loss = 1.57971042\n",
            "Iteration 31, loss = 1.57891913\n",
            "Iteration 32, loss = 1.57814915\n",
            "Iteration 33, loss = 1.57736681\n",
            "Iteration 34, loss = 1.57667483\n",
            "Iteration 35, loss = 1.57598375\n",
            "Iteration 36, loss = 1.57518869\n",
            "Iteration 37, loss = 1.57435360\n",
            "Iteration 38, loss = 1.57361365\n",
            "Iteration 39, loss = 1.57278195\n",
            "Iteration 40, loss = 1.57202879\n",
            "Iteration 41, loss = 1.57125397\n",
            "Iteration 42, loss = 1.57045541\n",
            "Iteration 43, loss = 1.56969981\n",
            "Iteration 44, loss = 1.56875121\n",
            "Iteration 45, loss = 1.56795706\n",
            "Iteration 46, loss = 1.56708486\n",
            "Iteration 47, loss = 1.56625316\n",
            "Iteration 48, loss = 1.56537621\n",
            "Iteration 49, loss = 1.56450010\n",
            "Iteration 50, loss = 1.56362421\n",
            "Iteration 51, loss = 1.56277147\n",
            "Iteration 52, loss = 1.56188998"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 53, loss = 1.56099520\n",
            "Iteration 54, loss = 1.56002599\n",
            "Iteration 55, loss = 1.55908398\n",
            "Iteration 56, loss = 1.55813774\n",
            "Iteration 57, loss = 1.55719965\n",
            "Iteration 58, loss = 1.55620340\n",
            "Iteration 59, loss = 1.55521720\n",
            "Iteration 60, loss = 1.55426687\n",
            "Iteration 61, loss = 1.55327478\n",
            "Iteration 62, loss = 1.55226814\n",
            "Iteration 63, loss = 1.55119494\n",
            "Iteration 64, loss = 1.55021371\n",
            "Iteration 65, loss = 1.54917082\n",
            "Iteration 66, loss = 1.54807771\n",
            "Iteration 67, loss = 1.54700099\n",
            "Iteration 68, loss = 1.54589698\n",
            "Iteration 69, loss = 1.54489832\n",
            "Iteration 70, loss = 1.54375455\n",
            "Iteration 71, loss = 1.54260878\n",
            "Iteration 72, loss = 1.54154424\n",
            "Iteration 73, loss = 1.54035005\n",
            "Iteration 74, loss = 1.53915450\n",
            "Iteration 75, loss = 1.53801072\n",
            "Iteration 76, loss = 1.53681011\n",
            "Iteration 77, loss = 1.53563620\n",
            "Iteration 78, loss = 1.53442649\n",
            "Iteration 79, loss = 1.53329560\n",
            "Iteration 80, loss = 1.53199764\n",
            "Iteration 81, loss = 1.53074754\n",
            "Iteration 82, loss = 1.52947764\n",
            "Iteration 83, loss = 1.52817195\n",
            "Iteration 84, loss = 1.52698739\n",
            "Iteration 85, loss = 1.52562778\n",
            "Iteration 86, loss = 1.52428787\n",
            "Iteration 87, loss = 1.52295750\n",
            "Iteration 88, loss = 1.52163515\n",
            "Iteration 89, loss = 1.52029983\n",
            "Iteration 90, loss = 1.51890907\n",
            "Iteration 91, loss = 1.51752985\n",
            "Iteration 92, loss = 1.51614367\n",
            "Iteration 93, loss = 1.51477849\n",
            "Iteration 94, loss = 1.51336590\n",
            "Iteration 95, loss = 1.51189039\n",
            "Iteration 96, loss = 1.51048007\n",
            "Iteration 97, loss = 1.50896218\n",
            "Iteration 98, loss = 1.50754233\n",
            "Iteration 99, loss = 1.50605293\n",
            "Iteration 100, loss = 1.50452747\n",
            "Iteration 1, loss = 1.61538146\n",
            "Iteration 2, loss = 1.59093315\n",
            "Iteration 3, loss = 1.58894157\n",
            "Iteration 4, loss = 1.58433279\n",
            "Iteration 5, loss = 1.57744998\n",
            "Iteration 6, loss = 1.56908086\n",
            "Iteration 7, loss = 1.56294105\n",
            "Iteration 8, loss = 1.55233423\n",
            "Iteration 9, loss = 1.54586499\n",
            "Iteration 10, loss = 1.53675291\n",
            "Iteration 11, loss = 1.52779204\n",
            "Iteration 12, loss = 1.51733131\n",
            "Iteration 13, loss = 1.50520671\n",
            "Iteration 14, loss = 1.49405624\n",
            "Iteration 15, loss = 1.48122352\n",
            "Iteration 16, loss = 1.46865952\n",
            "Iteration 17, loss = 1.45294408\n",
            "Iteration 18, loss = 1.43758565\n",
            "Iteration 19, loss = 1.42327227\n",
            "Iteration 20, loss = 1.40634341\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 1.38995228\n",
            "Iteration 22, loss = 1.37467580\n",
            "Iteration 23, loss = 1.35780733\n",
            "Iteration 24, loss = 1.34142006\n",
            "Iteration 25, loss = 1.32566097\n",
            "Iteration 26, loss = 1.31181080\n",
            "Iteration 27, loss = 1.29616298\n",
            "Iteration 28, loss = 1.28268473\n",
            "Iteration 29, loss = 1.26942291\n",
            "Iteration 30, loss = 1.25827258\n",
            "Iteration 31, loss = 1.24575642\n",
            "Iteration 32, loss = 1.23471389\n",
            "Iteration 33, loss = 1.22465871\n",
            "Iteration 34, loss = 1.21615318\n",
            "Iteration 35, loss = 1.20857039\n",
            "Iteration 36, loss = 1.20006776\n",
            "Iteration 37, loss = 1.19298143\n",
            "Iteration 38, loss = 1.18637292\n",
            "Iteration 39, loss = 1.18023148\n",
            "Iteration 40, loss = 1.17498757\n",
            "Iteration 41, loss = 1.16885583\n",
            "Iteration 42, loss = 1.16458200\n",
            "Iteration 43, loss = 1.16026195\n",
            "Iteration 44, loss = 1.15488953\n",
            "Iteration 45, loss = 1.15109035\n",
            "Iteration 46, loss = 1.14733129\n",
            "Iteration 47, loss = 1.14382260\n",
            "Iteration 48, loss = 1.14024768\n",
            "Iteration 49, loss = 1.13722402\n",
            "Iteration 50, loss = 1.13420726\n",
            "Iteration 51, loss = 1.13184383\n",
            "Iteration 52, loss = 1.12883556\n",
            "Iteration 53, loss = 1.12652647\n",
            "Iteration 54, loss = 1.12387106\n",
            "Iteration 55, loss = 1.12080913\n",
            "Iteration 56, loss = 1.11877863\n",
            "Iteration 57, loss = 1.11783405\n",
            "Iteration 58, loss = 1.11414939\n",
            "Iteration 59, loss = 1.11195355\n",
            "Iteration 60, loss = 1.11080400\n",
            "Iteration 61, loss = 1.10837572\n",
            "Iteration 62, loss = 1.10701560\n",
            "Iteration 63, loss = 1.10419885\n",
            "Iteration 64, loss = 1.10271129\n",
            "Iteration 65, loss = 1.10101965\n",
            "Iteration 66, loss = 1.09929287\n",
            "Iteration 67, loss = 1.09681121\n",
            "Iteration 68, loss = 1.09508239\n",
            "Iteration 69, loss = 1.09437782\n",
            "Iteration 70, loss = 1.09192271\n",
            "Iteration 71, loss = 1.09048635\n",
            "Iteration 72, loss = 1.09071721\n",
            "Iteration 73, loss = 1.08693759\n",
            "Iteration 74, loss = 1.08510863\n",
            "Iteration 75, loss = 1.08430048\n",
            "Iteration 76, loss = 1.08198390\n",
            "Iteration 77, loss = 1.08054095\n",
            "Iteration 78, loss = 1.07851794\n",
            "Iteration 79, loss = 1.07791960\n",
            "Iteration 80, loss = 1.07556045\n",
            "Iteration 81, loss = 1.07437160\n",
            "Iteration 82, loss = 1.07249419\n",
            "Iteration 83, loss = 1.07100279\n",
            "Iteration 84, loss = 1.06903481\n",
            "Iteration 85, loss = 1.06868379\n",
            "Iteration 86, loss = 1.06525463\n",
            "Iteration 87, loss = 1.06378660\n",
            "Iteration 88, loss = 1.06213197\n",
            "Iteration 89, loss = 1.05988634\n",
            "Iteration 90, loss = 1.05794958\n",
            "Iteration 91, loss = 1.05748622\n",
            "Iteration 92, loss = 1.05449607\n",
            "Iteration 93, loss = 1.05521553\n",
            "Iteration 94, loss = 1.05157653\n",
            "Iteration 95, loss = 1.04890500\n",
            "Iteration 96, loss = 1.04848755\n",
            "Iteration 97, loss = 1.04571882\n",
            "Iteration 98, loss = 1.04518872\n",
            "Iteration 99, loss = 1.04170750\n",
            "Iteration 100, loss = 1.03979859\n",
            "Iteration 1, loss = 1.73905193\n",
            "Iteration 2, loss = 1.67730927\n",
            "Iteration 3, loss = 1.61374920\n",
            "Iteration 4, loss = 1.55588297\n",
            "Iteration 5, loss = 1.53894976\n",
            "Iteration 6, loss = 1.49180246\n",
            "Iteration 7, loss = 1.40682251\n",
            "Iteration 8, loss = 1.34885197\n",
            "Iteration 9, loss = 1.26544542\n",
            "Iteration 10, loss = 1.21292182\n",
            "Iteration 11, loss = 1.17799687\n",
            "Iteration 12, loss = 1.13980138\n",
            "Iteration 13, loss = 1.13378487\n",
            "Iteration 14, loss = 1.11483221\n",
            "Iteration 15, loss = 1.10445952\n",
            "Iteration 16, loss = 1.11288021\n",
            "Iteration 17, loss = 1.09755868\n",
            "Iteration 18, loss = 1.10095114\n",
            "Iteration 19, loss = 1.11041641\n",
            "Iteration 20, loss = 1.08883556\n",
            "Iteration 21, loss = 1.09633885\n",
            "Iteration 22, loss = 1.10012065\n",
            "Iteration 23, loss = 1.08943481\n",
            "Iteration 24, loss = 1.07002472\n",
            "Iteration 25, loss = 1.08032257\n",
            "Iteration 26, loss = 1.08354091\n",
            "Iteration 27, loss = 1.05854928\n",
            "Iteration 28, loss = 1.06029610\n",
            "Iteration 29, loss = 1.04719505\n",
            "Iteration 30, loss = 1.06048628\n",
            "Iteration 31, loss = 1.06044166\n",
            "Iteration 32, loss = 1.02978977\n",
            "Iteration 33, loss = 1.03336393\n",
            "Iteration 34, loss = 1.01853168\n",
            "Iteration 35, loss = 1.00332699\n",
            "Iteration 36, loss = 1.02535190\n",
            "Iteration 37, loss = 0.99888423\n",
            "Iteration 38, loss = 0.99145045\n",
            "Iteration 39, loss = 0.99801914\n",
            "Iteration 40, loss = 0.99229635\n",
            "Iteration 41, loss = 1.04400205\n",
            "Iteration 42, loss = 1.01138108\n",
            "Iteration 43, loss = 0.99277649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 44, loss = 0.96201153\n",
            "Iteration 45, loss = 0.95440947\n",
            "Iteration 46, loss = 0.92811892\n",
            "Iteration 47, loss = 0.92400051\n",
            "Iteration 48, loss = 0.91640279\n",
            "Iteration 49, loss = 0.91949359\n",
            "Iteration 50, loss = 0.91340487\n",
            "Iteration 51, loss = 0.93393645\n",
            "Iteration 52, loss = 0.90667886\n",
            "Iteration 53, loss = 0.94123661\n",
            "Iteration 54, loss = 0.89545174\n",
            "Iteration 55, loss = 0.87991164\n",
            "Iteration 56, loss = 0.90777701\n",
            "Iteration 57, loss = 0.87403089\n",
            "Iteration 58, loss = 0.86332297\n",
            "Iteration 59, loss = 0.85730417\n",
            "Iteration 60, loss = 0.94651884\n",
            "Iteration 61, loss = 0.88092160\n",
            "Iteration 62, loss = 0.94006904\n",
            "Iteration 63, loss = 0.88865357\n",
            "Iteration 64, loss = 0.93878591\n",
            "Iteration 65, loss = 0.91657754\n",
            "Iteration 66, loss = 0.91887880\n",
            "Iteration 67, loss = 0.88348469\n",
            "Iteration 68, loss = 0.89359715\n",
            "Iteration 69, loss = 0.87640641\n",
            "Iteration 70, loss = 0.90439744\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.67228993\n",
            "Iteration 2, loss = 1.66304169\n",
            "Iteration 3, loss = 1.65328653\n",
            "Iteration 4, loss = 1.64528049\n",
            "Iteration 5, loss = 1.63838408\n",
            "Iteration 6, loss = 1.63154324\n",
            "Iteration 7, loss = 1.62648235\n",
            "Iteration 8, loss = 1.62039144\n",
            "Iteration 9, loss = 1.61624529\n",
            "Iteration 10, loss = 1.61250350\n",
            "Iteration 11, loss = 1.60887575\n",
            "Iteration 12, loss = 1.60678899\n",
            "Iteration 13, loss = 1.60474133\n",
            "Iteration 14, loss = 1.60319317\n",
            "Iteration 15, loss = 1.60196263\n",
            "Iteration 16, loss = 1.60070714\n",
            "Iteration 17, loss = 1.60010573\n",
            "Iteration 18, loss = 1.59942819\n",
            "Iteration 19, loss = 1.59920604\n",
            "Iteration 20, loss = 1.59885959\n",
            "Iteration 21, loss = 1.59858687\n",
            "Iteration 22, loss = 1.59852996\n",
            "Iteration 23, loss = 1.59843854\n",
            "Iteration 24, loss = 1.59832903\n",
            "Iteration 25, loss = 1.59828526\n",
            "Iteration 26, loss = 1.59823060\n",
            "Iteration 27, loss = 1.59805509\n",
            "Iteration 28, loss = 1.59803548\n",
            "Iteration 29, loss = 1.59800866\n",
            "Iteration 30, loss = 1.59800043\n",
            "Iteration 31, loss = 1.59782322\n",
            "Iteration 32, loss = 1.59772383\n",
            "Iteration 33, loss = 1.59768209\n",
            "Iteration 34, loss = 1.59763467\n",
            "Iteration 35, loss = 1.59768121\n",
            "Iteration 36, loss = 1.59757107\n",
            "Iteration 37, loss = 1.59748956\n",
            "Iteration 38, loss = 1.59738671\n",
            "Iteration 39, loss = 1.59728787\n",
            "Iteration 40, loss = 1.59726984\n",
            "Iteration 41, loss = 1.59719684\n",
            "Iteration 42, loss = 1.59712870\n",
            "Iteration 43, loss = 1.59716709\n",
            "Iteration 44, loss = 1.59697415\n",
            "Iteration 45, loss = 1.59693823\n",
            "Iteration 46, loss = 1.59685789\n",
            "Iteration 47, loss = 1.59679877\n",
            "Iteration 48, loss = 1.59672008\n",
            "Iteration 49, loss = 1.59665750\n",
            "Iteration 50, loss = 1.59660324\n",
            "Iteration 51, loss = 1.59658039\n",
            "Iteration 52, loss = 1.59656314\n",
            "Iteration 53, loss = 1.59649831\n",
            "Iteration 54, loss = 1.59637585\n",
            "Iteration 55, loss = 1.59631577\n",
            "Iteration 56, loss = 1.59623574\n",
            "Iteration 57, loss = 1.59622114\n",
            "Iteration 58, loss = 1.59615170\n",
            "Iteration 59, loss = 1.59606402\n",
            "Iteration 60, loss = 1.59600941\n",
            "Iteration 61, loss = 1.59593141\n",
            "Iteration 62, loss = 1.59589917\n",
            "Iteration 63, loss = 1.59579625\n",
            "Iteration 64, loss = 1.59576823\n",
            "Iteration 65, loss = 1.59567770\n",
            "Iteration 66, loss = 1.59561814\n",
            "Iteration 67, loss = 1.59550037\n",
            "Iteration 68, loss = 1.59543615\n",
            "Iteration 69, loss = 1.59544051\n",
            "Iteration 70, loss = 1.59532100\n",
            "Iteration 71, loss = 1.59525687\n",
            "Iteration 72, loss = 1.59522762\n",
            "Iteration 73, loss = 1.59512400\n",
            "Iteration 74, loss = 1.59505807\n",
            "Iteration 75, loss = 1.59496190\n",
            "Iteration 76, loss = 1.59491155\n",
            "Iteration 77, loss = 1.59488123\n",
            "Iteration 78, loss = 1.59476725\n",
            "Iteration 79, loss = 1.59480278\n",
            "Iteration 80, loss = 1.59462775\n",
            "Iteration 81, loss = 1.59464376\n",
            "Iteration 82, loss = 1.59449541\n",
            "Iteration 83, loss = 1.59438188\n",
            "Iteration 84, loss = 1.59438307\n",
            "Iteration 85, loss = 1.59427218\n",
            "Iteration 86, loss = 1.59416113\n",
            "Iteration 87, loss = 1.59407249\n",
            "Iteration 88, loss = 1.59404336\n",
            "Iteration 89, loss = 1.59394115\n",
            "Iteration 90, loss = 1.59384516\n",
            "Iteration 91, loss = 1.59377210\n",
            "Iteration 92, loss = 1.59370352\n",
            "Iteration 93, loss = 1.59364738\n",
            "Iteration 94, loss = 1.59355715\n",
            "Iteration 95, loss = 1.59345221\n",
            "Iteration 96, loss = 1.59339933\n",
            "Iteration 97, loss = 1.59328539\n",
            "Iteration 98, loss = 1.59329218\n",
            "Iteration 99, loss = 1.59314160\n",
            "Iteration 100, loss = 1.59307169\n",
            "Iteration 1, loss = 1.65443250\n",
            "Iteration 2, loss = 1.61239399\n",
            "Iteration 3, loss = 1.60468066\n",
            "Iteration 4, loss = 1.61104358\n",
            "Iteration 5, loss = 1.60948336\n",
            "Iteration 6, loss = 1.60458461\n",
            "Iteration 7, loss = 1.60059944\n",
            "Iteration 8, loss = 1.59827919\n",
            "Iteration 9, loss = 1.59772692\n",
            "Iteration 10, loss = 1.59833159\n",
            "Iteration 11, loss = 1.59751194\n",
            "Iteration 12, loss = 1.59625313\n",
            "Iteration 13, loss = 1.59350185\n",
            "Iteration 14, loss = 1.59356359\n",
            "Iteration 15, loss = 1.59324780\n",
            "Iteration 16, loss = 1.59351559\n",
            "Iteration 17, loss = 1.59154167\n",
            "Iteration 18, loss = 1.58947189\n",
            "Iteration 19, loss = 1.58866958\n",
            "Iteration 20, loss = 1.58702832\n",
            "Iteration 21, loss = 1.58619021\n",
            "Iteration 22, loss = 1.58632618\n",
            "Iteration 23, loss = 1.58463558\n",
            "Iteration 24, loss = 1.58275460\n",
            "Iteration 25, loss = 1.58150040\n",
            "Iteration 26, loss = 1.58073769\n",
            "Iteration 27, loss = 1.57828980\n",
            "Iteration 28, loss = 1.57636853\n",
            "Iteration 29, loss = 1.57463223\n",
            "Iteration 30, loss = 1.57305360\n",
            "Iteration 31, loss = 1.57028612\n",
            "Iteration 32, loss = 1.56763472\n",
            "Iteration 33, loss = 1.56477615\n",
            "Iteration 34, loss = 1.56256951\n",
            "Iteration 35, loss = 1.56024012\n",
            "Iteration 36, loss = 1.55687069\n",
            "Iteration 37, loss = 1.55306651\n",
            "Iteration 38, loss = 1.54929152\n",
            "Iteration 39, loss = 1.54497087\n",
            "Iteration 40, loss = 1.54131146\n",
            "Iteration 41, loss = 1.53707605\n",
            "Iteration 42, loss = 1.53236613\n",
            "Iteration 43, loss = 1.52824548\n",
            "Iteration 44, loss = 1.52219708"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 45, loss = 1.51675032\n",
            "Iteration 46, loss = 1.51130355\n",
            "Iteration 47, loss = 1.50553293\n",
            "Iteration 48, loss = 1.49892235\n",
            "Iteration 49, loss = 1.49268153\n",
            "Iteration 50, loss = 1.48594954\n",
            "Iteration 51, loss = 1.47942268\n",
            "Iteration 52, loss = 1.47280282\n",
            "Iteration 53, loss = 1.46570909\n",
            "Iteration 54, loss = 1.45720555\n",
            "Iteration 55, loss = 1.44936206\n",
            "Iteration 56, loss = 1.44130566\n",
            "Iteration 57, loss = 1.43345430\n",
            "Iteration 58, loss = 1.42507175\n",
            "Iteration 59, loss = 1.41694866\n",
            "Iteration 60, loss = 1.40896264\n",
            "Iteration 61, loss = 1.40102790\n",
            "Iteration 62, loss = 1.39291524\n",
            "Iteration 63, loss = 1.38420892\n",
            "Iteration 64, loss = 1.37615852\n",
            "Iteration 65, loss = 1.36809165\n",
            "Iteration 66, loss = 1.35970259\n",
            "Iteration 67, loss = 1.35147753\n",
            "Iteration 68, loss = 1.34345116\n",
            "Iteration 69, loss = 1.33594375\n",
            "Iteration 70, loss = 1.32826353\n",
            "Iteration 71, loss = 1.32076168\n",
            "Iteration 72, loss = 1.31443577\n",
            "Iteration 73, loss = 1.30690820\n",
            "Iteration 74, loss = 1.29979013\n",
            "Iteration 75, loss = 1.29364417\n",
            "Iteration 76, loss = 1.28690283\n",
            "Iteration 77, loss = 1.28115644\n",
            "Iteration 78, loss = 1.27499079\n",
            "Iteration 79, loss = 1.27048329\n",
            "Iteration 80, loss = 1.26388417\n",
            "Iteration 81, loss = 1.25900826\n",
            "Iteration 82, loss = 1.25387575\n",
            "Iteration 83, loss = 1.24834970\n",
            "Iteration 84, loss = 1.24439305\n",
            "Iteration 85, loss = 1.24022554\n",
            "Iteration 86, loss = 1.23524008\n",
            "Iteration 87, loss = 1.23150864\n",
            "Iteration 88, loss = 1.22782089\n",
            "Iteration 89, loss = 1.22378826\n",
            "Iteration 90, loss = 1.21972054\n",
            "Iteration 91, loss = 1.21673100\n",
            "Iteration 92, loss = 1.21329983\n",
            "Iteration 93, loss = 1.21067006\n",
            "Iteration 94, loss = 1.20748953\n",
            "Iteration 95, loss = 1.20403769\n",
            "Iteration 96, loss = 1.20121875\n",
            "Iteration 97, loss = 1.19872350\n",
            "Iteration 98, loss = 1.19650731\n",
            "Iteration 99, loss = 1.19334292\n",
            "Iteration 100, loss = 1.19144026\n",
            "Iteration 1, loss = 1.92357234\n",
            "Iteration 2, loss = 1.65453024\n",
            "Iteration 3, loss = 1.78944025\n",
            "Iteration 4, loss = 1.68272092\n",
            "Iteration 5, loss = 1.61995124\n",
            "Iteration 6, loss = 1.65483225\n",
            "Iteration 7, loss = 1.63623608\n",
            "Iteration 8, loss = 1.60763131\n",
            "Iteration 9, loss = 1.59902538\n",
            "Iteration 10, loss = 1.60338593\n",
            "Iteration 11, loss = 1.60593113\n",
            "Iteration 12, loss = 1.59706772\n",
            "Iteration 13, loss = 1.58693513\n",
            "Iteration 14, loss = 1.57986752\n",
            "Iteration 15, loss = 1.57554343\n",
            "Iteration 16, loss = 1.57041337\n",
            "Iteration 17, loss = 1.56146749\n",
            "Iteration 18, loss = 1.54820147\n",
            "Iteration 19, loss = 1.53306717\n",
            "Iteration 20, loss = 1.51278893\n",
            "Iteration 21, loss = 1.48892723\n",
            "Iteration 22, loss = 1.46388427\n",
            "Iteration 23, loss = 1.43206700\n",
            "Iteration 24, loss = 1.39466860\n",
            "Iteration 25, loss = 1.35675630\n",
            "Iteration 26, loss = 1.32107899\n",
            "Iteration 27, loss = 1.28395694\n",
            "Iteration 28, loss = 1.25362233\n",
            "Iteration 29, loss = 1.22398463\n",
            "Iteration 30, loss = 1.21344168\n",
            "Iteration 31, loss = 1.18914758\n",
            "Iteration 32, loss = 1.17897059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 1.17071362\n",
            "Iteration 34, loss = 1.15743130\n",
            "Iteration 35, loss = 1.16172975\n",
            "Iteration 36, loss = 1.15492713\n",
            "Iteration 37, loss = 1.13972304\n",
            "Iteration 38, loss = 1.14032332\n",
            "Iteration 39, loss = 1.13484072\n",
            "Iteration 40, loss = 1.13279384\n",
            "Iteration 41, loss = 1.12425036\n",
            "Iteration 42, loss = 1.12645207\n",
            "Iteration 43, loss = 1.12534914\n",
            "Iteration 44, loss = 1.11989141\n",
            "Iteration 45, loss = 1.11961314\n",
            "Iteration 46, loss = 1.11359391\n",
            "Iteration 47, loss = 1.11366937\n",
            "Iteration 48, loss = 1.11075298\n",
            "Iteration 49, loss = 1.11112050\n",
            "Iteration 50, loss = 1.10915168\n",
            "Iteration 51, loss = 1.11122518\n",
            "Iteration 52, loss = 1.10926984\n",
            "Iteration 53, loss = 1.11067396\n",
            "Iteration 54, loss = 1.10544973\n",
            "Iteration 55, loss = 1.10739710\n",
            "Iteration 56, loss = 1.10437698\n",
            "Iteration 57, loss = 1.10847731\n",
            "Iteration 58, loss = 1.10370213\n",
            "Iteration 59, loss = 1.10495228\n",
            "Iteration 60, loss = 1.10019709\n",
            "Iteration 61, loss = 1.10402712\n",
            "Iteration 62, loss = 1.10931035\n",
            "Iteration 63, loss = 1.09890255\n",
            "Iteration 64, loss = 1.10902351\n",
            "Iteration 65, loss = 1.09643244\n",
            "Iteration 66, loss = 1.09981475\n",
            "Iteration 67, loss = 1.09479150\n",
            "Iteration 68, loss = 1.09491694\n",
            "Iteration 69, loss = 1.09598173\n",
            "Iteration 70, loss = 1.09210306\n",
            "Iteration 71, loss = 1.09417208\n",
            "Iteration 72, loss = 1.09899693\n",
            "Iteration 73, loss = 1.08959893\n",
            "Iteration 74, loss = 1.08868684\n",
            "Iteration 75, loss = 1.09058587\n",
            "Iteration 76, loss = 1.08373582\n",
            "Iteration 77, loss = 1.08966248\n",
            "Iteration 78, loss = 1.07739865\n",
            "Iteration 79, loss = 1.09059963\n",
            "Iteration 80, loss = 1.07422972\n",
            "Iteration 81, loss = 1.08606507\n",
            "Iteration 82, loss = 1.07373993\n",
            "Iteration 83, loss = 1.07675080\n",
            "Iteration 84, loss = 1.06907992\n",
            "Iteration 85, loss = 1.07228730\n",
            "Iteration 86, loss = 1.07423499\n",
            "Iteration 87, loss = 1.06940495\n",
            "Iteration 88, loss = 1.06899622\n",
            "Iteration 89, loss = 1.05758927\n",
            "Iteration 90, loss = 1.05840443\n",
            "Iteration 91, loss = 1.04387024\n",
            "Iteration 92, loss = 1.05064037\n",
            "Iteration 93, loss = 1.05344796\n",
            "Iteration 94, loss = 1.05152256\n",
            "Iteration 95, loss = 1.03621155\n",
            "Iteration 96, loss = 1.03818861\n",
            "Iteration 97, loss = 1.04856268\n",
            "Iteration 98, loss = 1.05946081\n",
            "Iteration 99, loss = 1.02181128\n",
            "Iteration 100, loss = 1.04380053\n",
            "Iteration 1, loss = 1.61955179\n",
            "Iteration 2, loss = 1.61490879\n",
            "Iteration 3, loss = 1.61168403\n",
            "Iteration 4, loss = 1.60847435\n",
            "Iteration 5, loss = 1.60558737\n",
            "Iteration 6, loss = 1.60283299\n",
            "Iteration 7, loss = 1.60084123\n",
            "Iteration 8, loss = 1.59903392\n",
            "Iteration 9, loss = 1.59715274\n",
            "Iteration 10, loss = 1.59574659\n",
            "Iteration 11, loss = 1.59485440\n",
            "Iteration 12, loss = 1.59383539\n",
            "Iteration 13, loss = 1.59262372\n",
            "Iteration 14, loss = 1.59168953\n",
            "Iteration 15, loss = 1.59084885\n",
            "Iteration 16, loss = 1.59020729\n",
            "Iteration 17, loss = 1.58956345\n",
            "Iteration 18, loss = 1.58883354\n",
            "Iteration 19, loss = 1.58816727\n",
            "Iteration 20, loss = 1.58754248\n",
            "Iteration 21, loss = 1.58692432\n",
            "Iteration 22, loss = 1.58631160\n",
            "Iteration 23, loss = 1.58561269\n",
            "Iteration 24, loss = 1.58497676\n",
            "Iteration 25, loss = 1.58432117\n",
            "Iteration 26, loss = 1.58375437\n",
            "Iteration 27, loss = 1.58300997\n",
            "Iteration 28, loss = 1.58233826\n",
            "Iteration 29, loss = 1.58168866\n",
            "Iteration 30, loss = 1.58105503\n",
            "Iteration 31, loss = 1.58031311\n",
            "Iteration 32, loss = 1.57959139\n",
            "Iteration 33, loss = 1.57885866\n",
            "Iteration 34, loss = 1.57821101\n",
            "Iteration 35, loss = 1.57756473\n",
            "Iteration 36, loss = 1.57681924\n",
            "Iteration 37, loss = 1.57603305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 1.57534036\n",
            "Iteration 39, loss = 1.57455721\n",
            "Iteration 40, loss = 1.57385121\n",
            "Iteration 41, loss = 1.57312394\n",
            "Iteration 42, loss = 1.57237198\n",
            "Iteration 43, loss = 1.57166456\n",
            "Iteration 44, loss = 1.57076818\n",
            "Iteration 45, loss = 1.57002119\n",
            "Iteration 46, loss = 1.56919959\n",
            "Iteration 47, loss = 1.56841588\n",
            "Iteration 48, loss = 1.56758912\n",
            "Iteration 49, loss = 1.56676321\n",
            "Iteration 50, loss = 1.56593637\n",
            "Iteration 51, loss = 1.56513197\n",
            "Iteration 52, loss = 1.56429958\n",
            "Iteration 53, loss = 1.56345379\n",
            "Iteration 54, loss = 1.56253717\n",
            "Iteration 55, loss = 1.56164594\n",
            "Iteration 56, loss = 1.56074978\n",
            "Iteration 57, loss = 1.55986236\n",
            "Iteration 58, loss = 1.55891807\n",
            "Iteration 59, loss = 1.55798296\n",
            "Iteration 60, loss = 1.55708056\n",
            "Iteration 61, loss = 1.55613929\n",
            "Iteration 62, loss = 1.55518318\n",
            "Iteration 63, loss = 1.55416370\n",
            "Iteration 64, loss = 1.55322976\n",
            "Iteration 65, loss = 1.55223636\n",
            "Iteration 66, loss = 1.55119682\n",
            "Iteration 67, loss = 1.55017003\n",
            "Iteration 68, loss = 1.54911768\n",
            "Iteration 69, loss = 1.54816527\n",
            "Iteration 70, loss = 1.54707287\n",
            "Iteration 71, loss = 1.54597912\n",
            "Iteration 72, loss = 1.54496083\n",
            "Iteration 73, loss = 1.54381957\n",
            "Iteration 74, loss = 1.54267651\n",
            "Iteration 75, loss = 1.54158086\n",
            "Iteration 76, loss = 1.54043045\n",
            "Iteration 77, loss = 1.53930429\n",
            "Iteration 78, loss = 1.53814418\n",
            "Iteration 79, loss = 1.53705645\n",
            "Iteration 80, loss = 1.53581269\n",
            "Iteration 81, loss = 1.53461171\n",
            "Iteration 82, loss = 1.53338877\n",
            "Iteration 83, loss = 1.53213299\n",
            "Iteration 84, loss = 1.53099073\n",
            "Iteration 85, loss = 1.52967926\n",
            "Iteration 86, loss = 1.52838879\n",
            "Iteration 87, loss = 1.52710483\n",
            "Iteration 88, loss = 1.52582741\n",
            "Iteration 89, loss = 1.52453688\n",
            "Iteration 90, loss = 1.52319266\n",
            "Iteration 91, loss = 1.52185787\n",
            "Iteration 92, loss = 1.52051619\n",
            "Iteration 93, loss = 1.51919150\n",
            "Iteration 94, loss = 1.51782254\n",
            "Iteration 95, loss = 1.51639197\n",
            "Iteration 96, loss = 1.51502288\n",
            "Iteration 97, loss = 1.51354842\n",
            "Iteration 98, loss = 1.51216705\n",
            "Iteration 99, loss = 1.51071947\n",
            "Iteration 100, loss = 1.50923324\n",
            "Iteration 1, loss = 1.61467672\n",
            "Iteration 2, loss = 1.59146597\n",
            "Iteration 3, loss = 1.58946901\n",
            "Iteration 4, loss = 1.58523424\n",
            "Iteration 5, loss = 1.57883951\n",
            "Iteration 6, loss = 1.57086971\n",
            "Iteration 7, loss = 1.56500987\n",
            "Iteration 8, loss = 1.55477671\n",
            "Iteration 9, loss = 1.54862558\n",
            "Iteration 10, loss = 1.53987250\n",
            "Iteration 11, loss = 1.53124925\n",
            "Iteration 12, loss = 1.52109164\n",
            "Iteration 13, loss = 1.50925574\n",
            "Iteration 14, loss = 1.49830369\n",
            "Iteration 15, loss = 1.48576299\n",
            "Iteration 16, loss = 1.47329398\n",
            "Iteration 17, loss = 1.45783280\n",
            "Iteration 18, loss = 1.44256695\n",
            "Iteration 19, loss = 1.42829944\n",
            "Iteration 20, loss = 1.41130398\n",
            "Iteration 21, loss = 1.39479327\n",
            "Iteration 22, loss = 1.37919100\n",
            "Iteration 23, loss = 1.36214919\n",
            "Iteration 24, loss = 1.34548151\n",
            "Iteration 25, loss = 1.32932939\n",
            "Iteration 26, loss = 1.31492467\n",
            "Iteration 27, loss = 1.29886584\n",
            "Iteration 28, loss = 1.28497877\n",
            "Iteration 29, loss = 1.27130741\n",
            "Iteration 30, loss = 1.25966652\n",
            "Iteration 31, loss = 1.24690475\n",
            "Iteration 32, loss = 1.23556330\n",
            "Iteration 33, loss = 1.22520747\n",
            "Iteration 34, loss = 1.21634325\n",
            "Iteration 35, loss = 1.20834263\n",
            "Iteration 36, loss = 1.19981715\n",
            "Iteration 37, loss = 1.19252081\n",
            "Iteration 38, loss = 1.18576860\n",
            "Iteration 39, loss = 1.17962320\n",
            "Iteration 40, loss = 1.17411290\n",
            "Iteration 41, loss = 1.16798838\n",
            "Iteration 42, loss = 1.16350908\n",
            "Iteration 43, loss = 1.15919641\n",
            "Iteration 44, loss = 1.15385739\n",
            "Iteration 45, loss = 1.15000519\n",
            "Iteration 46, loss = 1.14619116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 47, loss = 1.14266344\n",
            "Iteration 48, loss = 1.13914855\n",
            "Iteration 49, loss = 1.13606823\n",
            "Iteration 50, loss = 1.13305597\n",
            "Iteration 51, loss = 1.13063746\n",
            "Iteration 52, loss = 1.12752352\n",
            "Iteration 53, loss = 1.12512133\n",
            "Iteration 54, loss = 1.12264017\n",
            "Iteration 55, loss = 1.11972347\n",
            "Iteration 56, loss = 1.11758388\n",
            "Iteration 57, loss = 1.11643847\n",
            "Iteration 58, loss = 1.11301004\n",
            "Iteration 59, loss = 1.11090060\n",
            "Iteration 60, loss = 1.10957384\n",
            "Iteration 61, loss = 1.10716442\n",
            "Iteration 62, loss = 1.10577160\n",
            "Iteration 63, loss = 1.10312072\n",
            "Iteration 64, loss = 1.10133354\n",
            "Iteration 65, loss = 1.09971582\n",
            "Iteration 66, loss = 1.09806169\n",
            "Iteration 67, loss = 1.09569653\n",
            "Iteration 68, loss = 1.09368902\n",
            "Iteration 69, loss = 1.09280998\n",
            "Iteration 70, loss = 1.09058852\n",
            "Iteration 71, loss = 1.08907334\n",
            "Iteration 72, loss = 1.08909385\n",
            "Iteration 73, loss = 1.08533566\n",
            "Iteration 74, loss = 1.08348525\n",
            "Iteration 75, loss = 1.08257530\n",
            "Iteration 76, loss = 1.08025398\n",
            "Iteration 77, loss = 1.07881567\n",
            "Iteration 78, loss = 1.07678865\n",
            "Iteration 79, loss = 1.07563872\n",
            "Iteration 80, loss = 1.07356152\n",
            "Iteration 81, loss = 1.07247853\n",
            "Iteration 82, loss = 1.07021030\n",
            "Iteration 83, loss = 1.06863344\n",
            "Iteration 84, loss = 1.06657206\n",
            "Iteration 85, loss = 1.06608653\n",
            "Iteration 86, loss = 1.06278770\n",
            "Iteration 87, loss = 1.06117649\n",
            "Iteration 88, loss = 1.05925875\n",
            "Iteration 89, loss = 1.05707994\n",
            "Iteration 90, loss = 1.05513492\n",
            "Iteration 91, loss = 1.05447258\n",
            "Iteration 92, loss = 1.05139757\n",
            "Iteration 93, loss = 1.05204592\n",
            "Iteration 94, loss = 1.04824584\n",
            "Iteration 95, loss = 1.04541258\n",
            "Iteration 96, loss = 1.04498572\n",
            "Iteration 97, loss = 1.04208799\n",
            "Iteration 98, loss = 1.04123921\n",
            "Iteration 99, loss = 1.03768088\n",
            "Iteration 100, loss = 1.03551866\n",
            "Iteration 1, loss = 1.72661017\n",
            "Iteration 2, loss = 1.66451066\n",
            "Iteration 3, loss = 1.60278286\n",
            "Iteration 4, loss = 1.54879983\n",
            "Iteration 5, loss = 1.52948628\n",
            "Iteration 6, loss = 1.45947437\n",
            "Iteration 7, loss = 1.39021152\n",
            "Iteration 8, loss = 1.30858192\n",
            "Iteration 9, loss = 1.23432689\n",
            "Iteration 10, loss = 1.18717066\n",
            "Iteration 11, loss = 1.15652926\n",
            "Iteration 12, loss = 1.13843291\n",
            "Iteration 13, loss = 1.12277101\n",
            "Iteration 14, loss = 1.10736829\n",
            "Iteration 15, loss = 1.10209050\n",
            "Iteration 16, loss = 1.10993524\n",
            "Iteration 17, loss = 1.09193028\n",
            "Iteration 18, loss = 1.09153875\n",
            "Iteration 19, loss = 1.10645024\n",
            "Iteration 20, loss = 1.08897769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 1.08751552\n",
            "Iteration 22, loss = 1.08828645\n",
            "Iteration 23, loss = 1.09805988\n",
            "Iteration 24, loss = 1.06310104\n",
            "Iteration 25, loss = 1.06516030\n",
            "Iteration 26, loss = 1.09831333\n",
            "Iteration 27, loss = 1.06862835\n",
            "Iteration 28, loss = 1.05174807\n",
            "Iteration 29, loss = 1.05677996\n",
            "Iteration 30, loss = 1.07095121\n",
            "Iteration 31, loss = 1.05847096\n",
            "Iteration 32, loss = 1.03207780\n",
            "Iteration 33, loss = 1.01866757\n",
            "Iteration 34, loss = 1.00835715\n",
            "Iteration 35, loss = 1.00259062\n",
            "Iteration 36, loss = 0.99233066\n",
            "Iteration 37, loss = 0.97752046\n",
            "Iteration 38, loss = 0.98144304\n",
            "Iteration 39, loss = 0.97487236\n",
            "Iteration 40, loss = 0.98139295\n",
            "Iteration 41, loss = 1.04373737\n",
            "Iteration 42, loss = 1.00264536\n",
            "Iteration 43, loss = 0.97425169\n",
            "Iteration 44, loss = 0.95878366\n",
            "Iteration 45, loss = 0.94476153\n",
            "Iteration 46, loss = 0.91916821\n",
            "Iteration 47, loss = 0.91034743\n",
            "Iteration 48, loss = 0.90409880\n",
            "Iteration 49, loss = 0.91236984\n",
            "Iteration 50, loss = 0.91265837\n",
            "Iteration 51, loss = 0.89898324\n",
            "Iteration 52, loss = 0.89084229\n",
            "Iteration 53, loss = 0.92072882\n",
            "Iteration 54, loss = 0.87828356\n",
            "Iteration 55, loss = 0.87818009\n",
            "Iteration 56, loss = 0.91298358\n",
            "Iteration 57, loss = 0.86563952\n",
            "Iteration 58, loss = 0.85627700\n",
            "Iteration 59, loss = 0.85498600\n",
            "Iteration 60, loss = 0.90388168\n",
            "Iteration 61, loss = 0.91150710\n",
            "Iteration 62, loss = 1.00126257\n",
            "Iteration 63, loss = 0.91601220\n",
            "Iteration 64, loss = 0.95838502\n",
            "Iteration 65, loss = 0.93366655\n",
            "Iteration 66, loss = 0.93490021\n",
            "Iteration 67, loss = 0.88622905\n",
            "Iteration 68, loss = 0.88618196\n",
            "Iteration 69, loss = 0.88457263\n",
            "Iteration 70, loss = 0.91824390\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.61143971\n",
            "Iteration 2, loss = 1.60955738\n",
            "Iteration 3, loss = 1.60774277\n",
            "Iteration 4, loss = 1.60619277\n",
            "Iteration 5, loss = 1.60483398\n",
            "Iteration 6, loss = 1.60321928\n",
            "Iteration 7, loss = 1.60217256\n",
            "Iteration 8, loss = 1.60082002\n",
            "Iteration 9, loss = 1.59977048\n",
            "Iteration 10, loss = 1.59872031\n",
            "Iteration 11, loss = 1.59766211\n",
            "Iteration 12, loss = 1.59704040\n",
            "Iteration 13, loss = 1.59612998\n",
            "Iteration 14, loss = 1.59539521\n",
            "Iteration 15, loss = 1.59466656\n",
            "Iteration 16, loss = 1.59394027\n",
            "Iteration 17, loss = 1.59343605\n",
            "Iteration 18, loss = 1.59278640\n",
            "Iteration 19, loss = 1.59227192\n",
            "Iteration 20, loss = 1.59179680\n",
            "Iteration 21, loss = 1.59129627\n",
            "Iteration 22, loss = 1.59081097\n",
            "Iteration 23, loss = 1.59042015\n",
            "Iteration 24, loss = 1.59001725\n",
            "Iteration 25, loss = 1.58961414\n",
            "Iteration 26, loss = 1.58919659\n",
            "Iteration 27, loss = 1.58880125\n",
            "Iteration 28, loss = 1.58847214\n",
            "Iteration 29, loss = 1.58812427\n",
            "Iteration 30, loss = 1.58774174\n",
            "Iteration 31, loss = 1.58734301\n",
            "Iteration 32, loss = 1.58700765\n",
            "Iteration 33, loss = 1.58665282\n",
            "Iteration 34, loss = 1.58632390\n",
            "Iteration 35, loss = 1.58596668\n",
            "Iteration 36, loss = 1.58565331\n",
            "Iteration 37, loss = 1.58524081\n",
            "Iteration 38, loss = 1.58489002\n",
            "Iteration 39, loss = 1.58450697\n",
            "Iteration 40, loss = 1.58416133\n",
            "Iteration 41, loss = 1.58378352\n",
            "Iteration 42, loss = 1.58342828\n",
            "Iteration 43, loss = 1.58307144\n",
            "Iteration 44, loss = 1.58263541\n",
            "Iteration 45, loss = 1.58225830\n",
            "Iteration 46, loss = 1.58185468\n",
            "Iteration 47, loss = 1.58146328\n",
            "Iteration 48, loss = 1.58105254\n",
            "Iteration 49, loss = 1.58064127\n",
            "Iteration 50, loss = 1.58022656\n",
            "Iteration 51, loss = 1.57982270\n",
            "Iteration 52, loss = 1.57941949\n",
            "Iteration 53, loss = 1.57898564\n",
            "Iteration 54, loss = 1.57852205\n",
            "Iteration 55, loss = 1.57807448\n",
            "Iteration 56, loss = 1.57762146\n",
            "Iteration 57, loss = 1.57717667\n",
            "Iteration 58, loss = 1.57669694\n",
            "Iteration 59, loss = 1.57621882\n",
            "Iteration 60, loss = 1.57574991\n",
            "Iteration 61, loss = 1.57527900\n",
            "Iteration 62, loss = 1.57479074\n",
            "Iteration 63, loss = 1.57426696\n",
            "Iteration 64, loss = 1.57378331\n",
            "Iteration 65, loss = 1.57327446\n",
            "Iteration 66, loss = 1.57273357\n",
            "Iteration 67, loss = 1.57220027\n",
            "Iteration 68, loss = 1.57165473\n",
            "Iteration 69, loss = 1.57116527\n",
            "Iteration 70, loss = 1.57059252\n",
            "Iteration 71, loss = 1.57001314\n",
            "Iteration 72, loss = 1.56947944\n",
            "Iteration 73, loss = 1.56888184\n",
            "Iteration 74, loss = 1.56827547\n",
            "Iteration 75, loss = 1.56769697\n",
            "Iteration 76, loss = 1.56708756\n",
            "Iteration 77, loss = 1.56647453\n",
            "Iteration 78, loss = 1.56585730\n",
            "Iteration 79, loss = 1.56528977\n",
            "Iteration 80, loss = 1.56460590\n",
            "Iteration 81, loss = 1.56396726\n",
            "Iteration 82, loss = 1.56330013\n",
            "Iteration 83, loss = 1.56261477\n",
            "Iteration 84, loss = 1.56199463\n",
            "Iteration 85, loss = 1.56126019\n",
            "Iteration 86, loss = 1.56056992\n",
            "Iteration 87, loss = 1.55986276\n",
            "Iteration 88, loss = 1.55915696\n",
            "Iteration 89, loss = 1.55844115\n",
            "Iteration 90, loss = 1.55769410\n",
            "Iteration 91, loss = 1.55694945\n",
            "Iteration 92, loss = 1.55619906\n",
            "Iteration 93, loss = 1.55544064\n",
            "Iteration 94, loss = 1.55467746\n",
            "Iteration 95, loss = 1.55386934\n",
            "Iteration 96, loss = 1.55309032\n",
            "Iteration 97, loss = 1.55224587\n",
            "Iteration 98, loss = 1.55144306\n",
            "Iteration 99, loss = 1.55062734\n",
            "Iteration 100, loss = 1.54976153\n",
            "Iteration 1, loss = 1.60947255\n",
            "Iteration 2, loss = 1.59707533\n",
            "Iteration 3, loss = 1.59052863\n",
            "Iteration 4, loss = 1.58778386\n",
            "Iteration 5, loss = 1.58608237\n",
            "Iteration 6, loss = 1.58270701\n",
            "Iteration 7, loss = 1.57908224\n",
            "Iteration 8, loss = 1.57359613\n",
            "Iteration 9, loss = 1.56926463\n",
            "Iteration 10, loss = 1.56438349\n",
            "Iteration 11, loss = 1.55945937\n",
            "Iteration 12, loss = 1.55358562\n",
            "Iteration 13, loss = 1.54682650\n",
            "Iteration 14, loss = 1.54021334\n",
            "Iteration 15, loss = 1.53247060\n",
            "Iteration 16, loss = 1.52430368\n",
            "Iteration 17, loss = 1.51483735\n",
            "Iteration 18, loss = 1.50462886\n",
            "Iteration 19, loss = 1.49483293\n",
            "Iteration 20, loss = 1.48309431\n",
            "Iteration 21, loss = 1.47115506\n",
            "Iteration 22, loss = 1.45919550\n",
            "Iteration 23, loss = 1.44603154\n",
            "Iteration 24, loss = 1.43238322\n",
            "Iteration 25, loss = 1.41846079\n",
            "Iteration 26, loss = 1.40516473\n",
            "Iteration 27, loss = 1.39049444\n",
            "Iteration 28, loss = 1.37664953\n",
            "Iteration 29, loss = 1.36248296\n",
            "Iteration 30, loss = 1.34960963\n",
            "Iteration 31, loss = 1.33536313\n",
            "Iteration 32, loss = 1.32227092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 1.30946731\n",
            "Iteration 34, loss = 1.29785362\n",
            "Iteration 35, loss = 1.28656851\n",
            "Iteration 36, loss = 1.27549457\n",
            "Iteration 37, loss = 1.26521228\n",
            "Iteration 38, loss = 1.25560480\n",
            "Iteration 39, loss = 1.24671758\n",
            "Iteration 40, loss = 1.23863792\n",
            "Iteration 41, loss = 1.23035800\n",
            "Iteration 42, loss = 1.22315080\n",
            "Iteration 43, loss = 1.21708707\n",
            "Iteration 44, loss = 1.20988762\n",
            "Iteration 45, loss = 1.20422731\n",
            "Iteration 46, loss = 1.19866665\n",
            "Iteration 47, loss = 1.19375426\n",
            "Iteration 48, loss = 1.18891310\n",
            "Iteration 49, loss = 1.18453411\n",
            "Iteration 50, loss = 1.18041014\n",
            "Iteration 51, loss = 1.17672441\n",
            "Iteration 52, loss = 1.17307752\n",
            "Iteration 53, loss = 1.16975601\n",
            "Iteration 54, loss = 1.16631385\n",
            "Iteration 55, loss = 1.16310634\n",
            "Iteration 56, loss = 1.16049680\n",
            "Iteration 57, loss = 1.15848651\n",
            "Iteration 58, loss = 1.15489158\n",
            "Iteration 59, loss = 1.15249313\n",
            "Iteration 60, loss = 1.15100788\n",
            "Iteration 61, loss = 1.14818083\n",
            "Iteration 62, loss = 1.14613868\n",
            "Iteration 63, loss = 1.14390189\n",
            "Iteration 64, loss = 1.14260960\n",
            "Iteration 65, loss = 1.14042227\n",
            "Iteration 66, loss = 1.13858028\n",
            "Iteration 67, loss = 1.13682563\n",
            "Iteration 68, loss = 1.13509724\n",
            "Iteration 69, loss = 1.13377968\n",
            "Iteration 70, loss = 1.13216635\n",
            "Iteration 71, loss = 1.13059874\n",
            "Iteration 72, loss = 1.13083634\n",
            "Iteration 73, loss = 1.12810926\n",
            "Iteration 74, loss = 1.12664737\n",
            "Iteration 75, loss = 1.12597731\n",
            "Iteration 76, loss = 1.12417380\n",
            "Iteration 77, loss = 1.12320261\n",
            "Iteration 78, loss = 1.12204563\n",
            "Iteration 79, loss = 1.12161285\n",
            "Iteration 80, loss = 1.11967542\n",
            "Iteration 81, loss = 1.11961703\n",
            "Iteration 82, loss = 1.11787119\n",
            "Iteration 83, loss = 1.11652966\n",
            "Iteration 84, loss = 1.11583546\n",
            "Iteration 85, loss = 1.11562180\n",
            "Iteration 86, loss = 1.11392696\n",
            "Iteration 87, loss = 1.11345397\n",
            "Iteration 88, loss = 1.11244789\n",
            "Iteration 89, loss = 1.11123678\n",
            "Iteration 90, loss = 1.11029875\n",
            "Iteration 91, loss = 1.11059349\n",
            "Iteration 92, loss = 1.10878737\n",
            "Iteration 93, loss = 1.10925459\n",
            "Iteration 94, loss = 1.10800432\n",
            "Iteration 95, loss = 1.10618005\n",
            "Iteration 96, loss = 1.10585681\n",
            "Iteration 97, loss = 1.10507979\n",
            "Iteration 98, loss = 1.10459840\n",
            "Iteration 99, loss = 1.10324458\n",
            "Iteration 100, loss = 1.10281712\n",
            "Iteration 1, loss = 1.65935748\n",
            "Iteration 2, loss = 1.59051897\n",
            "Iteration 3, loss = 1.58477562\n",
            "Iteration 4, loss = 1.55417547\n",
            "Iteration 5, loss = 1.50315516\n",
            "Iteration 6, loss = 1.43800478\n",
            "Iteration 7, loss = 1.38194994\n",
            "Iteration 8, loss = 1.30842531\n",
            "Iteration 9, loss = 1.25895622\n",
            "Iteration 10, loss = 1.21208101\n",
            "Iteration 11, loss = 1.18336687\n",
            "Iteration 12, loss = 1.16283273\n",
            "Iteration 13, loss = 1.14847479\n",
            "Iteration 14, loss = 1.13606519\n",
            "Iteration 15, loss = 1.13020820\n",
            "Iteration 16, loss = 1.12405987\n",
            "Iteration 17, loss = 1.11829504\n",
            "Iteration 18, loss = 1.11388248\n",
            "Iteration 19, loss = 1.11707348\n",
            "Iteration 20, loss = 1.10960945\n",
            "Iteration 21, loss = 1.10643148\n",
            "Iteration 22, loss = 1.10529231\n",
            "Iteration 23, loss = 1.10167232\n",
            "Iteration 24, loss = 1.09691911\n",
            "Iteration 25, loss = 1.09944716\n",
            "Iteration 26, loss = 1.09919493\n",
            "Iteration 27, loss = 1.10301597\n",
            "Iteration 28, loss = 1.09520109\n",
            "Iteration 29, loss = 1.08540928\n",
            "Iteration 30, loss = 1.09990317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 31, loss = 1.09280460\n",
            "Iteration 32, loss = 1.09484754\n",
            "Iteration 33, loss = 1.08709732\n",
            "Iteration 34, loss = 1.08719023\n",
            "Iteration 35, loss = 1.08443774\n",
            "Iteration 36, loss = 1.08688376\n",
            "Iteration 37, loss = 1.07161571\n",
            "Iteration 38, loss = 1.07319890\n",
            "Iteration 39, loss = 1.06954529\n",
            "Iteration 40, loss = 1.06100943\n",
            "Iteration 41, loss = 1.05819848\n",
            "Iteration 42, loss = 1.06773485\n",
            "Iteration 43, loss = 1.04956926\n",
            "Iteration 44, loss = 1.04538691\n",
            "Iteration 45, loss = 1.04229630\n",
            "Iteration 46, loss = 1.03586189\n",
            "Iteration 47, loss = 1.02536864\n",
            "Iteration 48, loss = 1.02070984\n",
            "Iteration 49, loss = 1.01182314\n",
            "Iteration 50, loss = 1.01229521\n",
            "Iteration 51, loss = 0.99272947\n",
            "Iteration 52, loss = 0.99516199\n",
            "Iteration 53, loss = 0.99310038\n",
            "Iteration 54, loss = 0.98300514\n",
            "Iteration 55, loss = 0.97162224\n",
            "Iteration 56, loss = 0.96721458\n",
            "Iteration 57, loss = 0.96377064\n",
            "Iteration 58, loss = 0.95180081\n",
            "Iteration 59, loss = 0.95035267\n",
            "Iteration 60, loss = 0.94355641\n",
            "Iteration 61, loss = 0.94823764\n",
            "Iteration 62, loss = 0.95906719\n",
            "Iteration 63, loss = 0.93185828\n",
            "Iteration 64, loss = 0.94850856\n",
            "Iteration 65, loss = 0.92541731\n",
            "Iteration 66, loss = 0.91269508\n",
            "Iteration 67, loss = 0.90927034\n",
            "Iteration 68, loss = 0.90120315\n",
            "Iteration 69, loss = 0.92303520\n",
            "Iteration 70, loss = 0.91697486\n",
            "Iteration 71, loss = 0.90861120\n",
            "Iteration 72, loss = 0.91622129\n",
            "Iteration 73, loss = 0.90434112\n",
            "Iteration 74, loss = 0.89434207\n",
            "Iteration 75, loss = 0.89020539\n",
            "Iteration 76, loss = 0.88293708\n",
            "Iteration 77, loss = 0.88452092\n",
            "Iteration 78, loss = 0.85637037\n",
            "Iteration 79, loss = 0.87082848\n",
            "Iteration 80, loss = 0.86988587\n",
            "Iteration 81, loss = 0.84966226\n",
            "Iteration 82, loss = 0.85818626\n",
            "Iteration 83, loss = 0.84427223\n",
            "Iteration 84, loss = 0.85248429\n",
            "Iteration 85, loss = 0.84953688\n",
            "Iteration 86, loss = 0.83799596\n",
            "Iteration 87, loss = 0.84536579\n",
            "Iteration 88, loss = 0.83616239\n",
            "Iteration 89, loss = 0.83521166\n",
            "Iteration 90, loss = 0.82853148\n",
            "Iteration 91, loss = 0.82835588\n",
            "Iteration 92, loss = 0.81882127\n",
            "Iteration 93, loss = 0.81794268\n",
            "Iteration 94, loss = 0.81863175\n",
            "Iteration 95, loss = 0.81105052\n",
            "Iteration 96, loss = 0.83116521\n",
            "Iteration 97, loss = 0.80744972\n",
            "Iteration 98, loss = 0.81022311\n",
            "Iteration 99, loss = 0.82834079\n",
            "Iteration 100, loss = 0.79805381\n",
            "Iteration 1, loss = 1.65059461\n",
            "Iteration 2, loss = 1.64410167\n",
            "Iteration 3, loss = 1.63815129\n",
            "Iteration 4, loss = 1.63270563\n",
            "Iteration 5, loss = 1.62724191\n",
            "Iteration 6, loss = 1.62294467\n",
            "Iteration 7, loss = 1.61843970\n",
            "Iteration 8, loss = 1.61511894\n",
            "Iteration 9, loss = 1.61128984\n",
            "Iteration 10, loss = 1.60831866\n",
            "Iteration 11, loss = 1.60546320\n",
            "Iteration 12, loss = 1.60319566\n",
            "Iteration 13, loss = 1.60084644\n",
            "Iteration 14, loss = 1.59936135\n",
            "Iteration 15, loss = 1.59752427\n",
            "Iteration 16, loss = 1.59585897\n",
            "Iteration 17, loss = 1.59467901\n",
            "Iteration 18, loss = 1.59334669\n",
            "Iteration 19, loss = 1.59242877\n",
            "Iteration 20, loss = 1.59133538\n",
            "Iteration 21, loss = 1.59040485\n",
            "Iteration 22, loss = 1.58954419\n",
            "Iteration 23, loss = 1.58863187\n",
            "Iteration 24, loss = 1.58784666"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 25, loss = 1.58691224\n",
            "Iteration 26, loss = 1.58627076\n",
            "Iteration 27, loss = 1.58537232\n",
            "Iteration 28, loss = 1.58457721\n",
            "Iteration 29, loss = 1.58392474\n",
            "Iteration 30, loss = 1.58298346\n",
            "Iteration 31, loss = 1.58219726\n",
            "Iteration 32, loss = 1.58139067\n",
            "Iteration 33, loss = 1.58061325\n",
            "Iteration 34, loss = 1.57975724\n",
            "Iteration 35, loss = 1.57896004\n",
            "Iteration 36, loss = 1.57811579\n",
            "Iteration 37, loss = 1.57726165\n",
            "Iteration 38, loss = 1.57640359\n",
            "Iteration 39, loss = 1.57551923\n",
            "Iteration 40, loss = 1.57469563\n",
            "Iteration 41, loss = 1.57376753\n",
            "Iteration 42, loss = 1.57298850\n",
            "Iteration 43, loss = 1.57204057\n",
            "Iteration 44, loss = 1.57110766\n",
            "Iteration 45, loss = 1.57018462\n",
            "Iteration 46, loss = 1.56931706\n",
            "Iteration 47, loss = 1.56834301\n",
            "Iteration 48, loss = 1.56743491\n",
            "Iteration 49, loss = 1.56645304\n",
            "Iteration 50, loss = 1.56554834\n",
            "Iteration 51, loss = 1.56453355\n",
            "Iteration 52, loss = 1.56353042\n",
            "Iteration 53, loss = 1.56258999\n",
            "Iteration 54, loss = 1.56166116\n",
            "Iteration 55, loss = 1.56055235\n",
            "Iteration 56, loss = 1.55952632\n",
            "Iteration 57, loss = 1.55851670\n",
            "Iteration 58, loss = 1.55744024\n",
            "Iteration 59, loss = 1.55639482\n",
            "Iteration 60, loss = 1.55531384\n",
            "Iteration 61, loss = 1.55424686\n",
            "Iteration 62, loss = 1.55318168\n",
            "Iteration 63, loss = 1.55201656\n",
            "Iteration 64, loss = 1.55093123\n",
            "Iteration 65, loss = 1.54974994\n",
            "Iteration 66, loss = 1.54875930\n",
            "Iteration 67, loss = 1.54748170\n",
            "Iteration 68, loss = 1.54635371\n",
            "Iteration 69, loss = 1.54511649\n",
            "Iteration 70, loss = 1.54392366\n",
            "Iteration 71, loss = 1.54277072\n",
            "Iteration 72, loss = 1.54150640\n",
            "Iteration 73, loss = 1.54028998\n",
            "Iteration 74, loss = 1.53903563\n",
            "Iteration 75, loss = 1.53776819\n",
            "Iteration 76, loss = 1.53651355\n",
            "Iteration 77, loss = 1.53524098\n",
            "Iteration 78, loss = 1.53396520\n",
            "Iteration 79, loss = 1.53268059\n",
            "Iteration 80, loss = 1.53130085\n",
            "Iteration 81, loss = 1.52993221\n",
            "Iteration 82, loss = 1.52864818\n",
            "Iteration 83, loss = 1.52723120\n",
            "Iteration 84, loss = 1.52585177\n",
            "Iteration 85, loss = 1.52450429\n",
            "Iteration 86, loss = 1.52316028\n",
            "Iteration 87, loss = 1.52163493\n",
            "Iteration 88, loss = 1.52026334\n",
            "Iteration 89, loss = 1.51875932\n",
            "Iteration 90, loss = 1.51732591\n",
            "Iteration 91, loss = 1.51588931\n",
            "Iteration 92, loss = 1.51441250\n",
            "Iteration 93, loss = 1.51287588\n",
            "Iteration 94, loss = 1.51133020\n",
            "Iteration 95, loss = 1.50985135\n",
            "Iteration 96, loss = 1.50828596\n",
            "Iteration 97, loss = 1.50674014\n",
            "Iteration 98, loss = 1.50518928\n",
            "Iteration 99, loss = 1.50360980\n",
            "Iteration 100, loss = 1.50205184\n",
            "Iteration 1, loss = 1.64227936\n",
            "Iteration 2, loss = 1.60562002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 1.59588268\n",
            "Iteration 4, loss = 1.59073430\n",
            "Iteration 5, loss = 1.58487352\n",
            "Iteration 6, loss = 1.57494778\n",
            "Iteration 7, loss = 1.56668876\n",
            "Iteration 8, loss = 1.55626913\n",
            "Iteration 9, loss = 1.54784825\n",
            "Iteration 10, loss = 1.53827878\n",
            "Iteration 11, loss = 1.52933849\n",
            "Iteration 12, loss = 1.51789267\n",
            "Iteration 13, loss = 1.50647421\n",
            "Iteration 14, loss = 1.49518036\n",
            "Iteration 15, loss = 1.48182499\n",
            "Iteration 16, loss = 1.46777732\n",
            "Iteration 17, loss = 1.45381718\n",
            "Iteration 18, loss = 1.43874227\n",
            "Iteration 19, loss = 1.42321162\n",
            "Iteration 20, loss = 1.40664711\n",
            "Iteration 21, loss = 1.39076345\n",
            "Iteration 22, loss = 1.37400438\n",
            "Iteration 23, loss = 1.35710430\n",
            "Iteration 24, loss = 1.34106392\n",
            "Iteration 25, loss = 1.32509960\n",
            "Iteration 26, loss = 1.31014598\n",
            "Iteration 27, loss = 1.29448639\n",
            "Iteration 28, loss = 1.28061114\n",
            "Iteration 29, loss = 1.26774956\n",
            "Iteration 30, loss = 1.25450345\n",
            "Iteration 31, loss = 1.24295404\n",
            "Iteration 32, loss = 1.23276142\n",
            "Iteration 33, loss = 1.22246163\n",
            "Iteration 34, loss = 1.21312393\n",
            "Iteration 35, loss = 1.20555433\n",
            "Iteration 36, loss = 1.19821077\n",
            "Iteration 37, loss = 1.19118028\n",
            "Iteration 38, loss = 1.18427723\n",
            "Iteration 39, loss = 1.17878702\n",
            "Iteration 40, loss = 1.17367501\n",
            "Iteration 41, loss = 1.16781894\n",
            "Iteration 42, loss = 1.16386589\n",
            "Iteration 43, loss = 1.15923726\n",
            "Iteration 44, loss = 1.15501987\n",
            "Iteration 45, loss = 1.15112335\n",
            "Iteration 46, loss = 1.14879135\n",
            "Iteration 47, loss = 1.14505440\n",
            "Iteration 48, loss = 1.14336605\n",
            "Iteration 49, loss = 1.13933297\n",
            "Iteration 50, loss = 1.13576155\n",
            "Iteration 51, loss = 1.13362920\n",
            "Iteration 52, loss = 1.13134671\n",
            "Iteration 53, loss = 1.12881598\n",
            "Iteration 54, loss = 1.12850013\n",
            "Iteration 55, loss = 1.12479701\n",
            "Iteration 56, loss = 1.12276329\n",
            "Iteration 57, loss = 1.12139462\n",
            "Iteration 58, loss = 1.11918864\n",
            "Iteration 59, loss = 1.11766696\n",
            "Iteration 60, loss = 1.11532790\n",
            "Iteration 61, loss = 1.11513298\n",
            "Iteration 62, loss = 1.11314031\n",
            "Iteration 63, loss = 1.11118092\n",
            "Iteration 64, loss = 1.10977052\n",
            "Iteration 65, loss = 1.10749864\n",
            "Iteration 66, loss = 1.10675328\n",
            "Iteration 67, loss = 1.10510808\n",
            "Iteration 68, loss = 1.10444423\n",
            "Iteration 69, loss = 1.10237480\n",
            "Iteration 70, loss = 1.10132765\n",
            "Iteration 71, loss = 1.09996022\n",
            "Iteration 72, loss = 1.09827083\n",
            "Iteration 73, loss = 1.09835780\n",
            "Iteration 74, loss = 1.09619716\n",
            "Iteration 75, loss = 1.09552410\n",
            "Iteration 76, loss = 1.09417755\n",
            "Iteration 77, loss = 1.09219015\n",
            "Iteration 78, loss = 1.09166691\n",
            "Iteration 79, loss = 1.09080775\n",
            "Iteration 80, loss = 1.08904365\n",
            "Iteration 81, loss = 1.08822900\n",
            "Iteration 82, loss = 1.08666908\n",
            "Iteration 83, loss = 1.08469631\n",
            "Iteration 84, loss = 1.08434585\n",
            "Iteration 85, loss = 1.08229579\n",
            "Iteration 86, loss = 1.08457656\n",
            "Iteration 87, loss = 1.08061462\n",
            "Iteration 88, loss = 1.07809462\n",
            "Iteration 89, loss = 1.07749106\n",
            "Iteration 90, loss = 1.07685745\n",
            "Iteration 91, loss = 1.07438590\n",
            "Iteration 92, loss = 1.07542399\n",
            "Iteration 93, loss = 1.07255690\n",
            "Iteration 94, loss = 1.06999891\n",
            "Iteration 95, loss = 1.06888000\n",
            "Iteration 96, loss = 1.06798208\n",
            "Iteration 97, loss = 1.06599576\n",
            "Iteration 98, loss = 1.06617863\n",
            "Iteration 99, loss = 1.06306598\n",
            "Iteration 100, loss = 1.06252374\n",
            "Iteration 1, loss = 1.78932962\n",
            "Iteration 2, loss = 1.61772476\n",
            "Iteration 3, loss = 1.58648416\n",
            "Iteration 4, loss = 1.49781084\n",
            "Iteration 5, loss = 1.42057213\n",
            "Iteration 6, loss = 1.32607129\n",
            "Iteration 7, loss = 1.28568952\n",
            "Iteration 8, loss = 1.22342046\n",
            "Iteration 9, loss = 1.18366921\n",
            "Iteration 10, loss = 1.15457002\n",
            "Iteration 11, loss = 1.15765908\n",
            "Iteration 12, loss = 1.14954823\n",
            "Iteration 13, loss = 1.12801586\n",
            "Iteration 14, loss = 1.12756580\n",
            "Iteration 15, loss = 1.12058226\n",
            "Iteration 16, loss = 1.11787546\n",
            "Iteration 17, loss = 1.11661391\n",
            "Iteration 18, loss = 1.11254053\n",
            "Iteration 19, loss = 1.11269219\n",
            "Iteration 20, loss = 1.10677196\n",
            "Iteration 21, loss = 1.10379460\n",
            "Iteration 22, loss = 1.10730091\n",
            "Iteration 23, loss = 1.09575470\n",
            "Iteration 24, loss = 1.09921102\n",
            "Iteration 25, loss = 1.09798840\n",
            "Iteration 26, loss = 1.11603958\n",
            "Iteration 27, loss = 1.09965086\n",
            "Iteration 28, loss = 1.08390759\n",
            "Iteration 29, loss = 1.09295373\n",
            "Iteration 30, loss = 1.09042077\n",
            "Iteration 31, loss = 1.09225876\n",
            "Iteration 32, loss = 1.09870486\n",
            "Iteration 33, loss = 1.08789924\n",
            "Iteration 34, loss = 1.07654938\n",
            "Iteration 35, loss = 1.06245146\n",
            "Iteration 36, loss = 1.08210385\n",
            "Iteration 37, loss = 1.04461558\n",
            "Iteration 38, loss = 1.05009545\n",
            "Iteration 39, loss = 1.04626780\n",
            "Iteration 40, loss = 1.06610153\n",
            "Iteration 41, loss = 1.05718829\n",
            "Iteration 42, loss = 1.03078218\n",
            "Iteration 43, loss = 1.04327229\n",
            "Iteration 44, loss = 0.99902474\n",
            "Iteration 45, loss = 1.01563535\n",
            "Iteration 46, loss = 1.00762914\n",
            "Iteration 47, loss = 1.01604649\n",
            "Iteration 48, loss = 1.00553871\n",
            "Iteration 49, loss = 0.98051760\n",
            "Iteration 50, loss = 0.96818671\n",
            "Iteration 51, loss = 0.95359478\n",
            "Iteration 52, loss = 0.93975509\n",
            "Iteration 53, loss = 0.93388076\n",
            "Iteration 54, loss = 0.94268770\n",
            "Iteration 55, loss = 1.00693356\n",
            "Iteration 56, loss = 1.00253372\n",
            "Iteration 57, loss = 0.93677884\n",
            "Iteration 58, loss = 0.90777533\n",
            "Iteration 59, loss = 0.91523683\n",
            "Iteration 60, loss = 0.93492352\n",
            "Iteration 61, loss = 0.97964008\n",
            "Iteration 62, loss = 0.99621060\n",
            "Iteration 63, loss = 0.95677900\n",
            "Iteration 64, loss = 0.93405166\n",
            "Iteration 65, loss = 0.90566664\n",
            "Iteration 66, loss = 0.88752073\n",
            "Iteration 67, loss = 0.90289502\n",
            "Iteration 68, loss = 0.87830859\n",
            "Iteration 69, loss = 0.86718347\n",
            "Iteration 70, loss = 0.84824495\n",
            "Iteration 71, loss = 0.84525894\n",
            "Iteration 72, loss = 0.84314690\n",
            "Iteration 73, loss = 0.84156665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 0.85504894\n",
            "Iteration 75, loss = 0.84308974\n",
            "Iteration 76, loss = 0.86684497\n",
            "Iteration 77, loss = 0.84287472\n",
            "Iteration 78, loss = 0.83530062\n",
            "Iteration 79, loss = 0.83124088\n",
            "Iteration 80, loss = 0.90535300\n",
            "Iteration 81, loss = 0.86911885\n",
            "Iteration 82, loss = 0.89112631\n",
            "Iteration 83, loss = 0.96347255\n",
            "Iteration 84, loss = 0.95443134\n",
            "Iteration 85, loss = 0.96129721\n",
            "Iteration 86, loss = 0.93698519\n",
            "Iteration 87, loss = 0.87275682\n",
            "Iteration 88, loss = 0.81908717\n",
            "Iteration 89, loss = 0.84760832\n",
            "Iteration 90, loss = 0.81805205\n",
            "Iteration 91, loss = 0.82495603\n",
            "Iteration 92, loss = 0.81360047\n",
            "Iteration 93, loss = 0.84677047\n",
            "Iteration 94, loss = 0.84100498\n",
            "Iteration 95, loss = 0.85150560\n",
            "Iteration 96, loss = 0.83168628\n",
            "Iteration 97, loss = 0.81324754\n",
            "Iteration 98, loss = 0.79968663\n",
            "Iteration 99, loss = 0.79871681\n",
            "Iteration 100, loss = 0.78980393\n",
            "Iteration 1, loss = 1.64446660\n",
            "Iteration 2, loss = 1.63797932\n",
            "Iteration 3, loss = 1.63204641\n",
            "Iteration 4, loss = 1.62769899\n",
            "Iteration 5, loss = 1.62284569\n",
            "Iteration 6, loss = 1.61896991\n",
            "Iteration 7, loss = 1.61516164\n",
            "Iteration 8, loss = 1.61236376\n",
            "Iteration 9, loss = 1.60953421\n",
            "Iteration 10, loss = 1.60710909\n",
            "Iteration 11, loss = 1.60563732\n",
            "Iteration 12, loss = 1.60361755\n",
            "Iteration 13, loss = 1.60229256\n",
            "Iteration 14, loss = 1.60119179\n",
            "Iteration 15, loss = 1.60010391\n",
            "Iteration 16, loss = 1.59945056\n",
            "Iteration 17, loss = 1.59912099\n",
            "Iteration 18, loss = 1.59859935\n",
            "Iteration 19, loss = 1.59813759\n",
            "Iteration 20, loss = 1.59790958\n",
            "Iteration 21, loss = 1.59767735\n",
            "Iteration 22, loss = 1.59743621\n",
            "Iteration 23, loss = 1.59730575\n",
            "Iteration 24, loss = 1.59726718\n",
            "Iteration 25, loss = 1.59725270\n",
            "Iteration 26, loss = 1.59714041\n",
            "Iteration 27, loss = 1.59698137\n",
            "Iteration 28, loss = 1.59696791\n",
            "Iteration 29, loss = 1.59696745\n",
            "Iteration 30, loss = 1.59678047\n",
            "Iteration 31, loss = 1.59669398\n",
            "Iteration 32, loss = 1.59662194\n",
            "Iteration 33, loss = 1.59654328\n",
            "Iteration 34, loss = 1.59642632\n",
            "Iteration 35, loss = 1.59637429\n",
            "Iteration 36, loss = 1.59625932\n",
            "Iteration 37, loss = 1.59620118\n",
            "Iteration 38, loss = 1.59613881\n",
            "Iteration 39, loss = 1.59596158\n",
            "Iteration 40, loss = 1.59596645\n",
            "Iteration 41, loss = 1.59578127\n",
            "Iteration 42, loss = 1.59584430\n",
            "Iteration 43, loss = 1.59564099\n",
            "Iteration 44, loss = 1.59557084\n",
            "Iteration 45, loss = 1.59540332\n",
            "Iteration 46, loss = 1.59536190\n",
            "Iteration 47, loss = 1.59523271\n",
            "Iteration 48, loss = 1.59518381\n",
            "Iteration 49, loss = 1.59506959\n",
            "Iteration 50, loss = 1.59501692\n",
            "Iteration 51, loss = 1.59487776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 52, loss = 1.59477604\n",
            "Iteration 53, loss = 1.59471925\n",
            "Iteration 54, loss = 1.59476181\n",
            "Iteration 55, loss = 1.59450057\n",
            "Iteration 56, loss = 1.59442888\n",
            "Iteration 57, loss = 1.59436414\n",
            "Iteration 58, loss = 1.59424699\n",
            "Iteration 59, loss = 1.59415911\n",
            "Iteration 60, loss = 1.59403595\n",
            "Iteration 61, loss = 1.59398825\n",
            "Iteration 62, loss = 1.59391305\n",
            "Iteration 63, loss = 1.59373389\n",
            "Iteration 64, loss = 1.59365683\n",
            "Iteration 65, loss = 1.59354298\n",
            "Iteration 66, loss = 1.59356105\n",
            "Iteration 67, loss = 1.59335141\n",
            "Iteration 68, loss = 1.59330502\n",
            "Iteration 69, loss = 1.59317014\n",
            "Iteration 70, loss = 1.59304128\n",
            "Iteration 71, loss = 1.59298537\n",
            "Iteration 72, loss = 1.59284424\n",
            "Iteration 73, loss = 1.59280248\n",
            "Iteration 74, loss = 1.59267952\n",
            "Iteration 75, loss = 1.59261664\n",
            "Iteration 76, loss = 1.59246600\n",
            "Iteration 77, loss = 1.59234422\n",
            "Iteration 78, loss = 1.59224560\n",
            "Iteration 79, loss = 1.59221231\n",
            "Iteration 80, loss = 1.59204395\n",
            "Iteration 81, loss = 1.59188510\n",
            "Iteration 82, loss = 1.59183139\n",
            "Iteration 83, loss = 1.59167241\n",
            "Iteration 84, loss = 1.59157547\n",
            "Iteration 85, loss = 1.59146961\n",
            "Iteration 86, loss = 1.59142867\n",
            "Iteration 87, loss = 1.59123408\n",
            "Iteration 88, loss = 1.59115589\n",
            "Iteration 89, loss = 1.59101456\n",
            "Iteration 90, loss = 1.59087735\n",
            "Iteration 91, loss = 1.59091094\n",
            "Iteration 92, loss = 1.59069311\n",
            "Iteration 93, loss = 1.59051961\n",
            "Iteration 94, loss = 1.59038582\n",
            "Iteration 95, loss = 1.59029354\n",
            "Iteration 96, loss = 1.59017539\n",
            "Iteration 97, loss = 1.59005661\n",
            "Iteration 98, loss = 1.59001096\n",
            "Iteration 99, loss = 1.58979076\n",
            "Iteration 100, loss = 1.58971070\n",
            "Iteration 1, loss = 1.63499704\n",
            "Iteration 2, loss = 1.60826757\n",
            "Iteration 3, loss = 1.60684089\n",
            "Iteration 4, loss = 1.60546479\n",
            "Iteration 5, loss = 1.60501404\n",
            "Iteration 6, loss = 1.60181574\n",
            "Iteration 7, loss = 1.59834319\n",
            "Iteration 8, loss = 1.59434120\n",
            "Iteration 9, loss = 1.59406953\n",
            "Iteration 10, loss = 1.59491362\n",
            "Iteration 11, loss = 1.59421576\n",
            "Iteration 12, loss = 1.59127741\n",
            "Iteration 13, loss = 1.59011654\n",
            "Iteration 14, loss = 1.58857701\n",
            "Iteration 15, loss = 1.58713418\n",
            "Iteration 16, loss = 1.58632949\n",
            "Iteration 17, loss = 1.58610706\n",
            "Iteration 18, loss = 1.58481809\n",
            "Iteration 19, loss = 1.58318813\n",
            "Iteration 20, loss = 1.58065684\n",
            "Iteration 21, loss = 1.57984856\n",
            "Iteration 22, loss = 1.57810118\n",
            "Iteration 23, loss = 1.57576990\n",
            "Iteration 24, loss = 1.57407524\n",
            "Iteration 25, loss = 1.57272286"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 26, loss = 1.57076679\n",
            "Iteration 27, loss = 1.56701120\n",
            "Iteration 28, loss = 1.56495116\n",
            "Iteration 29, loss = 1.56333861\n",
            "Iteration 30, loss = 1.55914519\n",
            "Iteration 31, loss = 1.55571139\n",
            "Iteration 32, loss = 1.55297715\n",
            "Iteration 33, loss = 1.54986508\n",
            "Iteration 34, loss = 1.54576572\n",
            "Iteration 35, loss = 1.54257798\n",
            "Iteration 36, loss = 1.53835773\n",
            "Iteration 37, loss = 1.53383713\n",
            "Iteration 38, loss = 1.52953093\n",
            "Iteration 39, loss = 1.52414689\n",
            "Iteration 40, loss = 1.52018822\n",
            "Iteration 41, loss = 1.51409498\n",
            "Iteration 42, loss = 1.51004075\n",
            "Iteration 43, loss = 1.50336055\n",
            "Iteration 44, loss = 1.49767797\n",
            "Iteration 45, loss = 1.49073110\n",
            "Iteration 46, loss = 1.48454409\n",
            "Iteration 47, loss = 1.47768077\n",
            "Iteration 48, loss = 1.47131715\n",
            "Iteration 49, loss = 1.46396237\n",
            "Iteration 50, loss = 1.45677975\n",
            "Iteration 51, loss = 1.44908977\n",
            "Iteration 52, loss = 1.44138092\n",
            "Iteration 53, loss = 1.43409176\n",
            "Iteration 54, loss = 1.42735725\n",
            "Iteration 55, loss = 1.41803107\n",
            "Iteration 56, loss = 1.41064010\n",
            "Iteration 57, loss = 1.40302385\n",
            "Iteration 58, loss = 1.39425038\n",
            "Iteration 59, loss = 1.38624248\n",
            "Iteration 60, loss = 1.37790140\n",
            "Iteration 61, loss = 1.37038084\n",
            "Iteration 62, loss = 1.36250389\n",
            "Iteration 63, loss = 1.35372355\n",
            "Iteration 64, loss = 1.34622242\n",
            "Iteration 65, loss = 1.33814172\n",
            "Iteration 66, loss = 1.33166208\n",
            "Iteration 67, loss = 1.32351594\n",
            "Iteration 68, loss = 1.31694088\n",
            "Iteration 69, loss = 1.30959559\n",
            "Iteration 70, loss = 1.30246666\n",
            "Iteration 71, loss = 1.29628388\n",
            "Iteration 72, loss = 1.28980979\n",
            "Iteration 73, loss = 1.28390062\n",
            "Iteration 74, loss = 1.27772271\n",
            "Iteration 75, loss = 1.27227667\n",
            "Iteration 76, loss = 1.26695172\n",
            "Iteration 77, loss = 1.26147389\n",
            "Iteration 78, loss = 1.25728911\n",
            "Iteration 79, loss = 1.25201213\n",
            "Iteration 80, loss = 1.24700886\n",
            "Iteration 81, loss = 1.24261381\n",
            "Iteration 82, loss = 1.23855129\n",
            "Iteration 83, loss = 1.23444244\n",
            "Iteration 84, loss = 1.23019561\n",
            "Iteration 85, loss = 1.22652669\n",
            "Iteration 86, loss = 1.22431509\n",
            "Iteration 87, loss = 1.21974830\n",
            "Iteration 88, loss = 1.21619283\n",
            "Iteration 89, loss = 1.21263269\n",
            "Iteration 90, loss = 1.21012556\n",
            "Iteration 91, loss = 1.20737254\n",
            "Iteration 92, loss = 1.20509910\n",
            "Iteration 93, loss = 1.20119459\n",
            "Iteration 94, loss = 1.19855306\n",
            "Iteration 95, loss = 1.19604560\n",
            "Iteration 96, loss = 1.19384835\n",
            "Iteration 97, loss = 1.19140778\n",
            "Iteration 98, loss = 1.19031951\n",
            "Iteration 99, loss = 1.18714810\n",
            "Iteration 100, loss = 1.18537011\n",
            "Iteration 1, loss = 1.76324269\n",
            "Iteration 2, loss = 1.75769852\n",
            "Iteration 3, loss = 1.63702528\n",
            "Iteration 4, loss = 1.67402785\n",
            "Iteration 5, loss = 1.58861967\n",
            "Iteration 6, loss = 1.59314579\n",
            "Iteration 7, loss = 1.59883699\n",
            "Iteration 8, loss = 1.57925956\n",
            "Iteration 9, loss = 1.56139710\n",
            "Iteration 10, loss = 1.54721237\n",
            "Iteration 11, loss = 1.53494698\n",
            "Iteration 12, loss = 1.51475965\n",
            "Iteration 13, loss = 1.48407367\n",
            "Iteration 14, loss = 1.45130331\n",
            "Iteration 15, loss = 1.41814632\n",
            "Iteration 16, loss = 1.38214888\n",
            "Iteration 17, loss = 1.34136914\n",
            "Iteration 18, loss = 1.30109589\n",
            "Iteration 19, loss = 1.27240906\n",
            "Iteration 20, loss = 1.24398470\n",
            "Iteration 21, loss = 1.21476959\n",
            "Iteration 22, loss = 1.19737356\n",
            "Iteration 23, loss = 1.17907309\n",
            "Iteration 24, loss = 1.16739094\n",
            "Iteration 25, loss = 1.15570459\n",
            "Iteration 26, loss = 1.16290736\n",
            "Iteration 27, loss = 1.14310371\n",
            "Iteration 28, loss = 1.14480851\n",
            "Iteration 29, loss = 1.12729117\n",
            "Iteration 30, loss = 1.13656778\n",
            "Iteration 31, loss = 1.12719113\n",
            "Iteration 32, loss = 1.12454375\n",
            "Iteration 33, loss = 1.11643123\n",
            "Iteration 34, loss = 1.11807109\n",
            "Iteration 35, loss = 1.11211119\n",
            "Iteration 36, loss = 1.12274744\n",
            "Iteration 37, loss = 1.11125934\n",
            "Iteration 38, loss = 1.12132548\n",
            "Iteration 39, loss = 1.11652637\n",
            "Iteration 40, loss = 1.12169690\n",
            "Iteration 41, loss = 1.10997628\n",
            "Iteration 42, loss = 1.11069976\n",
            "Iteration 43, loss = 1.10914230\n",
            "Iteration 44, loss = 1.10555550\n",
            "Iteration 45, loss = 1.10725010\n",
            "Iteration 46, loss = 1.11420524\n",
            "Iteration 47, loss = 1.10581949\n",
            "Iteration 48, loss = 1.11446069\n",
            "Iteration 49, loss = 1.10708740\n",
            "Iteration 50, loss = 1.11041809\n",
            "Iteration 51, loss = 1.10400375\n",
            "Iteration 52, loss = 1.10050746\n",
            "Iteration 53, loss = 1.10256323\n",
            "Iteration 54, loss = 1.11424327\n",
            "Iteration 55, loss = 1.10068703\n",
            "Iteration 56, loss = 1.10944115\n",
            "Iteration 57, loss = 1.09710559\n",
            "Iteration 58, loss = 1.10271478\n",
            "Iteration 59, loss = 1.09567188\n",
            "Iteration 60, loss = 1.10005143\n",
            "Iteration 61, loss = 1.09806423\n",
            "Iteration 62, loss = 1.10583468\n",
            "Iteration 63, loss = 1.09611516\n",
            "Iteration 64, loss = 1.10183563\n",
            "Iteration 65, loss = 1.09885895"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 66, loss = 1.09619206\n",
            "Iteration 67, loss = 1.09926206\n",
            "Iteration 68, loss = 1.09222204\n",
            "Iteration 69, loss = 1.09942131\n",
            "Iteration 70, loss = 1.09477994\n",
            "Iteration 71, loss = 1.09118445\n",
            "Iteration 72, loss = 1.09137266\n",
            "Iteration 73, loss = 1.09078134\n",
            "Iteration 74, loss = 1.08737944\n",
            "Iteration 75, loss = 1.08725536\n",
            "Iteration 76, loss = 1.08175274\n",
            "Iteration 77, loss = 1.08323929\n",
            "Iteration 78, loss = 1.08458841\n",
            "Iteration 79, loss = 1.07449059\n",
            "Iteration 80, loss = 1.07543798\n",
            "Iteration 81, loss = 1.07204181\n",
            "Iteration 82, loss = 1.07620346\n",
            "Iteration 83, loss = 1.06146734\n",
            "Iteration 84, loss = 1.06116529\n",
            "Iteration 85, loss = 1.05394961\n",
            "Iteration 86, loss = 1.06753813\n",
            "Iteration 87, loss = 1.05614551\n",
            "Iteration 88, loss = 1.06555705\n",
            "Iteration 89, loss = 1.04582453\n",
            "Iteration 90, loss = 1.04144773\n",
            "Iteration 91, loss = 1.04307454\n",
            "Iteration 92, loss = 1.03722409\n",
            "Iteration 93, loss = 1.03603613\n",
            "Iteration 94, loss = 1.02727078\n",
            "Iteration 95, loss = 1.02397326\n",
            "Iteration 96, loss = 1.02473664\n",
            "Iteration 97, loss = 1.02712669\n",
            "Iteration 98, loss = 1.00583185\n",
            "Iteration 99, loss = 1.01692590\n",
            "Iteration 100, loss = 1.00934068\n",
            "Iteration 1, loss = 1.64762893\n",
            "Iteration 2, loss = 1.64153912\n",
            "Iteration 3, loss = 1.63593738\n",
            "Iteration 4, loss = 1.63083221\n",
            "Iteration 5, loss = 1.62567843\n",
            "Iteration 6, loss = 1.62163298\n",
            "Iteration 7, loss = 1.61735996\n",
            "Iteration 8, loss = 1.61422729\n",
            "Iteration 9, loss = 1.61058009\n",
            "Iteration 10, loss = 1.60775357\n",
            "Iteration 11, loss = 1.60503135\n",
            "Iteration 12, loss = 1.60285882\n",
            "Iteration 13, loss = 1.60060536\n",
            "Iteration 14, loss = 1.59917857\n",
            "Iteration 15, loss = 1.59740551\n",
            "Iteration 16, loss = 1.59579967\n",
            "Iteration 17, loss = 1.59466794\n",
            "Iteration 18, loss = 1.59337518\n",
            "Iteration 19, loss = 1.59247314\n",
            "Iteration 20, loss = 1.59142137\n",
            "Iteration 21, loss = 1.59051059\n",
            "Iteration 22, loss = 1.58968427\n",
            "Iteration 23, loss = 1.58879817\n",
            "Iteration 24, loss = 1.58803916\n",
            "Iteration 25, loss = 1.58713583\n",
            "Iteration 26, loss = 1.58652686\n",
            "Iteration 27, loss = 1.58565654\n",
            "Iteration 28, loss = 1.58489590\n",
            "Iteration 29, loss = 1.58427479\n",
            "Iteration 30, loss = 1.58336966\n",
            "Iteration 31, loss = 1.58262031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 32, loss = 1.58185047\n",
            "Iteration 33, loss = 1.58110982\n",
            "Iteration 34, loss = 1.58029046\n",
            "Iteration 35, loss = 1.57953075\n",
            "Iteration 36, loss = 1.57872384\n",
            "Iteration 37, loss = 1.57790589\n",
            "Iteration 38, loss = 1.57708575\n",
            "Iteration 39, loss = 1.57624244\n",
            "Iteration 40, loss = 1.57545610\n",
            "Iteration 41, loss = 1.57456914\n",
            "Iteration 42, loss = 1.57382654\n",
            "Iteration 43, loss = 1.57291637\n",
            "Iteration 44, loss = 1.57202336\n",
            "Iteration 45, loss = 1.57113899\n",
            "Iteration 46, loss = 1.57030841\n",
            "Iteration 47, loss = 1.56937506\n",
            "Iteration 48, loss = 1.56850528\n",
            "Iteration 49, loss = 1.56756402\n",
            "Iteration 50, loss = 1.56669570\n",
            "Iteration 51, loss = 1.56572219\n",
            "Iteration 52, loss = 1.56475963\n",
            "Iteration 53, loss = 1.56385609\n",
            "Iteration 54, loss = 1.56296453\n",
            "Iteration 55, loss = 1.56189845\n",
            "Iteration 56, loss = 1.56091216\n",
            "Iteration 57, loss = 1.55994100\n",
            "Iteration 58, loss = 1.55890604\n",
            "Iteration 59, loss = 1.55789932\n",
            "Iteration 60, loss = 1.55685750\n",
            "Iteration 61, loss = 1.55582974\n",
            "Iteration 62, loss = 1.55480301\n",
            "Iteration 63, loss = 1.55367941\n",
            "Iteration 64, loss = 1.55263197\n",
            "Iteration 65, loss = 1.55149223\n",
            "Iteration 66, loss = 1.55053425\n",
            "Iteration 67, loss = 1.54930105\n",
            "Iteration 68, loss = 1.54821087\n",
            "Iteration 69, loss = 1.54701444\n",
            "Iteration 70, loss = 1.54586029\n",
            "Iteration 71, loss = 1.54474420\n",
            "Iteration 72, loss = 1.54352041\n",
            "Iteration 73, loss = 1.54234131\n",
            "Iteration 74, loss = 1.54112545\n",
            "Iteration 75, loss = 1.53989714\n",
            "Iteration 76, loss = 1.53867986\n",
            "Iteration 77, loss = 1.53744473\n",
            "Iteration 78, loss = 1.53620364\n",
            "Iteration 79, loss = 1.53495657\n",
            "Iteration 80, loss = 1.53361613\n",
            "Iteration 81, loss = 1.53228519\n",
            "Iteration 82, loss = 1.53103451\n",
            "Iteration 83, loss = 1.52965737\n",
            "Iteration 84, loss = 1.52831302\n",
            "Iteration 85, loss = 1.52700055\n",
            "Iteration 86, loss = 1.52568801\n",
            "Iteration 87, loss = 1.52420273\n",
            "Iteration 88, loss = 1.52286452\n",
            "Iteration 89, loss = 1.52139663\n",
            "Iteration 90, loss = 1.51999680\n",
            "Iteration 91, loss = 1.51858965\n",
            "Iteration 92, loss = 1.51714660\n",
            "Iteration 93, loss = 1.51564624\n",
            "Iteration 94, loss = 1.51413377\n",
            "Iteration 95, loss = 1.51268569\n",
            "Iteration 96, loss = 1.51115268\n",
            "Iteration 97, loss = 1.50963711\n",
            "Iteration 98, loss = 1.50811582\n",
            "Iteration 99, loss = 1.50656795\n",
            "Iteration 100, loss = 1.50503860\n",
            "Iteration 1, loss = 1.63954720\n",
            "Iteration 2, loss = 1.60482523\n",
            "Iteration 3, loss = 1.59549648\n",
            "Iteration 4, loss = 1.59046599\n",
            "Iteration 5, loss = 1.58482680\n",
            "Iteration 6, loss = 1.57536925\n",
            "Iteration 7, loss = 1.56742263\n",
            "Iteration 8, loss = 1.55728702\n",
            "Iteration 9, loss = 1.54919019\n",
            "Iteration 10, loss = 1.53989939\n",
            "Iteration 11, loss = 1.53115196\n",
            "Iteration 12, loss = 1.51988213\n",
            "Iteration 13, loss = 1.50859763\n",
            "Iteration 14, loss = 1.49742986\n",
            "Iteration 15, loss = 1.48423707\n",
            "Iteration 16, loss = 1.47036936\n",
            "Iteration 17, loss = 1.45650785\n",
            "Iteration 18, loss = 1.44144490\n",
            "Iteration 19, loss = 1.42591663\n",
            "Iteration 20, loss = 1.40930346\n",
            "Iteration 21, loss = 1.39339280\n",
            "Iteration 22, loss = 1.37655343\n",
            "Iteration 23, loss = 1.35955849\n",
            "Iteration 24, loss = 1.34335025\n",
            "Iteration 25, loss = 1.32712213\n",
            "Iteration 26, loss = 1.31190447\n",
            "Iteration 27, loss = 1.29608855\n",
            "Iteration 28, loss = 1.28202337\n",
            "Iteration 29, loss = 1.26894658\n",
            "Iteration 30, loss = 1.25552189\n",
            "Iteration 31, loss = 1.24377619\n",
            "Iteration 32, loss = 1.23337846\n",
            "Iteration 33, loss = 1.22299097\n",
            "Iteration 34, loss = 1.21353757\n",
            "Iteration 35, loss = 1.20583180\n",
            "Iteration 36, loss = 1.19834603\n",
            "Iteration 37, loss = 1.19123903\n",
            "Iteration 38, loss = 1.18425576\n",
            "Iteration 39, loss = 1.17879608\n",
            "Iteration 40, loss = 1.17355104\n",
            "Iteration 41, loss = 1.16774699\n",
            "Iteration 42, loss = 1.16368344\n",
            "Iteration 43, loss = 1.15906301\n",
            "Iteration 44, loss = 1.15489488\n",
            "Iteration 45, loss = 1.15099889\n",
            "Iteration 46, loss = 1.14865044\n",
            "Iteration 47, loss = 1.14496428\n",
            "Iteration 48, loss = 1.14321312\n",
            "Iteration 49, loss = 1.13921890\n",
            "Iteration 50, loss = 1.13573844\n",
            "Iteration 51, loss = 1.13357747\n",
            "Iteration 52, loss = 1.13130409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 1.12878231\n",
            "Iteration 54, loss = 1.12832953\n",
            "Iteration 55, loss = 1.12479119\n",
            "Iteration 56, loss = 1.12268294\n",
            "Iteration 57, loss = 1.12125033\n",
            "Iteration 58, loss = 1.11909260\n",
            "Iteration 59, loss = 1.11755311\n",
            "Iteration 60, loss = 1.11531306\n",
            "Iteration 61, loss = 1.11510311\n",
            "Iteration 62, loss = 1.11309163\n",
            "Iteration 63, loss = 1.11121371\n",
            "Iteration 64, loss = 1.10979672\n",
            "Iteration 65, loss = 1.10757832\n",
            "Iteration 66, loss = 1.10671056\n",
            "Iteration 67, loss = 1.10516972\n",
            "Iteration 68, loss = 1.10445106\n",
            "Iteration 69, loss = 1.10240730\n",
            "Iteration 70, loss = 1.10138771\n",
            "Iteration 71, loss = 1.10000043\n",
            "Iteration 72, loss = 1.09832290\n",
            "Iteration 73, loss = 1.09823448\n",
            "Iteration 74, loss = 1.09618051\n",
            "Iteration 75, loss = 1.09545638\n",
            "Iteration 76, loss = 1.09402111\n",
            "Iteration 77, loss = 1.09224063\n",
            "Iteration 78, loss = 1.09149775\n",
            "Iteration 79, loss = 1.09065167\n",
            "Iteration 80, loss = 1.08904486\n",
            "Iteration 81, loss = 1.08813087\n",
            "Iteration 82, loss = 1.08649960\n",
            "Iteration 83, loss = 1.08454151\n",
            "Iteration 84, loss = 1.08411915\n",
            "Iteration 85, loss = 1.08225430\n",
            "Iteration 86, loss = 1.08404083\n",
            "Iteration 87, loss = 1.08042823\n",
            "Iteration 88, loss = 1.07782810\n",
            "Iteration 89, loss = 1.07708048\n",
            "Iteration 90, loss = 1.07646067\n",
            "Iteration 91, loss = 1.07426705\n",
            "Iteration 92, loss = 1.07498754\n",
            "Iteration 93, loss = 1.07221570\n",
            "Iteration 94, loss = 1.06963427\n",
            "Iteration 95, loss = 1.06842999\n",
            "Iteration 96, loss = 1.06765895\n",
            "Iteration 97, loss = 1.06557469\n",
            "Iteration 98, loss = 1.06551513\n",
            "Iteration 99, loss = 1.06270053\n",
            "Iteration 100, loss = 1.06168174\n",
            "Iteration 1, loss = 1.77006957\n",
            "Iteration 2, loss = 1.60643629\n",
            "Iteration 3, loss = 1.56887945"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4, loss = 1.49297675\n",
            "Iteration 5, loss = 1.39683027\n",
            "Iteration 6, loss = 1.30393920\n",
            "Iteration 7, loss = 1.25644917\n",
            "Iteration 8, loss = 1.20346690\n",
            "Iteration 9, loss = 1.16506599\n",
            "Iteration 10, loss = 1.14972924\n",
            "Iteration 11, loss = 1.14831508\n",
            "Iteration 12, loss = 1.13449308\n",
            "Iteration 13, loss = 1.12494709\n",
            "Iteration 14, loss = 1.11672519\n",
            "Iteration 15, loss = 1.12146206\n",
            "Iteration 16, loss = 1.10734396\n",
            "Iteration 17, loss = 1.11322788\n",
            "Iteration 18, loss = 1.10752243\n",
            "Iteration 19, loss = 1.11043734\n",
            "Iteration 20, loss = 1.10547776\n",
            "Iteration 21, loss = 1.10115169\n",
            "Iteration 22, loss = 1.09902342\n",
            "Iteration 23, loss = 1.09282024\n",
            "Iteration 24, loss = 1.09302164\n",
            "Iteration 25, loss = 1.09227566\n",
            "Iteration 26, loss = 1.10211843\n",
            "Iteration 27, loss = 1.09623725\n",
            "Iteration 28, loss = 1.07240655\n",
            "Iteration 29, loss = 1.08368359\n",
            "Iteration 30, loss = 1.07828560\n",
            "Iteration 31, loss = 1.06670866\n",
            "Iteration 32, loss = 1.07069511\n",
            "Iteration 33, loss = 1.08136490\n",
            "Iteration 34, loss = 1.08576779\n",
            "Iteration 35, loss = 1.07181339\n",
            "Iteration 36, loss = 1.04127882\n",
            "Iteration 37, loss = 1.05402963\n",
            "Iteration 38, loss = 1.04432648\n",
            "Iteration 39, loss = 1.02791461\n",
            "Iteration 40, loss = 1.04663033\n",
            "Iteration 41, loss = 1.01122123\n",
            "Iteration 42, loss = 1.00492476\n",
            "Iteration 43, loss = 1.00519013\n",
            "Iteration 44, loss = 0.97462843\n",
            "Iteration 45, loss = 0.98029097\n",
            "Iteration 46, loss = 0.95685197\n",
            "Iteration 47, loss = 0.95995427\n",
            "Iteration 48, loss = 0.96682458\n",
            "Iteration 49, loss = 0.95086208\n",
            "Iteration 50, loss = 0.97963249\n",
            "Iteration 51, loss = 0.99135222\n",
            "Iteration 52, loss = 0.93966143\n",
            "Iteration 53, loss = 0.92756504\n",
            "Iteration 54, loss = 0.93577101\n",
            "Iteration 55, loss = 1.03139493\n",
            "Iteration 56, loss = 1.01507313\n",
            "Iteration 57, loss = 0.95482494\n",
            "Iteration 58, loss = 0.94421648\n",
            "Iteration 59, loss = 0.97105595\n",
            "Iteration 60, loss = 0.92975474\n",
            "Iteration 61, loss = 0.88888358\n",
            "Iteration 62, loss = 0.88163785\n",
            "Iteration 63, loss = 0.91311230\n",
            "Iteration 64, loss = 0.92497933\n",
            "Iteration 65, loss = 0.88715756\n",
            "Iteration 66, loss = 0.87808049\n",
            "Iteration 67, loss = 0.88420023\n",
            "Iteration 68, loss = 0.88331337\n",
            "Iteration 69, loss = 0.88604770\n",
            "Iteration 70, loss = 0.88545588\n",
            "Iteration 71, loss = 0.86704168\n",
            "Iteration 72, loss = 0.86053920\n",
            "Iteration 73, loss = 0.84527834\n",
            "Iteration 74, loss = 0.82472925\n",
            "Iteration 75, loss = 0.83443156\n",
            "Iteration 76, loss = 0.82375897\n",
            "Iteration 77, loss = 0.81983511\n",
            "Iteration 78, loss = 0.82861357\n",
            "Iteration 79, loss = 0.81826715\n",
            "Iteration 80, loss = 0.85097622\n",
            "Iteration 81, loss = 0.84148789\n",
            "Iteration 82, loss = 0.84988506\n",
            "Iteration 83, loss = 0.97355400\n",
            "Iteration 84, loss = 0.89428393\n",
            "Iteration 85, loss = 0.88093130\n",
            "Iteration 86, loss = 0.97207657\n",
            "Iteration 87, loss = 0.90410103\n",
            "Iteration 88, loss = 0.93569189\n",
            "Iteration 89, loss = 0.90514925\n",
            "Iteration 90, loss = 0.87187618\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.62323273\n",
            "Iteration 2, loss = 1.62019609\n",
            "Iteration 3, loss = 1.61756490\n",
            "Iteration 4, loss = 1.61509050\n",
            "Iteration 5, loss = 1.61255632\n",
            "Iteration 6, loss = 1.61076924\n",
            "Iteration 7, loss = 1.60865817\n",
            "Iteration 8, loss = 1.60709708\n",
            "Iteration 9, loss = 1.60522850\n",
            "Iteration 10, loss = 1.60386911\n",
            "Iteration 11, loss = 1.60260223\n",
            "Iteration 12, loss = 1.60132920\n",
            "Iteration 13, loss = 1.60025263\n",
            "Iteration 14, loss = 1.59942664\n",
            "Iteration 15, loss = 1.59839002\n",
            "Iteration 16, loss = 1.59754385\n",
            "Iteration 17, loss = 1.59696703\n",
            "Iteration 18, loss = 1.59622633\n",
            "Iteration 19, loss = 1.59549671\n",
            "Iteration 20, loss = 1.59503589\n",
            "Iteration 21, loss = 1.59438141\n",
            "Iteration 22, loss = 1.59398960\n",
            "Iteration 23, loss = 1.59343907\n",
            "Iteration 24, loss = 1.59297667\n",
            "Iteration 25, loss = 1.59254795\n",
            "Iteration 26, loss = 1.59218711\n",
            "Iteration 27, loss = 1.59165890\n",
            "Iteration 28, loss = 1.59126384\n",
            "Iteration 29, loss = 1.59086001\n",
            "Iteration 30, loss = 1.59043496\n",
            "Iteration 31, loss = 1.59002865\n",
            "Iteration 32, loss = 1.58964546\n",
            "Iteration 33, loss = 1.58920271\n",
            "Iteration 34, loss = 1.58877769\n",
            "Iteration 35, loss = 1.58838146\n",
            "Iteration 36, loss = 1.58795803\n",
            "Iteration 37, loss = 1.58753238\n",
            "Iteration 38, loss = 1.58712762\n",
            "Iteration 39, loss = 1.58666415\n",
            "Iteration 40, loss = 1.58625333\n",
            "Iteration 41, loss = 1.58577583\n",
            "Iteration 42, loss = 1.58538937\n",
            "Iteration 43, loss = 1.58491465\n",
            "Iteration 44, loss = 1.58444396\n",
            "Iteration 45, loss = 1.58396081\n",
            "Iteration 46, loss = 1.58352436\n",
            "Iteration 47, loss = 1.58301746\n",
            "Iteration 48, loss = 1.58255324\n",
            "Iteration 49, loss = 1.58204359\n",
            "Iteration 50, loss = 1.58157864\n",
            "Iteration 51, loss = 1.58104398\n",
            "Iteration 52, loss = 1.58051870\n",
            "Iteration 53, loss = 1.58002927\n",
            "Iteration 54, loss = 1.57954933\n",
            "Iteration 55, loss = 1.57895040\n",
            "Iteration 56, loss = 1.57841189\n",
            "Iteration 57, loss = 1.57787739\n",
            "Iteration 58, loss = 1.57730103\n",
            "Iteration 59, loss = 1.57673716\n",
            "Iteration 60, loss = 1.57615340\n",
            "Iteration 61, loss = 1.57558071\n",
            "Iteration 62, loss = 1.57500520\n",
            "Iteration 63, loss = 1.57436741\n",
            "Iteration 64, loss = 1.57377181\n",
            "Iteration 65, loss = 1.57313128\n",
            "Iteration 66, loss = 1.57258569\n",
            "Iteration 67, loss = 1.57188001\n",
            "Iteration 68, loss = 1.57125307\n",
            "Iteration 69, loss = 1.57056977\n",
            "Iteration 70, loss = 1.56990118\n",
            "Iteration 71, loss = 1.56926280\n",
            "Iteration 72, loss = 1.56854972\n",
            "Iteration 73, loss = 1.56786904\n",
            "Iteration 74, loss = 1.56715659\n",
            "Iteration 75, loss = 1.56644674\n",
            "Iteration 76, loss = 1.56572159\n",
            "Iteration 77, loss = 1.56499459\n",
            "Iteration 78, loss = 1.56426226\n",
            "Iteration 79, loss = 1.56355010\n",
            "Iteration 80, loss = 1.56275082\n",
            "Iteration 81, loss = 1.56195728\n",
            "Iteration 82, loss = 1.56121114\n",
            "Iteration 83, loss = 1.56039563\n",
            "Iteration 84, loss = 1.55958974\n",
            "Iteration 85, loss = 1.55880477\n",
            "Iteration 86, loss = 1.55801548\n",
            "Iteration 87, loss = 1.55712578\n",
            "Iteration 88, loss = 1.55631789\n",
            "Iteration 89, loss = 1.55543377\n",
            "Iteration 90, loss = 1.55457722\n",
            "Iteration 91, loss = 1.55372902\n",
            "Iteration 92, loss = 1.55283166\n",
            "Iteration 93, loss = 1.55192592\n",
            "Iteration 94, loss = 1.55099527\n",
            "Iteration 95, loss = 1.55010845\n",
            "Iteration 96, loss = 1.54916183\n",
            "Iteration 97, loss = 1.54822125\n",
            "Iteration 98, loss = 1.54728565\n",
            "Iteration 99, loss = 1.54631299\n",
            "Iteration 100, loss = 1.54535902\n",
            "Iteration 1, loss = 1.61899511\n",
            "Iteration 2, loss = 1.60132797\n",
            "Iteration 3, loss = 1.59604517\n",
            "Iteration 4, loss = 1.59390197\n",
            "Iteration 5, loss = 1.59022876\n",
            "Iteration 6, loss = 1.58517890\n",
            "Iteration 7, loss = 1.58055035\n",
            "Iteration 8, loss = 1.57493224\n",
            "Iteration 9, loss = 1.57091811\n",
            "Iteration 10, loss = 1.56580459\n",
            "Iteration 11, loss = 1.56043960\n",
            "Iteration 12, loss = 1.55379859\n",
            "Iteration 13, loss = 1.54675724"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 14, loss = 1.53957713\n",
            "Iteration 15, loss = 1.53096462\n",
            "Iteration 16, loss = 1.52204550\n",
            "Iteration 17, loss = 1.51316678\n",
            "Iteration 18, loss = 1.50288808\n",
            "Iteration 19, loss = 1.49189841\n",
            "Iteration 20, loss = 1.47999594\n",
            "Iteration 21, loss = 1.46796255\n",
            "Iteration 22, loss = 1.45492835\n",
            "Iteration 23, loss = 1.44144830\n",
            "Iteration 24, loss = 1.42813069\n",
            "Iteration 25, loss = 1.41372158\n",
            "Iteration 26, loss = 1.39986689\n",
            "Iteration 27, loss = 1.38469134\n",
            "Iteration 28, loss = 1.37029322\n",
            "Iteration 29, loss = 1.35681084\n",
            "Iteration 30, loss = 1.34161785\n",
            "Iteration 31, loss = 1.32819857\n",
            "Iteration 32, loss = 1.31543977\n",
            "Iteration 33, loss = 1.30262318\n",
            "Iteration 34, loss = 1.29003638\n",
            "Iteration 35, loss = 1.27916062\n",
            "Iteration 36, loss = 1.26823004\n",
            "Iteration 37, loss = 1.25813496\n",
            "Iteration 38, loss = 1.24821075\n",
            "Iteration 39, loss = 1.23953489\n",
            "Iteration 40, loss = 1.23152624\n",
            "Iteration 41, loss = 1.22325735\n",
            "Iteration 42, loss = 1.21654343\n",
            "Iteration 43, loss = 1.20972762\n",
            "Iteration 44, loss = 1.20364295\n",
            "Iteration 45, loss = 1.19780806\n",
            "Iteration 46, loss = 1.19309433\n",
            "Iteration 47, loss = 1.18794598\n",
            "Iteration 48, loss = 1.18397520\n",
            "Iteration 49, loss = 1.17890298\n",
            "Iteration 50, loss = 1.17460817\n",
            "Iteration 51, loss = 1.17099752\n",
            "Iteration 52, loss = 1.16738359\n",
            "Iteration 53, loss = 1.16399913\n",
            "Iteration 54, loss = 1.16214040\n",
            "Iteration 55, loss = 1.15818526\n",
            "Iteration 56, loss = 1.15562576\n",
            "Iteration 57, loss = 1.15309799\n",
            "Iteration 58, loss = 1.15033299\n",
            "Iteration 59, loss = 1.14814400\n",
            "Iteration 60, loss = 1.14551134\n",
            "Iteration 61, loss = 1.14453673\n",
            "Iteration 62, loss = 1.14206028\n",
            "Iteration 63, loss = 1.13997971\n",
            "Iteration 64, loss = 1.13803088\n",
            "Iteration 65, loss = 1.13580740\n",
            "Iteration 66, loss = 1.13440667\n",
            "Iteration 67, loss = 1.13284215\n",
            "Iteration 68, loss = 1.13192154\n",
            "Iteration 69, loss = 1.12969783\n",
            "Iteration 70, loss = 1.12826442\n",
            "Iteration 71, loss = 1.12700449\n",
            "Iteration 72, loss = 1.12548097\n",
            "Iteration 73, loss = 1.12469302\n",
            "Iteration 74, loss = 1.12310042\n",
            "Iteration 75, loss = 1.12222389\n",
            "Iteration 76, loss = 1.12085987\n",
            "Iteration 77, loss = 1.11961949\n",
            "Iteration 78, loss = 1.11881340\n",
            "Iteration 79, loss = 1.11799589\n",
            "Iteration 80, loss = 1.11675340\n",
            "Iteration 81, loss = 1.11574582\n",
            "Iteration 82, loss = 1.11457482\n",
            "Iteration 83, loss = 1.11328151\n",
            "Iteration 84, loss = 1.11279425\n",
            "Iteration 85, loss = 1.11139215\n",
            "Iteration 86, loss = 1.11165709\n",
            "Iteration 87, loss = 1.11010276\n",
            "Iteration 88, loss = 1.10856138\n",
            "Iteration 89, loss = 1.10724267\n",
            "Iteration 90, loss = 1.10694439\n",
            "Iteration 91, loss = 1.10614769\n",
            "Iteration 92, loss = 1.10521839\n",
            "Iteration 93, loss = 1.10378841\n",
            "Iteration 94, loss = 1.10284801\n",
            "Iteration 95, loss = 1.10161533\n",
            "Iteration 96, loss = 1.10129724\n",
            "Iteration 97, loss = 1.10070822\n",
            "Iteration 98, loss = 1.10010035\n",
            "Iteration 99, loss = 1.09841559\n",
            "Iteration 100, loss = 1.09836852\n",
            "Iteration 1, loss = 1.67216845\n",
            "Iteration 2, loss = 1.59789772\n",
            "Iteration 3, loss = 1.57989980\n",
            "Iteration 4, loss = 1.56088289\n",
            "Iteration 5, loss = 1.52130741\n",
            "Iteration 6, loss = 1.47263855\n",
            "Iteration 7, loss = 1.41893093\n",
            "Iteration 8, loss = 1.36787241\n",
            "Iteration 9, loss = 1.31232391\n",
            "Iteration 10, loss = 1.26411989\n",
            "Iteration 11, loss = 1.22624174\n",
            "Iteration 12, loss = 1.19140159\n",
            "Iteration 13, loss = 1.16976008\n",
            "Iteration 14, loss = 1.15356834\n",
            "Iteration 15, loss = 1.14543723\n",
            "Iteration 16, loss = 1.13149511\n",
            "Iteration 17, loss = 1.13199767\n",
            "Iteration 18, loss = 1.12427642\n",
            "Iteration 19, loss = 1.12211682\n",
            "Iteration 20, loss = 1.12468116\n",
            "Iteration 21, loss = 1.11573571\n",
            "Iteration 22, loss = 1.11662490\n",
            "Iteration 23, loss = 1.11597583\n",
            "Iteration 24, loss = 1.11438116\n",
            "Iteration 25, loss = 1.11248358\n",
            "Iteration 26, loss = 1.12564137\n",
            "Iteration 27, loss = 1.12105736\n",
            "Iteration 28, loss = 1.11010117\n",
            "Iteration 29, loss = 1.11979185\n",
            "Iteration 30, loss = 1.12755099\n",
            "Iteration 31, loss = 1.11271114\n",
            "Iteration 32, loss = 1.11638135\n",
            "Iteration 33, loss = 1.10185037\n",
            "Iteration 34, loss = 1.10710238\n",
            "Iteration 35, loss = 1.09964660\n",
            "Iteration 36, loss = 1.10356628\n",
            "Iteration 37, loss = 1.09537532\n",
            "Iteration 38, loss = 1.09843579\n",
            "Iteration 39, loss = 1.09201711\n",
            "Iteration 40, loss = 1.09436223\n",
            "Iteration 41, loss = 1.08897591\n",
            "Iteration 42, loss = 1.09030953\n",
            "Iteration 43, loss = 1.08751124\n",
            "Iteration 44, loss = 1.08540278\n",
            "Iteration 45, loss = 1.08515137\n",
            "Iteration 46, loss = 1.08609425\n",
            "Iteration 47, loss = 1.07987873\n",
            "Iteration 48, loss = 1.08452062\n",
            "Iteration 49, loss = 1.07745062\n",
            "Iteration 50, loss = 1.07779888\n",
            "Iteration 51, loss = 1.07110469\n",
            "Iteration 52, loss = 1.06577272\n",
            "Iteration 53, loss = 1.06489061\n",
            "Iteration 54, loss = 1.07577822\n",
            "Iteration 55, loss = 1.06032473\n",
            "Iteration 56, loss = 1.05831104\n",
            "Iteration 57, loss = 1.05530886\n",
            "Iteration 58, loss = 1.04593943\n",
            "Iteration 59, loss = 1.04540423\n",
            "Iteration 60, loss = 1.03873025\n",
            "Iteration 61, loss = 1.04071104\n",
            "Iteration 62, loss = 1.04299296\n",
            "Iteration 63, loss = 1.04085503\n",
            "Iteration 64, loss = 1.01791707\n",
            "Iteration 65, loss = 1.02363316\n",
            "Iteration 66, loss = 1.02480117\n",
            "Iteration 67, loss = 1.01209754\n",
            "Iteration 68, loss = 1.02458665"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 69, loss = 1.00797740\n",
            "Iteration 70, loss = 0.99593351\n",
            "Iteration 71, loss = 0.99859118\n",
            "Iteration 72, loss = 0.99456752\n",
            "Iteration 73, loss = 0.99351901\n",
            "Iteration 74, loss = 0.97250589\n",
            "Iteration 75, loss = 0.96681651\n",
            "Iteration 76, loss = 0.98116002\n",
            "Iteration 77, loss = 0.95861380\n",
            "Iteration 78, loss = 0.95641969\n",
            "Iteration 79, loss = 0.94374799\n",
            "Iteration 80, loss = 0.93810450\n",
            "Iteration 81, loss = 0.93641564\n",
            "Iteration 82, loss = 0.94020480\n",
            "Iteration 83, loss = 0.93835817\n",
            "Iteration 84, loss = 0.93278478\n",
            "Iteration 85, loss = 0.93122421\n",
            "Iteration 86, loss = 0.91688569\n",
            "Iteration 87, loss = 0.89806740\n",
            "Iteration 88, loss = 0.89833297\n",
            "Iteration 89, loss = 0.89659280\n",
            "Iteration 90, loss = 0.89207212\n",
            "Iteration 91, loss = 0.88596063\n",
            "Iteration 92, loss = 0.89962844\n",
            "Iteration 93, loss = 0.91582701\n",
            "Iteration 94, loss = 0.91857361\n",
            "Iteration 95, loss = 0.91062230\n",
            "Iteration 96, loss = 0.88863000\n",
            "Iteration 97, loss = 0.87404589\n",
            "Iteration 98, loss = 0.87198578\n",
            "Iteration 99, loss = 0.85424254\n",
            "Iteration 100, loss = 0.86398652\n",
            "Iteration 1, loss = 1.70453668\n",
            "Iteration 2, loss = 1.69475188\n",
            "Iteration 3, loss = 1.68586695\n",
            "Iteration 4, loss = 1.67800634\n",
            "Iteration 5, loss = 1.67015440\n",
            "Iteration 6, loss = 1.66295623\n",
            "Iteration 7, loss = 1.65610281\n",
            "Iteration 8, loss = 1.65004674\n",
            "Iteration 9, loss = 1.64481048\n",
            "Iteration 10, loss = 1.63980642\n",
            "Iteration 11, loss = 1.63505260\n",
            "Iteration 12, loss = 1.63059314\n",
            "Iteration 13, loss = 1.62665533\n",
            "Iteration 14, loss = 1.62325838\n",
            "Iteration 15, loss = 1.62012817\n",
            "Iteration 16, loss = 1.61716916\n",
            "Iteration 17, loss = 1.61471073\n",
            "Iteration 18, loss = 1.61189341\n",
            "Iteration 19, loss = 1.60920767\n",
            "Iteration 20, loss = 1.60753938\n",
            "Iteration 21, loss = 1.60565133\n",
            "Iteration 22, loss = 1.60363355\n",
            "Iteration 23, loss = 1.60217978\n",
            "Iteration 24, loss = 1.60044896\n",
            "Iteration 25, loss = 1.59928393\n",
            "Iteration 26, loss = 1.59793832\n",
            "Iteration 27, loss = 1.59659818\n",
            "Iteration 28, loss = 1.59556765\n",
            "Iteration 29, loss = 1.59445904\n",
            "Iteration 30, loss = 1.59346061\n",
            "Iteration 31, loss = 1.59262363\n",
            "Iteration 32, loss = 1.59184781\n",
            "Iteration 33, loss = 1.59085525\n",
            "Iteration 34, loss = 1.59000430\n",
            "Iteration 35, loss = 1.58927781\n",
            "Iteration 36, loss = 1.58852014\n",
            "Iteration 37, loss = 1.58771273\n",
            "Iteration 38, loss = 1.58699414\n",
            "Iteration 39, loss = 1.58630231\n",
            "Iteration 40, loss = 1.58558011\n",
            "Iteration 41, loss = 1.58491121\n",
            "Iteration 42, loss = 1.58434375\n",
            "Iteration 43, loss = 1.58348892\n",
            "Iteration 44, loss = 1.58284942\n",
            "Iteration 45, loss = 1.58213897\n",
            "Iteration 46, loss = 1.58141004\n",
            "Iteration 47, loss = 1.58072919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 48, loss = 1.58000508\n",
            "Iteration 49, loss = 1.57931091\n",
            "Iteration 50, loss = 1.57863226\n",
            "Iteration 51, loss = 1.57786536\n",
            "Iteration 52, loss = 1.57718583\n",
            "Iteration 53, loss = 1.57639926\n",
            "Iteration 54, loss = 1.57564152\n",
            "Iteration 55, loss = 1.57489389\n",
            "Iteration 56, loss = 1.57416570\n",
            "Iteration 57, loss = 1.57334149\n",
            "Iteration 58, loss = 1.57259974\n",
            "Iteration 59, loss = 1.57178124\n",
            "Iteration 60, loss = 1.57099250\n",
            "Iteration 61, loss = 1.57015780\n",
            "Iteration 62, loss = 1.56933969\n",
            "Iteration 63, loss = 1.56850380\n",
            "Iteration 64, loss = 1.56766231\n",
            "Iteration 65, loss = 1.56677923\n",
            "Iteration 66, loss = 1.56597320\n",
            "Iteration 67, loss = 1.56507020\n",
            "Iteration 68, loss = 1.56420448\n",
            "Iteration 69, loss = 1.56327656\n",
            "Iteration 70, loss = 1.56237745\n",
            "Iteration 71, loss = 1.56146960\n",
            "Iteration 72, loss = 1.56053182\n",
            "Iteration 73, loss = 1.55956536\n",
            "Iteration 74, loss = 1.55865327\n",
            "Iteration 75, loss = 1.55766846\n",
            "Iteration 76, loss = 1.55674730\n",
            "Iteration 77, loss = 1.55566830\n",
            "Iteration 78, loss = 1.55472385\n",
            "Iteration 79, loss = 1.55381898\n",
            "Iteration 80, loss = 1.55267406\n",
            "Iteration 81, loss = 1.55167516\n",
            "Iteration 82, loss = 1.55068252\n",
            "Iteration 83, loss = 1.54952484\n",
            "Iteration 84, loss = 1.54848785\n",
            "Iteration 85, loss = 1.54738143\n",
            "Iteration 86, loss = 1.54629782\n",
            "Iteration 87, loss = 1.54517287\n",
            "Iteration 88, loss = 1.54408866\n",
            "Iteration 89, loss = 1.54299616\n",
            "Iteration 90, loss = 1.54177149\n",
            "Iteration 91, loss = 1.54068673\n",
            "Iteration 92, loss = 1.53941457\n",
            "Iteration 93, loss = 1.53824974\n",
            "Iteration 94, loss = 1.53705931\n",
            "Iteration 95, loss = 1.53583135\n",
            "Iteration 96, loss = 1.53475665\n",
            "Iteration 97, loss = 1.53335636\n",
            "Iteration 98, loss = 1.53209600\n",
            "Iteration 99, loss = 1.53083644\n",
            "Iteration 100, loss = 1.52954205\n",
            "Iteration 1, loss = 1.68588094\n",
            "Iteration 2, loss = 1.62430266\n",
            "Iteration 3, loss = 1.60387842\n",
            "Iteration 4, loss = 1.59709075\n",
            "Iteration 5, loss = 1.59641956\n",
            "Iteration 6, loss = 1.59325984\n",
            "Iteration 7, loss = 1.58783748\n",
            "Iteration 8, loss = 1.57677746\n",
            "Iteration 9, loss = 1.56538870\n",
            "Iteration 10, loss = 1.55578979\n",
            "Iteration 11, loss = 1.54739337\n",
            "Iteration 12, loss = 1.54021368\n",
            "Iteration 13, loss = 1.53127479\n",
            "Iteration 14, loss = 1.52035769\n",
            "Iteration 15, loss = 1.50867591\n",
            "Iteration 16, loss = 1.49503340\n",
            "Iteration 17, loss = 1.48157784\n",
            "Iteration 18, loss = 1.46596097\n",
            "Iteration 19, loss = 1.45086970\n",
            "Iteration 20, loss = 1.43487875\n",
            "Iteration 21, loss = 1.41844924\n",
            "Iteration 22, loss = 1.40158588\n",
            "Iteration 23, loss = 1.38457661\n",
            "Iteration 24, loss = 1.36731508\n",
            "Iteration 25, loss = 1.35112029\n",
            "Iteration 26, loss = 1.33386383\n",
            "Iteration 27, loss = 1.31738497\n",
            "Iteration 28, loss = 1.30261931\n",
            "Iteration 29, loss = 1.28848421\n",
            "Iteration 30, loss = 1.27452354\n",
            "Iteration 31, loss = 1.26191129\n",
            "Iteration 32, loss = 1.25070495\n",
            "Iteration 33, loss = 1.23954145\n",
            "Iteration 34, loss = 1.22899363\n",
            "Iteration 35, loss = 1.22102304"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 36, loss = 1.21293051\n",
            "Iteration 37, loss = 1.20412429\n",
            "Iteration 38, loss = 1.19689574\n",
            "Iteration 39, loss = 1.19089976\n",
            "Iteration 40, loss = 1.18503340\n",
            "Iteration 41, loss = 1.17932471\n",
            "Iteration 42, loss = 1.17578492\n",
            "Iteration 43, loss = 1.17010352\n",
            "Iteration 44, loss = 1.16596074\n",
            "Iteration 45, loss = 1.16171655\n",
            "Iteration 46, loss = 1.15808892\n",
            "Iteration 47, loss = 1.15463075\n",
            "Iteration 48, loss = 1.15116566\n",
            "Iteration 49, loss = 1.14752129\n",
            "Iteration 50, loss = 1.14537079\n",
            "Iteration 51, loss = 1.14247053\n",
            "Iteration 52, loss = 1.14024451\n",
            "Iteration 53, loss = 1.13704893\n",
            "Iteration 54, loss = 1.13604660\n",
            "Iteration 55, loss = 1.13298250\n",
            "Iteration 56, loss = 1.13146777\n",
            "Iteration 57, loss = 1.12973967\n",
            "Iteration 58, loss = 1.12925929\n",
            "Iteration 59, loss = 1.12545015\n",
            "Iteration 60, loss = 1.12369744\n",
            "Iteration 61, loss = 1.12295979\n",
            "Iteration 62, loss = 1.12069490\n",
            "Iteration 63, loss = 1.11874708\n",
            "Iteration 64, loss = 1.11777190\n",
            "Iteration 65, loss = 1.11624595\n",
            "Iteration 66, loss = 1.11528243\n",
            "Iteration 67, loss = 1.11391024\n",
            "Iteration 68, loss = 1.11273797\n",
            "Iteration 69, loss = 1.11102080\n",
            "Iteration 70, loss = 1.11014072\n",
            "Iteration 71, loss = 1.10939849\n",
            "Iteration 72, loss = 1.10853243\n",
            "Iteration 73, loss = 1.10691693\n",
            "Iteration 74, loss = 1.10614090\n",
            "Iteration 75, loss = 1.10455971\n",
            "Iteration 76, loss = 1.10479352\n",
            "Iteration 77, loss = 1.10305498\n",
            "Iteration 78, loss = 1.10190168\n",
            "Iteration 79, loss = 1.10149500\n",
            "Iteration 80, loss = 1.10028906\n",
            "Iteration 81, loss = 1.09874766\n",
            "Iteration 82, loss = 1.09903022\n",
            "Iteration 83, loss = 1.09732270\n",
            "Iteration 84, loss = 1.09736412\n",
            "Iteration 85, loss = 1.09564503\n",
            "Iteration 86, loss = 1.09482943\n",
            "Iteration 87, loss = 1.09377226\n",
            "Iteration 88, loss = 1.09393498\n",
            "Iteration 89, loss = 1.09300829\n",
            "Iteration 90, loss = 1.09066090\n",
            "Iteration 91, loss = 1.08965449\n",
            "Iteration 92, loss = 1.08875840\n",
            "Iteration 93, loss = 1.08844155\n",
            "Iteration 94, loss = 1.08716748\n",
            "Iteration 95, loss = 1.08718752\n",
            "Iteration 96, loss = 1.08797663\n",
            "Iteration 97, loss = 1.08361329\n",
            "Iteration 98, loss = 1.08505835\n",
            "Iteration 99, loss = 1.08403753\n",
            "Iteration 100, loss = 1.08126736\n",
            "Iteration 1, loss = 1.72884238\n",
            "Iteration 2, loss = 1.64074750\n",
            "Iteration 3, loss = 1.56645291\n",
            "Iteration 4, loss = 1.52060405\n",
            "Iteration 5, loss = 1.43872559\n",
            "Iteration 6, loss = 1.36337355\n",
            "Iteration 7, loss = 1.28239082\n",
            "Iteration 8, loss = 1.22135314\n",
            "Iteration 9, loss = 1.17686588\n",
            "Iteration 10, loss = 1.14793639\n",
            "Iteration 11, loss = 1.13288341\n",
            "Iteration 12, loss = 1.12181362\n",
            "Iteration 13, loss = 1.12922082\n",
            "Iteration 14, loss = 1.11192996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 1.11615186\n",
            "Iteration 16, loss = 1.10945836\n",
            "Iteration 17, loss = 1.11210050\n",
            "Iteration 18, loss = 1.10753886\n",
            "Iteration 19, loss = 1.12538389\n",
            "Iteration 20, loss = 1.11342773\n",
            "Iteration 21, loss = 1.12212185\n",
            "Iteration 22, loss = 1.13164673\n",
            "Iteration 23, loss = 1.13627034\n",
            "Iteration 24, loss = 1.10349376\n",
            "Iteration 25, loss = 1.09745600\n",
            "Iteration 26, loss = 1.10257314\n",
            "Iteration 27, loss = 1.08704517\n",
            "Iteration 28, loss = 1.09230077\n",
            "Iteration 29, loss = 1.07891892\n",
            "Iteration 30, loss = 1.07073558\n",
            "Iteration 31, loss = 1.05931036\n",
            "Iteration 32, loss = 1.05516170\n",
            "Iteration 33, loss = 1.05183950\n",
            "Iteration 34, loss = 1.05516296\n",
            "Iteration 35, loss = 1.04743990\n",
            "Iteration 36, loss = 1.06206502\n",
            "Iteration 37, loss = 1.05615770\n",
            "Iteration 38, loss = 1.04556429\n",
            "Iteration 39, loss = 1.00384176\n",
            "Iteration 40, loss = 1.04693756\n",
            "Iteration 41, loss = 1.02138422\n",
            "Iteration 42, loss = 0.98591893\n",
            "Iteration 43, loss = 1.03592576\n",
            "Iteration 44, loss = 1.04610742\n",
            "Iteration 45, loss = 1.01898175\n",
            "Iteration 46, loss = 1.00361442\n",
            "Iteration 47, loss = 0.98417335\n",
            "Iteration 48, loss = 0.96605351\n",
            "Iteration 49, loss = 0.96845937\n",
            "Iteration 50, loss = 0.96185401\n",
            "Iteration 51, loss = 0.94255398\n",
            "Iteration 52, loss = 0.93061490\n",
            "Iteration 53, loss = 0.92673668\n",
            "Iteration 54, loss = 0.91390610\n",
            "Iteration 55, loss = 0.91071740\n",
            "Iteration 56, loss = 0.90962481\n",
            "Iteration 57, loss = 0.93299097\n",
            "Iteration 58, loss = 0.93344151\n",
            "Iteration 59, loss = 0.92209692\n",
            "Iteration 60, loss = 0.93989183\n",
            "Iteration 61, loss = 0.92441269\n",
            "Iteration 62, loss = 0.89193977\n",
            "Iteration 63, loss = 1.03722172\n",
            "Iteration 64, loss = 0.93597199\n",
            "Iteration 65, loss = 0.93853348\n",
            "Iteration 66, loss = 0.99029934\n",
            "Iteration 67, loss = 0.90421488\n",
            "Iteration 68, loss = 0.95950041\n",
            "Iteration 69, loss = 0.88259926\n",
            "Iteration 70, loss = 0.90121873\n",
            "Iteration 71, loss = 0.92564471\n",
            "Iteration 72, loss = 0.91905633\n",
            "Iteration 73, loss = 0.87391122\n",
            "Iteration 74, loss = 0.95634026\n",
            "Iteration 75, loss = 0.89504586\n",
            "Iteration 76, loss = 0.86771004\n",
            "Iteration 77, loss = 0.88612293\n",
            "Iteration 78, loss = 0.89256599\n",
            "Iteration 79, loss = 0.84595190\n",
            "Iteration 80, loss = 0.86979397\n",
            "Iteration 81, loss = 0.82654521\n",
            "Iteration 82, loss = 0.83287079\n",
            "Iteration 83, loss = 0.83365618\n",
            "Iteration 84, loss = 0.82412773\n",
            "Iteration 85, loss = 0.83022107\n",
            "Iteration 86, loss = 0.82427156\n",
            "Iteration 87, loss = 0.81252103\n",
            "Iteration 88, loss = 0.81384471\n",
            "Iteration 89, loss = 0.83537720\n",
            "Iteration 90, loss = 0.81072087\n",
            "Iteration 91, loss = 0.82751535\n",
            "Iteration 92, loss = 0.80946999\n",
            "Iteration 93, loss = 0.78381044\n",
            "Iteration 94, loss = 0.79820874\n",
            "Iteration 95, loss = 0.81260160\n",
            "Iteration 96, loss = 0.80894910\n",
            "Iteration 97, loss = 0.78718942\n",
            "Iteration 98, loss = 0.82540470\n",
            "Iteration 99, loss = 0.78620852\n",
            "Iteration 100, loss = 0.82585936\n",
            "Iteration 1, loss = 1.61156066\n",
            "Iteration 2, loss = 1.60800271\n",
            "Iteration 3, loss = 1.60555858\n",
            "Iteration 4, loss = 1.60325626\n",
            "Iteration 5, loss = 1.60172171\n",
            "Iteration 6, loss = 1.60045333\n",
            "Iteration 7, loss = 1.59960461\n",
            "Iteration 8, loss = 1.59936740\n",
            "Iteration 9, loss = 1.59932183\n",
            "Iteration 10, loss = 1.59912892\n",
            "Iteration 11, loss = 1.59888510\n",
            "Iteration 12, loss = 1.59891151\n",
            "Iteration 13, loss = 1.59884616\n",
            "Iteration 14, loss = 1.59873394\n",
            "Iteration 15, loss = 1.59872786\n",
            "Iteration 16, loss = 1.59860817\n",
            "Iteration 17, loss = 1.59856218\n",
            "Iteration 18, loss = 1.59843054\n",
            "Iteration 19, loss = 1.59839709\n",
            "Iteration 20, loss = 1.59809187\n",
            "Iteration 21, loss = 1.59802275\n",
            "Iteration 22, loss = 1.59788604\n",
            "Iteration 23, loss = 1.59784962\n",
            "Iteration 24, loss = 1.59773048\n",
            "Iteration 25, loss = 1.59779417\n",
            "Iteration 26, loss = 1.59760039\n",
            "Iteration 27, loss = 1.59747565\n",
            "Iteration 28, loss = 1.59743418\n",
            "Iteration 29, loss = 1.59738240\n",
            "Iteration 30, loss = 1.59728313\n",
            "Iteration 31, loss = 1.59718042\n",
            "Iteration 32, loss = 1.59719491\n",
            "Iteration 33, loss = 1.59703332\n",
            "Iteration 34, loss = 1.59695622\n",
            "Iteration 35, loss = 1.59688587\n",
            "Iteration 36, loss = 1.59682963\n",
            "Iteration 37, loss = 1.59668872\n",
            "Iteration 38, loss = 1.59656755\n",
            "Iteration 39, loss = 1.59649287\n",
            "Iteration 40, loss = 1.59644961\n",
            "Iteration 41, loss = 1.59632068\n",
            "Iteration 42, loss = 1.59644614\n",
            "Iteration 43, loss = 1.59619906\n",
            "Iteration 44, loss = 1.59612911\n",
            "Iteration 45, loss = 1.59599405\n",
            "Iteration 46, loss = 1.59592114\n",
            "Iteration 47, loss = 1.59586027\n",
            "Iteration 48, loss = 1.59575662\n",
            "Iteration 49, loss = 1.59566975\n",
            "Iteration 50, loss = 1.59560314\n",
            "Iteration 51, loss = 1.59545152\n",
            "Iteration 52, loss = 1.59548717\n",
            "Iteration 53, loss = 1.59530230\n",
            "Iteration 54, loss = 1.59523354\n",
            "Iteration 55, loss = 1.59518594\n",
            "Iteration 56, loss = 1.59511788\n",
            "Iteration 57, loss = 1.59498603\n",
            "Iteration 58, loss = 1.59492540\n",
            "Iteration 59, loss = 1.59475975\n",
            "Iteration 60, loss = 1.59472105\n",
            "Iteration 61, loss = 1.59457625\n",
            "Iteration 62, loss = 1.59453236\n",
            "Iteration 63, loss = 1.59443234\n",
            "Iteration 64, loss = 1.59433814\n",
            "Iteration 65, loss = 1.59419461\n",
            "Iteration 66, loss = 1.59417391\n",
            "Iteration 67, loss = 1.59401350\n",
            "Iteration 68, loss = 1.59396821\n",
            "Iteration 69, loss = 1.59379238\n",
            "Iteration 70, loss = 1.59372872\n",
            "Iteration 71, loss = 1.59369305\n",
            "Iteration 72, loss = 1.59355083"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 73, loss = 1.59340372\n",
            "Iteration 74, loss = 1.59343643\n",
            "Iteration 75, loss = 1.59325004\n",
            "Iteration 76, loss = 1.59325051\n",
            "Iteration 77, loss = 1.59301471\n",
            "Iteration 78, loss = 1.59304091\n",
            "Iteration 79, loss = 1.59295976\n",
            "Iteration 80, loss = 1.59273271\n",
            "Iteration 81, loss = 1.59266397\n",
            "Iteration 82, loss = 1.59261252\n",
            "Iteration 83, loss = 1.59239551\n",
            "Iteration 84, loss = 1.59235536\n",
            "Iteration 85, loss = 1.59216763\n",
            "Iteration 86, loss = 1.59210064\n",
            "Iteration 87, loss = 1.59195040\n",
            "Iteration 88, loss = 1.59197227\n",
            "Iteration 89, loss = 1.59185312\n",
            "Iteration 90, loss = 1.59163848\n",
            "Iteration 91, loss = 1.59157799\n",
            "Iteration 92, loss = 1.59139704\n",
            "Iteration 93, loss = 1.59133483\n",
            "Iteration 94, loss = 1.59119017\n",
            "Iteration 95, loss = 1.59107782\n",
            "Iteration 96, loss = 1.59122473\n",
            "Iteration 97, loss = 1.59083306\n",
            "Iteration 98, loss = 1.59072527\n",
            "Iteration 99, loss = 1.59061168\n",
            "Iteration 100, loss = 1.59048793\n",
            "Iteration 1, loss = 1.61074204\n",
            "Iteration 2, loss = 1.60395580\n",
            "Iteration 3, loss = 1.60330203\n",
            "Iteration 4, loss = 1.60083530\n",
            "Iteration 5, loss = 1.59791245\n",
            "Iteration 6, loss = 1.59807818\n",
            "Iteration 7, loss = 1.59912652\n",
            "Iteration 8, loss = 1.59653046\n",
            "Iteration 9, loss = 1.59605486\n",
            "Iteration 10, loss = 1.59483364\n",
            "Iteration 11, loss = 1.59292070\n",
            "Iteration 12, loss = 1.59225834\n",
            "Iteration 13, loss = 1.59149089\n",
            "Iteration 14, loss = 1.58956265\n",
            "Iteration 15, loss = 1.58851118\n",
            "Iteration 16, loss = 1.58700649\n",
            "Iteration 17, loss = 1.58699572\n",
            "Iteration 18, loss = 1.58538451\n",
            "Iteration 19, loss = 1.58486109\n",
            "Iteration 20, loss = 1.58124981\n",
            "Iteration 21, loss = 1.57977710\n",
            "Iteration 22, loss = 1.57741025\n",
            "Iteration 23, loss = 1.57591470\n",
            "Iteration 24, loss = 1.57323336\n",
            "Iteration 25, loss = 1.57248735\n",
            "Iteration 26, loss = 1.56882341\n",
            "Iteration 27, loss = 1.56567525\n",
            "Iteration 28, loss = 1.56311698\n",
            "Iteration 29, loss = 1.56111273\n",
            "Iteration 30, loss = 1.55728581\n",
            "Iteration 31, loss = 1.55292457\n",
            "Iteration 32, loss = 1.55053844\n",
            "Iteration 33, loss = 1.54571884\n",
            "Iteration 34, loss = 1.54134767\n",
            "Iteration 35, loss = 1.53696402\n",
            "Iteration 36, loss = 1.53197640\n",
            "Iteration 37, loss = 1.52667918\n",
            "Iteration 38, loss = 1.52106399\n",
            "Iteration 39, loss = 1.51554078\n",
            "Iteration 40, loss = 1.50956431\n",
            "Iteration 41, loss = 1.50288879\n",
            "Iteration 42, loss = 1.49817528\n",
            "Iteration 43, loss = 1.49041813\n",
            "Iteration 44, loss = 1.48340457\n",
            "Iteration 45, loss = 1.47541842\n",
            "Iteration 46, loss = 1.46790060\n",
            "Iteration 47, loss = 1.46026973\n",
            "Iteration 48, loss = 1.45244021\n",
            "Iteration 49, loss = 1.44456960\n",
            "Iteration 50, loss = 1.43687600\n",
            "Iteration 51, loss = 1.42785837\n",
            "Iteration 52, loss = 1.42025288\n",
            "Iteration 53, loss = 1.41079023\n",
            "Iteration 54, loss = 1.40289915\n",
            "Iteration 55, loss = 1.39366272\n",
            "Iteration 56, loss = 1.38635665\n",
            "Iteration 57, loss = 1.37704080\n",
            "Iteration 58, loss = 1.36893546\n",
            "Iteration 59, loss = 1.35989070\n",
            "Iteration 60, loss = 1.35236797\n",
            "Iteration 61, loss = 1.34410634\n",
            "Iteration 62, loss = 1.33602719"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 63, loss = 1.32785855\n",
            "Iteration 64, loss = 1.32066959\n",
            "Iteration 65, loss = 1.31355558\n",
            "Iteration 66, loss = 1.30689099\n",
            "Iteration 67, loss = 1.29972545\n",
            "Iteration 68, loss = 1.29328150\n",
            "Iteration 69, loss = 1.28662866\n",
            "Iteration 70, loss = 1.28018600\n",
            "Iteration 71, loss = 1.27480630\n",
            "Iteration 72, loss = 1.26927917\n",
            "Iteration 73, loss = 1.26301223\n",
            "Iteration 74, loss = 1.25837378\n",
            "Iteration 75, loss = 1.25297245\n",
            "Iteration 76, loss = 1.24889076\n",
            "Iteration 77, loss = 1.24336009\n",
            "Iteration 78, loss = 1.23978617\n",
            "Iteration 79, loss = 1.23533503\n",
            "Iteration 80, loss = 1.23091974\n",
            "Iteration 81, loss = 1.22707063\n",
            "Iteration 82, loss = 1.22459812\n",
            "Iteration 83, loss = 1.21992140\n",
            "Iteration 84, loss = 1.21616037\n",
            "Iteration 85, loss = 1.21257423\n",
            "Iteration 86, loss = 1.21021338\n",
            "Iteration 87, loss = 1.20676716\n",
            "Iteration 88, loss = 1.20448107\n",
            "Iteration 89, loss = 1.20173372\n",
            "Iteration 90, loss = 1.19825976\n",
            "Iteration 91, loss = 1.19620600\n",
            "Iteration 92, loss = 1.19344381\n",
            "Iteration 93, loss = 1.19137539\n",
            "Iteration 94, loss = 1.18894197\n",
            "Iteration 95, loss = 1.18727288\n",
            "Iteration 96, loss = 1.18715872\n",
            "Iteration 97, loss = 1.18248299\n",
            "Iteration 98, loss = 1.18079407\n",
            "Iteration 99, loss = 1.17932605\n",
            "Iteration 100, loss = 1.17730950\n",
            "Iteration 1, loss = 1.92788256\n",
            "Iteration 2, loss = 1.70661170\n",
            "Iteration 3, loss = 1.80872770\n",
            "Iteration 4, loss = 1.63687919\n",
            "Iteration 5, loss = 1.64482309\n",
            "Iteration 6, loss = 1.64678285\n",
            "Iteration 7, loss = 1.62265866\n",
            "Iteration 8, loss = 1.59689426\n",
            "Iteration 9, loss = 1.59226721\n",
            "Iteration 10, loss = 1.59131338\n",
            "Iteration 11, loss = 1.58398323\n",
            "Iteration 12, loss = 1.57353994\n",
            "Iteration 13, loss = 1.56157701\n",
            "Iteration 14, loss = 1.54472321\n",
            "Iteration 15, loss = 1.52871761\n",
            "Iteration 16, loss = 1.51408635\n",
            "Iteration 17, loss = 1.49362314\n",
            "Iteration 18, loss = 1.46149612\n",
            "Iteration 19, loss = 1.42881204\n",
            "Iteration 20, loss = 1.38915647\n",
            "Iteration 21, loss = 1.34976221\n",
            "Iteration 22, loss = 1.31015586\n",
            "Iteration 23, loss = 1.27447992\n",
            "Iteration 24, loss = 1.24537941\n",
            "Iteration 25, loss = 1.22519030\n",
            "Iteration 26, loss = 1.20210997\n",
            "Iteration 27, loss = 1.18723125\n",
            "Iteration 28, loss = 1.17509723\n",
            "Iteration 29, loss = 1.16548547\n",
            "Iteration 30, loss = 1.15624994\n",
            "Iteration 31, loss = 1.14990192\n",
            "Iteration 32, loss = 1.14461210\n",
            "Iteration 33, loss = 1.13978271\n",
            "Iteration 34, loss = 1.13686238\n",
            "Iteration 35, loss = 1.13240214\n",
            "Iteration 36, loss = 1.13185382\n",
            "Iteration 37, loss = 1.12837813\n",
            "Iteration 38, loss = 1.11907722\n",
            "Iteration 39, loss = 1.12216105\n",
            "Iteration 40, loss = 1.11890674\n",
            "Iteration 41, loss = 1.11791932\n",
            "Iteration 42, loss = 1.12294340\n",
            "Iteration 43, loss = 1.11976670\n",
            "Iteration 44, loss = 1.11693063\n",
            "Iteration 45, loss = 1.11427723\n",
            "Iteration 46, loss = 1.11981184\n",
            "Iteration 47, loss = 1.11036950\n",
            "Iteration 48, loss = 1.11529590\n",
            "Iteration 49, loss = 1.11039949\n",
            "Iteration 50, loss = 1.11242289\n",
            "Iteration 51, loss = 1.11154376\n",
            "Iteration 52, loss = 1.11494439\n",
            "Iteration 53, loss = 1.10632289\n",
            "Iteration 54, loss = 1.11456574\n",
            "Iteration 55, loss = 1.10945294\n",
            "Iteration 56, loss = 1.11166770\n",
            "Iteration 57, loss = 1.11241568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 1.11158589\n",
            "Iteration 59, loss = 1.11516969\n",
            "Iteration 60, loss = 1.11050656\n",
            "Iteration 61, loss = 1.11022166\n",
            "Iteration 62, loss = 1.10446171\n",
            "Iteration 63, loss = 1.10649523\n",
            "Iteration 64, loss = 1.10531417\n",
            "Iteration 65, loss = 1.10179088\n",
            "Iteration 66, loss = 1.10521318\n",
            "Iteration 67, loss = 1.10324162\n",
            "Iteration 68, loss = 1.10368877\n",
            "Iteration 69, loss = 1.09998878\n",
            "Iteration 70, loss = 1.10183254\n",
            "Iteration 71, loss = 1.09957856\n",
            "Iteration 72, loss = 1.10626416\n",
            "Iteration 73, loss = 1.10443136\n",
            "Iteration 74, loss = 1.10064470\n",
            "Iteration 75, loss = 1.10224877\n",
            "Iteration 76, loss = 1.09948589\n",
            "Iteration 77, loss = 1.09853734\n",
            "Iteration 78, loss = 1.10427860\n",
            "Iteration 79, loss = 1.09571356\n",
            "Iteration 80, loss = 1.09951555\n",
            "Iteration 81, loss = 1.10029123\n",
            "Iteration 82, loss = 1.09719895\n",
            "Iteration 83, loss = 1.09210399\n",
            "Iteration 84, loss = 1.09774674\n",
            "Iteration 85, loss = 1.08653465\n",
            "Iteration 86, loss = 1.09467355\n",
            "Iteration 87, loss = 1.09046509\n",
            "Iteration 88, loss = 1.09065657\n",
            "Iteration 89, loss = 1.10449964\n",
            "Iteration 90, loss = 1.08169429\n",
            "Iteration 91, loss = 1.09739393\n",
            "Iteration 92, loss = 1.08044418\n",
            "Iteration 93, loss = 1.08927761\n",
            "Iteration 94, loss = 1.08733256\n",
            "Iteration 95, loss = 1.08236852\n",
            "Iteration 96, loss = 1.09071667\n",
            "Iteration 97, loss = 1.07465742\n",
            "Iteration 98, loss = 1.07842015\n",
            "Iteration 99, loss = 1.07107272\n",
            "Iteration 100, loss = 1.07124403\n",
            "Iteration 1, loss = 1.69850262\n",
            "Iteration 2, loss = 1.68943294\n",
            "Iteration 3, loss = 1.68119599\n",
            "Iteration 4, loss = 1.67388012\n",
            "Iteration 5, loss = 1.66656406\n",
            "Iteration 6, loss = 1.65984669\n",
            "Iteration 7, loss = 1.65343421\n",
            "Iteration 8, loss = 1.64775278\n",
            "Iteration 9, loss = 1.64284563\n",
            "Iteration 10, loss = 1.63813002\n",
            "Iteration 11, loss = 1.63364483\n",
            "Iteration 12, loss = 1.62942613\n",
            "Iteration 13, loss = 1.62569728\n",
            "Iteration 14, loss = 1.62247507\n",
            "Iteration 15, loss = 1.61949681\n",
            "Iteration 16, loss = 1.61667906\n",
            "Iteration 17, loss = 1.61433093\n",
            "Iteration 18, loss = 1.61162842\n",
            "Iteration 19, loss = 1.60905779\n",
            "Iteration 20, loss = 1.60744953\n",
            "Iteration 21, loss = 1.60563065\n",
            "Iteration 22, loss = 1.60368163\n",
            "Iteration 23, loss = 1.60227471\n",
            "Iteration 24, loss = 1.60059708\n",
            "Iteration 25, loss = 1.59946811\n",
            "Iteration 26, loss = 1.59815808\n",
            "Iteration 27, loss = 1.59685401\n",
            "Iteration 28, loss = 1.59585219\n",
            "Iteration 29, loss = 1.59477342\n",
            "Iteration 30, loss = 1.59379866\n",
            "Iteration 31, loss = 1.59298638\n",
            "Iteration 32, loss = 1.59223020\n",
            "Iteration 33, loss = 1.59126622\n",
            "Iteration 34, loss = 1.59043675\n",
            "Iteration 35, loss = 1.58973625\n",
            "Iteration 36, loss = 1.58900022\n",
            "Iteration 37, loss = 1.58822183\n",
            "Iteration 38, loss = 1.58752810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 39, loss = 1.58685963\n",
            "Iteration 40, loss = 1.58616501\n",
            "Iteration 41, loss = 1.58552719\n",
            "Iteration 42, loss = 1.58498644\n",
            "Iteration 43, loss = 1.58415776\n",
            "Iteration 44, loss = 1.58354851\n",
            "Iteration 45, loss = 1.58287132\n",
            "Iteration 46, loss = 1.58217284\n",
            "Iteration 47, loss = 1.58152595\n",
            "Iteration 48, loss = 1.58083242\n",
            "Iteration 49, loss = 1.58017193\n",
            "Iteration 50, loss = 1.57952879\n",
            "Iteration 51, loss = 1.57879526\n",
            "Iteration 52, loss = 1.57814974\n",
            "Iteration 53, loss = 1.57739925\n",
            "Iteration 54, loss = 1.57667586\n",
            "Iteration 55, loss = 1.57596520\n",
            "Iteration 56, loss = 1.57527245\n",
            "Iteration 57, loss = 1.57448402\n",
            "Iteration 58, loss = 1.57377791\n",
            "Iteration 59, loss = 1.57299586\n",
            "Iteration 60, loss = 1.57224308\n",
            "Iteration 61, loss = 1.57144689\n",
            "Iteration 62, loss = 1.57066487\n",
            "Iteration 63, loss = 1.56986780\n",
            "Iteration 64, loss = 1.56906324\n",
            "Iteration 65, loss = 1.56821903\n",
            "Iteration 66, loss = 1.56744917\n",
            "Iteration 67, loss = 1.56658381\n",
            "Iteration 68, loss = 1.56575658\n",
            "Iteration 69, loss = 1.56486727\n",
            "Iteration 70, loss = 1.56400625\n",
            "Iteration 71, loss = 1.56313741\n",
            "Iteration 72, loss = 1.56223734\n",
            "Iteration 73, loss = 1.56131072\n",
            "Iteration 74, loss = 1.56043636\n",
            "Iteration 75, loss = 1.55949017\n",
            "Iteration 76, loss = 1.55860546\n",
            "Iteration 77, loss = 1.55756958\n",
            "Iteration 78, loss = 1.55666195\n",
            "Iteration 79, loss = 1.55579282\n",
            "Iteration 80, loss = 1.55469025\n",
            "Iteration 81, loss = 1.55372875\n",
            "Iteration 82, loss = 1.55277203\n",
            "Iteration 83, loss = 1.55165789\n",
            "Iteration 84, loss = 1.55065843\n",
            "Iteration 85, loss = 1.54959130\n",
            "Iteration 86, loss = 1.54854553\n",
            "Iteration 87, loss = 1.54745976\n",
            "Iteration 88, loss = 1.54641290\n",
            "Iteration 89, loss = 1.54535711\n",
            "Iteration 90, loss = 1.54417262\n",
            "Iteration 91, loss = 1.54312322\n",
            "Iteration 92, loss = 1.54189240\n",
            "Iteration 93, loss = 1.54076350\n",
            "Iteration 94, loss = 1.53961073\n",
            "Iteration 95, loss = 1.53842033\n",
            "Iteration 96, loss = 1.53737558\n",
            "Iteration 97, loss = 1.53601889\n",
            "Iteration 98, loss = 1.53479496\n",
            "Iteration 99, loss = 1.53357085\n",
            "Iteration 100, loss = 1.53231247\n",
            "Iteration 1, loss = 1.68113878\n",
            "Iteration 2, loss = 1.62334173\n",
            "Iteration 3, loss = 1.60362937\n",
            "Iteration 4, loss = 1.59664307\n",
            "Iteration 5, loss = 1.59558133\n",
            "Iteration 6, loss = 1.59265454\n",
            "Iteration 7, loss = 1.58796530\n",
            "Iteration 8, loss = 1.57763986\n",
            "Iteration 9, loss = 1.56665145\n",
            "Iteration 10, loss = 1.55703300\n",
            "Iteration 11, loss = 1.54855729\n",
            "Iteration 12, loss = 1.54140514\n",
            "Iteration 13, loss = 1.53263567\n",
            "Iteration 14, loss = 1.52198480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 1.51054471\n",
            "Iteration 16, loss = 1.49706725\n",
            "Iteration 17, loss = 1.48366362\n",
            "Iteration 18, loss = 1.46809576\n",
            "Iteration 19, loss = 1.45289286\n",
            "Iteration 20, loss = 1.43686258\n",
            "Iteration 21, loss = 1.42033869\n",
            "Iteration 22, loss = 1.40333507\n",
            "Iteration 23, loss = 1.38618257\n",
            "Iteration 24, loss = 1.36873139\n",
            "Iteration 25, loss = 1.35227336\n",
            "Iteration 26, loss = 1.33475052\n",
            "Iteration 27, loss = 1.31796580\n",
            "Iteration 28, loss = 1.30289836\n",
            "Iteration 29, loss = 1.28845477\n",
            "Iteration 30, loss = 1.27429708\n",
            "Iteration 31, loss = 1.26153552\n",
            "Iteration 32, loss = 1.25002564\n",
            "Iteration 33, loss = 1.23876606\n",
            "Iteration 34, loss = 1.22807701\n",
            "Iteration 35, loss = 1.21987415\n",
            "Iteration 36, loss = 1.21173963\n",
            "Iteration 37, loss = 1.20303236\n",
            "Iteration 38, loss = 1.19575460\n",
            "Iteration 39, loss = 1.18971383\n",
            "Iteration 40, loss = 1.18383115\n",
            "Iteration 41, loss = 1.17815294\n",
            "Iteration 42, loss = 1.17448596\n",
            "Iteration 43, loss = 1.16891610\n",
            "Iteration 44, loss = 1.16470300\n",
            "Iteration 45, loss = 1.16056044\n",
            "Iteration 46, loss = 1.15690836\n",
            "Iteration 47, loss = 1.15348064\n",
            "Iteration 48, loss = 1.15005252\n",
            "Iteration 49, loss = 1.14645907\n",
            "Iteration 50, loss = 1.14424191\n",
            "Iteration 51, loss = 1.14142825\n",
            "Iteration 52, loss = 1.13919812\n",
            "Iteration 53, loss = 1.13608496\n",
            "Iteration 54, loss = 1.13510504\n",
            "Iteration 55, loss = 1.13209477\n",
            "Iteration 56, loss = 1.13049741\n",
            "Iteration 57, loss = 1.12882793\n",
            "Iteration 58, loss = 1.12828017\n",
            "Iteration 59, loss = 1.12458603\n",
            "Iteration 60, loss = 1.12280637\n",
            "Iteration 61, loss = 1.12211956\n",
            "Iteration 62, loss = 1.11985980\n",
            "Iteration 63, loss = 1.11796372\n",
            "Iteration 64, loss = 1.11697070\n",
            "Iteration 65, loss = 1.11549105\n",
            "Iteration 66, loss = 1.11448097\n",
            "Iteration 67, loss = 1.11310074\n",
            "Iteration 68, loss = 1.11206256\n",
            "Iteration 69, loss = 1.11027311\n",
            "Iteration 70, loss = 1.10931070\n",
            "Iteration 71, loss = 1.10852739\n",
            "Iteration 72, loss = 1.10786053\n",
            "Iteration 73, loss = 1.10621947\n",
            "Iteration 74, loss = 1.10535451\n",
            "Iteration 75, loss = 1.10374589\n",
            "Iteration 76, loss = 1.10382327\n",
            "Iteration 77, loss = 1.10226694\n",
            "Iteration 78, loss = 1.10103372\n",
            "Iteration 79, loss = 1.10059887\n",
            "Iteration 80, loss = 1.09936891\n",
            "Iteration 81, loss = 1.09786188\n",
            "Iteration 82, loss = 1.09798588\n",
            "Iteration 83, loss = 1.09632573\n",
            "Iteration 84, loss = 1.09639176\n",
            "Iteration 85, loss = 1.09463508\n",
            "Iteration 86, loss = 1.09369774\n",
            "Iteration 87, loss = 1.09261853\n",
            "Iteration 88, loss = 1.09271736\n",
            "Iteration 89, loss = 1.09177488\n",
            "Iteration 90, loss = 1.08943792\n",
            "Iteration 91, loss = 1.08836201\n",
            "Iteration 92, loss = 1.08735160\n",
            "Iteration 93, loss = 1.08689352\n",
            "Iteration 94, loss = 1.08568214\n",
            "Iteration 95, loss = 1.08560858\n",
            "Iteration 96, loss = 1.08629094\n",
            "Iteration 97, loss = 1.08203518\n",
            "Iteration 98, loss = 1.08322848\n",
            "Iteration 99, loss = 1.08215780\n",
            "Iteration 100, loss = 1.07942671\n",
            "Iteration 1, loss = 1.71503418\n",
            "Iteration 2, loss = 1.63623788\n",
            "Iteration 3, loss = 1.55809364\n",
            "Iteration 4, loss = 1.51238385\n",
            "Iteration 5, loss = 1.43071556\n",
            "Iteration 6, loss = 1.35215962\n",
            "Iteration 7, loss = 1.26914993\n",
            "Iteration 8, loss = 1.21586721\n",
            "Iteration 9, loss = 1.17122663\n",
            "Iteration 10, loss = 1.14093357\n",
            "Iteration 11, loss = 1.12931764\n",
            "Iteration 12, loss = 1.12384284\n",
            "Iteration 13, loss = 1.12500056\n",
            "Iteration 14, loss = 1.11293540\n",
            "Iteration 15, loss = 1.12094439\n",
            "Iteration 16, loss = 1.11318263\n",
            "Iteration 17, loss = 1.11414569\n",
            "Iteration 18, loss = 1.11081762\n",
            "Iteration 19, loss = 1.12710459\n",
            "Iteration 20, loss = 1.11329272\n",
            "Iteration 21, loss = 1.12246419\n",
            "Iteration 22, loss = 1.12244403\n",
            "Iteration 23, loss = 1.13263148\n",
            "Iteration 24, loss = 1.09742639\n",
            "Iteration 25, loss = 1.09526141\n",
            "Iteration 26, loss = 1.09824654\n",
            "Iteration 27, loss = 1.08127448\n",
            "Iteration 28, loss = 1.09254296\n",
            "Iteration 29, loss = 1.07152710\n",
            "Iteration 30, loss = 1.06679045\n",
            "Iteration 31, loss = 1.05969041\n",
            "Iteration 32, loss = 1.05675674\n",
            "Iteration 33, loss = 1.05130011\n",
            "Iteration 34, loss = 1.05746124\n",
            "Iteration 35, loss = 1.04647620\n",
            "Iteration 36, loss = 1.06093426\n",
            "Iteration 37, loss = 1.05260934\n",
            "Iteration 38, loss = 1.04416966\n",
            "Iteration 39, loss = 1.00085873\n",
            "Iteration 40, loss = 1.04633116\n",
            "Iteration 41, loss = 1.02324960\n",
            "Iteration 42, loss = 0.97744391\n",
            "Iteration 43, loss = 1.01214149\n",
            "Iteration 44, loss = 1.04083875\n",
            "Iteration 45, loss = 1.00110671\n",
            "Iteration 46, loss = 1.00372484\n",
            "Iteration 47, loss = 0.97194354\n",
            "Iteration 48, loss = 0.96459544\n",
            "Iteration 49, loss = 0.95009463\n",
            "Iteration 50, loss = 0.94673421\n",
            "Iteration 51, loss = 0.94565657\n",
            "Iteration 52, loss = 0.95121251\n",
            "Iteration 53, loss = 0.91921956\n",
            "Iteration 54, loss = 0.91362291\n",
            "Iteration 55, loss = 0.90545320\n",
            "Iteration 56, loss = 0.92879747\n",
            "Iteration 57, loss = 0.91436363\n",
            "Iteration 58, loss = 0.92430861\n",
            "Iteration 59, loss = 0.91815369\n",
            "Iteration 60, loss = 0.99467439\n",
            "Iteration 61, loss = 0.91304118\n",
            "Iteration 62, loss = 0.94090006\n",
            "Iteration 63, loss = 0.99500885\n",
            "Iteration 64, loss = 0.93899030\n",
            "Iteration 65, loss = 0.94521680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 66, loss = 0.95209332\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.64844688\n",
            "Iteration 2, loss = 1.64523036\n",
            "Iteration 3, loss = 1.64244655\n",
            "Iteration 4, loss = 1.63976839\n",
            "Iteration 5, loss = 1.63713241\n",
            "Iteration 6, loss = 1.63459972\n",
            "Iteration 7, loss = 1.63210922\n",
            "Iteration 8, loss = 1.62982782\n",
            "Iteration 9, loss = 1.62788314\n",
            "Iteration 10, loss = 1.62581620\n",
            "Iteration 11, loss = 1.62380420\n",
            "Iteration 12, loss = 1.62184778\n",
            "Iteration 13, loss = 1.62009059\n",
            "Iteration 14, loss = 1.61849418\n",
            "Iteration 15, loss = 1.61699300\n",
            "Iteration 16, loss = 1.61549144\n",
            "Iteration 17, loss = 1.61422109\n",
            "Iteration 18, loss = 1.61266851\n",
            "Iteration 19, loss = 1.61119829\n",
            "Iteration 20, loss = 1.61020793\n",
            "Iteration 21, loss = 1.60907149\n",
            "Iteration 22, loss = 1.60782693\n",
            "Iteration 23, loss = 1.60686906\n",
            "Iteration 24, loss = 1.60572262\n",
            "Iteration 25, loss = 1.60492267\n",
            "Iteration 26, loss = 1.60393503\n",
            "Iteration 27, loss = 1.60296665\n",
            "Iteration 28, loss = 1.60219663\n",
            "Iteration 29, loss = 1.60135351\n",
            "Iteration 30, loss = 1.60052843\n",
            "Iteration 31, loss = 1.59984944\n",
            "Iteration 32, loss = 1.59917416\n",
            "Iteration 33, loss = 1.59838551\n",
            "Iteration 34, loss = 1.59763649\n",
            "Iteration 35, loss = 1.59703730\n",
            "Iteration 36, loss = 1.59636933\n",
            "Iteration 37, loss = 1.59572118\n",
            "Iteration 38, loss = 1.59509874\n",
            "Iteration 39, loss = 1.59448099\n",
            "Iteration 40, loss = 1.59387867\n",
            "Iteration 41, loss = 1.59334303\n",
            "Iteration 42, loss = 1.59285721\n",
            "Iteration 43, loss = 1.59208105\n",
            "Iteration 44, loss = 1.59155867\n",
            "Iteration 45, loss = 1.59100274\n",
            "Iteration 46, loss = 1.59038440\n",
            "Iteration 47, loss = 1.58987014\n",
            "Iteration 48, loss = 1.58925740\n",
            "Iteration 49, loss = 1.58871579\n",
            "Iteration 50, loss = 1.58823315\n",
            "Iteration 51, loss = 1.58760935\n",
            "Iteration 52, loss = 1.58709652\n",
            "Iteration 53, loss = 1.58650310\n",
            "Iteration 54, loss = 1.58591466\n",
            "Iteration 55, loss = 1.58538837\n",
            "Iteration 56, loss = 1.58488036\n",
            "Iteration 57, loss = 1.58423729\n",
            "Iteration 58, loss = 1.58371963\n",
            "Iteration 59, loss = 1.58310937\n",
            "Iteration 60, loss = 1.58252988\n",
            "Iteration 61, loss = 1.58195699\n",
            "Iteration 62, loss = 1.58134845\n",
            "Iteration 63, loss = 1.58077544\n",
            "Iteration 64, loss = 1.58017731\n",
            "Iteration 65, loss = 1.57955685\n",
            "Iteration 66, loss = 1.57898747\n",
            "Iteration 67, loss = 1.57833918\n",
            "Iteration 68, loss = 1.57774025\n",
            "Iteration 69, loss = 1.57707125\n",
            "Iteration 70, loss = 1.57643899\n",
            "Iteration 71, loss = 1.57579700\n",
            "Iteration 72, loss = 1.57512578\n",
            "Iteration 73, loss = 1.57444460\n",
            "Iteration 74, loss = 1.57380522\n",
            "Iteration 75, loss = 1.57310272\n",
            "Iteration 76, loss = 1.57242755\n",
            "Iteration 77, loss = 1.57168906\n",
            "Iteration 78, loss = 1.57101446\n",
            "Iteration 79, loss = 1.57038014\n",
            "Iteration 80, loss = 1.56956990\n",
            "Iteration 81, loss = 1.56886727\n",
            "Iteration 82, loss = 1.56816049\n",
            "Iteration 83, loss = 1.56734403\n",
            "Iteration 84, loss = 1.56661624\n",
            "Iteration 85, loss = 1.56583113\n",
            "Iteration 86, loss = 1.56506077\n",
            "Iteration 87, loss = 1.56426912\n",
            "Iteration 88, loss = 1.56350495\n",
            "Iteration 89, loss = 1.56273085\n",
            "Iteration 90, loss = 1.56186574\n",
            "Iteration 91, loss = 1.56109779\n",
            "Iteration 92, loss = 1.56020352\n",
            "Iteration 93, loss = 1.55937784\n",
            "Iteration 94, loss = 1.55853843\n",
            "Iteration 95, loss = 1.55766965\n",
            "Iteration 96, loss = 1.55690982\n",
            "Iteration 97, loss = 1.55592160\n",
            "Iteration 98, loss = 1.55503436\n",
            "Iteration 99, loss = 1.55414431\n",
            "Iteration 100, loss = 1.55323261\n",
            "Iteration 1, loss = 1.64299749\n",
            "Iteration 2, loss = 1.61863008\n",
            "Iteration 3, loss = 1.60656299\n",
            "Iteration 4, loss = 1.59986034\n",
            "Iteration 5, loss = 1.59537223\n",
            "Iteration 6, loss = 1.59132973\n",
            "Iteration 7, loss = 1.58873105\n",
            "Iteration 8, loss = 1.58429679\n",
            "Iteration 9, loss = 1.57938682\n",
            "Iteration 10, loss = 1.57352030\n",
            "Iteration 11, loss = 1.56714339\n",
            "Iteration 12, loss = 1.56060437\n",
            "Iteration 13, loss = 1.55336008\n",
            "Iteration 14, loss = 1.54543339\n",
            "Iteration 15, loss = 1.53725187\n",
            "Iteration 16, loss = 1.52792493\n",
            "Iteration 17, loss = 1.51816886\n",
            "Iteration 18, loss = 1.50692677\n",
            "Iteration 19, loss = 1.49553960\n",
            "Iteration 20, loss = 1.48316713\n",
            "Iteration 21, loss = 1.47000076\n",
            "Iteration 22, loss = 1.45620213\n",
            "Iteration 23, loss = 1.44216341\n",
            "Iteration 24, loss = 1.42736487\n",
            "Iteration 25, loss = 1.41322932\n",
            "Iteration 26, loss = 1.39758725\n",
            "Iteration 27, loss = 1.38175527\n",
            "Iteration 28, loss = 1.36691786\n",
            "Iteration 29, loss = 1.35210156\n",
            "Iteration 30, loss = 1.33735718\n",
            "Iteration 31, loss = 1.32313806\n",
            "Iteration 32, loss = 1.31016629\n",
            "Iteration 33, loss = 1.29704258\n",
            "Iteration 34, loss = 1.28441724\n",
            "Iteration 35, loss = 1.27376402\n",
            "Iteration 36, loss = 1.26324170\n",
            "Iteration 37, loss = 1.25271957\n",
            "Iteration 38, loss = 1.24322749\n",
            "Iteration 39, loss = 1.23488495\n",
            "Iteration 40, loss = 1.22694303\n",
            "Iteration 41, loss = 1.21934754\n",
            "Iteration 42, loss = 1.21340267\n",
            "Iteration 43, loss = 1.20644595\n",
            "Iteration 44, loss = 1.20046204\n",
            "Iteration 45, loss = 1.19510353\n",
            "Iteration 46, loss = 1.19014055\n",
            "Iteration 47, loss = 1.18522795\n",
            "Iteration 48, loss = 1.18104640\n",
            "Iteration 49, loss = 1.17675374\n",
            "Iteration 50, loss = 1.17324259\n",
            "Iteration 51, loss = 1.16949910\n",
            "Iteration 52, loss = 1.16689785\n",
            "Iteration 53, loss = 1.16300925\n",
            "Iteration 54, loss = 1.16108920\n",
            "Iteration 55, loss = 1.15774808\n",
            "Iteration 56, loss = 1.15541609\n",
            "Iteration 57, loss = 1.15307763\n",
            "Iteration 58, loss = 1.15182911\n",
            "Iteration 59, loss = 1.14805547\n",
            "Iteration 60, loss = 1.14600204\n",
            "Iteration 61, loss = 1.14466722\n",
            "Iteration 62, loss = 1.14236613\n",
            "Iteration 63, loss = 1.14038218\n",
            "Iteration 64, loss = 1.13915015\n",
            "Iteration 65, loss = 1.13737776\n",
            "Iteration 66, loss = 1.13603309\n",
            "Iteration 67, loss = 1.13438632\n",
            "Iteration 68, loss = 1.13347737\n",
            "Iteration 69, loss = 1.13163711\n",
            "Iteration 70, loss = 1.13026585\n",
            "Iteration 71, loss = 1.12944000\n",
            "Iteration 72, loss = 1.12896896\n",
            "Iteration 73, loss = 1.12705028\n",
            "Iteration 74, loss = 1.12619910\n",
            "Iteration 75, loss = 1.12484578\n",
            "Iteration 76, loss = 1.12433948\n",
            "Iteration 77, loss = 1.12339448\n",
            "Iteration 78, loss = 1.12223490\n",
            "Iteration 79, loss = 1.12144590\n",
            "Iteration 80, loss = 1.12048758\n",
            "Iteration 81, loss = 1.11957095\n",
            "Iteration 82, loss = 1.11909784\n",
            "Iteration 83, loss = 1.11784643\n",
            "Iteration 84, loss = 1.11790856\n",
            "Iteration 85, loss = 1.11660858\n",
            "Iteration 86, loss = 1.11559357\n",
            "Iteration 87, loss = 1.11463540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 88, loss = 1.11501824\n",
            "Iteration 89, loss = 1.11460361\n",
            "Iteration 90, loss = 1.11265748\n",
            "Iteration 91, loss = 1.11190648\n",
            "Iteration 92, loss = 1.11146030\n",
            "Iteration 93, loss = 1.11114993\n",
            "Iteration 94, loss = 1.11017458\n",
            "Iteration 95, loss = 1.11047595\n",
            "Iteration 96, loss = 1.11183341\n",
            "Iteration 97, loss = 1.10814650\n",
            "Iteration 98, loss = 1.10920054\n",
            "Iteration 99, loss = 1.10889041\n",
            "Iteration 100, loss = 1.10681777\n",
            "Iteration 1, loss = 1.64111052\n",
            "Iteration 2, loss = 1.60195000\n",
            "Iteration 3, loss = 1.56904838\n",
            "Iteration 4, loss = 1.52149598\n",
            "Iteration 5, loss = 1.45728826\n",
            "Iteration 6, loss = 1.38110320\n",
            "Iteration 7, loss = 1.30702500\n",
            "Iteration 8, loss = 1.23996896\n",
            "Iteration 9, loss = 1.19200569\n",
            "Iteration 10, loss = 1.16302350\n",
            "Iteration 11, loss = 1.14332242\n",
            "Iteration 12, loss = 1.13390127\n",
            "Iteration 13, loss = 1.13371383\n",
            "Iteration 14, loss = 1.11978444\n",
            "Iteration 15, loss = 1.12307752\n",
            "Iteration 16, loss = 1.11507442\n",
            "Iteration 17, loss = 1.11341705\n",
            "Iteration 18, loss = 1.11626217\n",
            "Iteration 19, loss = 1.11432412\n",
            "Iteration 20, loss = 1.11127136\n",
            "Iteration 21, loss = 1.10786270\n",
            "Iteration 22, loss = 1.11279537\n",
            "Iteration 23, loss = 1.11791201\n",
            "Iteration 24, loss = 1.10171711\n",
            "Iteration 25, loss = 1.11172836\n",
            "Iteration 26, loss = 1.10948295\n",
            "Iteration 27, loss = 1.10280450\n",
            "Iteration 28, loss = 1.10494919\n",
            "Iteration 29, loss = 1.09457370\n",
            "Iteration 30, loss = 1.08651819\n",
            "Iteration 31, loss = 1.08777547\n",
            "Iteration 32, loss = 1.09284189\n",
            "Iteration 33, loss = 1.08193101\n",
            "Iteration 34, loss = 1.08421651\n",
            "Iteration 35, loss = 1.07922175\n",
            "Iteration 36, loss = 1.07497140\n",
            "Iteration 37, loss = 1.07446698\n",
            "Iteration 38, loss = 1.06881176\n",
            "Iteration 39, loss = 1.07183094\n",
            "Iteration 40, loss = 1.06862940\n",
            "Iteration 41, loss = 1.06361471\n",
            "Iteration 42, loss = 1.06744713\n",
            "Iteration 43, loss = 1.05521910\n",
            "Iteration 44, loss = 1.04835213\n",
            "Iteration 45, loss = 1.04535536\n",
            "Iteration 46, loss = 1.04645856\n",
            "Iteration 47, loss = 1.04558947\n",
            "Iteration 48, loss = 1.04501593\n",
            "Iteration 49, loss = 1.02551711\n",
            "Iteration 50, loss = 1.02500356\n",
            "Iteration 51, loss = 1.03164890\n",
            "Iteration 52, loss = 1.02283924\n",
            "Iteration 53, loss = 1.01293819\n",
            "Iteration 54, loss = 1.00925741\n",
            "Iteration 55, loss = 1.00268655\n",
            "Iteration 56, loss = 1.00272010\n",
            "Iteration 57, loss = 1.00111687\n",
            "Iteration 58, loss = 1.00123162\n",
            "Iteration 59, loss = 1.00573092\n",
            "Iteration 60, loss = 0.96990864\n",
            "Iteration 61, loss = 0.98564666\n",
            "Iteration 62, loss = 0.96367574\n",
            "Iteration 63, loss = 0.97776207\n",
            "Iteration 64, loss = 0.97141088\n",
            "Iteration 65, loss = 0.96830803\n",
            "Iteration 66, loss = 0.93751367\n",
            "Iteration 67, loss = 0.93267290\n",
            "Iteration 68, loss = 0.93270279\n",
            "Iteration 69, loss = 0.93732431\n",
            "Iteration 70, loss = 0.92881945\n",
            "Iteration 71, loss = 0.92307207\n",
            "Iteration 72, loss = 0.92030804\n",
            "Iteration 73, loss = 0.90744578\n",
            "Iteration 74, loss = 0.90498316\n",
            "Iteration 75, loss = 0.90407927\n",
            "Iteration 76, loss = 0.90205833\n",
            "Iteration 77, loss = 0.88261481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 78, loss = 0.88864922\n",
            "Iteration 79, loss = 0.87984615\n",
            "Iteration 80, loss = 0.89263167\n",
            "Iteration 81, loss = 0.89884431\n",
            "Iteration 82, loss = 0.86894536\n",
            "Iteration 83, loss = 0.85393065\n",
            "Iteration 84, loss = 0.85882316\n",
            "Iteration 85, loss = 0.84705086\n",
            "Iteration 86, loss = 0.84434162\n",
            "Iteration 87, loss = 0.84373533\n",
            "Iteration 88, loss = 0.84512164\n",
            "Iteration 89, loss = 0.84804367\n",
            "Iteration 90, loss = 0.85115688\n",
            "Iteration 91, loss = 0.85849927\n",
            "Iteration 92, loss = 0.84090721\n",
            "Iteration 93, loss = 0.83594473\n",
            "Iteration 94, loss = 0.83283570\n",
            "Iteration 95, loss = 0.84856370\n",
            "Iteration 96, loss = 0.82942090\n",
            "Iteration 97, loss = 0.81984783\n",
            "Iteration 98, loss = 0.82618448\n",
            "Iteration 99, loss = 0.81886790\n",
            "Iteration 100, loss = 0.83683869\n",
            "Iteration 1, loss = 1.75446173\n",
            "Iteration 2, loss = 1.74202297\n",
            "Iteration 3, loss = 1.73018708\n",
            "Iteration 4, loss = 1.71847280\n",
            "Iteration 5, loss = 1.70762913\n",
            "Iteration 6, loss = 1.69701158\n",
            "Iteration 7, loss = 1.68726095\n",
            "Iteration 8, loss = 1.67835112\n",
            "Iteration 9, loss = 1.67003189\n",
            "Iteration 10, loss = 1.66184460\n",
            "Iteration 11, loss = 1.65440806\n",
            "Iteration 12, loss = 1.64754266\n",
            "Iteration 13, loss = 1.64117252\n",
            "Iteration 14, loss = 1.63560607\n",
            "Iteration 15, loss = 1.62974177\n",
            "Iteration 16, loss = 1.62494079\n",
            "Iteration 17, loss = 1.62017928\n",
            "Iteration 18, loss = 1.61544245\n",
            "Iteration 19, loss = 1.61163640\n",
            "Iteration 20, loss = 1.60803749\n",
            "Iteration 21, loss = 1.60453096\n",
            "Iteration 22, loss = 1.60140783\n",
            "Iteration 23, loss = 1.59851759\n",
            "Iteration 24, loss = 1.59616696\n",
            "Iteration 25, loss = 1.59347596\n",
            "Iteration 26, loss = 1.59116205\n",
            "Iteration 27, loss = 1.58914819\n",
            "Iteration 28, loss = 1.58733918\n",
            "Iteration 29, loss = 1.58567166\n",
            "Iteration 30, loss = 1.58396746\n",
            "Iteration 31, loss = 1.58254435\n",
            "Iteration 32, loss = 1.58104246\n",
            "Iteration 33, loss = 1.57989620\n",
            "Iteration 34, loss = 1.57848130\n",
            "Iteration 35, loss = 1.57749861\n",
            "Iteration 36, loss = 1.57637804\n",
            "Iteration 37, loss = 1.57525291\n",
            "Iteration 38, loss = 1.57454498\n",
            "Iteration 39, loss = 1.57364136\n",
            "Iteration 40, loss = 1.57270128\n",
            "Iteration 41, loss = 1.57173852\n",
            "Iteration 42, loss = 1.57099494\n",
            "Iteration 43, loss = 1.57014824\n",
            "Iteration 44, loss = 1.56938931\n",
            "Iteration 45, loss = 1.56857321\n",
            "Iteration 46, loss = 1.56780405\n",
            "Iteration 47, loss = 1.56703615\n",
            "Iteration 48, loss = 1.56638876\n",
            "Iteration 49, loss = 1.56555480\n",
            "Iteration 50, loss = 1.56481042\n",
            "Iteration 51, loss = 1.56404735\n",
            "Iteration 52, loss = 1.56330361\n",
            "Iteration 53, loss = 1.56251358\n",
            "Iteration 54, loss = 1.56185133\n",
            "Iteration 55, loss = 1.56099715\n",
            "Iteration 56, loss = 1.56027935\n",
            "Iteration 57, loss = 1.55941742\n",
            "Iteration 58, loss = 1.55867514\n",
            "Iteration 59, loss = 1.55784456\n",
            "Iteration 60, loss = 1.55710219\n",
            "Iteration 61, loss = 1.55627381\n",
            "Iteration 62, loss = 1.55552528\n",
            "Iteration 63, loss = 1.55471308\n",
            "Iteration 64, loss = 1.55384467\n",
            "Iteration 65, loss = 1.55303450\n",
            "Iteration 66, loss = 1.55223504\n",
            "Iteration 67, loss = 1.55138134\n",
            "Iteration 68, loss = 1.55056223\n",
            "Iteration 69, loss = 1.54969007\n",
            "Iteration 70, loss = 1.54884085\n",
            "Iteration 71, loss = 1.54799770\n",
            "Iteration 72, loss = 1.54716454\n",
            "Iteration 73, loss = 1.54624986\n",
            "Iteration 74, loss = 1.54534977\n",
            "Iteration 75, loss = 1.54452445\n",
            "Iteration 76, loss = 1.54358347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 77, loss = 1.54264035\n",
            "Iteration 78, loss = 1.54175787\n",
            "Iteration 79, loss = 1.54084033\n",
            "Iteration 80, loss = 1.53992273\n",
            "Iteration 81, loss = 1.53895811\n",
            "Iteration 82, loss = 1.53803725\n",
            "Iteration 83, loss = 1.53706589\n",
            "Iteration 84, loss = 1.53611585\n",
            "Iteration 85, loss = 1.53515446\n",
            "Iteration 86, loss = 1.53419466\n",
            "Iteration 87, loss = 1.53315599\n",
            "Iteration 88, loss = 1.53216690\n",
            "Iteration 89, loss = 1.53118646\n",
            "Iteration 90, loss = 1.53020005\n",
            "Iteration 91, loss = 1.52915613\n",
            "Iteration 92, loss = 1.52814595\n",
            "Iteration 93, loss = 1.52709118\n",
            "Iteration 94, loss = 1.52608680\n",
            "Iteration 95, loss = 1.52497299\n",
            "Iteration 96, loss = 1.52391405\n",
            "Iteration 97, loss = 1.52286419\n",
            "Iteration 98, loss = 1.52180780\n",
            "Iteration 99, loss = 1.52073683\n",
            "Iteration 100, loss = 1.51962833\n",
            "Iteration 1, loss = 1.72771195\n",
            "Iteration 2, loss = 1.64042182\n",
            "Iteration 3, loss = 1.60006416\n",
            "Iteration 4, loss = 1.58307287\n",
            "Iteration 5, loss = 1.58148816\n",
            "Iteration 6, loss = 1.58163674\n",
            "Iteration 7, loss = 1.57699610\n",
            "Iteration 8, loss = 1.56759934\n",
            "Iteration 9, loss = 1.55477609\n",
            "Iteration 10, loss = 1.54463591\n",
            "Iteration 11, loss = 1.53391052\n",
            "Iteration 12, loss = 1.52440062\n",
            "Iteration 13, loss = 1.51608643\n",
            "Iteration 14, loss = 1.50812334\n",
            "Iteration 15, loss = 1.49811069\n",
            "Iteration 16, loss = 1.48784541\n",
            "Iteration 17, loss = 1.47657698\n",
            "Iteration 18, loss = 1.46348381\n",
            "Iteration 19, loss = 1.45150507\n",
            "Iteration 20, loss = 1.43922247\n",
            "Iteration 21, loss = 1.42560597\n",
            "Iteration 22, loss = 1.41236771\n",
            "Iteration 23, loss = 1.39872531\n",
            "Iteration 24, loss = 1.38513325\n",
            "Iteration 25, loss = 1.37090461\n",
            "Iteration 26, loss = 1.35687450\n",
            "Iteration 27, loss = 1.34295691\n",
            "Iteration 28, loss = 1.32868004\n",
            "Iteration 29, loss = 1.31633988\n",
            "Iteration 30, loss = 1.30262686\n",
            "Iteration 31, loss = 1.29030588\n",
            "Iteration 32, loss = 1.27920065\n",
            "Iteration 33, loss = 1.26716098\n",
            "Iteration 34, loss = 1.25679943\n",
            "Iteration 35, loss = 1.24680668\n",
            "Iteration 36, loss = 1.23731709\n",
            "Iteration 37, loss = 1.22844322\n",
            "Iteration 38, loss = 1.22070537\n",
            "Iteration 39, loss = 1.21305762\n",
            "Iteration 40, loss = 1.20637300\n",
            "Iteration 41, loss = 1.19945630\n",
            "Iteration 42, loss = 1.19258972\n",
            "Iteration 43, loss = 1.18742888\n",
            "Iteration 44, loss = 1.18192507\n",
            "Iteration 45, loss = 1.17646108\n",
            "Iteration 46, loss = 1.17279027\n",
            "Iteration 47, loss = 1.16807261\n",
            "Iteration 48, loss = 1.16387431\n",
            "Iteration 49, loss = 1.15993713\n",
            "Iteration 50, loss = 1.15656797\n",
            "Iteration 51, loss = 1.15304468\n",
            "Iteration 52, loss = 1.15030835\n",
            "Iteration 53, loss = 1.14704497\n",
            "Iteration 54, loss = 1.14541377\n",
            "Iteration 55, loss = 1.14183798\n",
            "Iteration 56, loss = 1.14005945\n",
            "Iteration 57, loss = 1.13657814\n",
            "Iteration 58, loss = 1.13442303\n",
            "Iteration 59, loss = 1.13181555\n",
            "Iteration 60, loss = 1.13177930\n",
            "Iteration 61, loss = 1.12805944\n",
            "Iteration 62, loss = 1.12665807\n",
            "Iteration 63, loss = 1.12494280\n",
            "Iteration 64, loss = 1.12366537\n",
            "Iteration 65, loss = 1.12092311\n",
            "Iteration 66, loss = 1.11950547\n",
            "Iteration 67, loss = 1.11864935\n",
            "Iteration 68, loss = 1.11699724\n",
            "Iteration 69, loss = 1.11522533\n",
            "Iteration 70, loss = 1.11376967\n",
            "Iteration 71, loss = 1.11245532\n",
            "Iteration 72, loss = 1.11193127\n",
            "Iteration 73, loss = 1.10976489\n",
            "Iteration 74, loss = 1.10851850\n",
            "Iteration 75, loss = 1.10880571\n",
            "Iteration 76, loss = 1.10696765\n",
            "Iteration 77, loss = 1.10463066\n",
            "Iteration 78, loss = 1.10391663\n",
            "Iteration 79, loss = 1.10356290\n",
            "Iteration 80, loss = 1.10126563\n",
            "Iteration 81, loss = 1.10014125\n",
            "Iteration 82, loss = 1.09918801\n",
            "Iteration 83, loss = 1.09814226\n",
            "Iteration 84, loss = 1.09829966\n",
            "Iteration 85, loss = 1.09658978\n",
            "Iteration 86, loss = 1.09513354\n",
            "Iteration 87, loss = 1.09385703\n",
            "Iteration 88, loss = 1.09266694\n",
            "Iteration 89, loss = 1.09209885\n",
            "Iteration 90, loss = 1.09392988\n",
            "Iteration 91, loss = 1.08964833\n",
            "Iteration 92, loss = 1.08815562\n",
            "Iteration 93, loss = 1.08799731\n",
            "Iteration 94, loss = 1.08635922\n",
            "Iteration 95, loss = 1.08538882\n",
            "Iteration 96, loss = 1.08428816\n",
            "Iteration 97, loss = 1.08295856\n",
            "Iteration 98, loss = 1.08371152\n",
            "Iteration 99, loss = 1.08069672\n",
            "Iteration 100, loss = 1.08028050\n",
            "Iteration 1, loss = 1.70039544\n",
            "Iteration 2, loss = 1.61329746\n",
            "Iteration 3, loss = 1.57248987\n",
            "Iteration 4, loss = 1.48297517\n",
            "Iteration 5, loss = 1.42042637\n",
            "Iteration 6, loss = 1.34416699\n",
            "Iteration 7, loss = 1.29203030\n",
            "Iteration 8, loss = 1.22096952\n",
            "Iteration 9, loss = 1.18884013\n",
            "Iteration 10, loss = 1.16276017\n",
            "Iteration 11, loss = 1.14480279\n",
            "Iteration 12, loss = 1.12651346\n",
            "Iteration 13, loss = 1.12076665\n",
            "Iteration 14, loss = 1.11891393\n",
            "Iteration 15, loss = 1.13086813\n",
            "Iteration 16, loss = 1.13045724\n",
            "Iteration 17, loss = 1.11274530\n",
            "Iteration 18, loss = 1.12635402\n",
            "Iteration 19, loss = 1.12957887\n",
            "Iteration 20, loss = 1.12599014\n",
            "Iteration 21, loss = 1.11220582\n",
            "Iteration 22, loss = 1.11797410\n",
            "Iteration 23, loss = 1.09537782\n",
            "Iteration 24, loss = 1.09715377\n",
            "Iteration 25, loss = 1.10110484\n",
            "Iteration 26, loss = 1.08208755\n",
            "Iteration 27, loss = 1.09597545\n",
            "Iteration 28, loss = 1.08766411\n",
            "Iteration 29, loss = 1.08620464\n",
            "Iteration 30, loss = 1.07893409\n",
            "Iteration 31, loss = 1.05979178\n",
            "Iteration 32, loss = 1.06955913\n",
            "Iteration 33, loss = 1.06851121\n",
            "Iteration 34, loss = 1.05910805\n",
            "Iteration 35, loss = 1.05653793\n",
            "Iteration 36, loss = 1.03861650\n",
            "Iteration 37, loss = 1.03620495\n",
            "Iteration 38, loss = 1.02136416\n",
            "Iteration 39, loss = 1.00969567\n",
            "Iteration 40, loss = 1.00690299\n",
            "Iteration 41, loss = 1.02324430\n",
            "Iteration 42, loss = 1.01876868\n",
            "Iteration 43, loss = 1.00070623\n",
            "Iteration 44, loss = 1.02255095\n",
            "Iteration 45, loss = 1.02279539\n",
            "Iteration 46, loss = 1.06244039\n",
            "Iteration 47, loss = 0.98376223\n",
            "Iteration 48, loss = 0.97319735\n",
            "Iteration 49, loss = 0.97489546\n",
            "Iteration 50, loss = 0.94782048\n",
            "Iteration 51, loss = 0.95505382\n",
            "Iteration 52, loss = 0.98354621\n",
            "Iteration 53, loss = 0.97563538\n",
            "Iteration 54, loss = 0.94156391\n",
            "Iteration 55, loss = 1.00294561\n",
            "Iteration 56, loss = 1.05508764\n",
            "Iteration 57, loss = 1.00291118\n",
            "Iteration 58, loss = 0.98260357\n",
            "Iteration 59, loss = 0.96816268\n",
            "Iteration 60, loss = 0.93700212\n",
            "Iteration 61, loss = 0.93956797\n",
            "Iteration 62, loss = 0.97256854\n",
            "Iteration 63, loss = 0.90859350\n",
            "Iteration 64, loss = 0.91612282\n",
            "Iteration 65, loss = 0.88687816\n",
            "Iteration 66, loss = 0.90415683\n",
            "Iteration 67, loss = 0.89259360\n",
            "Iteration 68, loss = 0.87994929\n",
            "Iteration 69, loss = 0.88467698\n",
            "Iteration 70, loss = 0.86131127\n",
            "Iteration 71, loss = 0.87305136\n",
            "Iteration 72, loss = 0.86789411\n",
            "Iteration 73, loss = 0.87705671\n",
            "Iteration 74, loss = 0.86600732\n",
            "Iteration 75, loss = 0.84346972\n",
            "Iteration 76, loss = 0.86733073\n",
            "Iteration 77, loss = 0.89860165\n",
            "Iteration 78, loss = 0.95046766\n",
            "Iteration 79, loss = 0.87806928\n",
            "Iteration 80, loss = 0.85850464\n",
            "Iteration 81, loss = 0.85767828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 82, loss = 0.84168233\n",
            "Iteration 83, loss = 0.88493909\n",
            "Iteration 84, loss = 0.82029822\n",
            "Iteration 85, loss = 0.82265599\n",
            "Iteration 86, loss = 0.81271984\n",
            "Iteration 87, loss = 0.81603205\n",
            "Iteration 88, loss = 0.79776354\n",
            "Iteration 89, loss = 0.80789817\n",
            "Iteration 90, loss = 0.83282908\n",
            "Iteration 91, loss = 0.80116812\n",
            "Iteration 92, loss = 0.80348908\n",
            "Iteration 93, loss = 0.82723228\n",
            "Iteration 94, loss = 0.79684080\n",
            "Iteration 95, loss = 0.83723281\n",
            "Iteration 96, loss = 0.79440325\n",
            "Iteration 97, loss = 0.81405659\n",
            "Iteration 98, loss = 0.78408135\n",
            "Iteration 99, loss = 0.78126239\n",
            "Iteration 100, loss = 0.77129689\n",
            "Iteration 1, loss = 1.62109092\n",
            "Iteration 2, loss = 1.61679941\n",
            "Iteration 3, loss = 1.61259187\n",
            "Iteration 4, loss = 1.60969603\n",
            "Iteration 5, loss = 1.60700204\n",
            "Iteration 6, loss = 1.60474409\n",
            "Iteration 7, loss = 1.60257755\n",
            "Iteration 8, loss = 1.60107456\n",
            "Iteration 9, loss = 1.60033302\n",
            "Iteration 10, loss = 1.59880825\n",
            "Iteration 11, loss = 1.59838004\n",
            "Iteration 12, loss = 1.59808140\n",
            "Iteration 13, loss = 1.59770874\n",
            "Iteration 14, loss = 1.59721335\n",
            "Iteration 15, loss = 1.59732737\n",
            "Iteration 16, loss = 1.59706117\n",
            "Iteration 17, loss = 1.59697796\n",
            "Iteration 18, loss = 1.59688356\n",
            "Iteration 19, loss = 1.59693994\n",
            "Iteration 20, loss = 1.59700689\n",
            "Iteration 21, loss = 1.59674675\n",
            "Iteration 22, loss = 1.59673602\n",
            "Iteration 23, loss = 1.59659783\n",
            "Iteration 24, loss = 1.59653117\n",
            "Iteration 25, loss = 1.59638887\n",
            "Iteration 26, loss = 1.59630688\n",
            "Iteration 27, loss = 1.59620831\n",
            "Iteration 28, loss = 1.59603021\n",
            "Iteration 29, loss = 1.59608904\n",
            "Iteration 30, loss = 1.59587775\n",
            "Iteration 31, loss = 1.59575130\n",
            "Iteration 32, loss = 1.59571746\n",
            "Iteration 33, loss = 1.59556302\n",
            "Iteration 34, loss = 1.59559732\n",
            "Iteration 35, loss = 1.59545511\n",
            "Iteration 36, loss = 1.59537343\n",
            "Iteration 37, loss = 1.59531598\n",
            "Iteration 38, loss = 1.59518219\n",
            "Iteration 39, loss = 1.59510197\n",
            "Iteration 40, loss = 1.59502150\n",
            "Iteration 41, loss = 1.59495336\n",
            "Iteration 42, loss = 1.59481216\n",
            "Iteration 43, loss = 1.59478348\n",
            "Iteration 44, loss = 1.59462702\n",
            "Iteration 45, loss = 1.59455540\n",
            "Iteration 46, loss = 1.59451950\n",
            "Iteration 47, loss = 1.59443624\n",
            "Iteration 48, loss = 1.59435604\n",
            "Iteration 49, loss = 1.59426321\n",
            "Iteration 50, loss = 1.59415225\n",
            "Iteration 51, loss = 1.59404044\n",
            "Iteration 52, loss = 1.59398123\n",
            "Iteration 53, loss = 1.59388672\n",
            "Iteration 54, loss = 1.59391113\n",
            "Iteration 55, loss = 1.59372273\n",
            "Iteration 56, loss = 1.59368861\n",
            "Iteration 57, loss = 1.59348653\n",
            "Iteration 58, loss = 1.59346088\n",
            "Iteration 59, loss = 1.59329468\n",
            "Iteration 60, loss = 1.59327660\n",
            "Iteration 61, loss = 1.59313922\n",
            "Iteration 62, loss = 1.59315754\n",
            "Iteration 63, loss = 1.59298045\n",
            "Iteration 64, loss = 1.59282728\n",
            "Iteration 65, loss = 1.59275100\n",
            "Iteration 66, loss = 1.59269738\n",
            "Iteration 67, loss = 1.59258352\n",
            "Iteration 68, loss = 1.59251670\n",
            "Iteration 69, loss = 1.59238831\n",
            "Iteration 70, loss = 1.59226525\n",
            "Iteration 71, loss = 1.59221804\n",
            "Iteration 72, loss = 1.59217544\n",
            "Iteration 73, loss = 1.59197938\n",
            "Iteration 74, loss = 1.59188209\n",
            "Iteration 75, loss = 1.59193412\n",
            "Iteration 76, loss = 1.59175224\n",
            "Iteration 77, loss = 1.59159328\n",
            "Iteration 78, loss = 1.59151840\n",
            "Iteration 79, loss = 1.59140245\n",
            "Iteration 80, loss = 1.59130518\n",
            "Iteration 81, loss = 1.59119003\n",
            "Iteration 82, loss = 1.59111280\n",
            "Iteration 83, loss = 1.59099925\n",
            "Iteration 84, loss = 1.59092249\n",
            "Iteration 85, loss = 1.59079987\n",
            "Iteration 86, loss = 1.59068846\n",
            "Iteration 87, loss = 1.59056416\n",
            "Iteration 88, loss = 1.59045139\n",
            "Iteration 89, loss = 1.59038570\n",
            "Iteration 90, loss = 1.59031865\n",
            "Iteration 91, loss = 1.59014665\n",
            "Iteration 92, loss = 1.59004400\n",
            "Iteration 93, loss = 1.58992233\n",
            "Iteration 94, loss = 1.58983562\n",
            "Iteration 95, loss = 1.58972332\n",
            "Iteration 96, loss = 1.58961904\n",
            "Iteration 97, loss = 1.58949356\n",
            "Iteration 98, loss = 1.58940967\n",
            "Iteration 99, loss = 1.58928242\n",
            "Iteration 100, loss = 1.58919394\n",
            "Iteration 1, loss = 1.62167768\n",
            "Iteration 2, loss = 1.60277275\n",
            "Iteration 3, loss = 1.60344837\n",
            "Iteration 4, loss = 1.60324649\n",
            "Iteration 5, loss = 1.59959398\n",
            "Iteration 6, loss = 1.59690689\n",
            "Iteration 7, loss = 1.59606925\n",
            "Iteration 8, loss = 1.59587263\n",
            "Iteration 9, loss = 1.59460987\n",
            "Iteration 10, loss = 1.59152540\n",
            "Iteration 11, loss = 1.59111273\n",
            "Iteration 12, loss = 1.59113491\n",
            "Iteration 13, loss = 1.59034250\n",
            "Iteration 14, loss = 1.58927329\n",
            "Iteration 15, loss = 1.58684482\n",
            "Iteration 16, loss = 1.58649236\n",
            "Iteration 17, loss = 1.58530317\n",
            "Iteration 18, loss = 1.58382208\n",
            "Iteration 19, loss = 1.58334865\n",
            "Iteration 20, loss = 1.58282485\n",
            "Iteration 21, loss = 1.57961391\n",
            "Iteration 22, loss = 1.57848627\n",
            "Iteration 23, loss = 1.57698434\n",
            "Iteration 24, loss = 1.57537118\n",
            "Iteration 25, loss = 1.57323647\n",
            "Iteration 26, loss = 1.57162276\n",
            "Iteration 27, loss = 1.57030438\n",
            "Iteration 28, loss = 1.56718438\n",
            "Iteration 29, loss = 1.56568550\n",
            "Iteration 30, loss = 1.56255429\n",
            "Iteration 31, loss = 1.56017935\n",
            "Iteration 32, loss = 1.55788931\n",
            "Iteration 33, loss = 1.55463351\n",
            "Iteration 34, loss = 1.55268248\n",
            "Iteration 35, loss = 1.54838591\n",
            "Iteration 36, loss = 1.54487810\n",
            "Iteration 37, loss = 1.54191597\n",
            "Iteration 38, loss = 1.53814771\n",
            "Iteration 39, loss = 1.53435090\n",
            "Iteration 40, loss = 1.53022397\n",
            "Iteration 41, loss = 1.52635880\n",
            "Iteration 42, loss = 1.52082444\n",
            "Iteration 43, loss = 1.51692984\n",
            "Iteration 44, loss = 1.51112963\n",
            "Iteration 45, loss = 1.50626873\n",
            "Iteration 46, loss = 1.50186155\n",
            "Iteration 47, loss = 1.49606197\n",
            "Iteration 48, loss = 1.49096538\n",
            "Iteration 49, loss = 1.48432154\n",
            "Iteration 50, loss = 1.47851374\n",
            "Iteration 51, loss = 1.47263242\n",
            "Iteration 52, loss = 1.46661819\n",
            "Iteration 53, loss = 1.46016996\n",
            "Iteration 54, loss = 1.45469757\n",
            "Iteration 55, loss = 1.44663765\n",
            "Iteration 56, loss = 1.44102838\n",
            "Iteration 57, loss = 1.43301788\n",
            "Iteration 58, loss = 1.42651269\n",
            "Iteration 59, loss = 1.41866279\n",
            "Iteration 60, loss = 1.41279413\n",
            "Iteration 61, loss = 1.40532137\n",
            "Iteration 62, loss = 1.39947218\n",
            "Iteration 63, loss = 1.39148761\n",
            "Iteration 64, loss = 1.38364612\n",
            "Iteration 65, loss = 1.37670147\n",
            "Iteration 66, loss = 1.36984476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 67, loss = 1.36295259\n",
            "Iteration 68, loss = 1.35620722\n",
            "Iteration 69, loss = 1.34908378\n",
            "Iteration 70, loss = 1.34218094\n",
            "Iteration 71, loss = 1.33618981\n",
            "Iteration 72, loss = 1.32994090\n",
            "Iteration 73, loss = 1.32276480\n",
            "Iteration 74, loss = 1.31647383\n",
            "Iteration 75, loss = 1.31186427\n",
            "Iteration 76, loss = 1.30570342\n",
            "Iteration 77, loss = 1.29866684\n",
            "Iteration 78, loss = 1.29320702\n",
            "Iteration 79, loss = 1.28810823\n",
            "Iteration 80, loss = 1.28258458\n",
            "Iteration 81, loss = 1.27719814\n",
            "Iteration 82, loss = 1.27203848\n",
            "Iteration 83, loss = 1.26744194\n",
            "Iteration 84, loss = 1.26298020\n",
            "Iteration 85, loss = 1.25843258\n",
            "Iteration 86, loss = 1.25393127\n",
            "Iteration 87, loss = 1.24968099\n",
            "Iteration 88, loss = 1.24545452\n",
            "Iteration 89, loss = 1.24172332\n",
            "Iteration 90, loss = 1.23847113\n",
            "Iteration 91, loss = 1.23416323\n",
            "Iteration 92, loss = 1.23072460\n",
            "Iteration 93, loss = 1.22751353\n",
            "Iteration 94, loss = 1.22448658\n",
            "Iteration 95, loss = 1.22104802\n",
            "Iteration 96, loss = 1.21814527\n",
            "Iteration 97, loss = 1.21521469\n",
            "Iteration 98, loss = 1.21253495\n",
            "Iteration 99, loss = 1.20950529\n",
            "Iteration 100, loss = 1.20717628\n",
            "Iteration 1, loss = 1.99973072\n",
            "Iteration 2, loss = 1.65333496\n",
            "Iteration 3, loss = 1.90045519\n",
            "Iteration 4, loss = 1.72973472\n",
            "Iteration 5, loss = 1.61226701\n",
            "Iteration 6, loss = 1.66982676\n",
            "Iteration 7, loss = 1.67110899\n",
            "Iteration 8, loss = 1.62720001\n",
            "Iteration 9, loss = 1.59421056\n",
            "Iteration 10, loss = 1.59340695\n",
            "Iteration 11, loss = 1.59749636\n",
            "Iteration 12, loss = 1.59403820\n",
            "Iteration 13, loss = 1.58347714\n",
            "Iteration 14, loss = 1.57023277\n",
            "Iteration 15, loss = 1.56565341\n",
            "Iteration 16, loss = 1.55695812\n",
            "Iteration 17, loss = 1.54692378\n",
            "Iteration 18, loss = 1.53412891\n",
            "Iteration 19, loss = 1.51834211\n",
            "Iteration 20, loss = 1.49883707\n",
            "Iteration 21, loss = 1.47678306\n",
            "Iteration 22, loss = 1.44838925\n",
            "Iteration 23, loss = 1.41523136\n",
            "Iteration 24, loss = 1.38095729\n",
            "Iteration 25, loss = 1.34503859\n",
            "Iteration 26, loss = 1.31344284\n",
            "Iteration 27, loss = 1.28479036\n",
            "Iteration 28, loss = 1.25723529\n",
            "Iteration 29, loss = 1.23797725\n",
            "Iteration 30, loss = 1.21922810\n",
            "Iteration 31, loss = 1.20288666\n",
            "Iteration 32, loss = 1.19684034\n",
            "Iteration 33, loss = 1.18416348\n",
            "Iteration 34, loss = 1.17383484\n",
            "Iteration 35, loss = 1.16579339\n",
            "Iteration 36, loss = 1.16111681\n",
            "Iteration 37, loss = 1.15505233\n",
            "Iteration 38, loss = 1.15319020\n",
            "Iteration 39, loss = 1.14601965\n",
            "Iteration 40, loss = 1.14502630\n",
            "Iteration 41, loss = 1.14078842\n",
            "Iteration 42, loss = 1.14024257\n",
            "Iteration 43, loss = 1.13317116\n",
            "Iteration 44, loss = 1.13390021\n",
            "Iteration 45, loss = 1.12458964\n",
            "Iteration 46, loss = 1.13121049\n",
            "Iteration 47, loss = 1.12356918\n",
            "Iteration 48, loss = 1.12739695\n",
            "Iteration 49, loss = 1.12258324\n",
            "Iteration 50, loss = 1.12001307\n",
            "Iteration 51, loss = 1.12025937\n",
            "Iteration 52, loss = 1.11882563\n",
            "Iteration 53, loss = 1.11717275\n",
            "Iteration 54, loss = 1.12137223\n",
            "Iteration 55, loss = 1.12111566\n",
            "Iteration 56, loss = 1.11977760\n",
            "Iteration 57, loss = 1.11462723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 1.11595165\n",
            "Iteration 59, loss = 1.11154720\n",
            "Iteration 60, loss = 1.11940238\n",
            "Iteration 61, loss = 1.11007772\n",
            "Iteration 62, loss = 1.11981276\n",
            "Iteration 63, loss = 1.11220608\n",
            "Iteration 64, loss = 1.11033395\n",
            "Iteration 65, loss = 1.10972972\n",
            "Iteration 66, loss = 1.11081128\n",
            "Iteration 67, loss = 1.11201437\n",
            "Iteration 68, loss = 1.10863505\n",
            "Iteration 69, loss = 1.10680188\n",
            "Iteration 70, loss = 1.10541598\n",
            "Iteration 71, loss = 1.10797501\n",
            "Iteration 72, loss = 1.10882804\n",
            "Iteration 73, loss = 1.10275626\n",
            "Iteration 74, loss = 1.10371434\n",
            "Iteration 75, loss = 1.10624995\n",
            "Iteration 76, loss = 1.10645485\n",
            "Iteration 77, loss = 1.10382996\n",
            "Iteration 78, loss = 1.10653229\n",
            "Iteration 79, loss = 1.10506754\n",
            "Iteration 80, loss = 1.09987600\n",
            "Iteration 81, loss = 1.10309464\n",
            "Iteration 82, loss = 1.09977497\n",
            "Iteration 83, loss = 1.09803336\n",
            "Iteration 84, loss = 1.10660545\n",
            "Iteration 85, loss = 1.10158770\n",
            "Iteration 86, loss = 1.09986895\n",
            "Iteration 87, loss = 1.09595842\n",
            "Iteration 88, loss = 1.09726869\n",
            "Iteration 89, loss = 1.09497189\n",
            "Iteration 90, loss = 1.09607395\n",
            "Iteration 91, loss = 1.09203035\n",
            "Iteration 92, loss = 1.09436263\n",
            "Iteration 93, loss = 1.09748062\n",
            "Iteration 94, loss = 1.08389120\n",
            "Iteration 95, loss = 1.08726591\n",
            "Iteration 96, loss = 1.08449703\n",
            "Iteration 97, loss = 1.08091161\n",
            "Iteration 98, loss = 1.08644151\n",
            "Iteration 99, loss = 1.07622813\n",
            "Iteration 100, loss = 1.07917275\n",
            "Iteration 1, loss = 1.74566493\n",
            "Iteration 2, loss = 1.73409941\n",
            "Iteration 3, loss = 1.72308479\n",
            "Iteration 4, loss = 1.71215729\n",
            "Iteration 5, loss = 1.70203231\n",
            "Iteration 6, loss = 1.69209930\n",
            "Iteration 7, loss = 1.68297235\n",
            "Iteration 8, loss = 1.67462442\n",
            "Iteration 9, loss = 1.66682014\n",
            "Iteration 10, loss = 1.65911686\n",
            "Iteration 11, loss = 1.65211413\n",
            "Iteration 12, loss = 1.64564380\n",
            "Iteration 13, loss = 1.63962864\n",
            "Iteration 14, loss = 1.63437461\n",
            "Iteration 15, loss = 1.62881334\n",
            "Iteration 16, loss = 1.62426989\n",
            "Iteration 17, loss = 1.61974608\n",
            "Iteration 18, loss = 1.61523527\n",
            "Iteration 19, loss = 1.61161305\n",
            "Iteration 20, loss = 1.60817822\n",
            "Iteration 21, loss = 1.60483128\n",
            "Iteration 22, loss = 1.60184350\n",
            "Iteration 23, loss = 1.59907668\n",
            "Iteration 24, loss = 1.59682064\n",
            "Iteration 25, loss = 1.59423574\n",
            "Iteration 26, loss = 1.59200769\n",
            "Iteration 27, loss = 1.59006432\n",
            "Iteration 28, loss = 1.58832080\n",
            "Iteration 29, loss = 1.58670764\n",
            "Iteration 30, loss = 1.58505949\n",
            "Iteration 31, loss = 1.58367921\n",
            "Iteration 32, loss = 1.58221982\n",
            "Iteration 33, loss = 1.58110784\n",
            "Iteration 34, loss = 1.57972903\n",
            "Iteration 35, loss = 1.57877493\n",
            "Iteration 36, loss = 1.57768496\n",
            "Iteration 37, loss = 1.57658986\n",
            "Iteration 38, loss = 1.57590414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 39, loss = 1.57502635\n",
            "Iteration 40, loss = 1.57411220\n",
            "Iteration 41, loss = 1.57317580\n",
            "Iteration 42, loss = 1.57246111\n",
            "Iteration 43, loss = 1.57163883\n",
            "Iteration 44, loss = 1.57091196\n",
            "Iteration 45, loss = 1.57012680\n",
            "Iteration 46, loss = 1.56938446\n",
            "Iteration 47, loss = 1.56864972\n",
            "Iteration 48, loss = 1.56803524\n",
            "Iteration 49, loss = 1.56723345\n",
            "Iteration 50, loss = 1.56652285\n",
            "Iteration 51, loss = 1.56579476\n",
            "Iteration 52, loss = 1.56508833\n",
            "Iteration 53, loss = 1.56433578\n",
            "Iteration 54, loss = 1.56370216\n",
            "Iteration 55, loss = 1.56288885\n",
            "Iteration 56, loss = 1.56220598\n",
            "Iteration 57, loss = 1.56138446\n",
            "Iteration 58, loss = 1.56067705\n",
            "Iteration 59, loss = 1.55988904\n",
            "Iteration 60, loss = 1.55918316\n",
            "Iteration 61, loss = 1.55839390\n",
            "Iteration 62, loss = 1.55768140\n",
            "Iteration 63, loss = 1.55690958\n",
            "Iteration 64, loss = 1.55608149\n",
            "Iteration 65, loss = 1.55531008\n",
            "Iteration 66, loss = 1.55454740\n",
            "Iteration 67, loss = 1.55373447\n",
            "Iteration 68, loss = 1.55295282\n",
            "Iteration 69, loss = 1.55212029\n",
            "Iteration 70, loss = 1.55131026\n",
            "Iteration 71, loss = 1.55050534\n",
            "Iteration 72, loss = 1.54970946\n",
            "Iteration 73, loss = 1.54883574\n",
            "Iteration 74, loss = 1.54797539\n",
            "Iteration 75, loss = 1.54718567\n",
            "Iteration 76, loss = 1.54628600\n",
            "Iteration 77, loss = 1.54538357\n",
            "Iteration 78, loss = 1.54453918\n",
            "Iteration 79, loss = 1.54366016\n",
            "Iteration 80, loss = 1.54278118\n",
            "Iteration 81, loss = 1.54185651\n",
            "Iteration 82, loss = 1.54097356\n",
            "Iteration 83, loss = 1.54004202\n",
            "Iteration 84, loss = 1.53913011\n",
            "Iteration 85, loss = 1.53820688\n",
            "Iteration 86, loss = 1.53728525\n",
            "Iteration 87, loss = 1.53628747\n",
            "Iteration 88, loss = 1.53533640\n",
            "Iteration 89, loss = 1.53439334\n",
            "Iteration 90, loss = 1.53344312\n",
            "Iteration 91, loss = 1.53243925\n",
            "Iteration 92, loss = 1.53146599\n",
            "Iteration 93, loss = 1.53044955\n",
            "Iteration 94, loss = 1.52948115\n",
            "Iteration 95, loss = 1.52840650\n",
            "Iteration 96, loss = 1.52738448\n",
            "Iteration 97, loss = 1.52637106\n",
            "Iteration 98, loss = 1.52535097\n",
            "Iteration 99, loss = 1.52431559\n",
            "Iteration 100, loss = 1.52324326\n",
            "Iteration 1, loss = 1.72065748\n",
            "Iteration 2, loss = 1.63893142\n",
            "Iteration 3, loss = 1.60051333\n",
            "Iteration 4, loss = 1.58372731\n",
            "Iteration 5, loss = 1.58164477\n",
            "Iteration 6, loss = 1.58158436\n",
            "Iteration 7, loss = 1.57743280\n",
            "Iteration 8, loss = 1.56894701\n",
            "Iteration 9, loss = 1.55694511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 1.54710439\n",
            "Iteration 11, loss = 1.53649026\n",
            "Iteration 12, loss = 1.52698766\n",
            "Iteration 13, loss = 1.51872606\n",
            "Iteration 14, loss = 1.51093490\n",
            "Iteration 15, loss = 1.50112676\n",
            "Iteration 16, loss = 1.49110547\n",
            "Iteration 17, loss = 1.48006158\n",
            "Iteration 18, loss = 1.46715673\n",
            "Iteration 19, loss = 1.45524791\n",
            "Iteration 20, loss = 1.44297068\n",
            "Iteration 21, loss = 1.42931824\n",
            "Iteration 22, loss = 1.41599958\n",
            "Iteration 23, loss = 1.40224003\n",
            "Iteration 24, loss = 1.38855091\n",
            "Iteration 25, loss = 1.37411906\n",
            "Iteration 26, loss = 1.35987787\n",
            "Iteration 27, loss = 1.34568600\n",
            "Iteration 28, loss = 1.33120684\n",
            "Iteration 29, loss = 1.31850328\n",
            "Iteration 30, loss = 1.30451510\n",
            "Iteration 31, loss = 1.29188882\n",
            "Iteration 32, loss = 1.28045828\n",
            "Iteration 33, loss = 1.26815053\n",
            "Iteration 34, loss = 1.25754404\n",
            "Iteration 35, loss = 1.24735877\n",
            "Iteration 36, loss = 1.23763771\n",
            "Iteration 37, loss = 1.22851973\n",
            "Iteration 38, loss = 1.22061478\n",
            "Iteration 39, loss = 1.21280444\n",
            "Iteration 40, loss = 1.20603569\n",
            "Iteration 41, loss = 1.19895426\n",
            "Iteration 42, loss = 1.19208831\n",
            "Iteration 43, loss = 1.18679414\n",
            "Iteration 44, loss = 1.18128814\n",
            "Iteration 45, loss = 1.17581705\n",
            "Iteration 46, loss = 1.17199260\n",
            "Iteration 47, loss = 1.16730121\n",
            "Iteration 48, loss = 1.16303538\n",
            "Iteration 49, loss = 1.15917456\n",
            "Iteration 50, loss = 1.15578939\n",
            "Iteration 51, loss = 1.15225414\n",
            "Iteration 52, loss = 1.14946007\n",
            "Iteration 53, loss = 1.14618194\n",
            "Iteration 54, loss = 1.14444674\n",
            "Iteration 55, loss = 1.14102034\n",
            "Iteration 56, loss = 1.13919132\n",
            "Iteration 57, loss = 1.13575152\n",
            "Iteration 58, loss = 1.13355358\n",
            "Iteration 59, loss = 1.13098728\n",
            "Iteration 60, loss = 1.13092276\n",
            "Iteration 61, loss = 1.12728558\n",
            "Iteration 62, loss = 1.12573327\n",
            "Iteration 63, loss = 1.12403936\n",
            "Iteration 64, loss = 1.12283727\n",
            "Iteration 65, loss = 1.12010885\n",
            "Iteration 66, loss = 1.11858450\n",
            "Iteration 67, loss = 1.11781262\n",
            "Iteration 68, loss = 1.11611233\n",
            "Iteration 69, loss = 1.11426014\n",
            "Iteration 70, loss = 1.11282492\n",
            "Iteration 71, loss = 1.11150549\n",
            "Iteration 72, loss = 1.11099587\n",
            "Iteration 73, loss = 1.10883539\n",
            "Iteration 74, loss = 1.10755698\n",
            "Iteration 75, loss = 1.10758248\n",
            "Iteration 76, loss = 1.10583902\n",
            "Iteration 77, loss = 1.10363554\n",
            "Iteration 78, loss = 1.10278164\n",
            "Iteration 79, loss = 1.10239503\n",
            "Iteration 80, loss = 1.10022960\n",
            "Iteration 81, loss = 1.09910293\n",
            "Iteration 82, loss = 1.09812710\n",
            "Iteration 83, loss = 1.09701105\n",
            "Iteration 84, loss = 1.09705415\n",
            "Iteration 85, loss = 1.09535454\n",
            "Iteration 86, loss = 1.09385379\n",
            "Iteration 87, loss = 1.09261345\n",
            "Iteration 88, loss = 1.09136968\n",
            "Iteration 89, loss = 1.09069170\n",
            "Iteration 90, loss = 1.09233953\n",
            "Iteration 91, loss = 1.08823042\n",
            "Iteration 92, loss = 1.08672936\n",
            "Iteration 93, loss = 1.08650310\n",
            "Iteration 94, loss = 1.08479925\n",
            "Iteration 95, loss = 1.08378261\n",
            "Iteration 96, loss = 1.08265877\n",
            "Iteration 97, loss = 1.08130033\n",
            "Iteration 98, loss = 1.08190619\n",
            "Iteration 99, loss = 1.07886819\n",
            "Iteration 100, loss = 1.07836933\n",
            "Iteration 1, loss = 1.69185287\n",
            "Iteration 2, loss = 1.60930034\n",
            "Iteration 3, loss = 1.56977649\n",
            "Iteration 4, loss = 1.47970055\n",
            "Iteration 5, loss = 1.41673785\n",
            "Iteration 6, loss = 1.33763201\n",
            "Iteration 7, loss = 1.28199554\n",
            "Iteration 8, loss = 1.21508059\n",
            "Iteration 9, loss = 1.18143769\n",
            "Iteration 10, loss = 1.15671983\n",
            "Iteration 11, loss = 1.14167539\n",
            "Iteration 12, loss = 1.12153785\n",
            "Iteration 13, loss = 1.11860947\n",
            "Iteration 14, loss = 1.11843077\n",
            "Iteration 15, loss = 1.12739448\n",
            "Iteration 16, loss = 1.13011595\n",
            "Iteration 17, loss = 1.10995974\n",
            "Iteration 18, loss = 1.11978465\n",
            "Iteration 19, loss = 1.12792830\n",
            "Iteration 20, loss = 1.12808706\n",
            "Iteration 21, loss = 1.10691572\n",
            "Iteration 22, loss = 1.12117772\n",
            "Iteration 23, loss = 1.09288204\n",
            "Iteration 24, loss = 1.09129126\n",
            "Iteration 25, loss = 1.09701938\n",
            "Iteration 26, loss = 1.07732574\n",
            "Iteration 27, loss = 1.08977206\n",
            "Iteration 28, loss = 1.08346441\n",
            "Iteration 29, loss = 1.08254609\n",
            "Iteration 30, loss = 1.07820294\n",
            "Iteration 31, loss = 1.05207129\n",
            "Iteration 32, loss = 1.06038826\n",
            "Iteration 33, loss = 1.06334634\n",
            "Iteration 34, loss = 1.04900694\n",
            "Iteration 35, loss = 1.05292507\n",
            "Iteration 36, loss = 1.03964345\n",
            "Iteration 37, loss = 1.02632676\n",
            "Iteration 38, loss = 1.01679575\n",
            "Iteration 39, loss = 1.00547376\n",
            "Iteration 40, loss = 0.99787227\n",
            "Iteration 41, loss = 1.01410030\n",
            "Iteration 42, loss = 0.99324741\n",
            "Iteration 43, loss = 0.97571141\n",
            "Iteration 44, loss = 0.99948388\n",
            "Iteration 45, loss = 0.98175447\n",
            "Iteration 46, loss = 1.02671276\n",
            "Iteration 47, loss = 1.00571591\n",
            "Iteration 48, loss = 0.98424238\n",
            "Iteration 49, loss = 0.94810501\n",
            "Iteration 50, loss = 0.95832455\n",
            "Iteration 51, loss = 0.95547516\n",
            "Iteration 52, loss = 0.94141986\n",
            "Iteration 53, loss = 0.92053028\n",
            "Iteration 54, loss = 0.93257737\n",
            "Iteration 55, loss = 0.96869498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 56, loss = 1.01049471\n",
            "Iteration 57, loss = 0.99218744\n",
            "Iteration 58, loss = 0.96003897\n",
            "Iteration 59, loss = 0.97438728\n",
            "Iteration 60, loss = 0.96324296\n",
            "Iteration 61, loss = 0.92382986\n",
            "Iteration 62, loss = 0.96236792\n",
            "Iteration 63, loss = 0.96380885\n",
            "Iteration 64, loss = 0.93746788\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.64617099\n",
            "Iteration 2, loss = 1.64196159\n",
            "Iteration 3, loss = 1.63796078\n",
            "Iteration 4, loss = 1.63388795\n",
            "Iteration 5, loss = 1.63012911\n",
            "Iteration 6, loss = 1.62640078\n",
            "Iteration 7, loss = 1.62296985\n",
            "Iteration 8, loss = 1.61986352\n",
            "Iteration 9, loss = 1.61699794\n",
            "Iteration 10, loss = 1.61390680\n",
            "Iteration 11, loss = 1.61118349\n",
            "Iteration 12, loss = 1.60869978\n",
            "Iteration 13, loss = 1.60629732\n",
            "Iteration 14, loss = 1.60418975\n",
            "Iteration 15, loss = 1.60188642\n",
            "Iteration 16, loss = 1.60002364\n",
            "Iteration 17, loss = 1.59807047\n",
            "Iteration 18, loss = 1.59606354\n",
            "Iteration 19, loss = 1.59449354\n",
            "Iteration 20, loss = 1.59296494\n",
            "Iteration 21, loss = 1.59139790\n",
            "Iteration 22, loss = 1.59001281\n",
            "Iteration 23, loss = 1.58865776\n",
            "Iteration 24, loss = 1.58756859\n",
            "Iteration 25, loss = 1.58623955\n",
            "Iteration 26, loss = 1.58505081\n",
            "Iteration 27, loss = 1.58402884\n",
            "Iteration 28, loss = 1.58304374\n",
            "Iteration 29, loss = 1.58210903\n",
            "Iteration 30, loss = 1.58112783\n",
            "Iteration 31, loss = 1.58026161\n",
            "Iteration 32, loss = 1.57928133\n",
            "Iteration 33, loss = 1.57853308\n",
            "Iteration 34, loss = 1.57765700\n",
            "Iteration 35, loss = 1.57701611\n",
            "Iteration 36, loss = 1.57625243\n",
            "Iteration 37, loss = 1.57544733\n",
            "Iteration 38, loss = 1.57488411\n",
            "Iteration 39, loss = 1.57417831\n",
            "Iteration 40, loss = 1.57347182\n",
            "Iteration 41, loss = 1.57275961\n",
            "Iteration 42, loss = 1.57214994\n",
            "Iteration 43, loss = 1.57146324\n",
            "Iteration 44, loss = 1.57082049\n",
            "Iteration 45, loss = 1.57016297\n",
            "Iteration 46, loss = 1.56951742\n",
            "Iteration 47, loss = 1.56889244\n",
            "Iteration 48, loss = 1.56832451\n",
            "Iteration 49, loss = 1.56762085\n",
            "Iteration 50, loss = 1.56698755\n",
            "Iteration 51, loss = 1.56634578\n",
            "Iteration 52, loss = 1.56571529\n",
            "Iteration 53, loss = 1.56503637\n",
            "Iteration 54, loss = 1.56445346\n",
            "Iteration 55, loss = 1.56373795\n",
            "Iteration 56, loss = 1.56311384\n",
            "Iteration 57, loss = 1.56237621\n",
            "Iteration 58, loss = 1.56173221\n",
            "Iteration 59, loss = 1.56102349\n",
            "Iteration 60, loss = 1.56037961\n",
            "Iteration 61, loss = 1.55966383\n",
            "Iteration 62, loss = 1.55900507\n",
            "Iteration 63, loss = 1.55830895\n",
            "Iteration 64, loss = 1.55754548\n",
            "Iteration 65, loss = 1.55683772\n",
            "Iteration 66, loss = 1.55612640\n",
            "Iteration 67, loss = 1.55538205\n",
            "Iteration 68, loss = 1.55466020\n",
            "Iteration 69, loss = 1.55388345\n",
            "Iteration 70, loss = 1.55313516\n",
            "Iteration 71, loss = 1.55238405\n",
            "Iteration 72, loss = 1.55163803\n",
            "Iteration 73, loss = 1.55082645\n",
            "Iteration 74, loss = 1.55001758\n",
            "Iteration 75, loss = 1.54926264\n",
            "Iteration 76, loss = 1.54843141\n",
            "Iteration 77, loss = 1.54757963\n",
            "Iteration 78, loss = 1.54677639\n",
            "Iteration 79, loss = 1.54594761\n",
            "Iteration 80, loss = 1.54511731\n",
            "Iteration 81, loss = 1.54424084\n",
            "Iteration 82, loss = 1.54339800\n",
            "Iteration 83, loss = 1.54251217\n",
            "Iteration 84, loss = 1.54163868\n",
            "Iteration 85, loss = 1.54075899\n",
            "Iteration 86, loss = 1.53987732\n",
            "Iteration 87, loss = 1.53892162\n",
            "Iteration 88, loss = 1.53801214\n",
            "Iteration 89, loss = 1.53710475\n",
            "Iteration 90, loss = 1.53619011\n",
            "Iteration 91, loss = 1.53522659\n",
            "Iteration 92, loss = 1.53429104\n",
            "Iteration 93, loss = 1.53330827\n",
            "Iteration 94, loss = 1.53237467\n",
            "Iteration 95, loss = 1.53133405\n",
            "Iteration 96, loss = 1.53034423\n",
            "Iteration 97, loss = 1.52936240\n",
            "Iteration 98, loss = 1.52836995\n",
            "Iteration 99, loss = 1.52736762\n",
            "Iteration 100, loss = 1.52632166\n",
            "Iteration 1, loss = 1.63640230\n",
            "Iteration 2, loss = 1.60566935\n",
            "Iteration 3, loss = 1.58886548\n",
            "Iteration 4, loss = 1.57894468\n",
            "Iteration 5, loss = 1.57484243\n",
            "Iteration 6, loss = 1.57164172\n",
            "Iteration 7, loss = 1.56700153\n",
            "Iteration 8, loss = 1.56136478\n",
            "Iteration 9, loss = 1.55359732\n",
            "Iteration 10, loss = 1.54691594\n",
            "Iteration 11, loss = 1.53876879\n",
            "Iteration 12, loss = 1.53017246\n",
            "Iteration 13, loss = 1.52190948\n",
            "Iteration 14, loss = 1.51398994\n",
            "Iteration 15, loss = 1.50401779\n",
            "Iteration 16, loss = 1.49463694\n",
            "Iteration 17, loss = 1.48414331\n",
            "Iteration 18, loss = 1.47217361\n",
            "Iteration 19, loss = 1.46072595\n",
            "Iteration 20, loss = 1.44857518\n",
            "Iteration 21, loss = 1.43519765\n",
            "Iteration 22, loss = 1.42199533\n",
            "Iteration 23, loss = 1.40833131\n",
            "Iteration 24, loss = 1.39479601\n",
            "Iteration 25, loss = 1.38037274\n",
            "Iteration 26, loss = 1.36634190\n",
            "Iteration 27, loss = 1.35243282\n",
            "Iteration 28, loss = 1.33821586\n",
            "Iteration 29, loss = 1.32549270\n",
            "Iteration 30, loss = 1.31183113\n",
            "Iteration 31, loss = 1.29941331\n",
            "Iteration 32, loss = 1.28784902\n",
            "Iteration 33, loss = 1.27566515\n",
            "Iteration 34, loss = 1.26522029\n",
            "Iteration 35, loss = 1.25511039\n",
            "Iteration 36, loss = 1.24538049\n",
            "Iteration 37, loss = 1.23638082\n",
            "Iteration 38, loss = 1.22835498\n",
            "Iteration 39, loss = 1.22057631\n",
            "Iteration 40, loss = 1.21385100\n",
            "Iteration 41, loss = 1.20672485\n",
            "Iteration 42, loss = 1.19984392\n",
            "Iteration 43, loss = 1.19456857\n",
            "Iteration 44, loss = 1.18913373\n",
            "Iteration 45, loss = 1.18354789\n",
            "Iteration 46, loss = 1.17961246\n",
            "Iteration 47, loss = 1.17505322\n",
            "Iteration 48, loss = 1.17078578\n",
            "Iteration 49, loss = 1.16689857\n",
            "Iteration 50, loss = 1.16346270\n",
            "Iteration 51, loss = 1.15994706\n",
            "Iteration 52, loss = 1.15711171\n",
            "Iteration 53, loss = 1.15379475\n",
            "Iteration 54, loss = 1.15191175\n",
            "Iteration 55, loss = 1.14858382\n",
            "Iteration 56, loss = 1.14685251\n",
            "Iteration 57, loss = 1.14344679\n",
            "Iteration 58, loss = 1.14126147\n",
            "Iteration 59, loss = 1.13880139\n",
            "Iteration 60, loss = 1.13842961\n",
            "Iteration 61, loss = 1.13484047\n",
            "Iteration 62, loss = 1.13345443\n",
            "Iteration 63, loss = 1.13189853\n",
            "Iteration 64, loss = 1.13035767\n",
            "Iteration 65, loss = 1.12784669\n",
            "Iteration 66, loss = 1.12662992\n",
            "Iteration 67, loss = 1.12581535\n",
            "Iteration 68, loss = 1.12375274\n",
            "Iteration 69, loss = 1.12233379\n",
            "Iteration 70, loss = 1.12099385\n",
            "Iteration 71, loss = 1.11963751\n",
            "Iteration 72, loss = 1.11906318\n",
            "Iteration 73, loss = 1.11723955\n",
            "Iteration 74, loss = 1.11602937\n",
            "Iteration 75, loss = 1.11572862\n",
            "Iteration 76, loss = 1.11417403\n",
            "Iteration 77, loss = 1.11246145\n",
            "Iteration 78, loss = 1.11167761\n",
            "Iteration 79, loss = 1.11161499\n",
            "Iteration 80, loss = 1.10958514\n",
            "Iteration 81, loss = 1.10848212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 82, loss = 1.10763183\n",
            "Iteration 83, loss = 1.10673177\n",
            "Iteration 84, loss = 1.10656301\n",
            "Iteration 85, loss = 1.10589564\n",
            "Iteration 86, loss = 1.10433561\n",
            "Iteration 87, loss = 1.10291816\n",
            "Iteration 88, loss = 1.10231890\n",
            "Iteration 89, loss = 1.10186604\n",
            "Iteration 90, loss = 1.10324747\n",
            "Iteration 91, loss = 1.09984861\n",
            "Iteration 92, loss = 1.09846280\n",
            "Iteration 93, loss = 1.09856551\n",
            "Iteration 94, loss = 1.09737205\n",
            "Iteration 95, loss = 1.09645106\n",
            "Iteration 96, loss = 1.09572929\n",
            "Iteration 97, loss = 1.09485539\n",
            "Iteration 98, loss = 1.09596623\n",
            "Iteration 99, loss = 1.09336885\n",
            "Iteration 100, loss = 1.09313068\n",
            "Iteration 1, loss = 1.61652924\n",
            "Iteration 2, loss = 1.57390011\n",
            "Iteration 3, loss = 1.52392057\n",
            "Iteration 4, loss = 1.44925233\n",
            "Iteration 5, loss = 1.39042485\n",
            "Iteration 6, loss = 1.31164947\n",
            "Iteration 7, loss = 1.25362212\n",
            "Iteration 8, loss = 1.20951254\n",
            "Iteration 9, loss = 1.17387736\n",
            "Iteration 10, loss = 1.15133569\n",
            "Iteration 11, loss = 1.13745091\n",
            "Iteration 12, loss = 1.12502870\n",
            "Iteration 13, loss = 1.11873493\n",
            "Iteration 14, loss = 1.12696213\n",
            "Iteration 15, loss = 1.14308753\n",
            "Iteration 16, loss = 1.13323651\n",
            "Iteration 17, loss = 1.11854131\n",
            "Iteration 18, loss = 1.12613921\n",
            "Iteration 19, loss = 1.13319581\n",
            "Iteration 20, loss = 1.12124771\n",
            "Iteration 21, loss = 1.12082004\n",
            "Iteration 22, loss = 1.10304349\n",
            "Iteration 23, loss = 1.11696324\n",
            "Iteration 24, loss = 1.11490782\n",
            "Iteration 25, loss = 1.11474301\n",
            "Iteration 26, loss = 1.11005687\n",
            "Iteration 27, loss = 1.11099228\n",
            "Iteration 28, loss = 1.10573258\n",
            "Iteration 29, loss = 1.11561753\n",
            "Iteration 30, loss = 1.09802427\n",
            "Iteration 31, loss = 1.10138306\n",
            "Iteration 32, loss = 1.11225768\n",
            "Iteration 33, loss = 1.10889946\n",
            "Iteration 34, loss = 1.10599075\n",
            "Iteration 35, loss = 1.09249478\n",
            "Iteration 36, loss = 1.08862462\n",
            "Iteration 37, loss = 1.09220070\n",
            "Iteration 38, loss = 1.08163315\n",
            "Iteration 39, loss = 1.08005519\n",
            "Iteration 40, loss = 1.07555762\n",
            "Iteration 41, loss = 1.07877680\n",
            "Iteration 42, loss = 1.07217985\n",
            "Iteration 43, loss = 1.07885104\n",
            "Iteration 44, loss = 1.06822400\n",
            "Iteration 45, loss = 1.06559403\n",
            "Iteration 46, loss = 1.06529124\n",
            "Iteration 47, loss = 1.06214902\n",
            "Iteration 48, loss = 1.05002530\n",
            "Iteration 49, loss = 1.03997524\n",
            "Iteration 50, loss = 1.03691424\n",
            "Iteration 51, loss = 1.04863144\n",
            "Iteration 52, loss = 1.03453745\n",
            "Iteration 53, loss = 1.01724219\n",
            "Iteration 54, loss = 1.04902952\n",
            "Iteration 55, loss = 1.02823145\n",
            "Iteration 56, loss = 1.01580803\n",
            "Iteration 57, loss = 0.99609145\n",
            "Iteration 58, loss = 1.00844797\n",
            "Iteration 59, loss = 0.98895074\n",
            "Iteration 60, loss = 0.98293110\n",
            "Iteration 61, loss = 0.98393781\n",
            "Iteration 62, loss = 0.99011615\n",
            "Iteration 63, loss = 0.96781249\n",
            "Iteration 64, loss = 0.95812258\n",
            "Iteration 65, loss = 0.94644629\n",
            "Iteration 66, loss = 0.94400203\n",
            "Iteration 67, loss = 0.94077064\n",
            "Iteration 68, loss = 0.93562454\n",
            "Iteration 69, loss = 0.95872923\n",
            "Iteration 70, loss = 0.91800185\n",
            "Iteration 71, loss = 0.93590809\n",
            "Iteration 72, loss = 0.90923808\n",
            "Iteration 73, loss = 0.91442096\n",
            "Iteration 74, loss = 0.90885928\n",
            "Iteration 75, loss = 0.93949191\n",
            "Iteration 76, loss = 0.90395966\n",
            "Iteration 77, loss = 0.89890471\n",
            "Iteration 78, loss = 0.91025394\n",
            "Iteration 79, loss = 0.89042350\n",
            "Iteration 80, loss = 0.87743989\n",
            "Iteration 81, loss = 0.87629137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 82, loss = 0.86839100\n",
            "Iteration 83, loss = 0.89024064\n",
            "Iteration 84, loss = 0.87681283\n",
            "Iteration 85, loss = 0.88085219\n",
            "Iteration 86, loss = 0.85632037\n",
            "Iteration 87, loss = 0.86629141\n",
            "Iteration 88, loss = 0.84882701\n",
            "Iteration 89, loss = 0.85348479\n",
            "Iteration 90, loss = 0.88134195\n",
            "Iteration 91, loss = 0.83578979\n",
            "Iteration 92, loss = 0.85689897\n",
            "Iteration 93, loss = 0.83724774\n",
            "Iteration 94, loss = 0.82878946\n",
            "Iteration 95, loss = 0.82698663\n",
            "Iteration 96, loss = 0.82554407\n",
            "Iteration 97, loss = 0.81735376\n",
            "Iteration 98, loss = 0.85104292\n",
            "Iteration 99, loss = 0.83614389\n",
            "Iteration 100, loss = 0.82789484\n",
            "Iteration 1, loss = 1.76769531\n",
            "Iteration 2, loss = 1.75420454\n",
            "Iteration 3, loss = 1.74279815\n",
            "Iteration 4, loss = 1.73096539\n",
            "Iteration 5, loss = 1.71956742\n",
            "Iteration 6, loss = 1.70853731\n",
            "Iteration 7, loss = 1.69922133\n",
            "Iteration 8, loss = 1.68901940\n",
            "Iteration 9, loss = 1.68044282\n",
            "Iteration 10, loss = 1.67101633\n",
            "Iteration 11, loss = 1.66326898\n",
            "Iteration 12, loss = 1.65640973\n",
            "Iteration 13, loss = 1.64842452\n",
            "Iteration 14, loss = 1.64211588\n",
            "Iteration 15, loss = 1.63603551\n",
            "Iteration 16, loss = 1.62993480\n",
            "Iteration 17, loss = 1.62470716\n",
            "Iteration 18, loss = 1.61941192\n",
            "Iteration 19, loss = 1.61477982\n",
            "Iteration 20, loss = 1.61103771\n",
            "Iteration 21, loss = 1.60679934\n",
            "Iteration 22, loss = 1.60295950\n",
            "Iteration 23, loss = 1.60001872\n",
            "Iteration 24, loss = 1.59722016\n",
            "Iteration 25, loss = 1.59412233\n",
            "Iteration 26, loss = 1.59139341\n",
            "Iteration 27, loss = 1.58917986\n",
            "Iteration 28, loss = 1.58734101\n",
            "Iteration 29, loss = 1.58528291\n",
            "Iteration 30, loss = 1.58353627\n",
            "Iteration 31, loss = 1.58183477\n",
            "Iteration 32, loss = 1.58047662\n",
            "Iteration 33, loss = 1.57902925\n",
            "Iteration 34, loss = 1.57761423\n",
            "Iteration 35, loss = 1.57664657\n",
            "Iteration 36, loss = 1.57546594\n",
            "Iteration 37, loss = 1.57455479\n",
            "Iteration 38, loss = 1.57340477\n",
            "Iteration 39, loss = 1.57245752\n",
            "Iteration 40, loss = 1.57157873\n",
            "Iteration 41, loss = 1.57063420\n",
            "Iteration 42, loss = 1.56976227\n",
            "Iteration 43, loss = 1.56894072\n",
            "Iteration 44, loss = 1.56807238\n",
            "Iteration 45, loss = 1.56729070\n",
            "Iteration 46, loss = 1.56640621\n",
            "Iteration 47, loss = 1.56560484\n",
            "Iteration 48, loss = 1.56476150\n",
            "Iteration 49, loss = 1.56404716\n",
            "Iteration 50, loss = 1.56311714\n",
            "Iteration 51, loss = 1.56233098\n",
            "Iteration 52, loss = 1.56144616\n",
            "Iteration 53, loss = 1.56062835\n",
            "Iteration 54, loss = 1.55981575\n",
            "Iteration 55, loss = 1.55897941\n",
            "Iteration 56, loss = 1.55809129\n",
            "Iteration 57, loss = 1.55716855\n",
            "Iteration 58, loss = 1.55632282\n",
            "Iteration 59, loss = 1.55545484\n",
            "Iteration 60, loss = 1.55456250\n",
            "Iteration 61, loss = 1.55370216\n",
            "Iteration 62, loss = 1.55277483\n",
            "Iteration 63, loss = 1.55186892\n",
            "Iteration 64, loss = 1.55097365\n",
            "Iteration 65, loss = 1.55003633\n",
            "Iteration 66, loss = 1.54914002\n",
            "Iteration 67, loss = 1.54820496\n",
            "Iteration 68, loss = 1.54724572\n",
            "Iteration 69, loss = 1.54632239\n",
            "Iteration 70, loss = 1.54537756\n",
            "Iteration 71, loss = 1.54439553\n",
            "Iteration 72, loss = 1.54342743\n",
            "Iteration 73, loss = 1.54243456\n",
            "Iteration 74, loss = 1.54143828\n",
            "Iteration 75, loss = 1.54043120\n",
            "Iteration 76, loss = 1.53950612\n",
            "Iteration 77, loss = 1.53845354\n",
            "Iteration 78, loss = 1.53744908\n",
            "Iteration 79, loss = 1.53640373\n",
            "Iteration 80, loss = 1.53536431\n",
            "Iteration 81, loss = 1.53431606\n",
            "Iteration 82, loss = 1.53329674\n",
            "Iteration 83, loss = 1.53222597\n",
            "Iteration 84, loss = 1.53117139\n",
            "Iteration 85, loss = 1.53007015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 86, loss = 1.52898743\n",
            "Iteration 87, loss = 1.52785533\n",
            "Iteration 88, loss = 1.52677999\n",
            "Iteration 89, loss = 1.52562999\n",
            "Iteration 90, loss = 1.52453122\n",
            "Iteration 91, loss = 1.52341333\n",
            "Iteration 92, loss = 1.52224560\n",
            "Iteration 93, loss = 1.52110554\n",
            "Iteration 94, loss = 1.51994110\n",
            "Iteration 95, loss = 1.51878330\n",
            "Iteration 96, loss = 1.51755608\n",
            "Iteration 97, loss = 1.51639651\n",
            "Iteration 98, loss = 1.51524402\n",
            "Iteration 99, loss = 1.51401128\n",
            "Iteration 100, loss = 1.51279280\n",
            "Iteration 1, loss = 1.74376055\n",
            "Iteration 2, loss = 1.64207521\n",
            "Iteration 3, loss = 1.59906386\n",
            "Iteration 4, loss = 1.57981154\n",
            "Iteration 5, loss = 1.58187587\n",
            "Iteration 6, loss = 1.58359628\n",
            "Iteration 7, loss = 1.57882560\n",
            "Iteration 8, loss = 1.56713070\n",
            "Iteration 9, loss = 1.55147280\n",
            "Iteration 10, loss = 1.54096626\n",
            "Iteration 11, loss = 1.52979215\n",
            "Iteration 12, loss = 1.52037012\n",
            "Iteration 13, loss = 1.51225103\n",
            "Iteration 14, loss = 1.50313345\n",
            "Iteration 15, loss = 1.49278048\n",
            "Iteration 16, loss = 1.48089477\n",
            "Iteration 17, loss = 1.46878428\n",
            "Iteration 18, loss = 1.45503660\n",
            "Iteration 19, loss = 1.44115010\n",
            "Iteration 20, loss = 1.42844004\n",
            "Iteration 21, loss = 1.41454140\n",
            "Iteration 22, loss = 1.40084922\n",
            "Iteration 23, loss = 1.38702434\n",
            "Iteration 24, loss = 1.37287568\n",
            "Iteration 25, loss = 1.35799119\n",
            "Iteration 26, loss = 1.34442433\n",
            "Iteration 27, loss = 1.33055456\n",
            "Iteration 28, loss = 1.31689996\n",
            "Iteration 29, loss = 1.30491398\n",
            "Iteration 30, loss = 1.29214030\n",
            "Iteration 31, loss = 1.28073197\n",
            "Iteration 32, loss = 1.26985008\n",
            "Iteration 33, loss = 1.25939435\n",
            "Iteration 34, loss = 1.24969691\n",
            "Iteration 35, loss = 1.24101875\n",
            "Iteration 36, loss = 1.23260280\n",
            "Iteration 37, loss = 1.22485656\n",
            "Iteration 38, loss = 1.21748955\n",
            "Iteration 39, loss = 1.21108084\n",
            "Iteration 40, loss = 1.20499027\n",
            "Iteration 41, loss = 1.19858965\n",
            "Iteration 42, loss = 1.19329791\n",
            "Iteration 43, loss = 1.18903915\n",
            "Iteration 44, loss = 1.18381129\n",
            "Iteration 45, loss = 1.17977796\n",
            "Iteration 46, loss = 1.17545277\n",
            "Iteration 47, loss = 1.17154628\n",
            "Iteration 48, loss = 1.16778559\n",
            "Iteration 49, loss = 1.16535284\n",
            "Iteration 50, loss = 1.16149711\n",
            "Iteration 51, loss = 1.15814567\n",
            "Iteration 52, loss = 1.15537645\n",
            "Iteration 53, loss = 1.15270566\n",
            "Iteration 54, loss = 1.15054468\n",
            "Iteration 55, loss = 1.14828478\n",
            "Iteration 56, loss = 1.14594355\n",
            "Iteration 57, loss = 1.14348590\n",
            "Iteration 58, loss = 1.14088644\n",
            "Iteration 59, loss = 1.13976868\n",
            "Iteration 60, loss = 1.13695263\n",
            "Iteration 61, loss = 1.13532739\n",
            "Iteration 62, loss = 1.13325866\n",
            "Iteration 63, loss = 1.13218571\n",
            "Iteration 64, loss = 1.12987180\n",
            "Iteration 65, loss = 1.12850816\n",
            "Iteration 66, loss = 1.12724162\n",
            "Iteration 67, loss = 1.12566735\n",
            "Iteration 68, loss = 1.12379026\n",
            "Iteration 69, loss = 1.12336976\n",
            "Iteration 70, loss = 1.12152773\n",
            "Iteration 71, loss = 1.11994323\n",
            "Iteration 72, loss = 1.11906124\n",
            "Iteration 73, loss = 1.11747374\n",
            "Iteration 74, loss = 1.11669948\n",
            "Iteration 75, loss = 1.11516179\n",
            "Iteration 76, loss = 1.11404003\n",
            "Iteration 77, loss = 1.11498582\n",
            "Iteration 78, loss = 1.11235625\n",
            "Iteration 79, loss = 1.11073521\n",
            "Iteration 80, loss = 1.11133833\n",
            "Iteration 81, loss = 1.10934737\n",
            "Iteration 82, loss = 1.10764184\n",
            "Iteration 83, loss = 1.10843943\n",
            "Iteration 84, loss = 1.10731821\n",
            "Iteration 85, loss = 1.10551205\n",
            "Iteration 86, loss = 1.10459508\n",
            "Iteration 87, loss = 1.10323040\n",
            "Iteration 88, loss = 1.10206434\n",
            "Iteration 89, loss = 1.10102745\n",
            "Iteration 90, loss = 1.10009714\n",
            "Iteration 91, loss = 1.09973688\n",
            "Iteration 92, loss = 1.09839328\n",
            "Iteration 93, loss = 1.09791742\n",
            "Iteration 94, loss = 1.09686045\n",
            "Iteration 95, loss = 1.09789917\n",
            "Iteration 96, loss = 1.09457861\n",
            "Iteration 97, loss = 1.09400830\n",
            "Iteration 98, loss = 1.09277650\n",
            "Iteration 99, loss = 1.09227097\n",
            "Iteration 100, loss = 1.09133930\n",
            "Iteration 1, loss = 1.74659407\n",
            "Iteration 2, loss = 1.65045524\n",
            "Iteration 3, loss = 1.55583792\n",
            "Iteration 4, loss = 1.49618439\n",
            "Iteration 5, loss = 1.41455220\n",
            "Iteration 6, loss = 1.33824656\n",
            "Iteration 7, loss = 1.27205742\n",
            "Iteration 8, loss = 1.21689547\n",
            "Iteration 9, loss = 1.19091153\n",
            "Iteration 10, loss = 1.16590683\n",
            "Iteration 11, loss = 1.15803284\n",
            "Iteration 12, loss = 1.15217297\n",
            "Iteration 13, loss = 1.12813641\n",
            "Iteration 14, loss = 1.13710849\n",
            "Iteration 15, loss = 1.14825803\n",
            "Iteration 16, loss = 1.11736455\n",
            "Iteration 17, loss = 1.13267377\n",
            "Iteration 18, loss = 1.11773857\n",
            "Iteration 19, loss = 1.11408083\n",
            "Iteration 20, loss = 1.11205617\n",
            "Iteration 21, loss = 1.10766565\n",
            "Iteration 22, loss = 1.10033240\n",
            "Iteration 23, loss = 1.09861231\n",
            "Iteration 24, loss = 1.09606098\n",
            "Iteration 25, loss = 1.08703097\n",
            "Iteration 26, loss = 1.08531192\n",
            "Iteration 27, loss = 1.09116449\n",
            "Iteration 28, loss = 1.09100650\n",
            "Iteration 29, loss = 1.11096854\n",
            "Iteration 30, loss = 1.10175540\n",
            "Iteration 31, loss = 1.07935285\n",
            "Iteration 32, loss = 1.07884598\n",
            "Iteration 33, loss = 1.06332878\n",
            "Iteration 34, loss = 1.05431891\n",
            "Iteration 35, loss = 1.04855616\n",
            "Iteration 36, loss = 1.06066330\n",
            "Iteration 37, loss = 1.03424110\n",
            "Iteration 38, loss = 1.03979384\n",
            "Iteration 39, loss = 1.00752004\n",
            "Iteration 40, loss = 1.01528221\n",
            "Iteration 41, loss = 0.99730922\n",
            "Iteration 42, loss = 0.98641067\n",
            "Iteration 43, loss = 0.98510153\n",
            "Iteration 44, loss = 0.98057495\n",
            "Iteration 45, loss = 1.06808657\n",
            "Iteration 46, loss = 0.97609820\n",
            "Iteration 47, loss = 0.96510156\n",
            "Iteration 48, loss = 0.94995413\n",
            "Iteration 49, loss = 0.95054969\n",
            "Iteration 50, loss = 0.97997642\n",
            "Iteration 51, loss = 0.93389420\n",
            "Iteration 52, loss = 0.96025571\n",
            "Iteration 53, loss = 1.01646953\n",
            "Iteration 54, loss = 1.08693310\n",
            "Iteration 55, loss = 1.00963163\n",
            "Iteration 56, loss = 0.96878361\n",
            "Iteration 57, loss = 0.96501820\n",
            "Iteration 58, loss = 0.93832488\n",
            "Iteration 59, loss = 0.90432106\n",
            "Iteration 60, loss = 0.93076284\n",
            "Iteration 61, loss = 0.92923668\n",
            "Iteration 62, loss = 0.88603863\n",
            "Iteration 63, loss = 0.88624843\n",
            "Iteration 64, loss = 0.90057154\n",
            "Iteration 65, loss = 0.89129595\n",
            "Iteration 66, loss = 0.88328825\n",
            "Iteration 67, loss = 0.90725465\n",
            "Iteration 68, loss = 0.89318196\n",
            "Iteration 69, loss = 0.90814925\n",
            "Iteration 70, loss = 0.90159503\n",
            "Iteration 71, loss = 0.88038612\n",
            "Iteration 72, loss = 0.86914495\n",
            "Iteration 73, loss = 0.85707485\n",
            "Iteration 74, loss = 0.83851586\n",
            "Iteration 75, loss = 0.83111048\n",
            "Iteration 76, loss = 0.83159833\n",
            "Iteration 77, loss = 0.84562183\n",
            "Iteration 78, loss = 0.81998916\n",
            "Iteration 79, loss = 0.81852113\n",
            "Iteration 80, loss = 0.81776516\n",
            "Iteration 81, loss = 0.80711991\n",
            "Iteration 82, loss = 0.82530705\n",
            "Iteration 83, loss = 0.79990953\n",
            "Iteration 84, loss = 0.80251586\n",
            "Iteration 85, loss = 0.83908776\n",
            "Iteration 86, loss = 0.79489605\n",
            "Iteration 87, loss = 0.79422007\n",
            "Iteration 88, loss = 0.79505247\n",
            "Iteration 89, loss = 0.81837429\n",
            "Iteration 90, loss = 0.81446774\n",
            "Iteration 91, loss = 0.81866828\n",
            "Iteration 92, loss = 0.81297626\n",
            "Iteration 93, loss = 0.78222992\n",
            "Iteration 94, loss = 0.77023728\n",
            "Iteration 95, loss = 0.82127990\n",
            "Iteration 96, loss = 0.81532827\n",
            "Iteration 97, loss = 0.77684227\n",
            "Iteration 98, loss = 0.77232919\n",
            "Iteration 99, loss = 0.80283625\n",
            "Iteration 100, loss = 0.79416842\n",
            "Iteration 1, loss = 1.67985000\n",
            "Iteration 2, loss = 1.67061177\n",
            "Iteration 3, loss = 1.66359170\n",
            "Iteration 4, loss = 1.65585044\n",
            "Iteration 5, loss = 1.64899302\n",
            "Iteration 6, loss = 1.64226418\n",
            "Iteration 7, loss = 1.63750109\n",
            "Iteration 8, loss = 1.63192438\n",
            "Iteration 9, loss = 1.62775855\n",
            "Iteration 10, loss = 1.62291816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 1.61961245\n",
            "Iteration 12, loss = 1.61713641\n",
            "Iteration 13, loss = 1.61345464\n",
            "Iteration 14, loss = 1.61110162\n",
            "Iteration 15, loss = 1.60892397\n",
            "Iteration 16, loss = 1.60670862\n",
            "Iteration 17, loss = 1.60496882\n",
            "Iteration 18, loss = 1.60331950\n",
            "Iteration 19, loss = 1.60214742\n",
            "Iteration 20, loss = 1.60123210\n",
            "Iteration 21, loss = 1.59997735\n",
            "Iteration 22, loss = 1.59922150\n",
            "Iteration 23, loss = 1.59865735\n",
            "Iteration 24, loss = 1.59806973\n",
            "Iteration 25, loss = 1.59754338\n",
            "Iteration 26, loss = 1.59693420\n",
            "Iteration 27, loss = 1.59675090\n",
            "Iteration 28, loss = 1.59653024\n",
            "Iteration 29, loss = 1.59623926\n",
            "Iteration 30, loss = 1.59612100\n",
            "Iteration 31, loss = 1.59585370\n",
            "Iteration 32, loss = 1.59574244\n",
            "Iteration 33, loss = 1.59562589\n",
            "Iteration 34, loss = 1.59544190\n",
            "Iteration 35, loss = 1.59538449\n",
            "Iteration 36, loss = 1.59528423\n",
            "Iteration 37, loss = 1.59521923\n",
            "Iteration 38, loss = 1.59505883\n",
            "Iteration 39, loss = 1.59500653\n",
            "Iteration 40, loss = 1.59487349\n",
            "Iteration 41, loss = 1.59474412\n",
            "Iteration 42, loss = 1.59463999\n",
            "Iteration 43, loss = 1.59459672\n",
            "Iteration 44, loss = 1.59447725\n",
            "Iteration 45, loss = 1.59443662\n",
            "Iteration 46, loss = 1.59433518\n",
            "Iteration 47, loss = 1.59412402\n",
            "Iteration 48, loss = 1.59401293\n",
            "Iteration 49, loss = 1.59396762\n",
            "Iteration 50, loss = 1.59382649\n",
            "Iteration 51, loss = 1.59374056\n",
            "Iteration 52, loss = 1.59362239\n",
            "Iteration 53, loss = 1.59353191\n",
            "Iteration 54, loss = 1.59343624\n",
            "Iteration 55, loss = 1.59340086\n",
            "Iteration 56, loss = 1.59321738\n",
            "Iteration 57, loss = 1.59306320\n",
            "Iteration 58, loss = 1.59295706\n",
            "Iteration 59, loss = 1.59289351\n",
            "Iteration 60, loss = 1.59272066\n",
            "Iteration 61, loss = 1.59263080\n",
            "Iteration 62, loss = 1.59250502\n",
            "Iteration 63, loss = 1.59241915\n",
            "Iteration 64, loss = 1.59228222\n",
            "Iteration 65, loss = 1.59217385\n",
            "Iteration 66, loss = 1.59208157\n",
            "Iteration 67, loss = 1.59197450\n",
            "Iteration 68, loss = 1.59183669\n",
            "Iteration 69, loss = 1.59174973\n",
            "Iteration 70, loss = 1.59165804\n",
            "Iteration 71, loss = 1.59150819\n",
            "Iteration 72, loss = 1.59139379\n",
            "Iteration 73, loss = 1.59126413\n",
            "Iteration 74, loss = 1.59115123\n",
            "Iteration 75, loss = 1.59105598\n",
            "Iteration 76, loss = 1.59109935\n",
            "Iteration 77, loss = 1.59082727\n",
            "Iteration 78, loss = 1.59072320\n",
            "Iteration 79, loss = 1.59058586\n",
            "Iteration 80, loss = 1.59044300\n",
            "Iteration 81, loss = 1.59032450\n",
            "Iteration 82, loss = 1.59021562\n",
            "Iteration 83, loss = 1.59012324\n",
            "Iteration 84, loss = 1.58998231\n",
            "Iteration 85, loss = 1.58985912\n",
            "Iteration 86, loss = 1.58970468\n",
            "Iteration 87, loss = 1.58956555\n",
            "Iteration 88, loss = 1.58947813\n",
            "Iteration 89, loss = 1.58930383\n",
            "Iteration 90, loss = 1.58920809\n",
            "Iteration 91, loss = 1.58914200\n",
            "Iteration 92, loss = 1.58890756\n",
            "Iteration 93, loss = 1.58882611\n",
            "Iteration 94, loss = 1.58868002\n",
            "Iteration 95, loss = 1.58854829\n",
            "Iteration 96, loss = 1.58840421\n",
            "Iteration 97, loss = 1.58826518\n",
            "Iteration 98, loss = 1.58816832\n",
            "Iteration 99, loss = 1.58802428\n",
            "Iteration 100, loss = 1.58787398\n",
            "Iteration 1, loss = 1.66675081\n",
            "Iteration 2, loss = 1.61053667\n",
            "Iteration 3, loss = 1.60198494\n",
            "Iteration 4, loss = 1.60313788\n",
            "Iteration 5, loss = 1.60840978\n",
            "Iteration 6, loss = 1.60921167\n",
            "Iteration 7, loss = 1.60255525\n",
            "Iteration 8, loss = 1.59739106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss = 1.59239754\n",
            "Iteration 10, loss = 1.59347388\n",
            "Iteration 11, loss = 1.59233544\n",
            "Iteration 12, loss = 1.59169961\n",
            "Iteration 13, loss = 1.58950129\n",
            "Iteration 14, loss = 1.58770393\n",
            "Iteration 15, loss = 1.58628676\n",
            "Iteration 16, loss = 1.58433599\n",
            "Iteration 17, loss = 1.58336523\n",
            "Iteration 18, loss = 1.58226332\n",
            "Iteration 19, loss = 1.58061555\n",
            "Iteration 20, loss = 1.57894939\n",
            "Iteration 21, loss = 1.57707800\n",
            "Iteration 22, loss = 1.57604168\n",
            "Iteration 23, loss = 1.57273743\n",
            "Iteration 24, loss = 1.57129628\n",
            "Iteration 25, loss = 1.56946577\n",
            "Iteration 26, loss = 1.56719256\n",
            "Iteration 27, loss = 1.56496352\n",
            "Iteration 28, loss = 1.56227560\n",
            "Iteration 29, loss = 1.55988065\n",
            "Iteration 30, loss = 1.55697628\n",
            "Iteration 31, loss = 1.55367397\n",
            "Iteration 32, loss = 1.55069255\n",
            "Iteration 33, loss = 1.54710341\n",
            "Iteration 34, loss = 1.54386977\n",
            "Iteration 35, loss = 1.54006348\n",
            "Iteration 36, loss = 1.53605087\n",
            "Iteration 37, loss = 1.53223045\n",
            "Iteration 38, loss = 1.52754304\n",
            "Iteration 39, loss = 1.52359485\n",
            "Iteration 40, loss = 1.51838829\n",
            "Iteration 41, loss = 1.51295098\n",
            "Iteration 42, loss = 1.50761708\n",
            "Iteration 43, loss = 1.50318876\n",
            "Iteration 44, loss = 1.49701585\n",
            "Iteration 45, loss = 1.49152926\n",
            "Iteration 46, loss = 1.48516908\n",
            "Iteration 47, loss = 1.47810321\n",
            "Iteration 48, loss = 1.47174377\n",
            "Iteration 49, loss = 1.46629626\n",
            "Iteration 50, loss = 1.45872606\n",
            "Iteration 51, loss = 1.45159148\n",
            "Iteration 52, loss = 1.44453725\n",
            "Iteration 53, loss = 1.43740405\n",
            "Iteration 54, loss = 1.43044254\n",
            "Iteration 55, loss = 1.42362957\n",
            "Iteration 56, loss = 1.41572578\n",
            "Iteration 57, loss = 1.40723802\n",
            "Iteration 58, loss = 1.39972517\n",
            "Iteration 59, loss = 1.39231691\n",
            "Iteration 60, loss = 1.38425415\n",
            "Iteration 61, loss = 1.37709518\n",
            "Iteration 62, loss = 1.36914226\n",
            "Iteration 63, loss = 1.36192869\n",
            "Iteration 64, loss = 1.35440051\n",
            "Iteration 65, loss = 1.34689415\n",
            "Iteration 66, loss = 1.33987043\n",
            "Iteration 67, loss = 1.33284692\n",
            "Iteration 68, loss = 1.32566924\n",
            "Iteration 69, loss = 1.31957847\n",
            "Iteration 70, loss = 1.31283271\n",
            "Iteration 71, loss = 1.30623158\n",
            "Iteration 72, loss = 1.29979212\n",
            "Iteration 73, loss = 1.29371160\n",
            "Iteration 74, loss = 1.28779860\n",
            "Iteration 75, loss = 1.28224285\n",
            "Iteration 76, loss = 1.27752425\n",
            "Iteration 77, loss = 1.27204619\n",
            "Iteration 78, loss = 1.26714267\n",
            "Iteration 79, loss = 1.26246585\n",
            "Iteration 80, loss = 1.25741022\n",
            "Iteration 81, loss = 1.25292249\n",
            "Iteration 82, loss = 1.24890413\n",
            "Iteration 83, loss = 1.24555305\n",
            "Iteration 84, loss = 1.24141907\n",
            "Iteration 85, loss = 1.23741144\n",
            "Iteration 86, loss = 1.23300971\n",
            "Iteration 87, loss = 1.22968959\n",
            "Iteration 88, loss = 1.22678951\n",
            "Iteration 89, loss = 1.22283391\n",
            "Iteration 90, loss = 1.21987535\n",
            "Iteration 91, loss = 1.21736838\n",
            "Iteration 92, loss = 1.21353334\n",
            "Iteration 93, loss = 1.21137895\n",
            "Iteration 94, loss = 1.20881283\n",
            "Iteration 95, loss = 1.20663781\n",
            "Iteration 96, loss = 1.20327564\n",
            "Iteration 97, loss = 1.20105290\n",
            "Iteration 98, loss = 1.19857262\n",
            "Iteration 99, loss = 1.19674728\n",
            "Iteration 100, loss = 1.19474538\n",
            "Iteration 1, loss = 1.89081222\n",
            "Iteration 2, loss = 1.71174282\n",
            "Iteration 3, loss = 1.73429736\n",
            "Iteration 4, loss = 1.64846546\n",
            "Iteration 5, loss = 1.60792700\n",
            "Iteration 6, loss = 1.62449120\n",
            "Iteration 7, loss = 1.61364445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 1.58948705\n",
            "Iteration 9, loss = 1.58084422\n",
            "Iteration 10, loss = 1.58588129\n",
            "Iteration 11, loss = 1.57720548\n",
            "Iteration 12, loss = 1.56661545\n",
            "Iteration 13, loss = 1.54689954\n",
            "Iteration 14, loss = 1.53731434\n",
            "Iteration 15, loss = 1.52356166\n",
            "Iteration 16, loss = 1.50270132\n",
            "Iteration 17, loss = 1.47462869\n",
            "Iteration 18, loss = 1.44562923\n",
            "Iteration 19, loss = 1.40889376\n",
            "Iteration 20, loss = 1.36932424\n",
            "Iteration 21, loss = 1.32699820\n",
            "Iteration 22, loss = 1.29090554\n",
            "Iteration 23, loss = 1.25916991\n",
            "Iteration 24, loss = 1.22881788\n",
            "Iteration 25, loss = 1.20530945\n",
            "Iteration 26, loss = 1.19025302\n",
            "Iteration 27, loss = 1.17448478\n",
            "Iteration 28, loss = 1.16890960\n",
            "Iteration 29, loss = 1.15600833\n",
            "Iteration 30, loss = 1.14637498\n",
            "Iteration 31, loss = 1.13783864\n",
            "Iteration 32, loss = 1.13482776\n",
            "Iteration 33, loss = 1.13019127\n",
            "Iteration 34, loss = 1.13106429\n",
            "Iteration 35, loss = 1.12631555\n",
            "Iteration 36, loss = 1.13042580\n",
            "Iteration 37, loss = 1.11680260\n",
            "Iteration 38, loss = 1.12306172\n",
            "Iteration 39, loss = 1.11669762\n",
            "Iteration 40, loss = 1.11994337\n",
            "Iteration 41, loss = 1.11600217\n",
            "Iteration 42, loss = 1.11377928\n",
            "Iteration 43, loss = 1.11676745\n",
            "Iteration 44, loss = 1.11576581\n",
            "Iteration 45, loss = 1.11304959\n",
            "Iteration 46, loss = 1.11187564\n",
            "Iteration 47, loss = 1.11377897\n",
            "Iteration 48, loss = 1.11133205\n",
            "Iteration 49, loss = 1.11299102\n",
            "Iteration 50, loss = 1.10947934\n",
            "Iteration 51, loss = 1.11269728\n",
            "Iteration 52, loss = 1.10965563\n",
            "Iteration 53, loss = 1.10911200\n",
            "Iteration 54, loss = 1.10969338\n",
            "Iteration 55, loss = 1.11193694\n",
            "Iteration 56, loss = 1.11664472\n",
            "Iteration 57, loss = 1.11142746\n",
            "Iteration 58, loss = 1.10881591\n",
            "Iteration 59, loss = 1.11619290\n",
            "Iteration 60, loss = 1.10344285\n",
            "Iteration 61, loss = 1.11456145\n",
            "Iteration 62, loss = 1.10311719\n",
            "Iteration 63, loss = 1.11607720\n",
            "Iteration 64, loss = 1.10702376\n",
            "Iteration 65, loss = 1.10755443\n",
            "Iteration 66, loss = 1.10550898\n",
            "Iteration 67, loss = 1.10861102\n",
            "Iteration 68, loss = 1.10083491\n",
            "Iteration 69, loss = 1.10431773\n",
            "Iteration 70, loss = 1.10573846\n",
            "Iteration 71, loss = 1.10029220\n",
            "Iteration 72, loss = 1.10189700\n",
            "Iteration 73, loss = 1.09592161\n",
            "Iteration 74, loss = 1.09511703\n",
            "Iteration 75, loss = 1.09465998\n",
            "Iteration 76, loss = 1.09871109\n",
            "Iteration 77, loss = 1.10400500\n",
            "Iteration 78, loss = 1.09221304\n",
            "Iteration 79, loss = 1.09754124\n",
            "Iteration 80, loss = 1.09684534\n",
            "Iteration 81, loss = 1.09369952\n",
            "Iteration 82, loss = 1.09353111\n",
            "Iteration 83, loss = 1.09550299\n",
            "Iteration 84, loss = 1.09174246\n",
            "Iteration 85, loss = 1.10007819\n",
            "Iteration 86, loss = 1.08742205\n",
            "Iteration 87, loss = 1.08976954\n",
            "Iteration 88, loss = 1.08154139\n",
            "Iteration 89, loss = 1.08221079\n",
            "Iteration 90, loss = 1.07556375\n",
            "Iteration 91, loss = 1.07652607\n",
            "Iteration 92, loss = 1.06697185\n",
            "Iteration 93, loss = 1.06929876\n",
            "Iteration 94, loss = 1.06733030\n",
            "Iteration 95, loss = 1.06910980\n",
            "Iteration 96, loss = 1.05662131\n",
            "Iteration 97, loss = 1.05544199\n",
            "Iteration 98, loss = 1.05342796\n",
            "Iteration 99, loss = 1.04824523\n",
            "Iteration 100, loss = 1.05214488\n",
            "Iteration 1, loss = 1.75759640\n",
            "Iteration 2, loss = 1.74509412\n",
            "Iteration 3, loss = 1.73455054\n",
            "Iteration 4, loss = 1.72356448\n",
            "Iteration 5, loss = 1.71298050\n",
            "Iteration 6, loss = 1.70269876\n",
            "Iteration 7, loss = 1.69405239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 1.68453500\n",
            "Iteration 9, loss = 1.67651712\n",
            "Iteration 10, loss = 1.66769574\n",
            "Iteration 11, loss = 1.66043978\n",
            "Iteration 12, loss = 1.65402282\n",
            "Iteration 13, loss = 1.64648719\n",
            "Iteration 14, loss = 1.64055175\n",
            "Iteration 15, loss = 1.63480636\n",
            "Iteration 16, loss = 1.62902288\n",
            "Iteration 17, loss = 1.62407381\n",
            "Iteration 18, loss = 1.61903080\n",
            "Iteration 19, loss = 1.61461616\n",
            "Iteration 20, loss = 1.61103914\n",
            "Iteration 21, loss = 1.60698174\n",
            "Iteration 22, loss = 1.60328359\n",
            "Iteration 23, loss = 1.60045438\n",
            "Iteration 24, loss = 1.59774719\n",
            "Iteration 25, loss = 1.59474189\n",
            "Iteration 26, loss = 1.59209036\n",
            "Iteration 27, loss = 1.58993186\n",
            "Iteration 28, loss = 1.58812612\n",
            "Iteration 29, loss = 1.58611019\n",
            "Iteration 30, loss = 1.58438996\n",
            "Iteration 31, loss = 1.58271056\n",
            "Iteration 32, loss = 1.58136408\n",
            "Iteration 33, loss = 1.57992878\n",
            "Iteration 34, loss = 1.57852638\n",
            "Iteration 35, loss = 1.57756479\n",
            "Iteration 36, loss = 1.57639278\n",
            "Iteration 37, loss = 1.57548089\n",
            "Iteration 38, loss = 1.57433940\n",
            "Iteration 39, loss = 1.57339558\n",
            "Iteration 40, loss = 1.57252760\n",
            "Iteration 41, loss = 1.57159716\n",
            "Iteration 42, loss = 1.57073977\n",
            "Iteration 43, loss = 1.56993681\n",
            "Iteration 44, loss = 1.56908026\n",
            "Iteration 45, loss = 1.56831133\n",
            "Iteration 46, loss = 1.56744658\n",
            "Iteration 47, loss = 1.56666884\n",
            "Iteration 48, loss = 1.56584427\n",
            "Iteration 49, loss = 1.56515440\n",
            "Iteration 50, loss = 1.56424472\n",
            "Iteration 51, loss = 1.56348440\n",
            "Iteration 52, loss = 1.56261700\n",
            "Iteration 53, loss = 1.56183059\n",
            "Iteration 54, loss = 1.56103754\n",
            "Iteration 55, loss = 1.56021891\n",
            "Iteration 56, loss = 1.55936552\n",
            "Iteration 57, loss = 1.55847037\n",
            "Iteration 58, loss = 1.55764762\n",
            "Iteration 59, loss = 1.55680330\n",
            "Iteration 60, loss = 1.55593827\n",
            "Iteration 61, loss = 1.55510343\n",
            "Iteration 62, loss = 1.55420191\n",
            "Iteration 63, loss = 1.55332060\n",
            "Iteration 64, loss = 1.55245120\n",
            "Iteration 65, loss = 1.55153931\n",
            "Iteration 66, loss = 1.55066827\n",
            "Iteration 67, loss = 1.54975808\n",
            "Iteration 68, loss = 1.54882505\n",
            "Iteration 69, loss = 1.54792615\n",
            "Iteration 70, loss = 1.54700569\n",
            "Iteration 71, loss = 1.54604957\n",
            "Iteration 72, loss = 1.54510752\n",
            "Iteration 73, loss = 1.54414087\n",
            "Iteration 74, loss = 1.54316909\n",
            "Iteration 75, loss = 1.54218739\n",
            "Iteration 76, loss = 1.54128691\n",
            "Iteration 77, loss = 1.54025977\n",
            "Iteration 78, loss = 1.53927969\n",
            "Iteration 79, loss = 1.53825962\n",
            "Iteration 80, loss = 1.53724623\n",
            "Iteration 81, loss = 1.53622215\n",
            "Iteration 82, loss = 1.53522588\n",
            "Iteration 83, loss = 1.53418012\n",
            "Iteration 84, loss = 1.53315043\n",
            "Iteration 85, loss = 1.53207465\n",
            "Iteration 86, loss = 1.53101640\n",
            "Iteration 87, loss = 1.52990959\n",
            "Iteration 88, loss = 1.52885803\n",
            "Iteration 89, loss = 1.52773373\n",
            "Iteration 90, loss = 1.52665729\n",
            "Iteration 91, loss = 1.52556298\n",
            "Iteration 92, loss = 1.52442087\n",
            "Iteration 93, loss = 1.52330366\n",
            "Iteration 94, loss = 1.52216220\n",
            "Iteration 95, loss = 1.52102662\n",
            "Iteration 96, loss = 1.51982462\n",
            "Iteration 97, loss = 1.51868813\n",
            "Iteration 98, loss = 1.51755699\n",
            "Iteration 99, loss = 1.51634750\n",
            "Iteration 100, loss = 1.51515154\n",
            "Iteration 1, loss = 1.73532938\n",
            "Iteration 2, loss = 1.64048577\n",
            "Iteration 3, loss = 1.59958444\n",
            "Iteration 4, loss = 1.58009002\n",
            "Iteration 5, loss = 1.58109005\n",
            "Iteration 6, loss = 1.58262541\n",
            "Iteration 7, loss = 1.57871859\n",
            "Iteration 8, loss = 1.56781508\n",
            "Iteration 9, loss = 1.55283374\n",
            "Iteration 10, loss = 1.54222484\n",
            "Iteration 11, loss = 1.53087607\n",
            "Iteration 12, loss = 1.52127698\n",
            "Iteration 13, loss = 1.51318046\n",
            "Iteration 14, loss = 1.50415039\n",
            "Iteration 15, loss = 1.49384591\n",
            "Iteration 16, loss = 1.48207382\n",
            "Iteration 17, loss = 1.46993828\n",
            "Iteration 18, loss = 1.45614497\n",
            "Iteration 19, loss = 1.44222635\n",
            "Iteration 20, loss = 1.42942334\n",
            "Iteration 21, loss = 1.41537077\n",
            "Iteration 22, loss = 1.40147836\n",
            "Iteration 23, loss = 1.38757439\n",
            "Iteration 24, loss = 1.37336468\n",
            "Iteration 25, loss = 1.35838687\n",
            "Iteration 26, loss = 1.34468079\n",
            "Iteration 27, loss = 1.33063149\n",
            "Iteration 28, loss = 1.31685453\n",
            "Iteration 29, loss = 1.30473215\n",
            "Iteration 30, loss = 1.29181990\n",
            "Iteration 31, loss = 1.28033518\n",
            "Iteration 32, loss = 1.26935167\n",
            "Iteration 33, loss = 1.25882487\n",
            "Iteration 34, loss = 1.24906152\n",
            "Iteration 35, loss = 1.24031611\n",
            "Iteration 36, loss = 1.23183156\n",
            "Iteration 37, loss = 1.22403561\n",
            "Iteration 38, loss = 1.21662637\n",
            "Iteration 39, loss = 1.21015409\n",
            "Iteration 40, loss = 1.20409681\n",
            "Iteration 41, loss = 1.19768702\n",
            "Iteration 42, loss = 1.19239274\n",
            "Iteration 43, loss = 1.18802774\n",
            "Iteration 44, loss = 1.18280598\n",
            "Iteration 45, loss = 1.17869113\n",
            "Iteration 46, loss = 1.17449046\n",
            "Iteration 47, loss = 1.17061293\n",
            "Iteration 48, loss = 1.16685163\n",
            "Iteration 49, loss = 1.16434815\n",
            "Iteration 50, loss = 1.16053082\n",
            "Iteration 51, loss = 1.15721353\n",
            "Iteration 52, loss = 1.15440377\n",
            "Iteration 53, loss = 1.15175479\n",
            "Iteration 54, loss = 1.14960234\n",
            "Iteration 55, loss = 1.14722445\n",
            "Iteration 56, loss = 1.14491963\n",
            "Iteration 57, loss = 1.14252668\n",
            "Iteration 58, loss = 1.13996067\n",
            "Iteration 59, loss = 1.13885149\n",
            "Iteration 60, loss = 1.13606634\n",
            "Iteration 61, loss = 1.13440786\n",
            "Iteration 62, loss = 1.13235641\n",
            "Iteration 63, loss = 1.13122952\n",
            "Iteration 64, loss = 1.12896475\n",
            "Iteration 65, loss = 1.12759933\n",
            "Iteration 66, loss = 1.12632453\n",
            "Iteration 67, loss = 1.12471906\n",
            "Iteration 68, loss = 1.12287574\n",
            "Iteration 69, loss = 1.12238136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 1.12057729\n",
            "Iteration 71, loss = 1.11897070\n",
            "Iteration 72, loss = 1.11811625\n",
            "Iteration 73, loss = 1.11651949\n",
            "Iteration 74, loss = 1.11571039\n",
            "Iteration 75, loss = 1.11415177\n",
            "Iteration 76, loss = 1.11300121\n",
            "Iteration 77, loss = 1.11392953\n",
            "Iteration 78, loss = 1.11129502\n",
            "Iteration 79, loss = 1.10963056\n",
            "Iteration 80, loss = 1.11024705\n",
            "Iteration 81, loss = 1.10828053\n",
            "Iteration 82, loss = 1.10649207\n",
            "Iteration 83, loss = 1.10719647\n",
            "Iteration 84, loss = 1.10605117\n",
            "Iteration 85, loss = 1.10429059\n",
            "Iteration 86, loss = 1.10340976\n",
            "Iteration 87, loss = 1.10200831\n",
            "Iteration 88, loss = 1.10076456\n",
            "Iteration 89, loss = 1.09973105\n",
            "Iteration 90, loss = 1.09879159\n",
            "Iteration 91, loss = 1.09832562\n",
            "Iteration 92, loss = 1.09700374\n",
            "Iteration 93, loss = 1.09646978\n",
            "Iteration 94, loss = 1.09538057\n",
            "Iteration 95, loss = 1.09631102\n",
            "Iteration 96, loss = 1.09300238\n",
            "Iteration 97, loss = 1.09236345\n",
            "Iteration 98, loss = 1.09116361\n",
            "Iteration 99, loss = 1.09053906\n",
            "Iteration 100, loss = 1.08953830\n",
            "Iteration 1, loss = 1.73261654\n",
            "Iteration 2, loss = 1.64337887\n",
            "Iteration 3, loss = 1.55157931\n",
            "Iteration 4, loss = 1.48913357\n",
            "Iteration 5, loss = 1.40700085\n",
            "Iteration 6, loss = 1.33437975\n",
            "Iteration 7, loss = 1.26333464\n",
            "Iteration 8, loss = 1.20878940\n",
            "Iteration 9, loss = 1.18831400\n",
            "Iteration 10, loss = 1.16577066\n",
            "Iteration 11, loss = 1.15181140\n",
            "Iteration 12, loss = 1.15065932\n",
            "Iteration 13, loss = 1.12713126\n",
            "Iteration 14, loss = 1.12962967\n",
            "Iteration 15, loss = 1.14686292\n",
            "Iteration 16, loss = 1.11620768\n",
            "Iteration 17, loss = 1.12506669\n",
            "Iteration 18, loss = 1.11972691\n",
            "Iteration 19, loss = 1.11204618\n",
            "Iteration 20, loss = 1.10918168\n",
            "Iteration 21, loss = 1.10542084\n",
            "Iteration 22, loss = 1.09950447\n",
            "Iteration 23, loss = 1.09191186\n",
            "Iteration 24, loss = 1.09597924\n",
            "Iteration 25, loss = 1.08535664\n",
            "Iteration 26, loss = 1.07959982\n",
            "Iteration 27, loss = 1.08114413\n",
            "Iteration 28, loss = 1.08584029\n",
            "Iteration 29, loss = 1.10302526\n",
            "Iteration 30, loss = 1.09518738\n",
            "Iteration 31, loss = 1.08038043\n",
            "Iteration 32, loss = 1.07176652\n",
            "Iteration 33, loss = 1.05985377\n",
            "Iteration 34, loss = 1.05597016\n",
            "Iteration 35, loss = 1.04011861\n",
            "Iteration 36, loss = 1.04468755\n",
            "Iteration 37, loss = 1.02893571\n",
            "Iteration 38, loss = 1.03083866\n",
            "Iteration 39, loss = 1.00685226\n",
            "Iteration 40, loss = 1.00770561\n",
            "Iteration 41, loss = 0.99027036\n",
            "Iteration 42, loss = 0.98359796\n",
            "Iteration 43, loss = 0.96773878\n",
            "Iteration 44, loss = 0.96780198\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 45, loss = 1.06992960\n",
            "Iteration 46, loss = 0.98928331\n",
            "Iteration 47, loss = 0.97808587\n",
            "Iteration 48, loss = 0.95383213\n",
            "Iteration 49, loss = 0.95606676\n",
            "Iteration 50, loss = 0.97270544\n",
            "Iteration 51, loss = 0.95030524\n",
            "Iteration 52, loss = 1.00328269\n",
            "Iteration 53, loss = 1.07414606\n",
            "Iteration 54, loss = 1.04763870\n",
            "Iteration 55, loss = 0.95977427\n",
            "Iteration 56, loss = 0.94038302\n",
            "Iteration 57, loss = 0.96418398\n",
            "Iteration 58, loss = 0.91797921\n",
            "Iteration 59, loss = 0.92955822\n",
            "Iteration 60, loss = 0.92870433\n",
            "Iteration 61, loss = 0.89425886\n",
            "Iteration 62, loss = 0.89044216\n",
            "Iteration 63, loss = 0.89086463\n",
            "Iteration 64, loss = 0.87872650\n",
            "Iteration 65, loss = 0.86992900\n",
            "Iteration 66, loss = 0.89803625\n",
            "Iteration 67, loss = 0.87950486\n",
            "Iteration 68, loss = 0.85654950\n",
            "Iteration 69, loss = 0.88131482\n",
            "Iteration 70, loss = 0.86638017\n",
            "Iteration 71, loss = 0.85157147\n",
            "Iteration 72, loss = 0.84850541\n",
            "Iteration 73, loss = 0.83630835\n",
            "Iteration 74, loss = 0.83458057\n",
            "Iteration 75, loss = 0.81685247\n",
            "Iteration 76, loss = 0.82108896\n",
            "Iteration 77, loss = 0.83457472\n",
            "Iteration 78, loss = 0.83137416\n",
            "Iteration 79, loss = 0.80268760\n",
            "Iteration 80, loss = 0.79942523\n",
            "Iteration 81, loss = 0.79827931\n",
            "Iteration 82, loss = 0.80602960\n",
            "Iteration 83, loss = 0.82212481\n",
            "Iteration 84, loss = 0.79060987\n",
            "Iteration 85, loss = 0.80404376\n",
            "Iteration 86, loss = 0.85169748\n",
            "Iteration 87, loss = 0.79743896\n",
            "Iteration 88, loss = 0.77171098\n",
            "Iteration 89, loss = 0.77567255\n",
            "Iteration 90, loss = 0.77020792\n",
            "Iteration 91, loss = 0.81685145\n",
            "Iteration 92, loss = 0.81097072\n",
            "Iteration 93, loss = 0.89877027\n",
            "Iteration 94, loss = 0.76416942\n",
            "Iteration 95, loss = 0.77654175\n",
            "Iteration 96, loss = 0.77594436\n",
            "Iteration 97, loss = 0.76757956\n",
            "Iteration 98, loss = 0.75702338\n",
            "Iteration 99, loss = 0.76388298\n",
            "Iteration 100, loss = 0.74449435\n",
            "Iteration 1, loss = 1.76038982\n",
            "Iteration 2, loss = 1.75338148\n",
            "Iteration 3, loss = 1.74745364\n",
            "Iteration 4, loss = 1.74118661\n",
            "Iteration 5, loss = 1.73500233\n",
            "Iteration 6, loss = 1.72897126\n",
            "Iteration 7, loss = 1.72368115\n",
            "Iteration 8, loss = 1.71779046\n",
            "Iteration 9, loss = 1.71276400\n",
            "Iteration 10, loss = 1.70704743\n",
            "Iteration 11, loss = 1.70220073\n",
            "Iteration 12, loss = 1.69771735\n",
            "Iteration 13, loss = 1.69260257\n",
            "Iteration 14, loss = 1.68828973\n",
            "Iteration 15, loss = 1.68408905\n",
            "Iteration 16, loss = 1.67981095\n",
            "Iteration 17, loss = 1.67588335\n",
            "Iteration 18, loss = 1.67191519\n",
            "Iteration 19, loss = 1.66824406\n",
            "Iteration 20, loss = 1.66501208\n",
            "Iteration 21, loss = 1.66135155\n",
            "Iteration 22, loss = 1.65799643\n",
            "Iteration 23, loss = 1.65495646\n",
            "Iteration 24, loss = 1.65202138\n",
            "Iteration 25, loss = 1.64890851\n",
            "Iteration 26, loss = 1.64584362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 27, loss = 1.64316120\n",
            "Iteration 28, loss = 1.64068233\n",
            "Iteration 29, loss = 1.63792650\n",
            "Iteration 30, loss = 1.63558020\n",
            "Iteration 31, loss = 1.63309895\n",
            "Iteration 32, loss = 1.63091990\n",
            "Iteration 33, loss = 1.62863959\n",
            "Iteration 34, loss = 1.62637516\n",
            "Iteration 35, loss = 1.62457188\n",
            "Iteration 36, loss = 1.62257755\n",
            "Iteration 37, loss = 1.62076176\n",
            "Iteration 38, loss = 1.61880978\n",
            "Iteration 39, loss = 1.61704892\n",
            "Iteration 40, loss = 1.61545934\n",
            "Iteration 41, loss = 1.61383857\n",
            "Iteration 42, loss = 1.61234635\n",
            "Iteration 43, loss = 1.61097750\n",
            "Iteration 44, loss = 1.60954351\n",
            "Iteration 45, loss = 1.60825595\n",
            "Iteration 46, loss = 1.60692614\n",
            "Iteration 47, loss = 1.60582941\n",
            "Iteration 48, loss = 1.60465026\n",
            "Iteration 49, loss = 1.60374018\n",
            "Iteration 50, loss = 1.60246066\n",
            "Iteration 51, loss = 1.60160123\n",
            "Iteration 52, loss = 1.60041906\n",
            "Iteration 53, loss = 1.59967604\n",
            "Iteration 54, loss = 1.59874946\n",
            "Iteration 55, loss = 1.59776686\n",
            "Iteration 56, loss = 1.59712530\n",
            "Iteration 57, loss = 1.59626077\n",
            "Iteration 58, loss = 1.59541657\n",
            "Iteration 59, loss = 1.59468376\n",
            "Iteration 60, loss = 1.59395211\n",
            "Iteration 61, loss = 1.59334045\n",
            "Iteration 62, loss = 1.59258831\n",
            "Iteration 63, loss = 1.59191709\n",
            "Iteration 64, loss = 1.59132819\n",
            "Iteration 65, loss = 1.59066935\n",
            "Iteration 66, loss = 1.59008405\n",
            "Iteration 67, loss = 1.58938091\n",
            "Iteration 68, loss = 1.58877343\n",
            "Iteration 69, loss = 1.58822582\n",
            "Iteration 70, loss = 1.58765717\n",
            "Iteration 71, loss = 1.58714636\n",
            "Iteration 72, loss = 1.58652314\n",
            "Iteration 73, loss = 1.58600087\n",
            "Iteration 74, loss = 1.58544171\n",
            "Iteration 75, loss = 1.58487016\n",
            "Iteration 76, loss = 1.58436306\n",
            "Iteration 77, loss = 1.58381304\n",
            "Iteration 78, loss = 1.58328540\n",
            "Iteration 79, loss = 1.58279602\n",
            "Iteration 80, loss = 1.58220452\n",
            "Iteration 81, loss = 1.58170699\n",
            "Iteration 82, loss = 1.58126255\n",
            "Iteration 83, loss = 1.58073034\n",
            "Iteration 84, loss = 1.58018768\n",
            "Iteration 85, loss = 1.57967883\n",
            "Iteration 86, loss = 1.57918210\n",
            "Iteration 87, loss = 1.57863715\n",
            "Iteration 88, loss = 1.57811671\n",
            "Iteration 89, loss = 1.57760695\n",
            "Iteration 90, loss = 1.57709166\n",
            "Iteration 91, loss = 1.57656883\n",
            "Iteration 92, loss = 1.57604709\n",
            "Iteration 93, loss = 1.57551766\n",
            "Iteration 94, loss = 1.57501731\n",
            "Iteration 95, loss = 1.57450429\n",
            "Iteration 96, loss = 1.57395146\n",
            "Iteration 97, loss = 1.57342292\n",
            "Iteration 98, loss = 1.57292888\n",
            "Iteration 99, loss = 1.57235480\n",
            "Iteration 100, loss = 1.57180949\n",
            "Iteration 1, loss = 1.74787904\n",
            "Iteration 2, loss = 1.68906524\n",
            "Iteration 3, loss = 1.65428489\n",
            "Iteration 4, loss = 1.62568387\n",
            "Iteration 5, loss = 1.60687936\n",
            "Iteration 6, loss = 1.59622413\n",
            "Iteration 7, loss = 1.59447205\n",
            "Iteration 8, loss = 1.59210098\n",
            "Iteration 9, loss = 1.59122791\n",
            "Iteration 10, loss = 1.58869102\n",
            "Iteration 11, loss = 1.58553680\n",
            "Iteration 12, loss = 1.58144295\n",
            "Iteration 13, loss = 1.57658583\n",
            "Iteration 14, loss = 1.57161209\n",
            "Iteration 15, loss = 1.56632471\n",
            "Iteration 16, loss = 1.56143988\n",
            "Iteration 17, loss = 1.55615098\n",
            "Iteration 18, loss = 1.55098041\n",
            "Iteration 19, loss = 1.54503458\n",
            "Iteration 20, loss = 1.53909359\n",
            "Iteration 21, loss = 1.53239403\n",
            "Iteration 22, loss = 1.52530792\n",
            "Iteration 23, loss = 1.51798536\n",
            "Iteration 24, loss = 1.51074213\n",
            "Iteration 25, loss = 1.50201173\n",
            "Iteration 26, loss = 1.49359990\n",
            "Iteration 27, loss = 1.48444623\n",
            "Iteration 28, loss = 1.47488058\n",
            "Iteration 29, loss = 1.46532797\n",
            "Iteration 30, loss = 1.45456295\n",
            "Iteration 31, loss = 1.44436307\n",
            "Iteration 32, loss = 1.43356978\n",
            "Iteration 33, loss = 1.42256334\n",
            "Iteration 34, loss = 1.41128943\n",
            "Iteration 35, loss = 1.40009791\n",
            "Iteration 36, loss = 1.38867056\n",
            "Iteration 37, loss = 1.37795242\n",
            "Iteration 38, loss = 1.36649452\n",
            "Iteration 39, loss = 1.35575336\n",
            "Iteration 40, loss = 1.34497106\n",
            "Iteration 41, loss = 1.33425996\n",
            "Iteration 42, loss = 1.32421282\n",
            "Iteration 43, loss = 1.31470585\n",
            "Iteration 44, loss = 1.30487482\n",
            "Iteration 45, loss = 1.29603116\n",
            "Iteration 46, loss = 1.28744138\n",
            "Iteration 47, loss = 1.27911194\n",
            "Iteration 48, loss = 1.27110678\n",
            "Iteration 49, loss = 1.26431277\n",
            "Iteration 50, loss = 1.25672069\n",
            "Iteration 51, loss = 1.24999097\n",
            "Iteration 52, loss = 1.24364458\n",
            "Iteration 53, loss = 1.23767118\n",
            "Iteration 54, loss = 1.23223133\n",
            "Iteration 55, loss = 1.22720177\n",
            "Iteration 56, loss = 1.22187513\n",
            "Iteration 57, loss = 1.21695028\n",
            "Iteration 58, loss = 1.21238206\n",
            "Iteration 59, loss = 1.20830741\n",
            "Iteration 60, loss = 1.20396994\n",
            "Iteration 61, loss = 1.20035773\n",
            "Iteration 62, loss = 1.19656129\n",
            "Iteration 63, loss = 1.19333168\n",
            "Iteration 64, loss = 1.18979145\n",
            "Iteration 65, loss = 1.18672515\n",
            "Iteration 66, loss = 1.18382239\n",
            "Iteration 67, loss = 1.18093339\n",
            "Iteration 68, loss = 1.17800263\n",
            "Iteration 69, loss = 1.17631543\n",
            "Iteration 70, loss = 1.17321052\n",
            "Iteration 71, loss = 1.17061332\n",
            "Iteration 72, loss = 1.16844379\n",
            "Iteration 73, loss = 1.16611722\n",
            "Iteration 74, loss = 1.16411212\n",
            "Iteration 75, loss = 1.16205979\n",
            "Iteration 76, loss = 1.16022136\n",
            "Iteration 77, loss = 1.15907033\n",
            "Iteration 78, loss = 1.15648895\n",
            "Iteration 79, loss = 1.15437826\n",
            "Iteration 80, loss = 1.15351651\n",
            "Iteration 81, loss = 1.15181587\n",
            "Iteration 82, loss = 1.14986808\n",
            "Iteration 83, loss = 1.14895222\n",
            "Iteration 84, loss = 1.14729931\n",
            "Iteration 85, loss = 1.14559553\n",
            "Iteration 86, loss = 1.14398959\n",
            "Iteration 87, loss = 1.14254498\n",
            "Iteration 88, loss = 1.14140179\n",
            "Iteration 89, loss = 1.13964983\n",
            "Iteration 90, loss = 1.13869103\n",
            "Iteration 91, loss = 1.13784539\n",
            "Iteration 92, loss = 1.13625181\n",
            "Iteration 93, loss = 1.13515281\n",
            "Iteration 94, loss = 1.13445492\n",
            "Iteration 95, loss = 1.13418823\n",
            "Iteration 96, loss = 1.13175094\n",
            "Iteration 97, loss = 1.13082661\n",
            "Iteration 98, loss = 1.12980771\n",
            "Iteration 99, loss = 1.12892533\n",
            "Iteration 100, loss = 1.12810963\n",
            "Iteration 1, loss = 1.70054986\n",
            "Iteration 2, loss = 1.61860738\n",
            "Iteration 3, loss = 1.59159396\n",
            "Iteration 4, loss = 1.57243043\n",
            "Iteration 5, loss = 1.55060951\n",
            "Iteration 6, loss = 1.52256691\n",
            "Iteration 7, loss = 1.48378458\n",
            "Iteration 8, loss = 1.43415590\n",
            "Iteration 9, loss = 1.38075678\n",
            "Iteration 10, loss = 1.32522366\n",
            "Iteration 11, loss = 1.27837935\n",
            "Iteration 12, loss = 1.23526736\n",
            "Iteration 13, loss = 1.20424344\n",
            "Iteration 14, loss = 1.19088947\n",
            "Iteration 15, loss = 1.16817247\n",
            "Iteration 16, loss = 1.15772583\n",
            "Iteration 17, loss = 1.15166094\n",
            "Iteration 18, loss = 1.13981095\n",
            "Iteration 19, loss = 1.13411688\n",
            "Iteration 20, loss = 1.12667624\n",
            "Iteration 21, loss = 1.12372880\n",
            "Iteration 22, loss = 1.12098729\n",
            "Iteration 23, loss = 1.11736172\n",
            "Iteration 24, loss = 1.11499380\n",
            "Iteration 25, loss = 1.11328713\n",
            "Iteration 26, loss = 1.11040713\n",
            "Iteration 27, loss = 1.10992242\n",
            "Iteration 28, loss = 1.10754337\n",
            "Iteration 29, loss = 1.10495605\n",
            "Iteration 30, loss = 1.10611005\n",
            "Iteration 31, loss = 1.10372125\n",
            "Iteration 32, loss = 1.10401978\n",
            "Iteration 33, loss = 1.10274026\n",
            "Iteration 34, loss = 1.10229080\n",
            "Iteration 35, loss = 1.10295324\n",
            "Iteration 36, loss = 1.11775820\n",
            "Iteration 37, loss = 1.09876072\n",
            "Iteration 38, loss = 1.10872836\n",
            "Iteration 39, loss = 1.10157136\n",
            "Iteration 40, loss = 1.10580436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 1.09883561\n",
            "Iteration 42, loss = 1.09834004\n",
            "Iteration 43, loss = 1.10042009\n",
            "Iteration 44, loss = 1.09946096\n",
            "Iteration 45, loss = 1.09985737\n",
            "Iteration 46, loss = 1.09793949\n",
            "Iteration 47, loss = 1.09746752\n",
            "Iteration 48, loss = 1.09587082\n",
            "Iteration 49, loss = 1.09900263\n",
            "Iteration 50, loss = 1.09617308\n",
            "Iteration 51, loss = 1.09410920\n",
            "Iteration 52, loss = 1.09768431\n",
            "Iteration 53, loss = 1.09613520\n",
            "Iteration 54, loss = 1.09360643\n",
            "Iteration 55, loss = 1.09441966\n",
            "Iteration 56, loss = 1.09337100\n",
            "Iteration 57, loss = 1.09147004\n",
            "Iteration 58, loss = 1.08975509\n",
            "Iteration 59, loss = 1.09369042\n",
            "Iteration 60, loss = 1.08712723\n",
            "Iteration 61, loss = 1.09150074\n",
            "Iteration 62, loss = 1.08745502\n",
            "Iteration 63, loss = 1.08979589\n",
            "Iteration 64, loss = 1.08723647\n",
            "Iteration 65, loss = 1.08424792\n",
            "Iteration 66, loss = 1.09744667\n",
            "Iteration 67, loss = 1.09238004\n",
            "Iteration 68, loss = 1.08863560\n",
            "Iteration 69, loss = 1.09177078\n",
            "Iteration 70, loss = 1.07691671\n",
            "Iteration 71, loss = 1.09084373\n",
            "Iteration 72, loss = 1.08387932\n",
            "Iteration 73, loss = 1.07745590\n",
            "Iteration 74, loss = 1.08246705\n",
            "Iteration 75, loss = 1.07000041\n",
            "Iteration 76, loss = 1.07301739\n",
            "Iteration 77, loss = 1.07871584\n",
            "Iteration 78, loss = 1.06445121\n",
            "Iteration 79, loss = 1.07390948\n",
            "Iteration 80, loss = 1.06604548\n",
            "Iteration 81, loss = 1.06484015\n",
            "Iteration 82, loss = 1.06564036\n",
            "Iteration 83, loss = 1.06456917\n",
            "Iteration 84, loss = 1.05771737\n",
            "Iteration 85, loss = 1.07101029\n",
            "Iteration 86, loss = 1.04790692\n",
            "Iteration 87, loss = 1.05962381\n",
            "Iteration 88, loss = 1.04815103\n",
            "Iteration 89, loss = 1.04359554\n",
            "Iteration 90, loss = 1.04946293\n",
            "Iteration 91, loss = 1.03990704\n",
            "Iteration 92, loss = 1.04217981\n",
            "Iteration 93, loss = 1.03066106\n",
            "Iteration 94, loss = 1.02717988\n",
            "Iteration 95, loss = 1.03484265\n",
            "Iteration 96, loss = 1.01681636\n",
            "Iteration 97, loss = 1.02806852\n",
            "Iteration 98, loss = 1.00973666\n",
            "Iteration 99, loss = 1.01312518\n",
            "Iteration 100, loss = 0.99974378\n",
            "Iteration 1, loss = 1.62072848\n",
            "Iteration 2, loss = 1.61727264\n",
            "Iteration 3, loss = 1.61475916\n",
            "Iteration 4, loss = 1.61196036\n",
            "Iteration 5, loss = 1.60990374\n",
            "Iteration 6, loss = 1.60769361\n",
            "Iteration 7, loss = 1.60595607\n",
            "Iteration 8, loss = 1.60448450\n",
            "Iteration 9, loss = 1.60293406\n",
            "Iteration 10, loss = 1.60144637\n",
            "Iteration 11, loss = 1.60045972\n",
            "Iteration 12, loss = 1.59939598\n",
            "Iteration 13, loss = 1.59835382\n",
            "Iteration 14, loss = 1.59734919\n",
            "Iteration 15, loss = 1.59645348\n",
            "Iteration 16, loss = 1.59571701\n",
            "Iteration 17, loss = 1.59482801\n",
            "Iteration 18, loss = 1.59401064\n",
            "Iteration 19, loss = 1.59315410\n",
            "Iteration 20, loss = 1.59236996\n",
            "Iteration 21, loss = 1.59153017\n",
            "Iteration 22, loss = 1.59080265\n",
            "Iteration 23, loss = 1.58994830\n",
            "Iteration 24, loss = 1.58915878\n",
            "Iteration 25, loss = 1.58829568\n",
            "Iteration 26, loss = 1.58752341\n",
            "Iteration 27, loss = 1.58672128\n",
            "Iteration 28, loss = 1.58586859\n",
            "Iteration 29, loss = 1.58500171\n",
            "Iteration 30, loss = 1.58421487\n",
            "Iteration 31, loss = 1.58340199\n",
            "Iteration 32, loss = 1.58261730\n",
            "Iteration 33, loss = 1.58168882\n",
            "Iteration 34, loss = 1.58084577\n",
            "Iteration 35, loss = 1.58001012\n",
            "Iteration 36, loss = 1.57917287\n",
            "Iteration 37, loss = 1.57830316\n",
            "Iteration 38, loss = 1.57747434\n",
            "Iteration 39, loss = 1.57658807\n",
            "Iteration 40, loss = 1.57574408\n",
            "Iteration 41, loss = 1.57482640\n",
            "Iteration 42, loss = 1.57392980\n",
            "Iteration 43, loss = 1.57305562\n",
            "Iteration 44, loss = 1.57229653\n",
            "Iteration 45, loss = 1.57122868\n",
            "Iteration 46, loss = 1.57031337\n",
            "Iteration 47, loss = 1.56942353\n",
            "Iteration 48, loss = 1.56849111\n",
            "Iteration 49, loss = 1.56753881\n",
            "Iteration 50, loss = 1.56662798\n",
            "Iteration 51, loss = 1.56565313\n",
            "Iteration 52, loss = 1.56469907\n",
            "Iteration 53, loss = 1.56375948\n",
            "Iteration 54, loss = 1.56279965\n",
            "Iteration 55, loss = 1.56177242\n",
            "Iteration 56, loss = 1.56076462\n",
            "Iteration 57, loss = 1.55986528\n",
            "Iteration 58, loss = 1.55876812\n",
            "Iteration 59, loss = 1.55778256\n",
            "Iteration 60, loss = 1.55668653\n",
            "Iteration 61, loss = 1.55567074\n",
            "Iteration 62, loss = 1.55463467\n",
            "Iteration 63, loss = 1.55362302\n",
            "Iteration 64, loss = 1.55250396\n",
            "Iteration 65, loss = 1.55143917\n",
            "Iteration 66, loss = 1.55033463\n",
            "Iteration 67, loss = 1.54923748\n",
            "Iteration 68, loss = 1.54811532\n",
            "Iteration 69, loss = 1.54696138\n",
            "Iteration 70, loss = 1.54589190\n",
            "Iteration 71, loss = 1.54466506\n",
            "Iteration 72, loss = 1.54357748\n",
            "Iteration 73, loss = 1.54248130\n",
            "Iteration 74, loss = 1.54117877\n",
            "Iteration 75, loss = 1.53998634\n",
            "Iteration 76, loss = 1.53879840\n",
            "Iteration 77, loss = 1.53759576\n",
            "Iteration 78, loss = 1.53631810\n",
            "Iteration 79, loss = 1.53517571\n",
            "Iteration 80, loss = 1.53384216\n",
            "Iteration 81, loss = 1.53258055\n",
            "Iteration 82, loss = 1.53135334\n",
            "Iteration 83, loss = 1.53005339\n",
            "Iteration 84, loss = 1.52870848\n",
            "Iteration 85, loss = 1.52743501\n",
            "Iteration 86, loss = 1.52614564\n",
            "Iteration 87, loss = 1.52474928\n",
            "Iteration 88, loss = 1.52340099\n",
            "Iteration 89, loss = 1.52201918\n",
            "Iteration 90, loss = 1.52064646\n",
            "Iteration 91, loss = 1.51937721\n",
            "Iteration 92, loss = 1.51786359\n",
            "Iteration 93, loss = 1.51646167\n",
            "Iteration 94, loss = 1.51502233\n",
            "Iteration 95, loss = 1.51366787\n",
            "Iteration 96, loss = 1.51212879\n",
            "Iteration 97, loss = 1.51071156\n",
            "Iteration 98, loss = 1.50932222\n",
            "Iteration 99, loss = 1.50780559\n",
            "Iteration 100, loss = 1.50627405\n",
            "Iteration 1, loss = 1.61894229\n",
            "Iteration 2, loss = 1.60020553\n",
            "Iteration 3, loss = 1.59655156\n",
            "Iteration 4, loss = 1.58739835\n",
            "Iteration 5, loss = 1.57944938\n",
            "Iteration 6, loss = 1.57177942\n",
            "Iteration 7, loss = 1.56405073\n",
            "Iteration 8, loss = 1.55572257\n",
            "Iteration 9, loss = 1.54742117\n",
            "Iteration 10, loss = 1.53818441\n",
            "Iteration 11, loss = 1.52853166\n",
            "Iteration 12, loss = 1.51814237\n",
            "Iteration 13, loss = 1.50814165\n",
            "Iteration 14, loss = 1.49485252\n",
            "Iteration 15, loss = 1.48234399\n",
            "Iteration 16, loss = 1.46953990\n",
            "Iteration 17, loss = 1.45547446\n",
            "Iteration 18, loss = 1.44045746\n",
            "Iteration 19, loss = 1.42538679\n",
            "Iteration 20, loss = 1.40932222\n",
            "Iteration 21, loss = 1.39294427\n",
            "Iteration 22, loss = 1.37737393\n",
            "Iteration 23, loss = 1.36083695\n",
            "Iteration 24, loss = 1.34472810\n",
            "Iteration 25, loss = 1.32838780\n",
            "Iteration 26, loss = 1.31334119\n",
            "Iteration 27, loss = 1.29899953\n",
            "Iteration 28, loss = 1.28460368\n",
            "Iteration 29, loss = 1.27075043\n",
            "Iteration 30, loss = 1.25865025\n",
            "Iteration 31, loss = 1.24741715\n",
            "Iteration 32, loss = 1.23668707\n",
            "Iteration 33, loss = 1.22594434\n",
            "Iteration 34, loss = 1.21638201\n",
            "Iteration 35, loss = 1.20791249\n",
            "Iteration 36, loss = 1.20037857\n",
            "Iteration 37, loss = 1.19295832\n",
            "Iteration 38, loss = 1.18620202\n",
            "Iteration 39, loss = 1.17999214\n",
            "Iteration 40, loss = 1.17459374\n",
            "Iteration 41, loss = 1.16871132\n",
            "Iteration 42, loss = 1.16442110\n",
            "Iteration 43, loss = 1.15948718\n",
            "Iteration 44, loss = 1.15706000\n",
            "Iteration 45, loss = 1.15220776\n",
            "Iteration 46, loss = 1.14774876\n",
            "Iteration 47, loss = 1.14436311\n",
            "Iteration 48, loss = 1.14139695\n",
            "Iteration 49, loss = 1.13867149\n",
            "Iteration 50, loss = 1.13588971\n",
            "Iteration 51, loss = 1.13310495\n",
            "Iteration 52, loss = 1.13060591\n",
            "Iteration 53, loss = 1.12827874\n",
            "Iteration 54, loss = 1.12735935\n",
            "Iteration 55, loss = 1.12492606\n",
            "Iteration 56, loss = 1.12211934\n",
            "Iteration 57, loss = 1.12156242\n",
            "Iteration 58, loss = 1.11905296\n",
            "Iteration 59, loss = 1.11738317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 60, loss = 1.11490793\n",
            "Iteration 61, loss = 1.11402247\n",
            "Iteration 62, loss = 1.11224220\n",
            "Iteration 63, loss = 1.11132053\n",
            "Iteration 64, loss = 1.10944389\n",
            "Iteration 65, loss = 1.10871273\n",
            "Iteration 66, loss = 1.10609853\n",
            "Iteration 67, loss = 1.10714116\n",
            "Iteration 68, loss = 1.10401488\n",
            "Iteration 69, loss = 1.10207588\n",
            "Iteration 70, loss = 1.10375260\n",
            "Iteration 71, loss = 1.10070601\n",
            "Iteration 72, loss = 1.09916279\n",
            "Iteration 73, loss = 1.10032502\n",
            "Iteration 74, loss = 1.09679758\n",
            "Iteration 75, loss = 1.09527077\n",
            "Iteration 76, loss = 1.09536922\n",
            "Iteration 77, loss = 1.09380661\n",
            "Iteration 78, loss = 1.09384402\n",
            "Iteration 79, loss = 1.09218526\n",
            "Iteration 80, loss = 1.08946455\n",
            "Iteration 81, loss = 1.08891549\n",
            "Iteration 82, loss = 1.08925217\n",
            "Iteration 83, loss = 1.08873514\n",
            "Iteration 84, loss = 1.08628673\n",
            "Iteration 85, loss = 1.08506472\n",
            "Iteration 86, loss = 1.08324934\n",
            "Iteration 87, loss = 1.08180147\n",
            "Iteration 88, loss = 1.08049498\n",
            "Iteration 89, loss = 1.07915939\n",
            "Iteration 90, loss = 1.07829184\n",
            "Iteration 91, loss = 1.07765852\n",
            "Iteration 92, loss = 1.07552129\n",
            "Iteration 93, loss = 1.07455134\n",
            "Iteration 94, loss = 1.07314451\n",
            "Iteration 95, loss = 1.07291415\n",
            "Iteration 96, loss = 1.07012319\n",
            "Iteration 97, loss = 1.07011447\n",
            "Iteration 98, loss = 1.06956707\n",
            "Iteration 99, loss = 1.06653300\n",
            "Iteration 100, loss = 1.06585839\n",
            "Iteration 1, loss = 1.76838732\n",
            "Iteration 2, loss = 1.61300286\n",
            "Iteration 3, loss = 1.57074302\n",
            "Iteration 4, loss = 1.49477591\n",
            "Iteration 5, loss = 1.42772464\n",
            "Iteration 6, loss = 1.32771806\n",
            "Iteration 7, loss = 1.24683963\n",
            "Iteration 8, loss = 1.19392677\n",
            "Iteration 9, loss = 1.15814943\n",
            "Iteration 10, loss = 1.13792131\n",
            "Iteration 11, loss = 1.12824838\n",
            "Iteration 12, loss = 1.12629921\n",
            "Iteration 13, loss = 1.12300571\n",
            "Iteration 14, loss = 1.12085570\n",
            "Iteration 15, loss = 1.13406143\n",
            "Iteration 16, loss = 1.11874585\n",
            "Iteration 17, loss = 1.11343054\n",
            "Iteration 18, loss = 1.13035492\n",
            "Iteration 19, loss = 1.11434855\n",
            "Iteration 20, loss = 1.11420890\n",
            "Iteration 21, loss = 1.12477586\n",
            "Iteration 22, loss = 1.10867939\n",
            "Iteration 23, loss = 1.11434972\n",
            "Iteration 24, loss = 1.12343040\n",
            "Iteration 25, loss = 1.10313593\n",
            "Iteration 26, loss = 1.10445449\n",
            "Iteration 27, loss = 1.10702655\n",
            "Iteration 28, loss = 1.08477051\n",
            "Iteration 29, loss = 1.09213003\n",
            "Iteration 30, loss = 1.07943801\n",
            "Iteration 31, loss = 1.07730182\n",
            "Iteration 32, loss = 1.07689870\n",
            "Iteration 33, loss = 1.07426539\n",
            "Iteration 34, loss = 1.06224094\n",
            "Iteration 35, loss = 1.05961413\n",
            "Iteration 36, loss = 1.05876739\n",
            "Iteration 37, loss = 1.05221954\n",
            "Iteration 38, loss = 1.04214324\n",
            "Iteration 39, loss = 1.04347126\n",
            "Iteration 40, loss = 1.03504674\n",
            "Iteration 41, loss = 1.01500266\n",
            "Iteration 42, loss = 1.00561947\n",
            "Iteration 43, loss = 1.00513579\n",
            "Iteration 44, loss = 1.02125700\n",
            "Iteration 45, loss = 1.06366027\n",
            "Iteration 46, loss = 1.08763953\n",
            "Iteration 47, loss = 1.06327691\n",
            "Iteration 48, loss = 1.04040055\n",
            "Iteration 49, loss = 1.03345510\n",
            "Iteration 50, loss = 0.96915817\n",
            "Iteration 51, loss = 0.98434207\n",
            "Iteration 52, loss = 1.00436434\n",
            "Iteration 53, loss = 0.99537859\n",
            "Iteration 54, loss = 1.03336471\n",
            "Iteration 55, loss = 0.97078567\n",
            "Iteration 56, loss = 0.99371978\n",
            "Iteration 57, loss = 0.98390866\n",
            "Iteration 58, loss = 0.94479171\n",
            "Iteration 59, loss = 0.94788942\n",
            "Iteration 60, loss = 0.93171420\n",
            "Iteration 61, loss = 0.94469512\n",
            "Iteration 62, loss = 0.92937796\n",
            "Iteration 63, loss = 0.93582267\n",
            "Iteration 64, loss = 0.93739265\n",
            "Iteration 65, loss = 0.90459229\n",
            "Iteration 66, loss = 0.91472477\n",
            "Iteration 67, loss = 0.89752027\n",
            "Iteration 68, loss = 0.89882040\n",
            "Iteration 69, loss = 0.87045084\n",
            "Iteration 70, loss = 0.89824452\n",
            "Iteration 71, loss = 0.87869131\n",
            "Iteration 72, loss = 0.89955948\n",
            "Iteration 73, loss = 0.93304309\n",
            "Iteration 74, loss = 0.85747887\n",
            "Iteration 75, loss = 0.84304570\n",
            "Iteration 76, loss = 0.84200410\n",
            "Iteration 77, loss = 0.84387432\n",
            "Iteration 78, loss = 0.83799754\n",
            "Iteration 79, loss = 0.85084244\n",
            "Iteration 80, loss = 0.83761091\n",
            "Iteration 81, loss = 0.81609166\n",
            "Iteration 82, loss = 0.81777153\n",
            "Iteration 83, loss = 0.85487794\n",
            "Iteration 84, loss = 0.81623477\n",
            "Iteration 85, loss = 0.93055247\n",
            "Iteration 86, loss = 0.88738192\n",
            "Iteration 87, loss = 0.88597411\n",
            "Iteration 88, loss = 0.93985641\n",
            "Iteration 89, loss = 0.83561210\n",
            "Iteration 90, loss = 0.80800313\n",
            "Iteration 91, loss = 0.79903965\n",
            "Iteration 92, loss = 0.78669543\n",
            "Iteration 93, loss = 0.78934144\n",
            "Iteration 94, loss = 0.78477565\n",
            "Iteration 95, loss = 0.84501272\n",
            "Iteration 96, loss = 0.81164919\n",
            "Iteration 97, loss = 0.82723995\n",
            "Iteration 98, loss = 0.84152965\n",
            "Iteration 99, loss = 0.85726833\n",
            "Iteration 100, loss = 0.78453301\n",
            "Iteration 1, loss = 1.70857386\n",
            "Iteration 2, loss = 1.69806525\n",
            "Iteration 3, loss = 1.68884894\n",
            "Iteration 4, loss = 1.67988833\n",
            "Iteration 5, loss = 1.67174127\n",
            "Iteration 6, loss = 1.66337883\n",
            "Iteration 7, loss = 1.65651549\n",
            "Iteration 8, loss = 1.64977125\n",
            "Iteration 9, loss = 1.64360872\n",
            "Iteration 10, loss = 1.63800454\n",
            "Iteration 11, loss = 1.63302199\n",
            "Iteration 12, loss = 1.62879758\n",
            "Iteration 13, loss = 1.62381507\n",
            "Iteration 14, loss = 1.62085053\n",
            "Iteration 15, loss = 1.61751370\n",
            "Iteration 16, loss = 1.61486940\n",
            "Iteration 17, loss = 1.61204915\n",
            "Iteration 18, loss = 1.61024062\n",
            "Iteration 19, loss = 1.60820720\n",
            "Iteration 20, loss = 1.60653061\n",
            "Iteration 21, loss = 1.60500727\n",
            "Iteration 22, loss = 1.60413641\n",
            "Iteration 23, loss = 1.60287053\n",
            "Iteration 24, loss = 1.60193179\n",
            "Iteration 25, loss = 1.60119373\n",
            "Iteration 26, loss = 1.60050259\n",
            "Iteration 27, loss = 1.59994596\n",
            "Iteration 28, loss = 1.59965528\n",
            "Iteration 29, loss = 1.59918326\n",
            "Iteration 30, loss = 1.59896847\n",
            "Iteration 31, loss = 1.59872103\n",
            "Iteration 32, loss = 1.59846453\n",
            "Iteration 33, loss = 1.59815115\n",
            "Iteration 34, loss = 1.59803126\n",
            "Iteration 35, loss = 1.59788486\n",
            "Iteration 36, loss = 1.59776273\n",
            "Iteration 37, loss = 1.59765461\n",
            "Iteration 38, loss = 1.59760906\n",
            "Iteration 39, loss = 1.59753370\n",
            "Iteration 40, loss = 1.59740999\n",
            "Iteration 41, loss = 1.59732993\n",
            "Iteration 42, loss = 1.59723474\n",
            "Iteration 43, loss = 1.59716059\n",
            "Iteration 44, loss = 1.59719680\n",
            "Iteration 45, loss = 1.59701331\n",
            "Iteration 46, loss = 1.59694080\n",
            "Iteration 47, loss = 1.59688623\n",
            "Iteration 48, loss = 1.59682144\n",
            "Iteration 49, loss = 1.59676375\n",
            "Iteration 50, loss = 1.59667532\n",
            "Iteration 51, loss = 1.59661080\n",
            "Iteration 52, loss = 1.59652494\n",
            "Iteration 53, loss = 1.59648281\n",
            "Iteration 54, loss = 1.59647417\n",
            "Iteration 55, loss = 1.59634087\n",
            "Iteration 56, loss = 1.59622988\n",
            "Iteration 57, loss = 1.59627613\n",
            "Iteration 58, loss = 1.59607665\n",
            "Iteration 59, loss = 1.59605313\n",
            "Iteration 60, loss = 1.59592205\n",
            "Iteration 61, loss = 1.59587113\n",
            "Iteration 62, loss = 1.59579020\n",
            "Iteration 63, loss = 1.59578586\n",
            "Iteration 64, loss = 1.59568800\n",
            "Iteration 65, loss = 1.59558765\n",
            "Iteration 66, loss = 1.59548499\n",
            "Iteration 67, loss = 1.59542355\n",
            "Iteration 68, loss = 1.59534874\n",
            "Iteration 69, loss = 1.59526456\n",
            "Iteration 70, loss = 1.59519774\n",
            "Iteration 71, loss = 1.59509213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 72, loss = 1.59505576\n",
            "Iteration 73, loss = 1.59503000\n",
            "Iteration 74, loss = 1.59491148\n",
            "Iteration 75, loss = 1.59479023\n",
            "Iteration 76, loss = 1.59473814\n",
            "Iteration 77, loss = 1.59467479\n",
            "Iteration 78, loss = 1.59455247\n",
            "Iteration 79, loss = 1.59453789\n",
            "Iteration 80, loss = 1.59439796\n",
            "Iteration 81, loss = 1.59432329\n",
            "Iteration 82, loss = 1.59427436\n",
            "Iteration 83, loss = 1.59416690\n",
            "Iteration 84, loss = 1.59407881\n",
            "Iteration 85, loss = 1.59404611\n",
            "Iteration 86, loss = 1.59394315\n",
            "Iteration 87, loss = 1.59383230\n",
            "Iteration 88, loss = 1.59382662\n",
            "Iteration 89, loss = 1.59366320\n",
            "Iteration 90, loss = 1.59358803\n",
            "Iteration 91, loss = 1.59356476\n",
            "Iteration 92, loss = 1.59341961\n",
            "Iteration 93, loss = 1.59333967\n",
            "Iteration 94, loss = 1.59323420\n",
            "Iteration 95, loss = 1.59321898\n",
            "Iteration 96, loss = 1.59306579\n",
            "Iteration 97, loss = 1.59305253\n",
            "Iteration 98, loss = 1.59297628\n",
            "Iteration 99, loss = 1.59289990\n",
            "Iteration 100, loss = 1.59277541\n",
            "Iteration 1, loss = 1.69470806\n",
            "Iteration 2, loss = 1.62220141\n",
            "Iteration 3, loss = 1.60294876\n",
            "Iteration 4, loss = 1.60605449\n",
            "Iteration 5, loss = 1.61373320\n",
            "Iteration 6, loss = 1.61204909\n",
            "Iteration 7, loss = 1.60858525\n",
            "Iteration 8, loss = 1.60455100\n",
            "Iteration 9, loss = 1.60054416\n",
            "Iteration 10, loss = 1.59702836\n",
            "Iteration 11, loss = 1.59529628\n",
            "Iteration 12, loss = 1.59493478\n",
            "Iteration 13, loss = 1.59552247\n",
            "Iteration 14, loss = 1.59478249\n",
            "Iteration 15, loss = 1.59312812\n",
            "Iteration 16, loss = 1.59168298\n",
            "Iteration 17, loss = 1.59013616\n",
            "Iteration 18, loss = 1.58907160\n",
            "Iteration 19, loss = 1.58845846\n",
            "Iteration 20, loss = 1.58753616\n",
            "Iteration 21, loss = 1.58636637\n",
            "Iteration 22, loss = 1.58577466\n",
            "Iteration 23, loss = 1.58411355\n",
            "Iteration 24, loss = 1.58277949\n",
            "Iteration 25, loss = 1.58143343\n",
            "Iteration 26, loss = 1.58081564\n",
            "Iteration 27, loss = 1.57954680\n",
            "Iteration 28, loss = 1.57737139\n",
            "Iteration 29, loss = 1.57528899\n",
            "Iteration 30, loss = 1.57401931\n",
            "Iteration 31, loss = 1.57242080\n",
            "Iteration 32, loss = 1.57112932\n",
            "Iteration 33, loss = 1.56819236\n",
            "Iteration 34, loss = 1.56574192\n",
            "Iteration 35, loss = 1.56359845\n",
            "Iteration 36, loss = 1.56155418\n",
            "Iteration 37, loss = 1.55856678\n",
            "Iteration 38, loss = 1.55602029\n",
            "Iteration 39, loss = 1.55323632\n",
            "Iteration 40, loss = 1.55055629\n",
            "Iteration 41, loss = 1.54688886\n",
            "Iteration 42, loss = 1.54314073\n",
            "Iteration 43, loss = 1.53973082\n",
            "Iteration 44, loss = 1.53711275\n",
            "Iteration 45, loss = 1.53189285\n",
            "Iteration 46, loss = 1.52746229\n",
            "Iteration 47, loss = 1.52326006\n",
            "Iteration 48, loss = 1.51853631\n",
            "Iteration 49, loss = 1.51360957\n",
            "Iteration 50, loss = 1.50856380\n",
            "Iteration 51, loss = 1.50323412\n",
            "Iteration 52, loss = 1.49761990\n",
            "Iteration 53, loss = 1.49210406\n",
            "Iteration 54, loss = 1.48690001\n",
            "Iteration 55, loss = 1.48049081\n",
            "Iteration 56, loss = 1.47372921\n",
            "Iteration 57, loss = 1.46837075\n",
            "Iteration 58, loss = 1.46060001\n",
            "Iteration 59, loss = 1.45425653\n",
            "Iteration 60, loss = 1.44669583\n",
            "Iteration 61, loss = 1.43989477\n",
            "Iteration 62, loss = 1.43259493\n",
            "Iteration 63, loss = 1.42590587\n",
            "Iteration 64, loss = 1.41826262\n",
            "Iteration 65, loss = 1.41068438\n",
            "Iteration 66, loss = 1.40294117\n",
            "Iteration 67, loss = 1.39592826\n",
            "Iteration 68, loss = 1.38817243\n",
            "Iteration 69, loss = 1.38045659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 1.37356550\n",
            "Iteration 71, loss = 1.36555623\n",
            "Iteration 72, loss = 1.35875929\n",
            "Iteration 73, loss = 1.35233181\n",
            "Iteration 74, loss = 1.34422701\n",
            "Iteration 75, loss = 1.33686842\n",
            "Iteration 76, loss = 1.33016430\n",
            "Iteration 77, loss = 1.32362971\n",
            "Iteration 78, loss = 1.31672247\n",
            "Iteration 79, loss = 1.31085912\n",
            "Iteration 80, loss = 1.30406724\n",
            "Iteration 81, loss = 1.29769729\n",
            "Iteration 82, loss = 1.29226609\n",
            "Iteration 83, loss = 1.28633910\n",
            "Iteration 84, loss = 1.28053446\n",
            "Iteration 85, loss = 1.27554820\n",
            "Iteration 86, loss = 1.27016655\n",
            "Iteration 87, loss = 1.26482524\n",
            "Iteration 88, loss = 1.26019884\n",
            "Iteration 89, loss = 1.25513125\n",
            "Iteration 90, loss = 1.25103716\n",
            "Iteration 91, loss = 1.24734420\n",
            "Iteration 92, loss = 1.24229355\n",
            "Iteration 93, loss = 1.23832291\n",
            "Iteration 94, loss = 1.23424836\n",
            "Iteration 95, loss = 1.23136502\n",
            "Iteration 96, loss = 1.22700857\n",
            "Iteration 97, loss = 1.22410821\n",
            "Iteration 98, loss = 1.22115023\n",
            "Iteration 99, loss = 1.21818198\n",
            "Iteration 100, loss = 1.21456882\n",
            "Iteration 1, loss = 1.84666488\n",
            "Iteration 2, loss = 1.74131382\n",
            "Iteration 3, loss = 1.67601667\n",
            "Iteration 4, loss = 1.66684712\n",
            "Iteration 5, loss = 1.61398026\n",
            "Iteration 6, loss = 1.60446475\n",
            "Iteration 7, loss = 1.61607121\n",
            "Iteration 8, loss = 1.59890848\n",
            "Iteration 9, loss = 1.58036454\n",
            "Iteration 10, loss = 1.57366579\n",
            "Iteration 11, loss = 1.56875112\n",
            "Iteration 12, loss = 1.55841045\n",
            "Iteration 13, loss = 1.53894655\n",
            "Iteration 14, loss = 1.51735073\n",
            "Iteration 15, loss = 1.49777435\n",
            "Iteration 16, loss = 1.47717041\n",
            "Iteration 17, loss = 1.44687585\n",
            "Iteration 18, loss = 1.41014292\n",
            "Iteration 19, loss = 1.37187426\n",
            "Iteration 20, loss = 1.33549832\n",
            "Iteration 21, loss = 1.29911309\n",
            "Iteration 22, loss = 1.27042467\n",
            "Iteration 23, loss = 1.23961298\n",
            "Iteration 24, loss = 1.21630991\n",
            "Iteration 25, loss = 1.19499906\n",
            "Iteration 26, loss = 1.18286197\n",
            "Iteration 27, loss = 1.17168712\n",
            "Iteration 28, loss = 1.15579839\n",
            "Iteration 29, loss = 1.14833491\n",
            "Iteration 30, loss = 1.14132968\n",
            "Iteration 31, loss = 1.13445380\n",
            "Iteration 32, loss = 1.13573150\n",
            "Iteration 33, loss = 1.12953360\n",
            "Iteration 34, loss = 1.12278013\n",
            "Iteration 35, loss = 1.12342813\n",
            "Iteration 36, loss = 1.12119807\n",
            "Iteration 37, loss = 1.11914584\n",
            "Iteration 38, loss = 1.11718325\n",
            "Iteration 39, loss = 1.11924202\n",
            "Iteration 40, loss = 1.11408711\n",
            "Iteration 41, loss = 1.11316415\n",
            "Iteration 42, loss = 1.11535619\n",
            "Iteration 43, loss = 1.11529176\n",
            "Iteration 44, loss = 1.12933335\n",
            "Iteration 45, loss = 1.11897490\n",
            "Iteration 46, loss = 1.10751739\n",
            "Iteration 47, loss = 1.10945152\n",
            "Iteration 48, loss = 1.10604520\n",
            "Iteration 49, loss = 1.10891558\n",
            "Iteration 50, loss = 1.10766224\n",
            "Iteration 51, loss = 1.10551416\n",
            "Iteration 52, loss = 1.10592338\n",
            "Iteration 53, loss = 1.10152375\n",
            "Iteration 54, loss = 1.10872455\n",
            "Iteration 55, loss = 1.10278205\n",
            "Iteration 56, loss = 1.10483084\n",
            "Iteration 57, loss = 1.10898825\n",
            "Iteration 58, loss = 1.10570669\n",
            "Iteration 59, loss = 1.10047666\n",
            "Iteration 60, loss = 1.10292593\n",
            "Iteration 61, loss = 1.10229986\n",
            "Iteration 62, loss = 1.10243732\n",
            "Iteration 63, loss = 1.09944623\n",
            "Iteration 64, loss = 1.10505864\n",
            "Iteration 65, loss = 1.10420740\n",
            "Iteration 66, loss = 1.10205386\n",
            "Iteration 67, loss = 1.10734806\n",
            "Iteration 68, loss = 1.09191819\n",
            "Iteration 69, loss = 1.10262949\n",
            "Iteration 70, loss = 1.09120780\n",
            "Iteration 71, loss = 1.09372902\n",
            "Iteration 72, loss = 1.09050655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 73, loss = 1.09209529\n",
            "Iteration 74, loss = 1.08601164\n",
            "Iteration 75, loss = 1.08814597\n",
            "Iteration 76, loss = 1.08599822\n",
            "Iteration 77, loss = 1.08641126\n",
            "Iteration 78, loss = 1.08363178\n",
            "Iteration 79, loss = 1.07699118\n",
            "Iteration 80, loss = 1.08087051\n",
            "Iteration 81, loss = 1.07614668\n",
            "Iteration 82, loss = 1.07216759\n",
            "Iteration 83, loss = 1.07276880\n",
            "Iteration 84, loss = 1.06959155\n",
            "Iteration 85, loss = 1.06427883\n",
            "Iteration 86, loss = 1.06698135\n",
            "Iteration 87, loss = 1.05992497\n",
            "Iteration 88, loss = 1.05984859\n",
            "Iteration 89, loss = 1.05530415\n",
            "Iteration 90, loss = 1.05487331\n",
            "Iteration 91, loss = 1.05662944\n",
            "Iteration 92, loss = 1.04806968\n",
            "Iteration 93, loss = 1.04609527\n",
            "Iteration 94, loss = 1.04118093\n",
            "Iteration 95, loss = 1.04902512\n",
            "Iteration 96, loss = 1.03096801\n",
            "Iteration 97, loss = 1.03534446\n",
            "Iteration 98, loss = 1.03558263\n",
            "Iteration 99, loss = 1.02728748\n",
            "Iteration 100, loss = 1.02225538\n",
            "Iteration 1, loss = 1.62042427\n",
            "Iteration 2, loss = 1.61705180\n",
            "Iteration 3, loss = 1.61458003\n",
            "Iteration 4, loss = 1.61185522\n",
            "Iteration 5, loss = 1.60982915\n",
            "Iteration 6, loss = 1.60766599\n",
            "Iteration 7, loss = 1.60594890\n",
            "Iteration 8, loss = 1.60449894\n",
            "Iteration 9, loss = 1.60297369\n",
            "Iteration 10, loss = 1.60151634\n",
            "Iteration 11, loss = 1.60054187\n",
            "Iteration 12, loss = 1.59950469\n",
            "Iteration 13, loss = 1.59846941\n",
            "Iteration 14, loss = 1.59749825\n",
            "Iteration 15, loss = 1.59662562\n",
            "Iteration 16, loss = 1.59591290\n",
            "Iteration 17, loss = 1.59505142\n",
            "Iteration 18, loss = 1.59427062\n",
            "Iteration 19, loss = 1.59344064\n",
            "Iteration 20, loss = 1.59269441\n",
            "Iteration 21, loss = 1.59188985\n",
            "Iteration 22, loss = 1.59120386\n",
            "Iteration 23, loss = 1.59038551\n",
            "Iteration 24, loss = 1.58963773\n",
            "Iteration 25, loss = 1.58880983\n",
            "Iteration 26, loss = 1.58807552\n",
            "Iteration 27, loss = 1.58731990\n",
            "Iteration 28, loss = 1.58650702\n",
            "Iteration 29, loss = 1.58567954\n",
            "Iteration 30, loss = 1.58493284\n",
            "Iteration 31, loss = 1.58415980\n",
            "Iteration 32, loss = 1.58341196\n",
            "Iteration 33, loss = 1.58252742\n",
            "Iteration 34, loss = 1.58172399\n",
            "Iteration 35, loss = 1.58092857\n",
            "Iteration 36, loss = 1.58013079\n",
            "Iteration 37, loss = 1.57930150\n",
            "Iteration 38, loss = 1.57851170\n",
            "Iteration 39, loss = 1.57766686\n",
            "Iteration 40, loss = 1.57686281\n",
            "Iteration 41, loss = 1.57598561\n",
            "Iteration 42, loss = 1.57513088\n",
            "Iteration 43, loss = 1.57429694\n",
            "Iteration 44, loss = 1.57357401\n",
            "Iteration 45, loss = 1.57255243\n",
            "Iteration 46, loss = 1.57167859\n",
            "Iteration 47, loss = 1.57082920\n",
            "Iteration 48, loss = 1.56993823\n",
            "Iteration 49, loss = 1.56902730\n",
            "Iteration 50, loss = 1.56815751\n",
            "Iteration 51, loss = 1.56722544\n",
            "Iteration 52, loss = 1.56631304\n",
            "Iteration 53, loss = 1.56541367\n",
            "Iteration 54, loss = 1.56449436"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 55, loss = 1.56351220\n",
            "Iteration 56, loss = 1.56254634\n",
            "Iteration 57, loss = 1.56168550\n",
            "Iteration 58, loss = 1.56063332\n",
            "Iteration 59, loss = 1.55968866\n",
            "Iteration 60, loss = 1.55863730\n",
            "Iteration 61, loss = 1.55766248\n",
            "Iteration 62, loss = 1.55666811\n",
            "Iteration 63, loss = 1.55569758\n",
            "Iteration 64, loss = 1.55462303\n",
            "Iteration 65, loss = 1.55359954\n",
            "Iteration 66, loss = 1.55253823\n",
            "Iteration 67, loss = 1.55148339\n",
            "Iteration 68, loss = 1.55040372\n",
            "Iteration 69, loss = 1.54929348\n",
            "Iteration 70, loss = 1.54826344\n",
            "Iteration 71, loss = 1.54708272\n",
            "Iteration 72, loss = 1.54603536\n",
            "Iteration 73, loss = 1.54497839\n",
            "Iteration 74, loss = 1.54372387\n",
            "Iteration 75, loss = 1.54257383\n",
            "Iteration 76, loss = 1.54142718\n",
            "Iteration 77, loss = 1.54026606\n",
            "Iteration 78, loss = 1.53903325\n",
            "Iteration 79, loss = 1.53792985\n",
            "Iteration 80, loss = 1.53664074\n",
            "Iteration 81, loss = 1.53542132\n",
            "Iteration 82, loss = 1.53423368\n",
            "Iteration 83, loss = 1.53297722\n",
            "Iteration 84, loss = 1.53167542\n",
            "Iteration 85, loss = 1.53044181\n",
            "Iteration 86, loss = 1.52919378\n",
            "Iteration 87, loss = 1.52784080\n",
            "Iteration 88, loss = 1.52653302\n",
            "Iteration 89, loss = 1.52519361\n",
            "Iteration 90, loss = 1.52386120\n",
            "Iteration 91, loss = 1.52262836\n",
            "Iteration 92, loss = 1.52115933\n",
            "Iteration 93, loss = 1.51979627\n",
            "Iteration 94, loss = 1.51839806\n",
            "Iteration 95, loss = 1.51708012\n",
            "Iteration 96, loss = 1.51558350\n",
            "Iteration 97, loss = 1.51420169\n",
            "Iteration 98, loss = 1.51284673\n",
            "Iteration 99, loss = 1.51137062\n",
            "Iteration 100, loss = 1.50987848\n",
            "Iteration 1, loss = 1.61846021\n",
            "Iteration 2, loss = 1.60005362\n",
            "Iteration 3, loss = 1.59655737\n",
            "Iteration 4, loss = 1.58784253\n",
            "Iteration 5, loss = 1.58036825\n",
            "Iteration 6, loss = 1.57299218\n",
            "Iteration 7, loss = 1.56547126\n",
            "Iteration 8, loss = 1.55739196\n",
            "Iteration 9, loss = 1.54943322\n",
            "Iteration 10, loss = 1.54060952\n",
            "Iteration 11, loss = 1.53123177\n",
            "Iteration 12, loss = 1.52119058\n",
            "Iteration 13, loss = 1.51133589\n",
            "Iteration 14, loss = 1.49835888\n",
            "Iteration 15, loss = 1.48610774\n",
            "Iteration 16, loss = 1.47356964\n",
            "Iteration 17, loss = 1.45975861\n",
            "Iteration 18, loss = 1.44488058\n",
            "Iteration 19, loss = 1.42991299\n",
            "Iteration 20, loss = 1.41389322\n",
            "Iteration 21, loss = 1.39752495\n",
            "Iteration 22, loss = 1.38186721\n",
            "Iteration 23, loss = 1.36528792\n",
            "Iteration 24, loss = 1.34896516\n",
            "Iteration 25, loss = 1.33240522\n",
            "Iteration 26, loss = 1.31704257\n",
            "Iteration 27, loss = 1.30239269\n",
            "Iteration 28, loss = 1.28776319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 29, loss = 1.27353159\n",
            "Iteration 30, loss = 1.26106975\n",
            "Iteration 31, loss = 1.24954759\n",
            "Iteration 32, loss = 1.23842348\n",
            "Iteration 33, loss = 1.22748846\n",
            "Iteration 34, loss = 1.21771513\n",
            "Iteration 35, loss = 1.20899637\n",
            "Iteration 36, loss = 1.20124482\n",
            "Iteration 37, loss = 1.19366744\n",
            "Iteration 38, loss = 1.18668644\n",
            "Iteration 39, loss = 1.18028942\n",
            "Iteration 40, loss = 1.17467460\n",
            "Iteration 41, loss = 1.16890263\n",
            "Iteration 42, loss = 1.16450083\n",
            "Iteration 43, loss = 1.15948547\n",
            "Iteration 44, loss = 1.15666971\n",
            "Iteration 45, loss = 1.15210335\n",
            "Iteration 46, loss = 1.14767714\n",
            "Iteration 47, loss = 1.14424182\n",
            "Iteration 48, loss = 1.14120120\n",
            "Iteration 49, loss = 1.13843727\n",
            "Iteration 50, loss = 1.13565605\n",
            "Iteration 51, loss = 1.13285055\n",
            "Iteration 52, loss = 1.13039708\n",
            "Iteration 53, loss = 1.12806957\n",
            "Iteration 54, loss = 1.12690908\n",
            "Iteration 55, loss = 1.12461557\n",
            "Iteration 56, loss = 1.12188956\n",
            "Iteration 57, loss = 1.12109659\n",
            "Iteration 58, loss = 1.11877116\n",
            "Iteration 59, loss = 1.11705410\n",
            "Iteration 60, loss = 1.11465114\n",
            "Iteration 61, loss = 1.11368365\n",
            "Iteration 62, loss = 1.11189224\n",
            "Iteration 63, loss = 1.11096170\n",
            "Iteration 64, loss = 1.10908313\n",
            "Iteration 65, loss = 1.10830262\n",
            "Iteration 66, loss = 1.10577959\n",
            "Iteration 67, loss = 1.10667033\n",
            "Iteration 68, loss = 1.10358621\n",
            "Iteration 69, loss = 1.10168799\n",
            "Iteration 70, loss = 1.10317228\n",
            "Iteration 71, loss = 1.10026231\n",
            "Iteration 72, loss = 1.09869178\n",
            "Iteration 73, loss = 1.09983252\n",
            "Iteration 74, loss = 1.09638065\n",
            "Iteration 75, loss = 1.09466296\n",
            "Iteration 76, loss = 1.09465150\n",
            "Iteration 77, loss = 1.09321451\n",
            "Iteration 78, loss = 1.09317299\n",
            "Iteration 79, loss = 1.09146477\n",
            "Iteration 80, loss = 1.08883851\n",
            "Iteration 81, loss = 1.08811172\n",
            "Iteration 82, loss = 1.08827705\n",
            "Iteration 83, loss = 1.08791030\n",
            "Iteration 84, loss = 1.08543911\n",
            "Iteration 85, loss = 1.08396530\n",
            "Iteration 86, loss = 1.08215833\n",
            "Iteration 87, loss = 1.08079087\n",
            "Iteration 88, loss = 1.07938982\n",
            "Iteration 89, loss = 1.07805165\n",
            "Iteration 90, loss = 1.07725705\n",
            "Iteration 91, loss = 1.07631360\n",
            "Iteration 92, loss = 1.07409430\n",
            "Iteration 93, loss = 1.07314246\n",
            "Iteration 94, loss = 1.07163527\n",
            "Iteration 95, loss = 1.07128072\n",
            "Iteration 96, loss = 1.06852293\n",
            "Iteration 97, loss = 1.06831908\n",
            "Iteration 98, loss = 1.06778646\n",
            "Iteration 99, loss = 1.06456457\n",
            "Iteration 100, loss = 1.06373913\n",
            "Iteration 1, loss = 1.74790226\n",
            "Iteration 2, loss = 1.60965919\n",
            "Iteration 3, loss = 1.57562012\n",
            "Iteration 4, loss = 1.48645989\n",
            "Iteration 5, loss = 1.41340480\n",
            "Iteration 6, loss = 1.31724913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss = 1.23909746\n",
            "Iteration 8, loss = 1.18572682\n",
            "Iteration 9, loss = 1.15288186\n",
            "Iteration 10, loss = 1.13483075\n",
            "Iteration 11, loss = 1.12408181\n",
            "Iteration 12, loss = 1.12087112\n",
            "Iteration 13, loss = 1.12152589\n",
            "Iteration 14, loss = 1.11791711\n",
            "Iteration 15, loss = 1.13100454\n",
            "Iteration 16, loss = 1.12331122\n",
            "Iteration 17, loss = 1.10931810\n",
            "Iteration 18, loss = 1.12716677\n",
            "Iteration 19, loss = 1.11489694\n",
            "Iteration 20, loss = 1.11289731\n",
            "Iteration 21, loss = 1.12105954\n",
            "Iteration 22, loss = 1.11273170\n",
            "Iteration 23, loss = 1.10496621\n",
            "Iteration 24, loss = 1.12336161\n",
            "Iteration 25, loss = 1.10485506\n",
            "Iteration 26, loss = 1.09751736\n",
            "Iteration 27, loss = 1.11069611\n",
            "Iteration 28, loss = 1.07932358\n",
            "Iteration 29, loss = 1.09179273\n",
            "Iteration 30, loss = 1.07927226\n",
            "Iteration 31, loss = 1.06905614\n",
            "Iteration 32, loss = 1.07670491\n",
            "Iteration 33, loss = 1.07213349\n",
            "Iteration 34, loss = 1.05915221\n",
            "Iteration 35, loss = 1.05836707\n",
            "Iteration 36, loss = 1.05378536\n",
            "Iteration 37, loss = 1.05199174\n",
            "Iteration 38, loss = 1.04095737\n",
            "Iteration 39, loss = 1.03945794\n",
            "Iteration 40, loss = 1.03387780\n",
            "Iteration 41, loss = 1.01337704\n",
            "Iteration 42, loss = 1.00092468\n",
            "Iteration 43, loss = 0.99946628\n",
            "Iteration 44, loss = 1.00729783\n",
            "Iteration 45, loss = 1.04134180\n",
            "Iteration 46, loss = 1.06753362\n",
            "Iteration 47, loss = 1.06149824\n",
            "Iteration 48, loss = 1.05255561\n",
            "Iteration 49, loss = 1.03767789\n",
            "Iteration 50, loss = 0.97162331\n",
            "Iteration 51, loss = 0.96014326\n",
            "Iteration 52, loss = 0.97183937\n",
            "Iteration 53, loss = 0.96625621\n",
            "Iteration 54, loss = 1.00218844\n",
            "Iteration 55, loss = 0.98003962\n",
            "Iteration 56, loss = 0.98897065\n",
            "Iteration 57, loss = 0.99153801\n",
            "Iteration 58, loss = 0.95976189\n",
            "Iteration 59, loss = 0.93236363\n",
            "Iteration 60, loss = 0.94780481\n",
            "Iteration 61, loss = 0.95183123\n",
            "Iteration 62, loss = 0.94642496\n",
            "Iteration 63, loss = 0.96229605\n",
            "Iteration 64, loss = 0.96610278\n",
            "Iteration 65, loss = 0.90833814\n",
            "Iteration 66, loss = 0.93196413\n",
            "Iteration 67, loss = 0.92734337\n",
            "Iteration 68, loss = 0.91129489\n",
            "Iteration 69, loss = 0.89943224\n",
            "Iteration 70, loss = 0.88818206\n",
            "Iteration 71, loss = 0.86731194\n",
            "Iteration 72, loss = 0.86115158\n",
            "Iteration 73, loss = 0.88741938\n",
            "Iteration 74, loss = 0.86640320\n",
            "Iteration 75, loss = 0.84939680\n",
            "Iteration 76, loss = 0.84558062\n",
            "Iteration 77, loss = 0.83786214\n",
            "Iteration 78, loss = 0.84753049\n",
            "Iteration 79, loss = 0.83935590\n",
            "Iteration 80, loss = 0.83364883\n",
            "Iteration 81, loss = 0.82430289\n",
            "Iteration 82, loss = 0.84505293\n",
            "Iteration 83, loss = 0.81954488\n",
            "Iteration 84, loss = 0.80723995\n",
            "Iteration 85, loss = 0.81429179\n",
            "Iteration 86, loss = 0.82255090\n",
            "Iteration 87, loss = 0.82340919\n",
            "Iteration 88, loss = 0.87706103\n",
            "Iteration 89, loss = 0.81801176\n",
            "Iteration 90, loss = 0.81098846\n",
            "Iteration 91, loss = 0.81764899\n",
            "Iteration 92, loss = 0.85245557\n",
            "Iteration 93, loss = 0.83475630\n",
            "Iteration 94, loss = 0.80873146\n",
            "Iteration 95, loss = 0.83328824\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.65861728\n",
            "Iteration 2, loss = 1.65473224\n",
            "Iteration 3, loss = 1.65133701\n",
            "Iteration 4, loss = 1.64792299\n",
            "Iteration 5, loss = 1.64481724\n",
            "Iteration 6, loss = 1.64139541\n",
            "Iteration 7, loss = 1.63867273\n",
            "Iteration 8, loss = 1.63583364\n",
            "Iteration 9, loss = 1.63319910\n",
            "Iteration 10, loss = 1.63066322\n",
            "Iteration 11, loss = 1.62834466\n",
            "Iteration 12, loss = 1.62628488\n",
            "Iteration 13, loss = 1.62369704\n",
            "Iteration 14, loss = 1.62208289\n",
            "Iteration 15, loss = 1.62016994\n",
            "Iteration 16, loss = 1.61852493\n",
            "Iteration 17, loss = 1.61666848\n",
            "Iteration 18, loss = 1.61539093\n",
            "Iteration 19, loss = 1.61384073\n",
            "Iteration 20, loss = 1.61249342\n",
            "Iteration 21, loss = 1.61112805\n",
            "Iteration 22, loss = 1.61023294\n",
            "Iteration 23, loss = 1.60896250\n",
            "Iteration 24, loss = 1.60788304\n",
            "Iteration 25, loss = 1.60685349\n",
            "Iteration 26, loss = 1.60590092\n",
            "Iteration 27, loss = 1.60519704\n",
            "Iteration 28, loss = 1.60443867\n",
            "Iteration 29, loss = 1.60354556\n",
            "Iteration 30, loss = 1.60287595\n",
            "Iteration 31, loss = 1.60222293\n",
            "Iteration 32, loss = 1.60143364\n",
            "Iteration 33, loss = 1.60090743\n",
            "Iteration 34, loss = 1.60030647\n",
            "Iteration 35, loss = 1.59971135\n",
            "Iteration 36, loss = 1.59915481\n",
            "Iteration 37, loss = 1.59861921\n",
            "Iteration 38, loss = 1.59811031\n",
            "Iteration 39, loss = 1.59756158\n",
            "Iteration 40, loss = 1.59703447\n",
            "Iteration 41, loss = 1.59662436\n",
            "Iteration 42, loss = 1.59609153\n",
            "Iteration 43, loss = 1.59562540\n",
            "Iteration 44, loss = 1.59527557\n",
            "Iteration 45, loss = 1.59471312\n",
            "Iteration 46, loss = 1.59426442\n",
            "Iteration 47, loss = 1.59381318\n",
            "Iteration 48, loss = 1.59336659\n",
            "Iteration 49, loss = 1.59294817\n",
            "Iteration 50, loss = 1.59248493\n",
            "Iteration 51, loss = 1.59205455\n",
            "Iteration 52, loss = 1.59160174\n",
            "Iteration 53, loss = 1.59119534\n",
            "Iteration 54, loss = 1.59078748\n",
            "Iteration 55, loss = 1.59029480\n",
            "Iteration 56, loss = 1.58986647\n",
            "Iteration 57, loss = 1.58948691\n",
            "Iteration 58, loss = 1.58899868\n",
            "Iteration 59, loss = 1.58856440\n",
            "Iteration 60, loss = 1.58810468\n",
            "Iteration 61, loss = 1.58767638\n",
            "Iteration 62, loss = 1.58722189\n",
            "Iteration 63, loss = 1.58678905\n",
            "Iteration 64, loss = 1.58632114\n",
            "Iteration 65, loss = 1.58587093\n",
            "Iteration 66, loss = 1.58540424\n",
            "Iteration 67, loss = 1.58495098\n",
            "Iteration 68, loss = 1.58448053\n",
            "Iteration 69, loss = 1.58400129\n",
            "Iteration 70, loss = 1.58355427\n",
            "Iteration 71, loss = 1.58304492\n",
            "Iteration 72, loss = 1.58260071\n",
            "Iteration 73, loss = 1.58213772\n",
            "Iteration 74, loss = 1.58160045\n",
            "Iteration 75, loss = 1.58110474\n",
            "Iteration 76, loss = 1.58061981\n",
            "Iteration 77, loss = 1.58011406\n",
            "Iteration 78, loss = 1.57958651\n",
            "Iteration 79, loss = 1.57911444\n",
            "Iteration 80, loss = 1.57856477\n",
            "Iteration 81, loss = 1.57804516\n",
            "Iteration 82, loss = 1.57753807\n",
            "Iteration 83, loss = 1.57699777\n",
            "Iteration 84, loss = 1.57643677\n",
            "Iteration 85, loss = 1.57591582\n",
            "Iteration 86, loss = 1.57537903\n",
            "Iteration 87, loss = 1.57480072\n",
            "Iteration 88, loss = 1.57423681\n",
            "Iteration 89, loss = 1.57366420\n",
            "Iteration 90, loss = 1.57309250\n",
            "Iteration 91, loss = 1.57256536\n",
            "Iteration 92, loss = 1.57193101\n",
            "Iteration 93, loss = 1.57134157\n",
            "Iteration 94, loss = 1.57073640\n",
            "Iteration 95, loss = 1.57016904\n",
            "Iteration 96, loss = 1.56951966\n",
            "Iteration 97, loss = 1.56891646\n",
            "Iteration 98, loss = 1.56834231\n",
            "Iteration 99, loss = 1.56769235\n",
            "Iteration 100, loss = 1.56703324\n",
            "Iteration 1, loss = 1.65237067\n",
            "Iteration 2, loss = 1.62240993\n",
            "Iteration 3, loss = 1.60847667\n",
            "Iteration 4, loss = 1.60137726\n",
            "Iteration 5, loss = 1.59977524\n",
            "Iteration 6, loss = 1.59752957\n",
            "Iteration 7, loss = 1.59392489\n",
            "Iteration 8, loss = 1.58979729\n",
            "Iteration 9, loss = 1.58457096\n",
            "Iteration 10, loss = 1.57998336\n",
            "Iteration 11, loss = 1.57545535\n",
            "Iteration 12, loss = 1.57117388\n",
            "Iteration 13, loss = 1.56737708\n",
            "Iteration 14, loss = 1.56195667\n",
            "Iteration 15, loss = 1.55636975\n",
            "Iteration 16, loss = 1.55056891\n",
            "Iteration 17, loss = 1.54381007\n",
            "Iteration 18, loss = 1.53727143\n",
            "Iteration 19, loss = 1.53013795\n",
            "Iteration 20, loss = 1.52221246\n",
            "Iteration 21, loss = 1.51348680\n",
            "Iteration 22, loss = 1.50481469\n",
            "Iteration 23, loss = 1.49483224\n",
            "Iteration 24, loss = 1.48489420\n",
            "Iteration 25, loss = 1.47383782\n",
            "Iteration 26, loss = 1.46281216\n",
            "Iteration 27, loss = 1.45132025\n",
            "Iteration 28, loss = 1.43949214\n",
            "Iteration 29, loss = 1.42640002\n",
            "Iteration 30, loss = 1.41419037\n",
            "Iteration 31, loss = 1.40166081\n",
            "Iteration 32, loss = 1.38864696\n",
            "Iteration 33, loss = 1.37547501\n",
            "Iteration 34, loss = 1.36252776\n",
            "Iteration 35, loss = 1.34989699\n",
            "Iteration 36, loss = 1.33723848\n",
            "Iteration 37, loss = 1.32501967\n",
            "Iteration 38, loss = 1.31328729\n",
            "Iteration 39, loss = 1.30152603\n",
            "Iteration 40, loss = 1.29069153\n",
            "Iteration 41, loss = 1.28009154\n",
            "Iteration 42, loss = 1.27004236\n",
            "Iteration 43, loss = 1.26055963\n",
            "Iteration 44, loss = 1.25272273\n",
            "Iteration 45, loss = 1.24368759\n",
            "Iteration 46, loss = 1.23549763\n",
            "Iteration 47, loss = 1.22810757\n",
            "Iteration 48, loss = 1.22124711\n",
            "Iteration 49, loss = 1.21482040\n",
            "Iteration 50, loss = 1.20873443\n",
            "Iteration 51, loss = 1.20290044\n",
            "Iteration 52, loss = 1.19768332\n",
            "Iteration 53, loss = 1.19278165\n",
            "Iteration 54, loss = 1.18840020\n",
            "Iteration 55, loss = 1.18425854\n",
            "Iteration 56, loss = 1.17965848\n",
            "Iteration 57, loss = 1.17650236\n",
            "Iteration 58, loss = 1.17234166\n",
            "Iteration 59, loss = 1.16920908\n",
            "Iteration 60, loss = 1.16548347\n",
            "Iteration 61, loss = 1.16271810\n",
            "Iteration 62, loss = 1.15997162\n",
            "Iteration 63, loss = 1.15761274\n",
            "Iteration 64, loss = 1.15475968\n",
            "Iteration 65, loss = 1.15239551\n",
            "Iteration 66, loss = 1.14971756\n",
            "Iteration 67, loss = 1.14922974\n",
            "Iteration 68, loss = 1.14609788\n",
            "Iteration 69, loss = 1.14335759\n",
            "Iteration 70, loss = 1.14306926\n",
            "Iteration 71, loss = 1.14085841\n",
            "Iteration 72, loss = 1.13865402\n",
            "Iteration 73, loss = 1.13817452\n",
            "Iteration 74, loss = 1.13594314\n",
            "Iteration 75, loss = 1.13361917\n",
            "Iteration 76, loss = 1.13211922\n",
            "Iteration 77, loss = 1.13142856\n",
            "Iteration 78, loss = 1.13042856\n",
            "Iteration 79, loss = 1.12890720\n",
            "Iteration 80, loss = 1.12739602\n",
            "Iteration 81, loss = 1.12569944\n",
            "Iteration 82, loss = 1.12504843\n",
            "Iteration 83, loss = 1.12447891\n",
            "Iteration 84, loss = 1.12290445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 1.12167001\n",
            "Iteration 86, loss = 1.12030243\n",
            "Iteration 87, loss = 1.11930139\n",
            "Iteration 88, loss = 1.11854577\n",
            "Iteration 89, loss = 1.11731073\n",
            "Iteration 90, loss = 1.11682190\n",
            "Iteration 91, loss = 1.11597090\n",
            "Iteration 92, loss = 1.11457930\n",
            "Iteration 93, loss = 1.11407025\n",
            "Iteration 94, loss = 1.11298988\n",
            "Iteration 95, loss = 1.11264787\n",
            "Iteration 96, loss = 1.11118874\n",
            "Iteration 97, loss = 1.11122394\n",
            "Iteration 98, loss = 1.11084753\n",
            "Iteration 99, loss = 1.10896797\n",
            "Iteration 100, loss = 1.10833700\n",
            "Iteration 1, loss = 1.65843068\n",
            "Iteration 2, loss = 1.60183820\n",
            "Iteration 3, loss = 1.57269616\n",
            "Iteration 4, loss = 1.54124520\n",
            "Iteration 5, loss = 1.50499304\n",
            "Iteration 6, loss = 1.43508563\n",
            "Iteration 7, loss = 1.36297435\n",
            "Iteration 8, loss = 1.29266877\n",
            "Iteration 9, loss = 1.23041506\n",
            "Iteration 10, loss = 1.18792649\n",
            "Iteration 11, loss = 1.15585385\n",
            "Iteration 12, loss = 1.14055433\n",
            "Iteration 13, loss = 1.13015994\n",
            "Iteration 14, loss = 1.12285919\n",
            "Iteration 15, loss = 1.12315393\n",
            "Iteration 16, loss = 1.10975332\n",
            "Iteration 17, loss = 1.11501210\n",
            "Iteration 18, loss = 1.10901856\n",
            "Iteration 19, loss = 1.10875770\n",
            "Iteration 20, loss = 1.10459473\n",
            "Iteration 21, loss = 1.10599991\n",
            "Iteration 22, loss = 1.10914979\n",
            "Iteration 23, loss = 1.11464597\n",
            "Iteration 24, loss = 1.10492478\n",
            "Iteration 25, loss = 1.10196650\n",
            "Iteration 26, loss = 1.11306045\n",
            "Iteration 27, loss = 1.09649643\n",
            "Iteration 28, loss = 1.10008643\n",
            "Iteration 29, loss = 1.10333002\n",
            "Iteration 30, loss = 1.09562132\n",
            "Iteration 31, loss = 1.09632877\n",
            "Iteration 32, loss = 1.09621404\n",
            "Iteration 33, loss = 1.08887292\n",
            "Iteration 34, loss = 1.08375826\n",
            "Iteration 35, loss = 1.07850330\n",
            "Iteration 36, loss = 1.06959531\n",
            "Iteration 37, loss = 1.07478778\n",
            "Iteration 38, loss = 1.06632390\n",
            "Iteration 39, loss = 1.07083475\n",
            "Iteration 40, loss = 1.04936216\n",
            "Iteration 41, loss = 1.05051159\n",
            "Iteration 42, loss = 1.04644583\n",
            "Iteration 43, loss = 1.02875523\n",
            "Iteration 44, loss = 1.06415997\n",
            "Iteration 45, loss = 1.07748407\n",
            "Iteration 46, loss = 1.03710387\n",
            "Iteration 47, loss = 1.05378573\n",
            "Iteration 48, loss = 1.04442694\n",
            "Iteration 49, loss = 1.01417840\n",
            "Iteration 50, loss = 1.03468524\n",
            "Iteration 51, loss = 1.00820690\n",
            "Iteration 52, loss = 0.99657521\n",
            "Iteration 53, loss = 0.99609175\n",
            "Iteration 54, loss = 1.01323137\n",
            "Iteration 55, loss = 0.99470695\n",
            "Iteration 56, loss = 0.99903804\n",
            "Iteration 57, loss = 0.99780802\n",
            "Iteration 58, loss = 0.97407812\n",
            "Iteration 59, loss = 0.96673830\n",
            "Iteration 60, loss = 0.96081559\n",
            "Iteration 61, loss = 0.96331294\n",
            "Iteration 62, loss = 0.95876210\n",
            "Iteration 63, loss = 0.96056202\n",
            "Iteration 64, loss = 0.97296176\n",
            "Iteration 65, loss = 0.92918468\n",
            "Iteration 66, loss = 0.93532444\n",
            "Iteration 67, loss = 0.93615500\n",
            "Iteration 68, loss = 0.91416185\n",
            "Iteration 69, loss = 0.91421976\n",
            "Iteration 70, loss = 0.90847739\n",
            "Iteration 71, loss = 0.90069715\n",
            "Iteration 72, loss = 0.89093335\n",
            "Iteration 73, loss = 0.91489693\n",
            "Iteration 74, loss = 0.91036159\n",
            "Iteration 75, loss = 0.88611100\n",
            "Iteration 76, loss = 0.87578856\n",
            "Iteration 77, loss = 0.86872971\n",
            "Iteration 78, loss = 0.87697302\n",
            "Iteration 79, loss = 0.85999098\n",
            "Iteration 80, loss = 0.85780643\n",
            "Iteration 81, loss = 0.85215546\n",
            "Iteration 82, loss = 0.85476627\n",
            "Iteration 83, loss = 0.84714121\n",
            "Iteration 84, loss = 0.84411317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 0.86095268\n",
            "Iteration 86, loss = 0.85951807\n",
            "Iteration 87, loss = 0.83977441\n",
            "Iteration 88, loss = 0.83671669\n",
            "Iteration 89, loss = 0.81857603\n",
            "Iteration 90, loss = 0.81936269\n",
            "Iteration 91, loss = 0.82493643\n",
            "Iteration 92, loss = 0.83296079\n",
            "Iteration 93, loss = 0.83677910\n",
            "Iteration 94, loss = 0.82400318\n",
            "Iteration 95, loss = 0.80904934\n",
            "Iteration 96, loss = 0.80071844\n",
            "Iteration 97, loss = 0.80638632\n",
            "Iteration 98, loss = 0.80545783\n",
            "Iteration 99, loss = 0.81017061\n",
            "Iteration 100, loss = 0.82459505\n",
            "Iteration 1, loss = 1.69844894\n",
            "Iteration 2, loss = 1.68975757\n",
            "Iteration 3, loss = 1.68216514\n",
            "Iteration 4, loss = 1.67465188\n",
            "Iteration 5, loss = 1.66779315\n",
            "Iteration 6, loss = 1.66123902\n",
            "Iteration 7, loss = 1.65526508\n",
            "Iteration 8, loss = 1.64888649\n",
            "Iteration 9, loss = 1.64419082\n",
            "Iteration 10, loss = 1.63911170\n",
            "Iteration 11, loss = 1.63440022\n",
            "Iteration 12, loss = 1.63008127\n",
            "Iteration 13, loss = 1.62617530\n",
            "Iteration 14, loss = 1.62271637\n",
            "Iteration 15, loss = 1.61937113\n",
            "Iteration 16, loss = 1.61591111\n",
            "Iteration 17, loss = 1.61314901\n",
            "Iteration 18, loss = 1.61087680\n",
            "Iteration 19, loss = 1.60809465\n",
            "Iteration 20, loss = 1.60589330\n",
            "Iteration 21, loss = 1.60394609\n",
            "Iteration 22, loss = 1.60188064\n",
            "Iteration 23, loss = 1.60022164\n",
            "Iteration 24, loss = 1.59852499\n",
            "Iteration 25, loss = 1.59699851\n",
            "Iteration 26, loss = 1.59557826\n",
            "Iteration 27, loss = 1.59434462\n",
            "Iteration 28, loss = 1.59298784\n",
            "Iteration 29, loss = 1.59194255\n",
            "Iteration 30, loss = 1.59056241\n",
            "Iteration 31, loss = 1.58965830\n",
            "Iteration 32, loss = 1.58861091\n",
            "Iteration 33, loss = 1.58770353\n",
            "Iteration 34, loss = 1.58676823\n",
            "Iteration 35, loss = 1.58583425\n",
            "Iteration 36, loss = 1.58490689\n",
            "Iteration 37, loss = 1.58418264\n",
            "Iteration 38, loss = 1.58326067\n",
            "Iteration 39, loss = 1.58235486\n",
            "Iteration 40, loss = 1.58160666\n",
            "Iteration 41, loss = 1.58083164\n",
            "Iteration 42, loss = 1.57991579\n",
            "Iteration 43, loss = 1.57917551\n",
            "Iteration 44, loss = 1.57838567\n",
            "Iteration 45, loss = 1.57755471\n",
            "Iteration 46, loss = 1.57680359\n",
            "Iteration 47, loss = 1.57591608\n",
            "Iteration 48, loss = 1.57515291\n",
            "Iteration 49, loss = 1.57431287\n",
            "Iteration 50, loss = 1.57357016\n",
            "Iteration 51, loss = 1.57269544\n",
            "Iteration 52, loss = 1.57185254\n",
            "Iteration 53, loss = 1.57112208\n",
            "Iteration 54, loss = 1.57022472\n",
            "Iteration 55, loss = 1.56938044\n",
            "Iteration 56, loss = 1.56853179\n",
            "Iteration 57, loss = 1.56764972\n",
            "Iteration 58, loss = 1.56682210\n",
            "Iteration 59, loss = 1.56598716\n",
            "Iteration 60, loss = 1.56503688\n",
            "Iteration 61, loss = 1.56417391\n",
            "Iteration 62, loss = 1.56327640\n",
            "Iteration 63, loss = 1.56244392\n",
            "Iteration 64, loss = 1.56149875\n",
            "Iteration 65, loss = 1.56053216\n",
            "Iteration 66, loss = 1.55964237\n",
            "Iteration 67, loss = 1.55867272\n",
            "Iteration 68, loss = 1.55776702\n",
            "Iteration 69, loss = 1.55685143\n",
            "Iteration 70, loss = 1.55585421\n",
            "Iteration 71, loss = 1.55487716\n",
            "Iteration 72, loss = 1.55390411\n",
            "Iteration 73, loss = 1.55297354\n",
            "Iteration 74, loss = 1.55192420\n",
            "Iteration 75, loss = 1.55091449\n",
            "Iteration 76, loss = 1.54997651\n",
            "Iteration 77, loss = 1.54887014\n",
            "Iteration 78, loss = 1.54784277\n",
            "Iteration 79, loss = 1.54679038\n",
            "Iteration 80, loss = 1.54581684\n",
            "Iteration 81, loss = 1.54468303\n",
            "Iteration 82, loss = 1.54364895\n",
            "Iteration 83, loss = 1.54252946\n",
            "Iteration 84, loss = 1.54144009\n",
            "Iteration 85, loss = 1.54035443\n",
            "Iteration 86, loss = 1.53925979\n",
            "Iteration 87, loss = 1.53815192\n",
            "Iteration 88, loss = 1.53696650\n",
            "Iteration 89, loss = 1.53584978\n",
            "Iteration 90, loss = 1.53479408\n",
            "Iteration 91, loss = 1.53352428\n",
            "Iteration 92, loss = 1.53233361\n",
            "Iteration 93, loss = 1.53116123\n",
            "Iteration 94, loss = 1.52997936\n",
            "Iteration 95, loss = 1.52881221\n",
            "Iteration 96, loss = 1.52754387\n",
            "Iteration 97, loss = 1.52633157\n",
            "Iteration 98, loss = 1.52514690\n",
            "Iteration 99, loss = 1.52384125\n",
            "Iteration 100, loss = 1.52253345\n",
            "Iteration 1, loss = 1.68330273\n",
            "Iteration 2, loss = 1.62373529\n",
            "Iteration 3, loss = 1.60038496\n",
            "Iteration 4, loss = 1.59073904\n",
            "Iteration 5, loss = 1.58899857\n",
            "Iteration 6, loss = 1.58539119\n",
            "Iteration 7, loss = 1.57935868\n",
            "Iteration 8, loss = 1.57118987\n",
            "Iteration 9, loss = 1.56070719\n",
            "Iteration 10, loss = 1.55060288\n",
            "Iteration 11, loss = 1.54122584\n",
            "Iteration 12, loss = 1.53172554\n",
            "Iteration 13, loss = 1.52304620\n",
            "Iteration 14, loss = 1.51237659\n",
            "Iteration 15, loss = 1.50204045\n",
            "Iteration 16, loss = 1.49038139\n",
            "Iteration 17, loss = 1.47837119\n",
            "Iteration 18, loss = 1.46560152\n",
            "Iteration 19, loss = 1.45005517\n",
            "Iteration 20, loss = 1.43589322\n",
            "Iteration 21, loss = 1.42227444\n",
            "Iteration 22, loss = 1.40656110\n",
            "Iteration 23, loss = 1.39172926\n",
            "Iteration 24, loss = 1.37637629\n",
            "Iteration 25, loss = 1.36084855\n",
            "Iteration 26, loss = 1.34612027\n",
            "Iteration 27, loss = 1.33106727\n",
            "Iteration 28, loss = 1.31713320\n",
            "Iteration 29, loss = 1.30329455\n",
            "Iteration 30, loss = 1.29044534\n",
            "Iteration 31, loss = 1.27738879\n",
            "Iteration 32, loss = 1.26572076\n",
            "Iteration 33, loss = 1.25472718\n",
            "Iteration 34, loss = 1.24442002\n",
            "Iteration 35, loss = 1.23467622\n",
            "Iteration 36, loss = 1.22558994\n",
            "Iteration 37, loss = 1.21797304\n",
            "Iteration 38, loss = 1.20979741\n",
            "Iteration 39, loss = 1.20225676\n",
            "Iteration 40, loss = 1.19584388\n",
            "Iteration 41, loss = 1.18985013\n",
            "Iteration 42, loss = 1.18415741\n",
            "Iteration 43, loss = 1.17911216\n",
            "Iteration 44, loss = 1.17378884\n",
            "Iteration 45, loss = 1.17036135\n",
            "Iteration 46, loss = 1.16526892\n",
            "Iteration 47, loss = 1.16118583\n",
            "Iteration 48, loss = 1.15709759\n",
            "Iteration 49, loss = 1.15457109\n",
            "Iteration 50, loss = 1.15122758\n",
            "Iteration 51, loss = 1.14744079\n",
            "Iteration 52, loss = 1.14458371\n",
            "Iteration 53, loss = 1.14268684\n",
            "Iteration 54, loss = 1.13943986\n",
            "Iteration 55, loss = 1.13700128\n",
            "Iteration 56, loss = 1.13502828\n",
            "Iteration 57, loss = 1.13238781\n",
            "Iteration 58, loss = 1.13062935\n",
            "Iteration 59, loss = 1.12842527\n",
            "Iteration 60, loss = 1.12601241\n",
            "Iteration 61, loss = 1.12472048\n",
            "Iteration 62, loss = 1.12270695\n",
            "Iteration 63, loss = 1.12239375\n",
            "Iteration 64, loss = 1.11970490\n",
            "Iteration 65, loss = 1.11799690\n",
            "Iteration 66, loss = 1.11626291\n",
            "Iteration 67, loss = 1.11574440\n",
            "Iteration 68, loss = 1.11573616\n",
            "Iteration 69, loss = 1.11614692\n",
            "Iteration 70, loss = 1.11123752\n",
            "Iteration 71, loss = 1.10935349\n",
            "Iteration 72, loss = 1.10787185\n",
            "Iteration 73, loss = 1.10828235\n",
            "Iteration 74, loss = 1.10638118\n",
            "Iteration 75, loss = 1.10372128\n",
            "Iteration 76, loss = 1.10414201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 77, loss = 1.10181338\n",
            "Iteration 78, loss = 1.10006968\n",
            "Iteration 79, loss = 1.10043315\n",
            "Iteration 80, loss = 1.09839623\n",
            "Iteration 81, loss = 1.09821914\n",
            "Iteration 82, loss = 1.09670176\n",
            "Iteration 83, loss = 1.09471563\n",
            "Iteration 84, loss = 1.09330461\n",
            "Iteration 85, loss = 1.09249436\n",
            "Iteration 86, loss = 1.09085882\n",
            "Iteration 87, loss = 1.09306226\n",
            "Iteration 88, loss = 1.09023777\n",
            "Iteration 89, loss = 1.08811621\n",
            "Iteration 90, loss = 1.08669439\n",
            "Iteration 91, loss = 1.08673993\n",
            "Iteration 92, loss = 1.08405915\n",
            "Iteration 93, loss = 1.08249072\n",
            "Iteration 94, loss = 1.08293389\n",
            "Iteration 95, loss = 1.08206438\n",
            "Iteration 96, loss = 1.07994818\n",
            "Iteration 97, loss = 1.07850014\n",
            "Iteration 98, loss = 1.07749158\n",
            "Iteration 99, loss = 1.07561943\n",
            "Iteration 100, loss = 1.07415663\n",
            "Iteration 1, loss = 1.68833171\n",
            "Iteration 2, loss = 1.62871140\n",
            "Iteration 3, loss = 1.56527942\n",
            "Iteration 4, loss = 1.50103345\n",
            "Iteration 5, loss = 1.43745508\n",
            "Iteration 6, loss = 1.36702073\n",
            "Iteration 7, loss = 1.30803082\n",
            "Iteration 8, loss = 1.25022615\n",
            "Iteration 9, loss = 1.20403907\n",
            "Iteration 10, loss = 1.16891169\n",
            "Iteration 11, loss = 1.14884510\n",
            "Iteration 12, loss = 1.13769767\n",
            "Iteration 13, loss = 1.12749966\n",
            "Iteration 14, loss = 1.11970919\n",
            "Iteration 15, loss = 1.12175220\n",
            "Iteration 16, loss = 1.11582800\n",
            "Iteration 17, loss = 1.11125160\n",
            "Iteration 18, loss = 1.11776638\n",
            "Iteration 19, loss = 1.11238840\n",
            "Iteration 20, loss = 1.12709282\n",
            "Iteration 21, loss = 1.10360716\n",
            "Iteration 22, loss = 1.10465547\n",
            "Iteration 23, loss = 1.11178925\n",
            "Iteration 24, loss = 1.09530087\n",
            "Iteration 25, loss = 1.09159505\n",
            "Iteration 26, loss = 1.09797120\n",
            "Iteration 27, loss = 1.08663726\n",
            "Iteration 28, loss = 1.08082323\n",
            "Iteration 29, loss = 1.09282105\n",
            "Iteration 30, loss = 1.08188192\n",
            "Iteration 31, loss = 1.07380651\n",
            "Iteration 32, loss = 1.08045971\n",
            "Iteration 33, loss = 1.08019996\n",
            "Iteration 34, loss = 1.06148278\n",
            "Iteration 35, loss = 1.05236740\n",
            "Iteration 36, loss = 1.04841129\n",
            "Iteration 37, loss = 1.06632843\n",
            "Iteration 38, loss = 1.04534380\n",
            "Iteration 39, loss = 1.02651677\n",
            "Iteration 40, loss = 1.01499408\n",
            "Iteration 41, loss = 1.01041200\n",
            "Iteration 42, loss = 1.01226200\n",
            "Iteration 43, loss = 1.01748840\n",
            "Iteration 44, loss = 0.97970032\n",
            "Iteration 45, loss = 0.99074565\n",
            "Iteration 46, loss = 0.97696744\n",
            "Iteration 47, loss = 1.00296210\n",
            "Iteration 48, loss = 0.95820406\n",
            "Iteration 49, loss = 0.94230761\n",
            "Iteration 50, loss = 0.98132706\n",
            "Iteration 51, loss = 0.96341567\n",
            "Iteration 52, loss = 0.98958625\n",
            "Iteration 53, loss = 0.93220475\n",
            "Iteration 54, loss = 0.97515359\n",
            "Iteration 55, loss = 0.96576033\n",
            "Iteration 56, loss = 0.93189633\n",
            "Iteration 57, loss = 0.91313912\n",
            "Iteration 58, loss = 0.91535637\n",
            "Iteration 59, loss = 0.91942870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 60, loss = 0.91221732\n",
            "Iteration 61, loss = 0.92066756\n",
            "Iteration 62, loss = 0.90231036\n",
            "Iteration 63, loss = 0.95164879\n",
            "Iteration 64, loss = 0.85979735\n",
            "Iteration 65, loss = 0.91078368\n",
            "Iteration 66, loss = 0.86415218\n",
            "Iteration 67, loss = 0.89089158\n",
            "Iteration 68, loss = 0.86611206\n",
            "Iteration 69, loss = 0.88298235\n",
            "Iteration 70, loss = 0.84884956\n",
            "Iteration 71, loss = 0.86760709\n",
            "Iteration 72, loss = 0.86585376\n",
            "Iteration 73, loss = 0.84895006\n",
            "Iteration 74, loss = 0.87645170\n",
            "Iteration 75, loss = 0.83757853\n",
            "Iteration 76, loss = 0.83096733\n",
            "Iteration 77, loss = 0.81651644\n",
            "Iteration 78, loss = 0.81062523\n",
            "Iteration 79, loss = 0.81369248\n",
            "Iteration 80, loss = 0.80367131\n",
            "Iteration 81, loss = 0.81584510\n",
            "Iteration 82, loss = 0.80374000\n",
            "Iteration 83, loss = 0.80669874\n",
            "Iteration 84, loss = 0.80855834\n",
            "Iteration 85, loss = 0.78906381\n",
            "Iteration 86, loss = 0.79660336\n",
            "Iteration 87, loss = 0.85099292\n",
            "Iteration 88, loss = 1.03595043\n",
            "Iteration 89, loss = 0.94499291\n",
            "Iteration 90, loss = 0.98534409\n",
            "Iteration 91, loss = 0.87615501\n",
            "Iteration 92, loss = 0.88643281\n",
            "Iteration 93, loss = 0.88066804\n",
            "Iteration 94, loss = 0.92028157\n",
            "Iteration 95, loss = 0.84562192\n",
            "Iteration 96, loss = 0.84994306\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.71244694\n",
            "Iteration 2, loss = 1.70201244\n",
            "Iteration 3, loss = 1.69332473\n",
            "Iteration 4, loss = 1.68444962\n",
            "Iteration 5, loss = 1.67644650\n",
            "Iteration 6, loss = 1.66905915\n",
            "Iteration 7, loss = 1.66201466\n",
            "Iteration 8, loss = 1.65486991\n",
            "Iteration 9, loss = 1.64965052\n",
            "Iteration 10, loss = 1.64404118\n",
            "Iteration 11, loss = 1.63913446\n",
            "Iteration 12, loss = 1.63439133\n",
            "Iteration 13, loss = 1.62998023\n",
            "Iteration 14, loss = 1.62654335\n",
            "Iteration 15, loss = 1.62314458\n",
            "Iteration 16, loss = 1.61969125\n",
            "Iteration 17, loss = 1.61685501\n",
            "Iteration 18, loss = 1.61492687\n",
            "Iteration 19, loss = 1.61216393\n",
            "Iteration 20, loss = 1.61031699\n",
            "Iteration 21, loss = 1.60870500\n",
            "Iteration 22, loss = 1.60690520\n",
            "Iteration 23, loss = 1.60564429\n",
            "Iteration 24, loss = 1.60436891\n",
            "Iteration 25, loss = 1.60332394\n",
            "Iteration 26, loss = 1.60246156\n",
            "Iteration 27, loss = 1.60173016\n",
            "Iteration 28, loss = 1.60089270\n",
            "Iteration 29, loss = 1.60043270\n",
            "Iteration 30, loss = 1.59972661\n",
            "Iteration 31, loss = 1.59940362\n",
            "Iteration 32, loss = 1.59899459\n",
            "Iteration 33, loss = 1.59873148\n",
            "Iteration 34, loss = 1.59846591\n",
            "Iteration 35, loss = 1.59823142\n",
            "Iteration 36, loss = 1.59796227\n",
            "Iteration 37, loss = 1.59788445\n",
            "Iteration 38, loss = 1.59771039\n",
            "Iteration 39, loss = 1.59748674\n",
            "Iteration 40, loss = 1.59736999\n",
            "Iteration 41, loss = 1.59733107\n",
            "Iteration 42, loss = 1.59716109\n",
            "Iteration 43, loss = 1.59705038\n",
            "Iteration 44, loss = 1.59697288\n",
            "Iteration 45, loss = 1.59688848\n",
            "Iteration 46, loss = 1.59681763\n",
            "Iteration 47, loss = 1.59672113\n",
            "Iteration 48, loss = 1.59662231\n",
            "Iteration 49, loss = 1.59653482\n",
            "Iteration 50, loss = 1.59650517\n",
            "Iteration 51, loss = 1.59639472\n",
            "Iteration 52, loss = 1.59632166\n",
            "Iteration 53, loss = 1.59635341\n",
            "Iteration 54, loss = 1.59621612\n",
            "Iteration 55, loss = 1.59611514\n",
            "Iteration 56, loss = 1.59604303\n",
            "Iteration 57, loss = 1.59595948\n",
            "Iteration 58, loss = 1.59590733\n",
            "Iteration 59, loss = 1.59587101\n",
            "Iteration 60, loss = 1.59574054\n",
            "Iteration 61, loss = 1.59568553\n",
            "Iteration 62, loss = 1.59561113\n",
            "Iteration 63, loss = 1.59560278\n",
            "Iteration 64, loss = 1.59549103\n",
            "Iteration 65, loss = 1.59537804\n",
            "Iteration 66, loss = 1.59530792\n",
            "Iteration 67, loss = 1.59521447\n",
            "Iteration 68, loss = 1.59518208\n",
            "Iteration 69, loss = 1.59521080\n",
            "Iteration 70, loss = 1.59504984\n",
            "Iteration 71, loss = 1.59493221\n",
            "Iteration 72, loss = 1.59485036\n",
            "Iteration 73, loss = 1.59482659\n",
            "Iteration 74, loss = 1.59467701\n",
            "Iteration 75, loss = 1.59458952\n",
            "Iteration 76, loss = 1.59458702\n",
            "Iteration 77, loss = 1.59444028\n",
            "Iteration 78, loss = 1.59435559\n",
            "Iteration 79, loss = 1.59427304\n",
            "Iteration 80, loss = 1.59424369\n",
            "Iteration 81, loss = 1.59412930\n",
            "Iteration 82, loss = 1.59404360\n",
            "Iteration 83, loss = 1.59394302\n",
            "Iteration 84, loss = 1.59386039\n",
            "Iteration 85, loss = 1.59378299\n",
            "Iteration 86, loss = 1.59371837\n",
            "Iteration 87, loss = 1.59368766\n",
            "Iteration 88, loss = 1.59352622\n",
            "Iteration 89, loss = 1.59349694\n",
            "Iteration 90, loss = 1.59347105\n",
            "Iteration 91, loss = 1.59327681\n",
            "Iteration 92, loss = 1.59317281\n",
            "Iteration 93, loss = 1.59309561\n",
            "Iteration 94, loss = 1.59302841\n",
            "Iteration 95, loss = 1.59296147\n",
            "Iteration 96, loss = 1.59284324\n",
            "Iteration 97, loss = 1.59278503\n",
            "Iteration 98, loss = 1.59269515\n",
            "Iteration 99, loss = 1.59258068\n",
            "Iteration 100, loss = 1.59245594\n",
            "Iteration 1, loss = 1.69449269\n",
            "Iteration 2, loss = 1.62768515\n",
            "Iteration 3, loss = 1.60620046\n",
            "Iteration 4, loss = 1.59969414\n",
            "Iteration 5, loss = 1.60672595\n",
            "Iteration 6, loss = 1.61078872\n",
            "Iteration 7, loss = 1.61199395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 1.60762001\n",
            "Iteration 9, loss = 1.60165631\n",
            "Iteration 10, loss = 1.59674087\n",
            "Iteration 11, loss = 1.59405703\n",
            "Iteration 12, loss = 1.59297117\n",
            "Iteration 13, loss = 1.59370192\n",
            "Iteration 14, loss = 1.59331430\n",
            "Iteration 15, loss = 1.59322486\n",
            "Iteration 16, loss = 1.59165640\n",
            "Iteration 17, loss = 1.58982402\n",
            "Iteration 18, loss = 1.58916427\n",
            "Iteration 19, loss = 1.58666730\n",
            "Iteration 20, loss = 1.58595136\n",
            "Iteration 21, loss = 1.58525339\n",
            "Iteration 22, loss = 1.58342268\n",
            "Iteration 23, loss = 1.58201615\n",
            "Iteration 24, loss = 1.58031458\n",
            "Iteration 25, loss = 1.57896876\n",
            "Iteration 26, loss = 1.57684839\n",
            "Iteration 27, loss = 1.57493007\n",
            "Iteration 28, loss = 1.57330280\n",
            "Iteration 29, loss = 1.57152254\n",
            "Iteration 30, loss = 1.56929076\n",
            "Iteration 31, loss = 1.56649283\n",
            "Iteration 32, loss = 1.56387833\n",
            "Iteration 33, loss = 1.56129296\n",
            "Iteration 34, loss = 1.55834016\n",
            "Iteration 35, loss = 1.55561641\n",
            "Iteration 36, loss = 1.55248200\n",
            "Iteration 37, loss = 1.54910448\n",
            "Iteration 38, loss = 1.54482069\n",
            "Iteration 39, loss = 1.54086890\n",
            "Iteration 40, loss = 1.53706924\n",
            "Iteration 41, loss = 1.53309519\n",
            "Iteration 42, loss = 1.52860778\n",
            "Iteration 43, loss = 1.52377043\n",
            "Iteration 44, loss = 1.51883445\n",
            "Iteration 45, loss = 1.51384037\n",
            "Iteration 46, loss = 1.50826349\n",
            "Iteration 47, loss = 1.50220808\n",
            "Iteration 48, loss = 1.49642229\n",
            "Iteration 49, loss = 1.49033019\n",
            "Iteration 50, loss = 1.48428774\n",
            "Iteration 51, loss = 1.47736201\n",
            "Iteration 52, loss = 1.47064822\n",
            "Iteration 53, loss = 1.46445477\n",
            "Iteration 54, loss = 1.45692404\n",
            "Iteration 55, loss = 1.44952570\n",
            "Iteration 56, loss = 1.44213128\n",
            "Iteration 57, loss = 1.43442220\n",
            "Iteration 58, loss = 1.42720382\n",
            "Iteration 59, loss = 1.41970158\n",
            "Iteration 60, loss = 1.41121657\n",
            "Iteration 61, loss = 1.40364831\n",
            "Iteration 62, loss = 1.39572139\n",
            "Iteration 63, loss = 1.38888144\n",
            "Iteration 64, loss = 1.38068911\n",
            "Iteration 65, loss = 1.37238605\n",
            "Iteration 66, loss = 1.36512749\n",
            "Iteration 67, loss = 1.35721523\n",
            "Iteration 68, loss = 1.35064759\n",
            "Iteration 69, loss = 1.34368815\n",
            "Iteration 70, loss = 1.33577171\n",
            "Iteration 71, loss = 1.32876468\n",
            "Iteration 72, loss = 1.32238743\n",
            "Iteration 73, loss = 1.31662481\n",
            "Iteration 74, loss = 1.30951235\n",
            "Iteration 75, loss = 1.30312526\n",
            "Iteration 76, loss = 1.29803608\n",
            "Iteration 77, loss = 1.29162442\n",
            "Iteration 78, loss = 1.28587959\n",
            "Iteration 79, loss = 1.28039946\n",
            "Iteration 80, loss = 1.27579881\n",
            "Iteration 81, loss = 1.27026685\n",
            "Iteration 82, loss = 1.26539015\n",
            "Iteration 83, loss = 1.26044365\n",
            "Iteration 84, loss = 1.25600000\n",
            "Iteration 85, loss = 1.25152574\n",
            "Iteration 86, loss = 1.24746317\n",
            "Iteration 87, loss = 1.24436882\n",
            "Iteration 88, loss = 1.23982359\n",
            "Iteration 89, loss = 1.23603487\n",
            "Iteration 90, loss = 1.23285780\n",
            "Iteration 91, loss = 1.22866414\n",
            "Iteration 92, loss = 1.22528036\n",
            "Iteration 93, loss = 1.22216745\n",
            "Iteration 94, loss = 1.21947713\n",
            "Iteration 95, loss = 1.21683037\n",
            "Iteration 96, loss = 1.21329959\n",
            "Iteration 97, loss = 1.21058336\n",
            "Iteration 98, loss = 1.20797622\n",
            "Iteration 99, loss = 1.20541638\n",
            "Iteration 100, loss = 1.20248340\n",
            "Iteration 1, loss = 1.73022561\n",
            "Iteration 2, loss = 1.80029311\n",
            "Iteration 3, loss = 1.62522239\n",
            "Iteration 4, loss = 1.63007859\n",
            "Iteration 5, loss = 1.63831159\n",
            "Iteration 6, loss = 1.60867677\n",
            "Iteration 7, loss = 1.59204930\n",
            "Iteration 8, loss = 1.58165538\n",
            "Iteration 9, loss = 1.57939019\n",
            "Iteration 10, loss = 1.56289542\n",
            "Iteration 11, loss = 1.54135879\n",
            "Iteration 12, loss = 1.51748954\n",
            "Iteration 13, loss = 1.49550185\n",
            "Iteration 14, loss = 1.46560320\n",
            "Iteration 15, loss = 1.42687978\n",
            "Iteration 16, loss = 1.37940939\n",
            "Iteration 17, loss = 1.33520478\n",
            "Iteration 18, loss = 1.29873366\n",
            "Iteration 19, loss = 1.25877632\n",
            "Iteration 20, loss = 1.22487569\n",
            "Iteration 21, loss = 1.20241509\n",
            "Iteration 22, loss = 1.18457019\n",
            "Iteration 23, loss = 1.16638626\n",
            "Iteration 24, loss = 1.15629947\n",
            "Iteration 25, loss = 1.14749962\n",
            "Iteration 26, loss = 1.14367656\n",
            "Iteration 27, loss = 1.13766398\n",
            "Iteration 28, loss = 1.13936171\n",
            "Iteration 29, loss = 1.13036930\n",
            "Iteration 30, loss = 1.13151928\n",
            "Iteration 31, loss = 1.12468632\n",
            "Iteration 32, loss = 1.12683867\n",
            "Iteration 33, loss = 1.12010893\n",
            "Iteration 34, loss = 1.11886584\n",
            "Iteration 35, loss = 1.11918864\n",
            "Iteration 36, loss = 1.11808128\n",
            "Iteration 37, loss = 1.11596014\n",
            "Iteration 38, loss = 1.11334056\n",
            "Iteration 39, loss = 1.10824238\n",
            "Iteration 40, loss = 1.11006173\n",
            "Iteration 41, loss = 1.11336922\n",
            "Iteration 42, loss = 1.11396895\n",
            "Iteration 43, loss = 1.11096096\n",
            "Iteration 44, loss = 1.10585969\n",
            "Iteration 45, loss = 1.13005118\n",
            "Iteration 46, loss = 1.11194967\n",
            "Iteration 47, loss = 1.11841456\n",
            "Iteration 48, loss = 1.10833773\n",
            "Iteration 49, loss = 1.10947538\n",
            "Iteration 50, loss = 1.11096798\n",
            "Iteration 51, loss = 1.11243056\n",
            "Iteration 52, loss = 1.10727181\n",
            "Iteration 53, loss = 1.12266084\n",
            "Iteration 54, loss = 1.10161064\n",
            "Iteration 55, loss = 1.11940760\n",
            "Iteration 56, loss = 1.11131496\n",
            "Iteration 57, loss = 1.10835454\n",
            "Iteration 58, loss = 1.10220667\n",
            "Iteration 59, loss = 1.11308179\n",
            "Iteration 60, loss = 1.10533773\n",
            "Iteration 61, loss = 1.10824344\n",
            "Iteration 62, loss = 1.09382476\n",
            "Iteration 63, loss = 1.11470565\n",
            "Iteration 64, loss = 1.09795726\n",
            "Iteration 65, loss = 1.09826904\n",
            "Iteration 66, loss = 1.09737323\n",
            "Iteration 67, loss = 1.10051301\n",
            "Iteration 68, loss = 1.09756834\n",
            "Iteration 69, loss = 1.10944722\n",
            "Iteration 70, loss = 1.10176018\n",
            "Iteration 71, loss = 1.09642260\n",
            "Iteration 72, loss = 1.09758751\n",
            "Iteration 73, loss = 1.10031874\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.69560924"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2, loss = 1.68742154\n",
            "Iteration 3, loss = 1.68023419\n",
            "Iteration 4, loss = 1.67311925\n",
            "Iteration 5, loss = 1.66660448\n",
            "Iteration 6, loss = 1.66039592\n",
            "Iteration 7, loss = 1.65468581\n",
            "Iteration 8, loss = 1.64860305\n",
            "Iteration 9, loss = 1.64412330\n",
            "Iteration 10, loss = 1.63923331\n",
            "Iteration 11, loss = 1.63471948\n",
            "Iteration 12, loss = 1.63054881\n",
            "Iteration 13, loss = 1.62674758\n",
            "Iteration 14, loss = 1.62340338\n",
            "Iteration 15, loss = 1.62015041\n",
            "Iteration 16, loss = 1.61677352\n",
            "Iteration 17, loss = 1.61406304\n",
            "Iteration 18, loss = 1.61183249\n",
            "Iteration 19, loss = 1.60907871\n",
            "Iteration 20, loss = 1.60689743\n",
            "Iteration 21, loss = 1.60497016\n",
            "Iteration 22, loss = 1.60291618\n",
            "Iteration 23, loss = 1.60125051\n",
            "Iteration 24, loss = 1.59955095\n",
            "Iteration 25, loss = 1.59801447\n",
            "Iteration 26, loss = 1.59658865\n",
            "Iteration 27, loss = 1.59533989\n",
            "Iteration 28, loss = 1.59397320\n",
            "Iteration 29, loss = 1.59291185\n",
            "Iteration 30, loss = 1.59152682\n",
            "Iteration 31, loss = 1.59061232\n",
            "Iteration 32, loss = 1.58955281\n",
            "Iteration 33, loss = 1.58863922\n",
            "Iteration 34, loss = 1.58769589\n",
            "Iteration 35, loss = 1.58675415\n",
            "Iteration 36, loss = 1.58582631\n",
            "Iteration 37, loss = 1.58510157\n",
            "Iteration 38, loss = 1.58417870\n",
            "Iteration 39, loss = 1.58327388\n",
            "Iteration 40, loss = 1.58253191\n",
            "Iteration 41, loss = 1.58176466\n",
            "Iteration 42, loss = 1.58085408\n",
            "Iteration 43, loss = 1.58012390\n",
            "Iteration 44, loss = 1.57934736\n",
            "Iteration 45, loss = 1.57852755\n",
            "Iteration 46, loss = 1.57779364\n",
            "Iteration 47, loss = 1.57691944\n",
            "Iteration 48, loss = 1.57617337\n",
            "Iteration 49, loss = 1.57534779\n",
            "Iteration 50, loss = 1.57462769\n",
            "Iteration 51, loss = 1.57376991\n",
            "Iteration 52, loss = 1.57294616\n",
            "Iteration 53, loss = 1.57223679\n",
            "Iteration 54, loss = 1.57136054\n",
            "Iteration 55, loss = 1.57053735\n",
            "Iteration 56, loss = 1.56971364\n",
            "Iteration 57, loss = 1.56885240\n",
            "Iteration 58, loss = 1.56804749\n",
            "Iteration 59, loss = 1.56723587\n",
            "Iteration 60, loss = 1.56630969\n",
            "Iteration 61, loss = 1.56547106\n",
            "Iteration 62, loss = 1.56459625\n",
            "Iteration 63, loss = 1.56378616\n",
            "Iteration 64, loss = 1.56286494\n",
            "Iteration 65, loss = 1.56192342\n",
            "Iteration 66, loss = 1.56105874\n",
            "Iteration 67, loss = 1.56011370\n",
            "Iteration 68, loss = 1.55923366\n",
            "Iteration 69, loss = 1.55833941\n",
            "Iteration 70, loss = 1.55736966\n",
            "Iteration 71, loss = 1.55641706\n",
            "Iteration 72, loss = 1.55546900\n",
            "Iteration 73, loss = 1.55456373\n",
            "Iteration 74, loss = 1.55354052\n",
            "Iteration 75, loss = 1.55255615\n",
            "Iteration 76, loss = 1.55164179\n",
            "Iteration 77, loss = 1.55056273\n",
            "Iteration 78, loss = 1.54956067\n",
            "Iteration 79, loss = 1.54853413\n",
            "Iteration 80, loss = 1.54758416\n",
            "Iteration 81, loss = 1.54647770\n",
            "Iteration 82, loss = 1.54546824\n",
            "Iteration 83, loss = 1.54437543\n",
            "Iteration 84, loss = 1.54331179\n",
            "Iteration 85, loss = 1.54225133\n",
            "Iteration 86, loss = 1.54118227\n",
            "Iteration 87, loss = 1.54009900\n",
            "Iteration 88, loss = 1.53894139\n",
            "Iteration 89, loss = 1.53784919\n",
            "Iteration 90, loss = 1.53681710\n",
            "Iteration 91, loss = 1.53557610\n",
            "Iteration 92, loss = 1.53441198\n",
            "Iteration 93, loss = 1.53326484\n",
            "Iteration 94, loss = 1.53210741\n",
            "Iteration 95, loss = 1.53096454\n",
            "Iteration 96, loss = 1.52972479\n",
            "Iteration 97, loss = 1.52853775\n",
            "Iteration 98, loss = 1.52737733\n",
            "Iteration 99, loss = 1.52609853\n",
            "Iteration 100, loss = 1.52481785\n",
            "Iteration 1, loss = 1.68127622\n",
            "Iteration 2, loss = 1.62455259\n",
            "Iteration 3, loss = 1.60120082\n",
            "Iteration 4, loss = 1.59084859\n",
            "Iteration 5, loss = 1.58856123\n",
            "Iteration 6, loss = 1.58517874\n",
            "Iteration 7, loss = 1.57987169\n",
            "Iteration 8, loss = 1.57221919\n",
            "Iteration 9, loss = 1.56247431\n",
            "Iteration 10, loss = 1.55267460\n",
            "Iteration 11, loss = 1.54328984\n",
            "Iteration 12, loss = 1.53370285\n",
            "Iteration 13, loss = 1.52490045\n",
            "Iteration 14, loss = 1.51427841\n",
            "Iteration 15, loss = 1.50411569\n",
            "Iteration 16, loss = 1.49280769\n",
            "Iteration 17, loss = 1.48107404\n",
            "Iteration 18, loss = 1.46838267\n",
            "Iteration 19, loss = 1.45308266\n",
            "Iteration 20, loss = 1.43892533\n",
            "Iteration 21, loss = 1.42522239\n",
            "Iteration 22, loss = 1.40946659\n",
            "Iteration 23, loss = 1.39458840\n",
            "Iteration 24, loss = 1.37919291\n",
            "Iteration 25, loss = 1.36360241\n",
            "Iteration 26, loss = 1.34883573\n",
            "Iteration 27, loss = 1.33367718\n",
            "Iteration 28, loss = 1.31953869\n",
            "Iteration 29, loss = 1.30550792\n",
            "Iteration 30, loss = 1.29246808\n",
            "Iteration 31, loss = 1.27923490\n",
            "Iteration 32, loss = 1.26737345\n",
            "Iteration 33, loss = 1.25618580\n",
            "Iteration 34, loss = 1.24573054\n",
            "Iteration 35, loss = 1.23577194"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 36, loss = 1.22654683\n",
            "Iteration 37, loss = 1.21881826\n",
            "Iteration 38, loss = 1.21053321\n",
            "Iteration 39, loss = 1.20289450\n",
            "Iteration 40, loss = 1.19639532\n",
            "Iteration 41, loss = 1.19026189\n",
            "Iteration 42, loss = 1.18449629\n",
            "Iteration 43, loss = 1.17938478\n",
            "Iteration 44, loss = 1.17403805\n",
            "Iteration 45, loss = 1.17054341\n",
            "Iteration 46, loss = 1.16543957\n",
            "Iteration 47, loss = 1.16133827\n",
            "Iteration 48, loss = 1.15724659\n",
            "Iteration 49, loss = 1.15468967\n",
            "Iteration 50, loss = 1.15128819\n",
            "Iteration 51, loss = 1.14753106\n",
            "Iteration 52, loss = 1.14468636\n",
            "Iteration 53, loss = 1.14268915\n",
            "Iteration 54, loss = 1.13948673\n",
            "Iteration 55, loss = 1.13704423\n",
            "Iteration 56, loss = 1.13507842\n",
            "Iteration 57, loss = 1.13245830\n",
            "Iteration 58, loss = 1.13070142\n",
            "Iteration 59, loss = 1.12850998\n",
            "Iteration 60, loss = 1.12612837\n",
            "Iteration 61, loss = 1.12479128\n",
            "Iteration 62, loss = 1.12283476\n",
            "Iteration 63, loss = 1.12253464\n",
            "Iteration 64, loss = 1.11988193\n",
            "Iteration 65, loss = 1.11820759\n",
            "Iteration 66, loss = 1.11649310\n",
            "Iteration 67, loss = 1.11595034\n",
            "Iteration 68, loss = 1.11595031\n",
            "Iteration 69, loss = 1.11645163\n",
            "Iteration 70, loss = 1.11151441\n",
            "Iteration 71, loss = 1.10972208\n",
            "Iteration 72, loss = 1.10818540\n",
            "Iteration 73, loss = 1.10868061\n",
            "Iteration 74, loss = 1.10685189\n",
            "Iteration 75, loss = 1.10422846\n",
            "Iteration 76, loss = 1.10455794\n",
            "Iteration 77, loss = 1.10236723\n",
            "Iteration 78, loss = 1.10073965\n",
            "Iteration 79, loss = 1.10109124\n",
            "Iteration 80, loss = 1.09899275\n",
            "Iteration 81, loss = 1.09875453\n",
            "Iteration 82, loss = 1.09728961\n",
            "Iteration 83, loss = 1.09547985\n",
            "Iteration 84, loss = 1.09407324\n",
            "Iteration 85, loss = 1.09324777\n",
            "Iteration 86, loss = 1.09175329\n",
            "Iteration 87, loss = 1.09367218\n",
            "Iteration 88, loss = 1.09115786\n",
            "Iteration 89, loss = 1.08897689\n",
            "Iteration 90, loss = 1.08773030\n",
            "Iteration 91, loss = 1.08771235\n",
            "Iteration 92, loss = 1.08512968\n",
            "Iteration 93, loss = 1.08365988\n",
            "Iteration 94, loss = 1.08392225\n",
            "Iteration 95, loss = 1.08311042\n",
            "Iteration 96, loss = 1.08095922\n",
            "Iteration 97, loss = 1.07947115\n",
            "Iteration 98, loss = 1.07874768\n",
            "Iteration 99, loss = 1.07709421\n",
            "Iteration 100, loss = 1.07533688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.68117774\n",
            "Iteration 2, loss = 1.62536808\n",
            "Iteration 3, loss = 1.56580839\n",
            "Iteration 4, loss = 1.50355838\n",
            "Iteration 5, loss = 1.43827924\n",
            "Iteration 6, loss = 1.37339020\n",
            "Iteration 7, loss = 1.30793494\n",
            "Iteration 8, loss = 1.24891262\n",
            "Iteration 9, loss = 1.20880340\n",
            "Iteration 10, loss = 1.16876844\n",
            "Iteration 11, loss = 1.14764779\n",
            "Iteration 12, loss = 1.14025757\n",
            "Iteration 13, loss = 1.12749229\n",
            "Iteration 14, loss = 1.11794336\n",
            "Iteration 15, loss = 1.12260501\n",
            "Iteration 16, loss = 1.11535821\n",
            "Iteration 17, loss = 1.11218710\n",
            "Iteration 18, loss = 1.11911603\n",
            "Iteration 19, loss = 1.11424137\n",
            "Iteration 20, loss = 1.12395756\n",
            "Iteration 21, loss = 1.10325002\n",
            "Iteration 22, loss = 1.10308501\n",
            "Iteration 23, loss = 1.10968588\n",
            "Iteration 24, loss = 1.09495915\n",
            "Iteration 25, loss = 1.09071520\n",
            "Iteration 26, loss = 1.09706654\n",
            "Iteration 27, loss = 1.08716889\n",
            "Iteration 28, loss = 1.07925054\n",
            "Iteration 29, loss = 1.09344515\n",
            "Iteration 30, loss = 1.07929232\n",
            "Iteration 31, loss = 1.07236301\n",
            "Iteration 32, loss = 1.07913131\n",
            "Iteration 33, loss = 1.08054619\n",
            "Iteration 34, loss = 1.05963795\n",
            "Iteration 35, loss = 1.05564167\n",
            "Iteration 36, loss = 1.05735851\n",
            "Iteration 37, loss = 1.06874429\n",
            "Iteration 38, loss = 1.04250579\n",
            "Iteration 39, loss = 1.02666274\n",
            "Iteration 40, loss = 1.01627683\n",
            "Iteration 41, loss = 1.01261558\n",
            "Iteration 42, loss = 1.01506901\n",
            "Iteration 43, loss = 1.01643885\n",
            "Iteration 44, loss = 0.97902717\n",
            "Iteration 45, loss = 0.99147574\n",
            "Iteration 46, loss = 0.98682442\n",
            "Iteration 47, loss = 1.00872247\n",
            "Iteration 48, loss = 0.97031610\n",
            "Iteration 49, loss = 0.96801169\n",
            "Iteration 50, loss = 1.01001172\n",
            "Iteration 51, loss = 1.01331821\n",
            "Iteration 52, loss = 1.02818692\n",
            "Iteration 53, loss = 1.01870020\n",
            "Iteration 54, loss = 0.99472115\n",
            "Iteration 55, loss = 0.95326114\n",
            "Iteration 56, loss = 0.94881225\n",
            "Iteration 57, loss = 0.93176761\n",
            "Iteration 58, loss = 0.91340016\n",
            "Iteration 59, loss = 0.92858152\n",
            "Iteration 60, loss = 0.92685802\n",
            "Iteration 61, loss = 0.92024403\n",
            "Iteration 62, loss = 0.91131641\n",
            "Iteration 63, loss = 0.90196496\n",
            "Iteration 64, loss = 0.91255337\n",
            "Iteration 65, loss = 0.88351917\n",
            "Iteration 66, loss = 0.87549194\n",
            "Iteration 67, loss = 0.86503827\n",
            "Iteration 68, loss = 0.86153296\n",
            "Iteration 69, loss = 0.87100926\n",
            "Iteration 70, loss = 0.86879681\n",
            "Iteration 71, loss = 0.85092860\n",
            "Iteration 72, loss = 0.84606958\n",
            "Iteration 73, loss = 0.84879338\n",
            "Iteration 74, loss = 0.88804073\n",
            "Iteration 75, loss = 0.82545841\n",
            "Iteration 76, loss = 0.82799603\n",
            "Iteration 77, loss = 0.87123801\n",
            "Iteration 78, loss = 0.83221151\n",
            "Iteration 79, loss = 0.81883924\n",
            "Iteration 80, loss = 0.81278629\n",
            "Iteration 81, loss = 0.80018517\n",
            "Iteration 82, loss = 0.80228930\n",
            "Iteration 83, loss = 0.81367203\n",
            "Iteration 84, loss = 0.80923468\n",
            "Iteration 85, loss = 0.78257188\n",
            "Iteration 86, loss = 0.78562793\n",
            "Iteration 87, loss = 0.85112451\n",
            "Iteration 88, loss = 1.01345813\n",
            "Iteration 89, loss = 0.95464634\n",
            "Iteration 90, loss = 0.92664266\n",
            "Iteration 91, loss = 0.87865203\n",
            "Iteration 92, loss = 0.92943722\n",
            "Iteration 93, loss = 0.88522905\n",
            "Iteration 94, loss = 0.92426760\n",
            "Iteration 95, loss = 0.78213579\n",
            "Iteration 96, loss = 0.79714147\n",
            "Iteration 97, loss = 0.80109306\n",
            "Iteration 98, loss = 0.79975068\n",
            "Iteration 99, loss = 0.81864230\n",
            "Iteration 100, loss = 0.77808106\n",
            "Iteration 1, loss = 1.70383477\n",
            "Iteration 2, loss = 1.69920862\n",
            "Iteration 3, loss = 1.69501290\n",
            "Iteration 4, loss = 1.69074857\n",
            "Iteration 5, loss = 1.68672007\n",
            "Iteration 6, loss = 1.68286383\n",
            "Iteration 7, loss = 1.67901802\n",
            "Iteration 8, loss = 1.67499855\n",
            "Iteration 9, loss = 1.67175366\n",
            "Iteration 10, loss = 1.66813512\n",
            "Iteration 11, loss = 1.66476243\n",
            "Iteration 12, loss = 1.66142028\n",
            "Iteration 13, loss = 1.65811617\n",
            "Iteration 14, loss = 1.65519388\n",
            "Iteration 15, loss = 1.65224076\n",
            "Iteration 16, loss = 1.64911737\n",
            "Iteration 17, loss = 1.64630580\n",
            "Iteration 18, loss = 1.64388346\n",
            "Iteration 19, loss = 1.64100617\n",
            "Iteration 20, loss = 1.63854332\n",
            "Iteration 21, loss = 1.63621384\n",
            "Iteration 22, loss = 1.63380262\n",
            "Iteration 23, loss = 1.63158390\n",
            "Iteration 24, loss = 1.62935379\n",
            "Iteration 25, loss = 1.62722514\n",
            "Iteration 26, loss = 1.62521729\n",
            "Iteration 27, loss = 1.62326130\n",
            "Iteration 28, loss = 1.62121758\n",
            "Iteration 29, loss = 1.61941634\n",
            "Iteration 30, loss = 1.61745624\n",
            "Iteration 31, loss = 1.61579574\n",
            "Iteration 32, loss = 1.61401797\n",
            "Iteration 33, loss = 1.61237941\n",
            "Iteration 34, loss = 1.61074303\n",
            "Iteration 35, loss = 1.60910421\n",
            "Iteration 36, loss = 1.60753469\n",
            "Iteration 37, loss = 1.60607281\n",
            "Iteration 38, loss = 1.60460396\n",
            "Iteration 39, loss = 1.60306952\n",
            "Iteration 40, loss = 1.60165714\n",
            "Iteration 41, loss = 1.60041690\n",
            "Iteration 42, loss = 1.59888590\n",
            "Iteration 43, loss = 1.59760387\n",
            "Iteration 44, loss = 1.59632779\n",
            "Iteration 45, loss = 1.59509157\n",
            "Iteration 46, loss = 1.59397300"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 47, loss = 1.59260682\n",
            "Iteration 48, loss = 1.59150985\n",
            "Iteration 49, loss = 1.59027828\n",
            "Iteration 50, loss = 1.58931025\n",
            "Iteration 51, loss = 1.58809748\n",
            "Iteration 52, loss = 1.58695792\n",
            "Iteration 53, loss = 1.58602207\n",
            "Iteration 54, loss = 1.58492726\n",
            "Iteration 55, loss = 1.58379174\n",
            "Iteration 56, loss = 1.58287360\n",
            "Iteration 57, loss = 1.58180655\n",
            "Iteration 58, loss = 1.58083528\n",
            "Iteration 59, loss = 1.57986117\n",
            "Iteration 60, loss = 1.57886222\n",
            "Iteration 61, loss = 1.57794425\n",
            "Iteration 62, loss = 1.57697061\n",
            "Iteration 63, loss = 1.57604442\n",
            "Iteration 64, loss = 1.57504926\n",
            "Iteration 65, loss = 1.57410810\n",
            "Iteration 66, loss = 1.57325815\n",
            "Iteration 67, loss = 1.57226396\n",
            "Iteration 68, loss = 1.57143251\n",
            "Iteration 69, loss = 1.57053569\n",
            "Iteration 70, loss = 1.56960781\n",
            "Iteration 71, loss = 1.56868798\n",
            "Iteration 72, loss = 1.56777999\n",
            "Iteration 73, loss = 1.56696123\n",
            "Iteration 74, loss = 1.56603255\n",
            "Iteration 75, loss = 1.56514628\n",
            "Iteration 76, loss = 1.56432410\n",
            "Iteration 77, loss = 1.56334373\n",
            "Iteration 78, loss = 1.56244340\n",
            "Iteration 79, loss = 1.56154175\n",
            "Iteration 80, loss = 1.56069549\n",
            "Iteration 81, loss = 1.55975254\n",
            "Iteration 82, loss = 1.55890361\n",
            "Iteration 83, loss = 1.55792443\n",
            "Iteration 84, loss = 1.55702146\n",
            "Iteration 85, loss = 1.55611292\n",
            "Iteration 86, loss = 1.55521955\n",
            "Iteration 87, loss = 1.55430334\n",
            "Iteration 88, loss = 1.55332136\n",
            "Iteration 89, loss = 1.55239184\n",
            "Iteration 90, loss = 1.55154687\n",
            "Iteration 91, loss = 1.55051937\n",
            "Iteration 92, loss = 1.54955028\n",
            "Iteration 93, loss = 1.54858564\n",
            "Iteration 94, loss = 1.54764459\n",
            "Iteration 95, loss = 1.54671475\n",
            "Iteration 96, loss = 1.54567333\n",
            "Iteration 97, loss = 1.54468882\n",
            "Iteration 98, loss = 1.54376246\n",
            "Iteration 99, loss = 1.54269586\n",
            "Iteration 100, loss = 1.54165956\n",
            "Iteration 1, loss = 1.69539741\n",
            "Iteration 2, loss = 1.65700478\n",
            "Iteration 3, loss = 1.63158300\n",
            "Iteration 4, loss = 1.61160594\n",
            "Iteration 5, loss = 1.59836418\n",
            "Iteration 6, loss = 1.59002224\n",
            "Iteration 7, loss = 1.58250501\n",
            "Iteration 8, loss = 1.57612121\n",
            "Iteration 9, loss = 1.57275520\n",
            "Iteration 10, loss = 1.56740704\n",
            "Iteration 11, loss = 1.56195083\n",
            "Iteration 12, loss = 1.55494097\n",
            "Iteration 13, loss = 1.54815718\n",
            "Iteration 14, loss = 1.53936492\n",
            "Iteration 15, loss = 1.53053129\n",
            "Iteration 16, loss = 1.52172368\n",
            "Iteration 17, loss = 1.51263240\n",
            "Iteration 18, loss = 1.50242835\n",
            "Iteration 19, loss = 1.49178859\n",
            "Iteration 20, loss = 1.48081252\n",
            "Iteration 21, loss = 1.46954877\n",
            "Iteration 22, loss = 1.45711517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 23, loss = 1.44448364\n",
            "Iteration 24, loss = 1.43144817\n",
            "Iteration 25, loss = 1.41781857\n",
            "Iteration 26, loss = 1.40493820\n",
            "Iteration 27, loss = 1.39136987\n",
            "Iteration 28, loss = 1.37799811\n",
            "Iteration 29, loss = 1.36503387\n",
            "Iteration 30, loss = 1.35201113\n",
            "Iteration 31, loss = 1.33903374\n",
            "Iteration 32, loss = 1.32661171\n",
            "Iteration 33, loss = 1.31455769\n",
            "Iteration 34, loss = 1.30305715\n",
            "Iteration 35, loss = 1.29171800\n",
            "Iteration 36, loss = 1.28118725\n",
            "Iteration 37, loss = 1.27167646\n",
            "Iteration 38, loss = 1.26175257\n",
            "Iteration 39, loss = 1.25272056\n",
            "Iteration 40, loss = 1.24457075\n",
            "Iteration 41, loss = 1.23679821\n",
            "Iteration 42, loss = 1.22938290\n",
            "Iteration 43, loss = 1.22278138\n",
            "Iteration 44, loss = 1.21603889\n",
            "Iteration 45, loss = 1.21072667\n",
            "Iteration 46, loss = 1.20470780\n",
            "Iteration 47, loss = 1.19931358\n",
            "Iteration 48, loss = 1.19411284\n",
            "Iteration 49, loss = 1.19024663\n",
            "Iteration 50, loss = 1.18573322\n",
            "Iteration 51, loss = 1.18122150\n",
            "Iteration 52, loss = 1.17753095\n",
            "Iteration 53, loss = 1.17440468\n",
            "Iteration 54, loss = 1.17059724\n",
            "Iteration 55, loss = 1.16737009\n",
            "Iteration 56, loss = 1.16455194\n",
            "Iteration 57, loss = 1.16143633\n",
            "Iteration 58, loss = 1.15902825\n",
            "Iteration 59, loss = 1.15638485\n",
            "Iteration 60, loss = 1.15353335\n",
            "Iteration 61, loss = 1.15162221\n",
            "Iteration 62, loss = 1.14929775\n",
            "Iteration 63, loss = 1.14842880\n",
            "Iteration 64, loss = 1.14548833\n",
            "Iteration 65, loss = 1.14340639\n",
            "Iteration 66, loss = 1.14199708\n",
            "Iteration 67, loss = 1.14042901\n",
            "Iteration 68, loss = 1.14023153\n",
            "Iteration 69, loss = 1.13958081\n",
            "Iteration 70, loss = 1.13520873\n",
            "Iteration 71, loss = 1.13397493\n",
            "Iteration 72, loss = 1.13242265\n",
            "Iteration 73, loss = 1.13190584\n",
            "Iteration 74, loss = 1.12997442\n",
            "Iteration 75, loss = 1.12833713\n",
            "Iteration 76, loss = 1.12789612\n",
            "Iteration 77, loss = 1.12613356\n",
            "Iteration 78, loss = 1.12492568\n",
            "Iteration 79, loss = 1.12471614\n",
            "Iteration 80, loss = 1.12316451\n",
            "Iteration 81, loss = 1.12237723\n",
            "Iteration 82, loss = 1.12094263\n",
            "Iteration 83, loss = 1.11977325\n",
            "Iteration 84, loss = 1.11869652\n",
            "Iteration 85, loss = 1.11783503\n",
            "Iteration 86, loss = 1.11709432\n",
            "Iteration 87, loss = 1.11794965\n",
            "Iteration 88, loss = 1.11615899\n",
            "Iteration 89, loss = 1.11453195\n",
            "Iteration 90, loss = 1.11395576\n",
            "Iteration 91, loss = 1.11364425\n",
            "Iteration 92, loss = 1.11215506\n",
            "Iteration 93, loss = 1.11120530\n",
            "Iteration 94, loss = 1.11098821\n",
            "Iteration 95, loss = 1.11071776\n",
            "Iteration 96, loss = 1.10911806\n",
            "Iteration 97, loss = 1.10827563\n",
            "Iteration 98, loss = 1.10791859\n",
            "Iteration 99, loss = 1.10738207\n",
            "Iteration 100, loss = 1.10606063\n",
            "Iteration 1, loss = 1.66388426\n",
            "Iteration 2, loss = 1.61520602\n",
            "Iteration 3, loss = 1.57423808\n",
            "Iteration 4, loss = 1.55135783\n",
            "Iteration 5, loss = 1.50884727\n",
            "Iteration 6, loss = 1.44937288\n",
            "Iteration 7, loss = 1.38549820\n",
            "Iteration 8, loss = 1.31509300\n",
            "Iteration 9, loss = 1.25794859\n",
            "Iteration 10, loss = 1.21072685\n",
            "Iteration 11, loss = 1.17855036\n",
            "Iteration 12, loss = 1.15609180\n",
            "Iteration 13, loss = 1.14144583\n",
            "Iteration 14, loss = 1.12959955\n",
            "Iteration 15, loss = 1.12494907\n",
            "Iteration 16, loss = 1.12024797\n",
            "Iteration 17, loss = 1.11483060\n",
            "Iteration 18, loss = 1.11719489\n",
            "Iteration 19, loss = 1.11204917\n",
            "Iteration 20, loss = 1.12015687\n",
            "Iteration 21, loss = 1.10081047\n",
            "Iteration 22, loss = 1.11635794\n",
            "Iteration 23, loss = 1.10681827\n",
            "Iteration 24, loss = 1.10178816\n",
            "Iteration 25, loss = 1.10760177\n",
            "Iteration 26, loss = 1.10743697\n",
            "Iteration 27, loss = 1.10035383\n",
            "Iteration 28, loss = 1.10711494\n",
            "Iteration 29, loss = 1.10840367\n",
            "Iteration 30, loss = 1.09880739\n",
            "Iteration 31, loss = 1.10374864\n",
            "Iteration 32, loss = 1.10881909\n",
            "Iteration 33, loss = 1.10061749\n",
            "Iteration 34, loss = 1.09742015\n",
            "Iteration 35, loss = 1.09126938\n",
            "Iteration 36, loss = 1.09154889\n",
            "Iteration 37, loss = 1.09352502\n",
            "Iteration 38, loss = 1.08987338\n",
            "Iteration 39, loss = 1.08328914\n",
            "Iteration 40, loss = 1.08576098\n",
            "Iteration 41, loss = 1.08025582\n",
            "Iteration 42, loss = 1.08138436\n",
            "Iteration 43, loss = 1.07611001\n",
            "Iteration 44, loss = 1.07162402\n",
            "Iteration 45, loss = 1.07966533\n",
            "Iteration 46, loss = 1.07167946\n",
            "Iteration 47, loss = 1.06331691\n",
            "Iteration 48, loss = 1.06722332\n",
            "Iteration 49, loss = 1.05671396\n",
            "Iteration 50, loss = 1.06092865\n",
            "Iteration 51, loss = 1.05956247\n",
            "Iteration 52, loss = 1.04935013\n",
            "Iteration 53, loss = 1.05841526\n",
            "Iteration 54, loss = 1.04115242\n",
            "Iteration 55, loss = 1.04252033\n",
            "Iteration 56, loss = 1.03428717\n",
            "Iteration 57, loss = 1.02660036\n",
            "Iteration 58, loss = 1.02538445\n",
            "Iteration 59, loss = 1.02217164\n",
            "Iteration 60, loss = 1.01118590\n",
            "Iteration 61, loss = 1.01078665\n",
            "Iteration 62, loss = 0.99926987\n",
            "Iteration 63, loss = 1.00765014\n",
            "Iteration 64, loss = 0.99983004\n",
            "Iteration 65, loss = 0.98998608\n",
            "Iteration 66, loss = 0.97608314\n",
            "Iteration 67, loss = 0.97507143\n",
            "Iteration 68, loss = 0.96716886\n",
            "Iteration 69, loss = 0.97782211\n",
            "Iteration 70, loss = 0.96706648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 71, loss = 0.97054033\n",
            "Iteration 72, loss = 0.97457490\n",
            "Iteration 73, loss = 0.96076369\n",
            "Iteration 74, loss = 0.96623488\n",
            "Iteration 75, loss = 0.93081933\n",
            "Iteration 76, loss = 0.91621703\n",
            "Iteration 77, loss = 0.93505385\n",
            "Iteration 78, loss = 0.91627762\n",
            "Iteration 79, loss = 0.91172890\n",
            "Iteration 80, loss = 0.91202728\n",
            "Iteration 81, loss = 0.90100061\n",
            "Iteration 82, loss = 0.89384400\n",
            "Iteration 83, loss = 0.88417665\n",
            "Iteration 84, loss = 0.87739900\n",
            "Iteration 85, loss = 0.87708074\n",
            "Iteration 86, loss = 0.87810854\n",
            "Iteration 87, loss = 0.93178203\n",
            "Iteration 88, loss = 0.87905811\n",
            "Iteration 89, loss = 0.89137641\n",
            "Iteration 90, loss = 0.89155828\n",
            "Iteration 91, loss = 0.88932378\n",
            "Iteration 92, loss = 0.86807774\n",
            "Iteration 93, loss = 0.85762539\n",
            "Iteration 94, loss = 0.85294935\n",
            "Iteration 95, loss = 0.85079468\n",
            "Iteration 96, loss = 0.84137311\n",
            "Iteration 97, loss = 0.85019088\n",
            "Iteration 98, loss = 0.85692920\n",
            "Iteration 99, loss = 0.84453354\n",
            "Iteration 100, loss = 0.82762366\n",
            "Iteration 1, loss = 1.65909370\n",
            "Iteration 2, loss = 1.65095018\n",
            "Iteration 3, loss = 1.64396387\n",
            "Iteration 4, loss = 1.63644368\n",
            "Iteration 5, loss = 1.63048308\n",
            "Iteration 6, loss = 1.62469661\n",
            "Iteration 7, loss = 1.61945389\n",
            "Iteration 8, loss = 1.61448339\n",
            "Iteration 9, loss = 1.60950127\n",
            "Iteration 10, loss = 1.60521745\n",
            "Iteration 11, loss = 1.60146718\n",
            "Iteration 12, loss = 1.59808572\n",
            "Iteration 13, loss = 1.59486085\n",
            "Iteration 14, loss = 1.59207743\n",
            "Iteration 15, loss = 1.58956765\n",
            "Iteration 16, loss = 1.58754623\n",
            "Iteration 17, loss = 1.58566513\n",
            "Iteration 18, loss = 1.58397383\n",
            "Iteration 19, loss = 1.58239161\n",
            "Iteration 20, loss = 1.58075468\n",
            "Iteration 21, loss = 1.57962613\n",
            "Iteration 22, loss = 1.57854926\n",
            "Iteration 23, loss = 1.57755528\n",
            "Iteration 24, loss = 1.57642514\n",
            "Iteration 25, loss = 1.57563878\n",
            "Iteration 26, loss = 1.57475519\n",
            "Iteration 27, loss = 1.57394784\n",
            "Iteration 28, loss = 1.57329219\n",
            "Iteration 29, loss = 1.57255113\n",
            "Iteration 30, loss = 1.57182965\n",
            "Iteration 31, loss = 1.57116350\n",
            "Iteration 32, loss = 1.57039403\n",
            "Iteration 33, loss = 1.56968442\n",
            "Iteration 34, loss = 1.56894757\n",
            "Iteration 35, loss = 1.56826085\n",
            "Iteration 36, loss = 1.56754481\n",
            "Iteration 37, loss = 1.56681073\n",
            "Iteration 38, loss = 1.56614317\n",
            "Iteration 39, loss = 1.56534434\n",
            "Iteration 40, loss = 1.56462792\n",
            "Iteration 41, loss = 1.56387951\n",
            "Iteration 42, loss = 1.56315148\n",
            "Iteration 43, loss = 1.56240669\n",
            "Iteration 44, loss = 1.56163138\n",
            "Iteration 45, loss = 1.56091094\n",
            "Iteration 46, loss = 1.56007184\n",
            "Iteration 47, loss = 1.55937279\n",
            "Iteration 48, loss = 1.55850958\n",
            "Iteration 49, loss = 1.55786200\n",
            "Iteration 50, loss = 1.55698913\n",
            "Iteration 51, loss = 1.55618953\n",
            "Iteration 52, loss = 1.55535790\n",
            "Iteration 53, loss = 1.55456124"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 54, loss = 1.55371209\n",
            "Iteration 55, loss = 1.55294038\n",
            "Iteration 56, loss = 1.55205682\n",
            "Iteration 57, loss = 1.55124192\n",
            "Iteration 58, loss = 1.55039222\n",
            "Iteration 59, loss = 1.54952120\n",
            "Iteration 60, loss = 1.54866366\n",
            "Iteration 61, loss = 1.54781722\n",
            "Iteration 62, loss = 1.54690465\n",
            "Iteration 63, loss = 1.54600453\n",
            "Iteration 64, loss = 1.54510664\n",
            "Iteration 65, loss = 1.54418962\n",
            "Iteration 66, loss = 1.54329563\n",
            "Iteration 67, loss = 1.54234427\n",
            "Iteration 68, loss = 1.54141690\n",
            "Iteration 69, loss = 1.54049067\n",
            "Iteration 70, loss = 1.53960901\n",
            "Iteration 71, loss = 1.53856143\n",
            "Iteration 72, loss = 1.53759331\n",
            "Iteration 73, loss = 1.53665362\n",
            "Iteration 74, loss = 1.53567706\n",
            "Iteration 75, loss = 1.53465004\n",
            "Iteration 76, loss = 1.53364907\n",
            "Iteration 77, loss = 1.53267190\n",
            "Iteration 78, loss = 1.53172602\n",
            "Iteration 79, loss = 1.53069309\n",
            "Iteration 80, loss = 1.52953362\n",
            "Iteration 81, loss = 1.52851346\n",
            "Iteration 82, loss = 1.52744877\n",
            "Iteration 83, loss = 1.52639604\n",
            "Iteration 84, loss = 1.52534606\n",
            "Iteration 85, loss = 1.52428168\n",
            "Iteration 86, loss = 1.52321098\n",
            "Iteration 87, loss = 1.52205860\n",
            "Iteration 88, loss = 1.52098513\n",
            "Iteration 89, loss = 1.51984711\n",
            "Iteration 90, loss = 1.51876317\n",
            "Iteration 91, loss = 1.51758302\n",
            "Iteration 92, loss = 1.51641116\n",
            "Iteration 93, loss = 1.51532347\n",
            "Iteration 94, loss = 1.51414068\n",
            "Iteration 95, loss = 1.51299733\n",
            "Iteration 96, loss = 1.51177460\n",
            "Iteration 97, loss = 1.51060012\n",
            "Iteration 98, loss = 1.50945104\n",
            "Iteration 99, loss = 1.50817770\n",
            "Iteration 100, loss = 1.50698967\n",
            "Iteration 1, loss = 1.64639672\n",
            "Iteration 2, loss = 1.59337522\n",
            "Iteration 3, loss = 1.58106501\n",
            "Iteration 4, loss = 1.57915491\n",
            "Iteration 5, loss = 1.57702736\n",
            "Iteration 6, loss = 1.56966137\n",
            "Iteration 7, loss = 1.55845344\n",
            "Iteration 8, loss = 1.54863495\n",
            "Iteration 9, loss = 1.53956020\n",
            "Iteration 10, loss = 1.53204456\n",
            "Iteration 11, loss = 1.52436390\n",
            "Iteration 12, loss = 1.51639775\n",
            "Iteration 13, loss = 1.50568061\n",
            "Iteration 14, loss = 1.49354746\n",
            "Iteration 15, loss = 1.48156716\n",
            "Iteration 16, loss = 1.46980031\n",
            "Iteration 17, loss = 1.45780866\n",
            "Iteration 18, loss = 1.44560728\n",
            "Iteration 19, loss = 1.43070886\n",
            "Iteration 20, loss = 1.41712264\n",
            "Iteration 21, loss = 1.40235034\n",
            "Iteration 22, loss = 1.38766004\n",
            "Iteration 23, loss = 1.37270070\n",
            "Iteration 24, loss = 1.35842592\n",
            "Iteration 25, loss = 1.34385894\n",
            "Iteration 26, loss = 1.32904983\n",
            "Iteration 27, loss = 1.31546510\n",
            "Iteration 28, loss = 1.30241392\n",
            "Iteration 29, loss = 1.28890910\n",
            "Iteration 30, loss = 1.27666698\n",
            "Iteration 31, loss = 1.26584273\n",
            "Iteration 32, loss = 1.25391811\n",
            "Iteration 33, loss = 1.24389348\n",
            "Iteration 34, loss = 1.23450671\n",
            "Iteration 35, loss = 1.22497120\n",
            "Iteration 36, loss = 1.21637388\n",
            "Iteration 37, loss = 1.20905080\n",
            "Iteration 38, loss = 1.20232525\n",
            "Iteration 39, loss = 1.19487492\n",
            "Iteration 40, loss = 1.18870046\n",
            "Iteration 41, loss = 1.18291366\n",
            "Iteration 42, loss = 1.17764129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 43, loss = 1.17291278\n",
            "Iteration 44, loss = 1.16870359\n",
            "Iteration 45, loss = 1.16424905\n",
            "Iteration 46, loss = 1.15951589\n",
            "Iteration 47, loss = 1.15723193\n",
            "Iteration 48, loss = 1.15227967\n",
            "Iteration 49, loss = 1.15129552\n",
            "Iteration 50, loss = 1.14751914\n",
            "Iteration 51, loss = 1.14379636\n",
            "Iteration 52, loss = 1.14037260\n",
            "Iteration 53, loss = 1.13894507\n",
            "Iteration 54, loss = 1.13582186\n",
            "Iteration 55, loss = 1.13340921\n",
            "Iteration 56, loss = 1.13147280\n",
            "Iteration 57, loss = 1.12913983\n",
            "Iteration 58, loss = 1.12719422\n",
            "Iteration 59, loss = 1.12566550\n",
            "Iteration 60, loss = 1.12320167\n",
            "Iteration 61, loss = 1.12171112\n",
            "Iteration 62, loss = 1.12005116\n",
            "Iteration 63, loss = 1.11857228\n",
            "Iteration 64, loss = 1.11684811\n",
            "Iteration 65, loss = 1.11482254\n",
            "Iteration 66, loss = 1.11334528\n",
            "Iteration 67, loss = 1.11181211\n",
            "Iteration 68, loss = 1.11037751\n",
            "Iteration 69, loss = 1.11035466\n",
            "Iteration 70, loss = 1.10941951\n",
            "Iteration 71, loss = 1.10693896\n",
            "Iteration 72, loss = 1.10599036\n",
            "Iteration 73, loss = 1.10542991\n",
            "Iteration 74, loss = 1.10356039\n",
            "Iteration 75, loss = 1.10234533\n",
            "Iteration 76, loss = 1.10177197\n",
            "Iteration 77, loss = 1.10049038\n",
            "Iteration 78, loss = 1.10123482\n",
            "Iteration 79, loss = 1.09844822\n",
            "Iteration 80, loss = 1.09735071\n",
            "Iteration 81, loss = 1.09600115\n",
            "Iteration 82, loss = 1.09451867\n",
            "Iteration 83, loss = 1.09454347\n",
            "Iteration 84, loss = 1.09208387\n",
            "Iteration 85, loss = 1.09160940\n",
            "Iteration 86, loss = 1.09038020\n",
            "Iteration 87, loss = 1.08906626\n",
            "Iteration 88, loss = 1.08842098\n",
            "Iteration 89, loss = 1.08752656\n",
            "Iteration 90, loss = 1.08646964\n",
            "Iteration 91, loss = 1.08530850\n",
            "Iteration 92, loss = 1.08377206\n",
            "Iteration 93, loss = 1.08336074\n",
            "Iteration 94, loss = 1.08340276\n",
            "Iteration 95, loss = 1.08140568\n",
            "Iteration 96, loss = 1.08180378\n",
            "Iteration 97, loss = 1.08034450\n",
            "Iteration 98, loss = 1.07991723\n",
            "Iteration 99, loss = 1.07877233\n",
            "Iteration 100, loss = 1.07728005\n",
            "Iteration 1, loss = 1.72844301\n",
            "Iteration 2, loss = 1.61252504\n",
            "Iteration 3, loss = 1.53815343\n",
            "Iteration 4, loss = 1.45485692\n",
            "Iteration 5, loss = 1.36471117\n",
            "Iteration 6, loss = 1.28925579\n",
            "Iteration 7, loss = 1.22741544\n",
            "Iteration 8, loss = 1.19381877\n",
            "Iteration 9, loss = 1.15182296\n",
            "Iteration 10, loss = 1.13778039\n",
            "Iteration 11, loss = 1.12800935\n",
            "Iteration 12, loss = 1.11657663\n",
            "Iteration 13, loss = 1.11788979\n",
            "Iteration 14, loss = 1.11098146\n",
            "Iteration 15, loss = 1.11351505\n",
            "Iteration 16, loss = 1.13031716\n",
            "Iteration 17, loss = 1.13129140\n",
            "Iteration 18, loss = 1.12382716\n",
            "Iteration 19, loss = 1.11441489\n",
            "Iteration 20, loss = 1.12879603\n",
            "Iteration 21, loss = 1.12016354\n",
            "Iteration 22, loss = 1.10060599\n",
            "Iteration 23, loss = 1.11326735\n",
            "Iteration 24, loss = 1.10095360\n",
            "Iteration 25, loss = 1.09449266\n",
            "Iteration 26, loss = 1.08780848\n",
            "Iteration 27, loss = 1.08931359\n",
            "Iteration 28, loss = 1.08528364\n",
            "Iteration 29, loss = 1.07601461\n",
            "Iteration 30, loss = 1.07340388\n",
            "Iteration 31, loss = 1.06824345\n",
            "Iteration 32, loss = 1.06853210\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 1.06821221\n",
            "Iteration 34, loss = 1.07130109\n",
            "Iteration 35, loss = 1.05328684\n",
            "Iteration 36, loss = 1.04796084\n",
            "Iteration 37, loss = 1.05386367\n",
            "Iteration 38, loss = 1.03446991\n",
            "Iteration 39, loss = 1.02606878\n",
            "Iteration 40, loss = 1.02143722\n",
            "Iteration 41, loss = 1.00173171\n",
            "Iteration 42, loss = 1.01353174\n",
            "Iteration 43, loss = 1.02294428\n",
            "Iteration 44, loss = 1.01794941\n",
            "Iteration 45, loss = 0.98841582\n",
            "Iteration 46, loss = 1.00863363\n",
            "Iteration 47, loss = 1.05076807\n",
            "Iteration 48, loss = 0.96439954\n",
            "Iteration 49, loss = 0.95684268\n",
            "Iteration 50, loss = 0.96203993\n",
            "Iteration 51, loss = 0.94778546\n",
            "Iteration 52, loss = 0.94699352\n",
            "Iteration 53, loss = 0.92349161\n",
            "Iteration 54, loss = 0.92913708\n",
            "Iteration 55, loss = 0.91210029\n",
            "Iteration 56, loss = 0.94111632\n",
            "Iteration 57, loss = 0.89744197\n",
            "Iteration 58, loss = 0.89697693\n",
            "Iteration 59, loss = 0.93948565\n",
            "Iteration 60, loss = 0.92867419\n",
            "Iteration 61, loss = 0.93720160\n",
            "Iteration 62, loss = 0.90184327\n",
            "Iteration 63, loss = 0.92368927\n",
            "Iteration 64, loss = 0.98632092\n",
            "Iteration 65, loss = 0.88867193\n",
            "Iteration 66, loss = 0.87926095\n",
            "Iteration 67, loss = 0.86631841\n",
            "Iteration 68, loss = 0.87625780\n",
            "Iteration 69, loss = 0.88924660\n",
            "Iteration 70, loss = 0.87627762\n",
            "Iteration 71, loss = 0.85347667\n",
            "Iteration 72, loss = 0.85954475\n",
            "Iteration 73, loss = 0.91278158\n",
            "Iteration 74, loss = 0.91553687\n",
            "Iteration 75, loss = 0.85786127\n",
            "Iteration 76, loss = 0.89962348\n",
            "Iteration 77, loss = 0.88603589\n",
            "Iteration 78, loss = 0.82532446\n",
            "Iteration 79, loss = 0.83711112\n",
            "Iteration 80, loss = 0.83836478\n",
            "Iteration 81, loss = 0.82088171\n",
            "Iteration 82, loss = 0.82529580\n",
            "Iteration 83, loss = 0.83704332\n",
            "Iteration 84, loss = 0.83615974\n",
            "Iteration 85, loss = 0.87733672\n",
            "Iteration 86, loss = 0.79897290\n",
            "Iteration 87, loss = 0.84671066\n",
            "Iteration 88, loss = 0.79778625\n",
            "Iteration 89, loss = 0.80541547\n",
            "Iteration 90, loss = 0.79608635\n",
            "Iteration 91, loss = 0.80593420\n",
            "Iteration 92, loss = 0.80783004\n",
            "Iteration 93, loss = 0.78522250\n",
            "Iteration 94, loss = 0.78825875\n",
            "Iteration 95, loss = 0.78208051\n",
            "Iteration 96, loss = 0.79201543\n",
            "Iteration 97, loss = 0.79094027\n",
            "Iteration 98, loss = 0.82034155\n",
            "Iteration 99, loss = 0.78937658\n",
            "Iteration 100, loss = 0.80636157\n",
            "Iteration 1, loss = 1.60975568\n",
            "Iteration 2, loss = 1.60742259\n",
            "Iteration 3, loss = 1.60509419\n",
            "Iteration 4, loss = 1.60352738\n",
            "Iteration 5, loss = 1.60198791\n",
            "Iteration 6, loss = 1.60070888\n",
            "Iteration 7, loss = 1.59959677\n",
            "Iteration 8, loss = 1.59872908\n",
            "Iteration 9, loss = 1.59840410\n",
            "Iteration 10, loss = 1.59797737\n",
            "Iteration 11, loss = 1.59764393\n",
            "Iteration 12, loss = 1.59742287\n",
            "Iteration 13, loss = 1.59736704\n",
            "Iteration 14, loss = 1.59714482"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 15, loss = 1.59708327\n",
            "Iteration 16, loss = 1.59699814\n",
            "Iteration 17, loss = 1.59687540\n",
            "Iteration 18, loss = 1.59682310\n",
            "Iteration 19, loss = 1.59674483\n",
            "Iteration 20, loss = 1.59673383\n",
            "Iteration 21, loss = 1.59662254\n",
            "Iteration 22, loss = 1.59657359\n",
            "Iteration 23, loss = 1.59647637\n",
            "Iteration 24, loss = 1.59638743\n",
            "Iteration 25, loss = 1.59630802\n",
            "Iteration 26, loss = 1.59621266\n",
            "Iteration 27, loss = 1.59614237\n",
            "Iteration 28, loss = 1.59607476\n",
            "Iteration 29, loss = 1.59594888\n",
            "Iteration 30, loss = 1.59585823\n",
            "Iteration 31, loss = 1.59584478\n",
            "Iteration 32, loss = 1.59566129\n",
            "Iteration 33, loss = 1.59560561\n",
            "Iteration 34, loss = 1.59550779\n",
            "Iteration 35, loss = 1.59542002\n",
            "Iteration 36, loss = 1.59532853\n",
            "Iteration 37, loss = 1.59525032\n",
            "Iteration 38, loss = 1.59523983\n",
            "Iteration 39, loss = 1.59505553\n",
            "Iteration 40, loss = 1.59503166\n",
            "Iteration 41, loss = 1.59488663\n",
            "Iteration 42, loss = 1.59483415\n",
            "Iteration 43, loss = 1.59477626\n",
            "Iteration 44, loss = 1.59467134\n",
            "Iteration 45, loss = 1.59460685\n",
            "Iteration 46, loss = 1.59447018\n",
            "Iteration 47, loss = 1.59448241\n",
            "Iteration 48, loss = 1.59428809\n",
            "Iteration 49, loss = 1.59442216\n",
            "Iteration 50, loss = 1.59422490\n",
            "Iteration 51, loss = 1.59407494\n",
            "Iteration 52, loss = 1.59396340\n",
            "Iteration 53, loss = 1.59390594\n",
            "Iteration 54, loss = 1.59376491\n",
            "Iteration 55, loss = 1.59372474\n",
            "Iteration 56, loss = 1.59360065\n",
            "Iteration 57, loss = 1.59350877\n",
            "Iteration 58, loss = 1.59341912\n",
            "Iteration 59, loss = 1.59336176\n",
            "Iteration 60, loss = 1.59321660\n",
            "Iteration 61, loss = 1.59315220\n",
            "Iteration 62, loss = 1.59305972\n",
            "Iteration 63, loss = 1.59293912\n",
            "Iteration 64, loss = 1.59286605\n",
            "Iteration 65, loss = 1.59273924\n",
            "Iteration 66, loss = 1.59268946\n",
            "Iteration 67, loss = 1.59253449\n",
            "Iteration 68, loss = 1.59248175\n",
            "Iteration 69, loss = 1.59239791\n",
            "Iteration 70, loss = 1.59233955\n",
            "Iteration 71, loss = 1.59214411\n",
            "Iteration 72, loss = 1.59208415\n",
            "Iteration 73, loss = 1.59200447\n",
            "Iteration 74, loss = 1.59189099\n",
            "Iteration 75, loss = 1.59180952\n",
            "Iteration 76, loss = 1.59169112\n",
            "Iteration 77, loss = 1.59161521\n",
            "Iteration 78, loss = 1.59159095\n",
            "Iteration 79, loss = 1.59147343\n",
            "Iteration 80, loss = 1.59123419\n",
            "Iteration 81, loss = 1.59116232\n",
            "Iteration 82, loss = 1.59102526\n",
            "Iteration 83, loss = 1.59090330\n",
            "Iteration 84, loss = 1.59078727\n",
            "Iteration 85, loss = 1.59076573\n",
            "Iteration 86, loss = 1.59061892\n",
            "Iteration 87, loss = 1.59047881\n",
            "Iteration 88, loss = 1.59038262\n",
            "Iteration 89, loss = 1.59024417\n",
            "Iteration 90, loss = 1.59019975\n",
            "Iteration 91, loss = 1.59002760\n",
            "Iteration 92, loss = 1.58990439\n",
            "Iteration 93, loss = 1.58981040\n",
            "Iteration 94, loss = 1.58971332\n",
            "Iteration 95, loss = 1.58959331\n",
            "Iteration 96, loss = 1.58949200\n",
            "Iteration 97, loss = 1.58935648\n",
            "Iteration 98, loss = 1.58930273\n",
            "Iteration 99, loss = 1.58909748\n",
            "Iteration 100, loss = 1.58901677\n",
            "Iteration 1, loss = 1.60549986\n",
            "Iteration 2, loss = 1.60105309\n",
            "Iteration 3, loss = 1.60182450\n",
            "Iteration 4, loss = 1.59990567\n",
            "Iteration 5, loss = 1.59583502\n",
            "Iteration 6, loss = 1.59465396\n",
            "Iteration 7, loss = 1.59496435\n",
            "Iteration 8, loss = 1.59569360\n",
            "Iteration 9, loss = 1.59272761\n",
            "Iteration 10, loss = 1.59117667\n",
            "Iteration 11, loss = 1.59030603\n",
            "Iteration 12, loss = 1.58990871\n",
            "Iteration 13, loss = 1.58894618\n",
            "Iteration 14, loss = 1.58685297\n",
            "Iteration 15, loss = 1.58599363\n",
            "Iteration 16, loss = 1.58459011\n",
            "Iteration 17, loss = 1.58341235\n",
            "Iteration 18, loss = 1.58258003\n",
            "Iteration 19, loss = 1.57960888\n",
            "Iteration 20, loss = 1.57878712\n",
            "Iteration 21, loss = 1.57609834\n",
            "Iteration 22, loss = 1.57450544\n",
            "Iteration 23, loss = 1.57231021\n",
            "Iteration 24, loss = 1.56996215\n",
            "Iteration 25, loss = 1.56729794\n",
            "Iteration 26, loss = 1.56454526\n",
            "Iteration 27, loss = 1.56192778\n",
            "Iteration 28, loss = 1.55953596\n",
            "Iteration 29, loss = 1.55586526\n",
            "Iteration 30, loss = 1.55234495\n",
            "Iteration 31, loss = 1.55023569\n",
            "Iteration 32, loss = 1.54465676\n",
            "Iteration 33, loss = 1.54128124\n",
            "Iteration 34, loss = 1.53673250\n",
            "Iteration 35, loss = 1.53204012\n",
            "Iteration 36, loss = 1.52729958\n",
            "Iteration 37, loss = 1.52235824\n",
            "Iteration 38, loss = 1.51749728\n",
            "Iteration 39, loss = 1.51149221\n",
            "Iteration 40, loss = 1.50663744\n",
            "Iteration 41, loss = 1.50042809\n",
            "Iteration 42, loss = 1.49461318\n",
            "Iteration 43, loss = 1.48812769\n",
            "Iteration 44, loss = 1.48162413\n",
            "Iteration 45, loss = 1.47539966\n",
            "Iteration 46, loss = 1.46792512\n",
            "Iteration 47, loss = 1.46237209\n",
            "Iteration 48, loss = 1.45384944\n",
            "Iteration 49, loss = 1.44884672\n",
            "Iteration 50, loss = 1.43986681\n",
            "Iteration 51, loss = 1.43245108\n",
            "Iteration 52, loss = 1.42445857\n",
            "Iteration 53, loss = 1.41722148\n",
            "Iteration 54, loss = 1.40943776\n",
            "Iteration 55, loss = 1.40234620\n",
            "Iteration 56, loss = 1.39428228\n",
            "Iteration 57, loss = 1.38691255\n",
            "Iteration 58, loss = 1.37906505\n",
            "Iteration 59, loss = 1.37153842\n",
            "Iteration 60, loss = 1.36391724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 61, loss = 1.35676316\n",
            "Iteration 62, loss = 1.34923653\n",
            "Iteration 63, loss = 1.34175725\n",
            "Iteration 64, loss = 1.33473911\n",
            "Iteration 65, loss = 1.32746038\n",
            "Iteration 66, loss = 1.32089306\n",
            "Iteration 67, loss = 1.31391496\n",
            "Iteration 68, loss = 1.30751843\n",
            "Iteration 69, loss = 1.30192987\n",
            "Iteration 70, loss = 1.29629853\n",
            "Iteration 71, loss = 1.28931312\n",
            "Iteration 72, loss = 1.28402584\n",
            "Iteration 73, loss = 1.27899502\n",
            "Iteration 74, loss = 1.27315441\n",
            "Iteration 75, loss = 1.26831123\n",
            "Iteration 76, loss = 1.26315196\n",
            "Iteration 77, loss = 1.25861223\n",
            "Iteration 78, loss = 1.25485182\n",
            "Iteration 79, loss = 1.25032643\n",
            "Iteration 80, loss = 1.24522562\n",
            "Iteration 81, loss = 1.24163844\n",
            "Iteration 82, loss = 1.23726299\n",
            "Iteration 83, loss = 1.23377823\n",
            "Iteration 84, loss = 1.22939775\n",
            "Iteration 85, loss = 1.22613809\n",
            "Iteration 86, loss = 1.22307417\n",
            "Iteration 87, loss = 1.21962963\n",
            "Iteration 88, loss = 1.21677994\n",
            "Iteration 89, loss = 1.21354200\n",
            "Iteration 90, loss = 1.21136834\n",
            "Iteration 91, loss = 1.20790803\n",
            "Iteration 92, loss = 1.20490106\n",
            "Iteration 93, loss = 1.20256967\n",
            "Iteration 94, loss = 1.20051047\n",
            "Iteration 95, loss = 1.19807065\n",
            "Iteration 96, loss = 1.19604238\n",
            "Iteration 97, loss = 1.19358002\n",
            "Iteration 98, loss = 1.19193159\n",
            "Iteration 99, loss = 1.18942496\n",
            "Iteration 100, loss = 1.18749029\n",
            "Iteration 1, loss = 1.76741387\n",
            "Iteration 2, loss = 1.68976925\n",
            "Iteration 3, loss = 1.74179431\n",
            "Iteration 4, loss = 1.65738167\n",
            "Iteration 5, loss = 1.61616693\n",
            "Iteration 6, loss = 1.63043261\n",
            "Iteration 7, loss = 1.61808968\n",
            "Iteration 8, loss = 1.58211543\n",
            "Iteration 9, loss = 1.56855649\n",
            "Iteration 10, loss = 1.57012968\n",
            "Iteration 11, loss = 1.56431542\n",
            "Iteration 12, loss = 1.54927075\n",
            "Iteration 13, loss = 1.52782725\n",
            "Iteration 14, loss = 1.50900090\n",
            "Iteration 15, loss = 1.48578739\n",
            "Iteration 16, loss = 1.46013215\n",
            "Iteration 17, loss = 1.42886234\n",
            "Iteration 18, loss = 1.39209816\n",
            "Iteration 19, loss = 1.35192704\n",
            "Iteration 20, loss = 1.32257227\n",
            "Iteration 21, loss = 1.28854315\n",
            "Iteration 22, loss = 1.26050528\n",
            "Iteration 23, loss = 1.23615596\n",
            "Iteration 24, loss = 1.21417607\n",
            "Iteration 25, loss = 1.19676343\n",
            "Iteration 26, loss = 1.18214529\n",
            "Iteration 27, loss = 1.17011305\n",
            "Iteration 28, loss = 1.16655917\n",
            "Iteration 29, loss = 1.15381730\n",
            "Iteration 30, loss = 1.14319886\n",
            "Iteration 31, loss = 1.14683760\n",
            "Iteration 32, loss = 1.13455106\n",
            "Iteration 33, loss = 1.13367477\n",
            "Iteration 34, loss = 1.13046211\n",
            "Iteration 35, loss = 1.12469484\n",
            "Iteration 36, loss = 1.12251539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 37, loss = 1.12571462\n",
            "Iteration 38, loss = 1.12074210\n",
            "Iteration 39, loss = 1.11979200\n",
            "Iteration 40, loss = 1.12561963\n",
            "Iteration 41, loss = 1.11240256\n",
            "Iteration 42, loss = 1.12069141\n",
            "Iteration 43, loss = 1.12127405\n",
            "Iteration 44, loss = 1.11818574\n",
            "Iteration 45, loss = 1.11154375\n",
            "Iteration 46, loss = 1.11034112\n",
            "Iteration 47, loss = 1.11813181\n",
            "Iteration 48, loss = 1.10843239\n",
            "Iteration 49, loss = 1.13320678\n",
            "Iteration 50, loss = 1.11414835\n",
            "Iteration 51, loss = 1.12223829\n",
            "Iteration 52, loss = 1.11454968\n",
            "Iteration 53, loss = 1.12126452\n",
            "Iteration 54, loss = 1.11553388\n",
            "Iteration 55, loss = 1.11188726\n",
            "Iteration 56, loss = 1.10888095\n",
            "Iteration 57, loss = 1.10757470\n",
            "Iteration 58, loss = 1.10879168\n",
            "Iteration 59, loss = 1.10923218\n",
            "Iteration 60, loss = 1.10479151\n",
            "Iteration 61, loss = 1.10430161\n",
            "Iteration 62, loss = 1.10267529\n",
            "Iteration 63, loss = 1.10483405\n",
            "Iteration 64, loss = 1.09894612\n",
            "Iteration 65, loss = 1.10155798\n",
            "Iteration 66, loss = 1.10140202\n",
            "Iteration 67, loss = 1.09926137\n",
            "Iteration 68, loss = 1.09639318\n",
            "Iteration 69, loss = 1.10275650\n",
            "Iteration 70, loss = 1.10096228\n",
            "Iteration 71, loss = 1.09789233\n",
            "Iteration 72, loss = 1.10071751\n",
            "Iteration 73, loss = 1.10997834\n",
            "Iteration 74, loss = 1.09714320\n",
            "Iteration 75, loss = 1.09591658\n",
            "Iteration 76, loss = 1.09910134\n",
            "Iteration 77, loss = 1.08908574\n",
            "Iteration 78, loss = 1.10021551\n",
            "Iteration 79, loss = 1.08796748\n",
            "Iteration 80, loss = 1.08695618\n",
            "Iteration 81, loss = 1.08510195\n",
            "Iteration 82, loss = 1.08418329\n",
            "Iteration 83, loss = 1.08796844\n",
            "Iteration 84, loss = 1.08075317\n",
            "Iteration 85, loss = 1.08099841\n",
            "Iteration 86, loss = 1.07546545\n",
            "Iteration 87, loss = 1.07508402\n",
            "Iteration 88, loss = 1.07328743\n",
            "Iteration 89, loss = 1.07398727\n",
            "Iteration 90, loss = 1.07372257\n",
            "Iteration 91, loss = 1.06822797\n",
            "Iteration 92, loss = 1.06177774\n",
            "Iteration 93, loss = 1.06008326\n",
            "Iteration 94, loss = 1.06829883\n",
            "Iteration 95, loss = 1.06009748\n",
            "Iteration 96, loss = 1.06181534\n",
            "Iteration 97, loss = 1.05590091\n",
            "Iteration 98, loss = 1.06609539\n",
            "Iteration 99, loss = 1.04470268\n",
            "Iteration 100, loss = 1.07284893\n",
            "Iteration 1, loss = 1.65407171\n",
            "Iteration 2, loss = 1.64671108\n",
            "Iteration 3, loss = 1.64040050\n",
            "Iteration 4, loss = 1.63357686\n",
            "Iteration 5, loss = 1.62817994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.62293745\n",
            "Iteration 7, loss = 1.61817444\n",
            "Iteration 8, loss = 1.61362563\n",
            "Iteration 9, loss = 1.60905721\n",
            "Iteration 10, loss = 1.60512214\n",
            "Iteration 11, loss = 1.60166841\n",
            "Iteration 12, loss = 1.59853870\n",
            "Iteration 13, loss = 1.59555016\n",
            "Iteration 14, loss = 1.59295674\n",
            "Iteration 15, loss = 1.59061329\n",
            "Iteration 16, loss = 1.58872217\n",
            "Iteration 17, loss = 1.58694913\n",
            "Iteration 18, loss = 1.58534973\n",
            "Iteration 19, loss = 1.58385155\n",
            "Iteration 20, loss = 1.58229454\n",
            "Iteration 21, loss = 1.58121651\n",
            "Iteration 22, loss = 1.58018683\n",
            "Iteration 23, loss = 1.57923703\n",
            "Iteration 24, loss = 1.57815340\n",
            "Iteration 25, loss = 1.57740364\n",
            "Iteration 26, loss = 1.57655757\n",
            "Iteration 27, loss = 1.57578731\n",
            "Iteration 28, loss = 1.57516457\n",
            "Iteration 29, loss = 1.57446450\n",
            "Iteration 30, loss = 1.57378165\n",
            "Iteration 31, loss = 1.57315770\n",
            "Iteration 32, loss = 1.57242885\n",
            "Iteration 33, loss = 1.57175686\n",
            "Iteration 34, loss = 1.57107100\n",
            "Iteration 35, loss = 1.57042975\n",
            "Iteration 36, loss = 1.56975573\n",
            "Iteration 37, loss = 1.56906645\n",
            "Iteration 38, loss = 1.56844402\n",
            "Iteration 39, loss = 1.56769724\n",
            "Iteration 40, loss = 1.56702595\n",
            "Iteration 41, loss = 1.56632420\n",
            "Iteration 42, loss = 1.56564278\n",
            "Iteration 43, loss = 1.56494405\n",
            "Iteration 44, loss = 1.56421725\n",
            "Iteration 45, loss = 1.56353835\n",
            "Iteration 46, loss = 1.56275093\n",
            "Iteration 47, loss = 1.56209631\n",
            "Iteration 48, loss = 1.56128143\n",
            "Iteration 49, loss = 1.56067209\n",
            "Iteration 50, loss = 1.55984959\n",
            "Iteration 51, loss = 1.55909538\n",
            "Iteration 52, loss = 1.55831003\n",
            "Iteration 53, loss = 1.55755823\n",
            "Iteration 54, loss = 1.55675554\n",
            "Iteration 55, loss = 1.55602615\n",
            "Iteration 56, loss = 1.55519003\n",
            "Iteration 57, loss = 1.55441919\n",
            "Iteration 58, loss = 1.55361461\n",
            "Iteration 59, loss = 1.55278910\n",
            "Iteration 60, loss = 1.55197603\n",
            "Iteration 61, loss = 1.55117286\n",
            "Iteration 62, loss = 1.55030611\n",
            "Iteration 63, loss = 1.54945138\n",
            "Iteration 64, loss = 1.54859788\n",
            "Iteration 65, loss = 1.54772531\n",
            "Iteration 66, loss = 1.54687432\n",
            "Iteration 67, loss = 1.54596844\n",
            "Iteration 68, loss = 1.54508459\n",
            "Iteration 69, loss = 1.54420067\n",
            "Iteration 70, loss = 1.54335806\n",
            "Iteration 71, loss = 1.54235964\n",
            "Iteration 72, loss = 1.54143439\n",
            "Iteration 73, loss = 1.54053440\n",
            "Iteration 74, loss = 1.53960037\n",
            "Iteration 75, loss = 1.53861666\n",
            "Iteration 76, loss = 1.53765769\n",
            "Iteration 77, loss = 1.53672011\n",
            "Iteration 78, loss = 1.53581075\n",
            "Iteration 79, loss = 1.53481894\n",
            "Iteration 80, loss = 1.53370671\n",
            "Iteration 81, loss = 1.53272540\n",
            "Iteration 82, loss = 1.53170101\n",
            "Iteration 83, loss = 1.53068706\n",
            "Iteration 84, loss = 1.52967544\n",
            "Iteration 85, loss = 1.52864762\n",
            "Iteration 86, loss = 1.52761420\n",
            "Iteration 87, loss = 1.52650258\n",
            "Iteration 88, loss = 1.52546477\n",
            "Iteration 89, loss = 1.52436494\n",
            "Iteration 90, loss = 1.52331430\n",
            "Iteration 91, loss = 1.52217385\n",
            "Iteration 92, loss = 1.52103846\n",
            "Iteration 93, loss = 1.51998319\n",
            "Iteration 94, loss = 1.51883406\n",
            "Iteration 95, loss = 1.51772461\n",
            "Iteration 96, loss = 1.51653667\n",
            "Iteration 97, loss = 1.51539505\n",
            "Iteration 98, loss = 1.51427436\n",
            "Iteration 99, loss = 1.51303774\n",
            "Iteration 100, loss = 1.51187823\n",
            "Iteration 1, loss = 1.64271859\n",
            "Iteration 2, loss = 1.59405143\n",
            "Iteration 3, loss = 1.58205240\n",
            "Iteration 4, loss = 1.57969115\n",
            "Iteration 5, loss = 1.57798366\n",
            "Iteration 6, loss = 1.57146641\n",
            "Iteration 7, loss = 1.56116622\n",
            "Iteration 8, loss = 1.55193584\n",
            "Iteration 9, loss = 1.54297597\n",
            "Iteration 10, loss = 1.53549996\n",
            "Iteration 11, loss = 1.52789601\n",
            "Iteration 12, loss = 1.52021473\n",
            "Iteration 13, loss = 1.50986754\n",
            "Iteration 14, loss = 1.49809050\n",
            "Iteration 15, loss = 1.48624721\n",
            "Iteration 16, loss = 1.47447551\n",
            "Iteration 17, loss = 1.46241377\n",
            "Iteration 18, loss = 1.45012204\n",
            "Iteration 19, loss = 1.43525357\n",
            "Iteration 20, loss = 1.42144536\n",
            "Iteration 21, loss = 1.40663530\n",
            "Iteration 22, loss = 1.39174124\n",
            "Iteration 23, loss = 1.37643754\n",
            "Iteration 24, loss = 1.36174461\n",
            "Iteration 25, loss = 1.34674279\n",
            "Iteration 26, loss = 1.33152342\n",
            "Iteration 27, loss = 1.31749970\n",
            "Iteration 28, loss = 1.30389022\n",
            "Iteration 29, loss = 1.29010332\n",
            "Iteration 30, loss = 1.27732739\n",
            "Iteration 31, loss = 1.26598212\n",
            "Iteration 32, loss = 1.25392263\n",
            "Iteration 33, loss = 1.24351518\n",
            "Iteration 34, loss = 1.23371116\n",
            "Iteration 35, loss = 1.22414230\n",
            "Iteration 36, loss = 1.21537556\n",
            "Iteration 37, loss = 1.20779845\n",
            "Iteration 38, loss = 1.20085602\n",
            "Iteration 39, loss = 1.19336849\n",
            "Iteration 40, loss = 1.18704265\n",
            "Iteration 41, loss = 1.18127082\n",
            "Iteration 42, loss = 1.17595943\n",
            "Iteration 43, loss = 1.17107333\n",
            "Iteration 44, loss = 1.16689741\n",
            "Iteration 45, loss = 1.16246779\n",
            "Iteration 46, loss = 1.15776753\n",
            "Iteration 47, loss = 1.15520885\n",
            "Iteration 48, loss = 1.15053562\n",
            "Iteration 49, loss = 1.14911652\n",
            "Iteration 50, loss = 1.14579007\n",
            "Iteration 51, loss = 1.14208127\n",
            "Iteration 52, loss = 1.13881451\n",
            "Iteration 53, loss = 1.13731016\n",
            "Iteration 54, loss = 1.13426637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 1.13178747\n",
            "Iteration 56, loss = 1.12997076\n",
            "Iteration 57, loss = 1.12763659\n",
            "Iteration 58, loss = 1.12579099\n",
            "Iteration 59, loss = 1.12416808\n",
            "Iteration 60, loss = 1.12180449\n",
            "Iteration 61, loss = 1.12025953\n",
            "Iteration 62, loss = 1.11864430\n",
            "Iteration 63, loss = 1.11724778\n",
            "Iteration 64, loss = 1.11550825\n",
            "Iteration 65, loss = 1.11356847\n",
            "Iteration 66, loss = 1.11203988\n",
            "Iteration 67, loss = 1.11056776\n",
            "Iteration 68, loss = 1.10913688\n",
            "Iteration 69, loss = 1.10899879\n",
            "Iteration 70, loss = 1.10808460\n",
            "Iteration 71, loss = 1.10564778\n",
            "Iteration 72, loss = 1.10455879\n",
            "Iteration 73, loss = 1.10381292\n",
            "Iteration 74, loss = 1.10207400\n",
            "Iteration 75, loss = 1.10079544\n",
            "Iteration 76, loss = 1.10027298\n",
            "Iteration 77, loss = 1.09882768\n",
            "Iteration 78, loss = 1.09942778\n",
            "Iteration 79, loss = 1.09672103\n",
            "Iteration 80, loss = 1.09564662\n",
            "Iteration 81, loss = 1.09424095\n",
            "Iteration 82, loss = 1.09280658\n",
            "Iteration 83, loss = 1.09278700\n",
            "Iteration 84, loss = 1.09028844\n",
            "Iteration 85, loss = 1.08969901\n",
            "Iteration 86, loss = 1.08840531\n",
            "Iteration 87, loss = 1.08709513\n",
            "Iteration 88, loss = 1.08637272\n",
            "Iteration 89, loss = 1.08536490\n",
            "Iteration 90, loss = 1.08424959\n",
            "Iteration 91, loss = 1.08306536\n",
            "Iteration 92, loss = 1.08146739\n",
            "Iteration 93, loss = 1.08092034\n",
            "Iteration 94, loss = 1.08071521\n",
            "Iteration 95, loss = 1.07876389\n",
            "Iteration 96, loss = 1.07895096\n",
            "Iteration 97, loss = 1.07736976\n",
            "Iteration 98, loss = 1.07696042\n",
            "Iteration 99, loss = 1.07558786\n",
            "Iteration 100, loss = 1.07390641\n",
            "Iteration 1, loss = 1.71228171\n",
            "Iteration 2, loss = 1.60571845\n",
            "Iteration 3, loss = 1.53539935\n",
            "Iteration 4, loss = 1.44767176\n",
            "Iteration 5, loss = 1.35746573\n",
            "Iteration 6, loss = 1.28371754\n",
            "Iteration 7, loss = 1.21858143\n",
            "Iteration 8, loss = 1.18980543\n",
            "Iteration 9, loss = 1.15258718\n",
            "Iteration 10, loss = 1.13613307\n",
            "Iteration 11, loss = 1.12844163\n",
            "Iteration 12, loss = 1.11658278\n",
            "Iteration 13, loss = 1.11835070\n",
            "Iteration 14, loss = 1.10897264\n",
            "Iteration 15, loss = 1.11305808\n",
            "Iteration 16, loss = 1.13280998\n",
            "Iteration 17, loss = 1.12157459\n",
            "Iteration 18, loss = 1.11727355\n",
            "Iteration 19, loss = 1.11467125\n",
            "Iteration 20, loss = 1.12996719\n",
            "Iteration 21, loss = 1.11000035\n",
            "Iteration 22, loss = 1.09958242\n",
            "Iteration 23, loss = 1.10675336\n",
            "Iteration 24, loss = 1.09983126\n",
            "Iteration 25, loss = 1.08340507\n",
            "Iteration 26, loss = 1.08642068\n",
            "Iteration 27, loss = 1.08442582\n",
            "Iteration 28, loss = 1.06470829\n",
            "Iteration 29, loss = 1.07217281\n",
            "Iteration 30, loss = 1.06548493\n",
            "Iteration 31, loss = 1.05342062\n",
            "Iteration 32, loss = 1.05179787\n",
            "Iteration 33, loss = 1.05893648\n",
            "Iteration 34, loss = 1.06228758\n",
            "Iteration 35, loss = 1.04901869\n",
            "Iteration 36, loss = 1.03988598\n",
            "Iteration 37, loss = 1.04606404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 1.03746817\n",
            "Iteration 39, loss = 1.02077204\n",
            "Iteration 40, loss = 1.01166680\n",
            "Iteration 41, loss = 0.99900447\n",
            "Iteration 42, loss = 1.01016709\n",
            "Iteration 43, loss = 1.01664026\n",
            "Iteration 44, loss = 0.98323281\n",
            "Iteration 45, loss = 1.00150879\n",
            "Iteration 46, loss = 0.98436731\n",
            "Iteration 47, loss = 0.97538367\n",
            "Iteration 48, loss = 0.94294664\n",
            "Iteration 49, loss = 0.99170482\n",
            "Iteration 50, loss = 0.95854351\n",
            "Iteration 51, loss = 0.94254098\n",
            "Iteration 52, loss = 0.94985464\n",
            "Iteration 53, loss = 0.92446237\n",
            "Iteration 54, loss = 0.94756702\n",
            "Iteration 55, loss = 0.91909167\n",
            "Iteration 56, loss = 0.96844972\n",
            "Iteration 57, loss = 0.90728552\n",
            "Iteration 58, loss = 0.92528496\n",
            "Iteration 59, loss = 0.94131653\n",
            "Iteration 60, loss = 0.94829875\n",
            "Iteration 61, loss = 0.96747302\n",
            "Iteration 62, loss = 0.98304144\n",
            "Iteration 63, loss = 0.94785242\n",
            "Iteration 64, loss = 0.87493273\n",
            "Iteration 65, loss = 0.92712696\n",
            "Iteration 66, loss = 0.89833444\n",
            "Iteration 67, loss = 0.88645728\n",
            "Iteration 68, loss = 0.85761377\n",
            "Iteration 69, loss = 0.85989435\n",
            "Iteration 70, loss = 0.87418372\n",
            "Iteration 71, loss = 0.85488973\n",
            "Iteration 72, loss = 0.83336211\n",
            "Iteration 73, loss = 0.85654408\n",
            "Iteration 74, loss = 0.85332595\n",
            "Iteration 75, loss = 0.82889256\n",
            "Iteration 76, loss = 0.85173208\n",
            "Iteration 77, loss = 0.84900405\n",
            "Iteration 78, loss = 0.88612079\n",
            "Iteration 79, loss = 0.81940956\n",
            "Iteration 80, loss = 0.81019333\n",
            "Iteration 81, loss = 0.82735711\n",
            "Iteration 82, loss = 0.80848429\n",
            "Iteration 83, loss = 0.81372558\n",
            "Iteration 84, loss = 0.80997849\n",
            "Iteration 85, loss = 0.86826176\n",
            "Iteration 86, loss = 0.80446675\n",
            "Iteration 87, loss = 0.80806416\n",
            "Iteration 88, loss = 0.81012013\n",
            "Iteration 89, loss = 0.79456731\n",
            "Iteration 90, loss = 0.79868352\n",
            "Iteration 91, loss = 0.82655171\n",
            "Iteration 92, loss = 0.83612080\n",
            "Iteration 93, loss = 0.79200352\n",
            "Iteration 94, loss = 0.78392819\n",
            "Iteration 95, loss = 0.77861674\n",
            "Iteration 96, loss = 0.77585114\n",
            "Iteration 97, loss = 0.78854027\n",
            "Iteration 98, loss = 0.81146250\n",
            "Iteration 99, loss = 0.83171184\n",
            "Iteration 100, loss = 0.80321365\n",
            "Iteration 1, loss = 1.60344068\n",
            "Iteration 2, loss = 1.60228602\n",
            "Iteration 3, loss = 1.60138475\n",
            "Iteration 4, loss = 1.60026243\n",
            "Iteration 5, loss = 1.59951184\n",
            "Iteration 6, loss = 1.59870047\n",
            "Iteration 7, loss = 1.59797899\n",
            "Iteration 8, loss = 1.59727846\n",
            "Iteration 9, loss = 1.59647582\n",
            "Iteration 10, loss = 1.59580537\n",
            "Iteration 11, loss = 1.59518914\n",
            "Iteration 12, loss = 1.59465189\n",
            "Iteration 13, loss = 1.59401495\n",
            "Iteration 14, loss = 1.59345992\n",
            "Iteration 15, loss = 1.59294432\n",
            "Iteration 16, loss = 1.59246973\n",
            "Iteration 17, loss = 1.59200343\n",
            "Iteration 18, loss = 1.59156652\n",
            "Iteration 19, loss = 1.59109272\n",
            "Iteration 20, loss = 1.59062245\n",
            "Iteration 21, loss = 1.59019126\n",
            "Iteration 22, loss = 1.58979734\n",
            "Iteration 23, loss = 1.58941667\n",
            "Iteration 24, loss = 1.58899673\n",
            "Iteration 25, loss = 1.58865263\n",
            "Iteration 26, loss = 1.58823093\n",
            "Iteration 27, loss = 1.58788947\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 1.58754924\n",
            "Iteration 29, loss = 1.58720307\n",
            "Iteration 30, loss = 1.58684304\n",
            "Iteration 31, loss = 1.58657297\n",
            "Iteration 32, loss = 1.58616349\n",
            "Iteration 33, loss = 1.58581297\n",
            "Iteration 34, loss = 1.58549694\n",
            "Iteration 35, loss = 1.58517479\n",
            "Iteration 36, loss = 1.58483325\n",
            "Iteration 37, loss = 1.58450153\n",
            "Iteration 38, loss = 1.58420717\n",
            "Iteration 39, loss = 1.58385064\n",
            "Iteration 40, loss = 1.58351967\n",
            "Iteration 41, loss = 1.58318639\n",
            "Iteration 42, loss = 1.58287797\n",
            "Iteration 43, loss = 1.58253913\n",
            "Iteration 44, loss = 1.58221991\n",
            "Iteration 45, loss = 1.58187433\n",
            "Iteration 46, loss = 1.58151784\n",
            "Iteration 47, loss = 1.58125127\n",
            "Iteration 48, loss = 1.58084763\n",
            "Iteration 49, loss = 1.58054499\n",
            "Iteration 50, loss = 1.58019059\n",
            "Iteration 51, loss = 1.57983472\n",
            "Iteration 52, loss = 1.57946395\n",
            "Iteration 53, loss = 1.57911361\n",
            "Iteration 54, loss = 1.57875800\n",
            "Iteration 55, loss = 1.57842500\n",
            "Iteration 56, loss = 1.57803788\n",
            "Iteration 57, loss = 1.57767597\n",
            "Iteration 58, loss = 1.57730260\n",
            "Iteration 59, loss = 1.57692143\n",
            "Iteration 60, loss = 1.57654128\n",
            "Iteration 61, loss = 1.57617071\n",
            "Iteration 62, loss = 1.57576550\n",
            "Iteration 63, loss = 1.57536278\n",
            "Iteration 64, loss = 1.57495764\n",
            "Iteration 65, loss = 1.57455279\n",
            "Iteration 66, loss = 1.57415295\n",
            "Iteration 67, loss = 1.57372302\n",
            "Iteration 68, loss = 1.57330937\n",
            "Iteration 69, loss = 1.57288929\n",
            "Iteration 70, loss = 1.57250060\n",
            "Iteration 71, loss = 1.57201596\n",
            "Iteration 72, loss = 1.57158145\n",
            "Iteration 73, loss = 1.57115472\n",
            "Iteration 74, loss = 1.57070712\n",
            "Iteration 75, loss = 1.57023074\n",
            "Iteration 76, loss = 1.56977498\n",
            "Iteration 77, loss = 1.56933204\n",
            "Iteration 78, loss = 1.56889798\n",
            "Iteration 79, loss = 1.56841612\n",
            "Iteration 80, loss = 1.56787857\n",
            "Iteration 81, loss = 1.56740346\n",
            "Iteration 82, loss = 1.56691088\n",
            "Iteration 83, loss = 1.56642184\n",
            "Iteration 84, loss = 1.56593150\n",
            "Iteration 85, loss = 1.56543419\n",
            "Iteration 86, loss = 1.56493267\n",
            "Iteration 87, loss = 1.56438627\n",
            "Iteration 88, loss = 1.56387729\n",
            "Iteration 89, loss = 1.56333926\n",
            "Iteration 90, loss = 1.56282176\n",
            "Iteration 91, loss = 1.56226157\n",
            "Iteration 92, loss = 1.56170002\n",
            "Iteration 93, loss = 1.56118202\n",
            "Iteration 94, loss = 1.56060861\n",
            "Iteration 95, loss = 1.56005474\n",
            "Iteration 96, loss = 1.55946667\n",
            "Iteration 97, loss = 1.55889757\n",
            "Iteration 98, loss = 1.55832996\n",
            "Iteration 99, loss = 1.55771837\n",
            "Iteration 100, loss = 1.55713252\n",
            "Iteration 1, loss = 1.60187400\n",
            "Iteration 2, loss = 1.59348654\n",
            "Iteration 3, loss = 1.59002603\n",
            "Iteration 4, loss = 1.58583568\n",
            "Iteration 5, loss = 1.58313941\n",
            "Iteration 6, loss = 1.58041665\n",
            "Iteration 7, loss = 1.57729660\n",
            "Iteration 8, loss = 1.57444987\n",
            "Iteration 9, loss = 1.56986709\n",
            "Iteration 10, loss = 1.56554772\n",
            "Iteration 11, loss = 1.56069178\n",
            "Iteration 12, loss = 1.55621883\n",
            "Iteration 13, loss = 1.55036022\n",
            "Iteration 14, loss = 1.54456733\n",
            "Iteration 15, loss = 1.53864966\n",
            "Iteration 16, loss = 1.53167820\n",
            "Iteration 17, loss = 1.52447261\n",
            "Iteration 18, loss = 1.51701049\n",
            "Iteration 19, loss = 1.50814901\n",
            "Iteration 20, loss = 1.49960132\n",
            "Iteration 21, loss = 1.49028410\n",
            "Iteration 22, loss = 1.48070503\n",
            "Iteration 23, loss = 1.47038385\n",
            "Iteration 24, loss = 1.45970305\n",
            "Iteration 25, loss = 1.44849154\n",
            "Iteration 26, loss = 1.43682970\n",
            "Iteration 27, loss = 1.42556301\n",
            "Iteration 28, loss = 1.41389019\n",
            "Iteration 29, loss = 1.40168185\n",
            "Iteration 30, loss = 1.38941893\n",
            "Iteration 31, loss = 1.37779507\n",
            "Iteration 32, loss = 1.36548659\n",
            "Iteration 33, loss = 1.35402292\n",
            "Iteration 34, loss = 1.34194701\n",
            "Iteration 35, loss = 1.33051329\n",
            "Iteration 36, loss = 1.31969486\n",
            "Iteration 37, loss = 1.30905625\n",
            "Iteration 38, loss = 1.29897726\n",
            "Iteration 39, loss = 1.28869837\n",
            "Iteration 40, loss = 1.27928993\n",
            "Iteration 41, loss = 1.27061634\n",
            "Iteration 42, loss = 1.26226170\n",
            "Iteration 43, loss = 1.25390092\n",
            "Iteration 44, loss = 1.24677664\n",
            "Iteration 45, loss = 1.23935557\n",
            "Iteration 46, loss = 1.23231808\n",
            "Iteration 47, loss = 1.22659714\n",
            "Iteration 48, loss = 1.22002447\n",
            "Iteration 49, loss = 1.21536255\n",
            "Iteration 50, loss = 1.21005377\n",
            "Iteration 51, loss = 1.20469139\n",
            "Iteration 52, loss = 1.19984367\n",
            "Iteration 53, loss = 1.19591359\n",
            "Iteration 54, loss = 1.19139301\n",
            "Iteration 55, loss = 1.18762462\n",
            "Iteration 56, loss = 1.18403383\n",
            "Iteration 57, loss = 1.18045929\n",
            "Iteration 58, loss = 1.17738278\n",
            "Iteration 59, loss = 1.17424814\n",
            "Iteration 60, loss = 1.17115672\n",
            "Iteration 61, loss = 1.16842566\n",
            "Iteration 62, loss = 1.16565423\n",
            "Iteration 63, loss = 1.16327648\n",
            "Iteration 64, loss = 1.16083969\n",
            "Iteration 65, loss = 1.15826586\n",
            "Iteration 66, loss = 1.15613086\n",
            "Iteration 67, loss = 1.15393397\n",
            "Iteration 68, loss = 1.15190644\n",
            "Iteration 69, loss = 1.15081861\n",
            "Iteration 70, loss = 1.14914918\n",
            "Iteration 71, loss = 1.14645643\n",
            "Iteration 72, loss = 1.14491343\n",
            "Iteration 73, loss = 1.14369172\n",
            "Iteration 74, loss = 1.14168533\n",
            "Iteration 75, loss = 1.14034608\n",
            "Iteration 76, loss = 1.13920716\n",
            "Iteration 77, loss = 1.13751865\n",
            "Iteration 78, loss = 1.13701905\n",
            "Iteration 79, loss = 1.13541463\n",
            "Iteration 80, loss = 1.13394291\n",
            "Iteration 81, loss = 1.13264195\n",
            "Iteration 82, loss = 1.13134507\n",
            "Iteration 83, loss = 1.13083692\n",
            "Iteration 84, loss = 1.12897255\n",
            "Iteration 85, loss = 1.12810388\n",
            "Iteration 86, loss = 1.12709758\n",
            "Iteration 87, loss = 1.12596586\n",
            "Iteration 88, loss = 1.12521795\n",
            "Iteration 89, loss = 1.12446870\n",
            "Iteration 90, loss = 1.12360725\n",
            "Iteration 91, loss = 1.12271944\n",
            "Iteration 92, loss = 1.12156210\n",
            "Iteration 93, loss = 1.12089561\n",
            "Iteration 94, loss = 1.12076680\n",
            "Iteration 95, loss = 1.11970860\n",
            "Iteration 96, loss = 1.11965281\n",
            "Iteration 97, loss = 1.11837397\n",
            "Iteration 98, loss = 1.11828699\n",
            "Iteration 99, loss = 1.11742841\n",
            "Iteration 100, loss = 1.11630725\n",
            "Iteration 1, loss = 1.61099503\n",
            "Iteration 2, loss = 1.56884206\n",
            "Iteration 3, loss = 1.53276138\n",
            "Iteration 4, loss = 1.47658822\n",
            "Iteration 5, loss = 1.39991452\n",
            "Iteration 6, loss = 1.32267600\n",
            "Iteration 7, loss = 1.25410999\n",
            "Iteration 8, loss = 1.21387915\n",
            "Iteration 9, loss = 1.17262509\n",
            "Iteration 10, loss = 1.15325799\n",
            "Iteration 11, loss = 1.13548849\n",
            "Iteration 12, loss = 1.12880460\n",
            "Iteration 13, loss = 1.12849850\n",
            "Iteration 14, loss = 1.11593159\n",
            "Iteration 15, loss = 1.12091302\n",
            "Iteration 16, loss = 1.13583191\n",
            "Iteration 17, loss = 1.11433097\n",
            "Iteration 18, loss = 1.12519191\n",
            "Iteration 19, loss = 1.12240074\n",
            "Iteration 20, loss = 1.12451886\n",
            "Iteration 21, loss = 1.11931493\n",
            "Iteration 22, loss = 1.11365343\n",
            "Iteration 23, loss = 1.10627981\n",
            "Iteration 24, loss = 1.11390497\n",
            "Iteration 25, loss = 1.10581660\n",
            "Iteration 26, loss = 1.10823541\n",
            "Iteration 27, loss = 1.10712836\n",
            "Iteration 28, loss = 1.10881003\n",
            "Iteration 29, loss = 1.10173953\n",
            "Iteration 30, loss = 1.10007730\n",
            "Iteration 31, loss = 1.10207278\n",
            "Iteration 32, loss = 1.09586664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 33, loss = 1.09550997\n",
            "Iteration 34, loss = 1.09521416\n",
            "Iteration 35, loss = 1.09313618\n",
            "Iteration 36, loss = 1.09158629\n",
            "Iteration 37, loss = 1.09151415\n",
            "Iteration 38, loss = 1.08598987\n",
            "Iteration 39, loss = 1.08528024\n",
            "Iteration 40, loss = 1.08732489\n",
            "Iteration 41, loss = 1.08219087\n",
            "Iteration 42, loss = 1.08092134\n",
            "Iteration 43, loss = 1.08146537\n",
            "Iteration 44, loss = 1.08190507\n",
            "Iteration 45, loss = 1.07923901\n",
            "Iteration 46, loss = 1.07308631\n",
            "Iteration 47, loss = 1.08617089\n",
            "Iteration 48, loss = 1.06704998\n",
            "Iteration 49, loss = 1.08457768\n",
            "Iteration 50, loss = 1.06783843\n",
            "Iteration 51, loss = 1.07938520\n",
            "Iteration 52, loss = 1.06186788\n",
            "Iteration 53, loss = 1.06441756\n",
            "Iteration 54, loss = 1.05992086\n",
            "Iteration 55, loss = 1.06145816\n",
            "Iteration 56, loss = 1.04404711\n",
            "Iteration 57, loss = 1.04086847\n",
            "Iteration 58, loss = 1.03685738\n",
            "Iteration 59, loss = 1.03458599\n",
            "Iteration 60, loss = 1.03337352\n",
            "Iteration 61, loss = 1.02925668\n",
            "Iteration 62, loss = 1.01511618\n",
            "Iteration 63, loss = 1.01740534\n",
            "Iteration 64, loss = 1.01183055\n",
            "Iteration 65, loss = 1.00415787\n",
            "Iteration 66, loss = 0.99709567\n",
            "Iteration 67, loss = 0.99401622\n",
            "Iteration 68, loss = 0.98226070\n",
            "Iteration 69, loss = 0.98242561\n",
            "Iteration 70, loss = 0.99010255\n",
            "Iteration 71, loss = 0.96712134\n",
            "Iteration 72, loss = 0.97221708\n",
            "Iteration 73, loss = 0.96765570\n",
            "Iteration 74, loss = 0.96775507\n",
            "Iteration 75, loss = 0.95370090\n",
            "Iteration 76, loss = 0.96844319\n",
            "Iteration 77, loss = 0.94872935\n",
            "Iteration 78, loss = 0.92569986\n",
            "Iteration 79, loss = 0.93791990\n",
            "Iteration 80, loss = 0.93294704\n",
            "Iteration 81, loss = 0.91116733\n",
            "Iteration 82, loss = 0.91364465\n",
            "Iteration 83, loss = 0.92211987\n",
            "Iteration 84, loss = 0.91552016\n",
            "Iteration 85, loss = 0.95168190\n",
            "Iteration 86, loss = 0.89270944\n",
            "Iteration 87, loss = 0.91084643\n",
            "Iteration 88, loss = 0.88753083\n",
            "Iteration 89, loss = 0.89190901\n",
            "Iteration 90, loss = 0.89821496\n",
            "Iteration 91, loss = 0.91711931\n",
            "Iteration 92, loss = 0.90037073\n",
            "Iteration 93, loss = 0.88018331\n",
            "Iteration 94, loss = 0.88372348\n",
            "Iteration 95, loss = 0.89213844\n",
            "Iteration 96, loss = 0.90660804\n",
            "Iteration 97, loss = 0.87908607\n",
            "Iteration 98, loss = 0.88355205\n",
            "Iteration 99, loss = 0.88094291\n",
            "Iteration 100, loss = 0.90994726\n",
            "Iteration 1, loss = 1.67076912\n",
            "Iteration 2, loss = 1.66371096\n",
            "Iteration 3, loss = 1.65731727\n",
            "Iteration 4, loss = 1.65083985\n",
            "Iteration 5, loss = 1.64563514\n",
            "Iteration 6, loss = 1.64088268\n",
            "Iteration 7, loss = 1.63598870\n",
            "Iteration 8, loss = 1.63180053\n",
            "Iteration 9, loss = 1.62800634\n",
            "Iteration 10, loss = 1.62411927\n",
            "Iteration 11, loss = 1.62156443\n",
            "Iteration 12, loss = 1.61849400\n",
            "Iteration 13, loss = 1.61607772\n",
            "Iteration 14, loss = 1.61363345\n",
            "Iteration 15, loss = 1.61122466\n",
            "Iteration 16, loss = 1.60946695\n",
            "Iteration 17, loss = 1.60786222\n",
            "Iteration 18, loss = 1.60617105\n",
            "Iteration 19, loss = 1.60485696\n",
            "Iteration 20, loss = 1.60360204\n",
            "Iteration 21, loss = 1.60224958\n",
            "Iteration 22, loss = 1.60129057\n",
            "Iteration 23, loss = 1.60022250\n",
            "Iteration 24, loss = 1.59944024\n",
            "Iteration 25, loss = 1.59844628\n",
            "Iteration 26, loss = 1.59771235\n",
            "Iteration 27, loss = 1.59686167\n",
            "Iteration 28, loss = 1.59610440\n",
            "Iteration 29, loss = 1.59533069\n",
            "Iteration 30, loss = 1.59466149\n",
            "Iteration 31, loss = 1.59392562\n",
            "Iteration 32, loss = 1.59328526\n",
            "Iteration 33, loss = 1.59253264\n",
            "Iteration 34, loss = 1.59189663\n",
            "Iteration 35, loss = 1.59123603\n",
            "Iteration 36, loss = 1.59052172\n",
            "Iteration 37, loss = 1.58985053\n",
            "Iteration 38, loss = 1.58914051\n",
            "Iteration 39, loss = 1.58851310\n",
            "Iteration 40, loss = 1.58780811\n",
            "Iteration 41, loss = 1.58714031\n",
            "Iteration 42, loss = 1.58642092\n",
            "Iteration 43, loss = 1.58576423\n",
            "Iteration 44, loss = 1.58505155\n",
            "Iteration 45, loss = 1.58435302\n",
            "Iteration 46, loss = 1.58365572\n",
            "Iteration 47, loss = 1.58297674\n",
            "Iteration 48, loss = 1.58222662\n",
            "Iteration 49, loss = 1.58154545\n",
            "Iteration 50, loss = 1.58082109\n",
            "Iteration 51, loss = 1.58009161\n",
            "Iteration 52, loss = 1.57941091\n",
            "Iteration 53, loss = 1.57868242\n",
            "Iteration 54, loss = 1.57797587\n",
            "Iteration 55, loss = 1.57727329\n",
            "Iteration 56, loss = 1.57647159\n",
            "Iteration 57, loss = 1.57570733\n",
            "Iteration 58, loss = 1.57499934\n",
            "Iteration 59, loss = 1.57424821\n",
            "Iteration 60, loss = 1.57347985\n",
            "Iteration 61, loss = 1.57270447\n",
            "Iteration 62, loss = 1.57194406\n",
            "Iteration 63, loss = 1.57117680\n",
            "Iteration 64, loss = 1.57039848\n",
            "Iteration 65, loss = 1.56959549\n",
            "Iteration 66, loss = 1.56880017\n",
            "Iteration 67, loss = 1.56800995\n",
            "Iteration 68, loss = 1.56720663\n",
            "Iteration 69, loss = 1.56636389\n",
            "Iteration 70, loss = 1.56557515\n",
            "Iteration 71, loss = 1.56475390\n",
            "Iteration 72, loss = 1.56389452\n",
            "Iteration 73, loss = 1.56307683\n",
            "Iteration 74, loss = 1.56225644\n",
            "Iteration 75, loss = 1.56138374\n",
            "Iteration 76, loss = 1.56054326\n",
            "Iteration 77, loss = 1.55969360\n",
            "Iteration 78, loss = 1.55878014\n",
            "Iteration 79, loss = 1.55793991\n",
            "Iteration 80, loss = 1.55703580\n",
            "Iteration 81, loss = 1.55610662\n",
            "Iteration 82, loss = 1.55523994\n",
            "Iteration 83, loss = 1.55429792\n",
            "Iteration 84, loss = 1.55339325\n",
            "Iteration 85, loss = 1.55248874\n",
            "Iteration 86, loss = 1.55150958\n",
            "Iteration 87, loss = 1.55062474\n",
            "Iteration 88, loss = 1.54964429\n",
            "Iteration 89, loss = 1.54869149\n",
            "Iteration 90, loss = 1.54770923\n",
            "Iteration 91, loss = 1.54674798\n",
            "Iteration 92, loss = 1.54574966\n",
            "Iteration 93, loss = 1.54475015\n",
            "Iteration 94, loss = 1.54375984\n",
            "Iteration 95, loss = 1.54274395\n",
            "Iteration 96, loss = 1.54169221\n",
            "Iteration 97, loss = 1.54071567\n",
            "Iteration 98, loss = 1.53968920\n",
            "Iteration 99, loss = 1.53865370\n",
            "Iteration 100, loss = 1.53757840\n",
            "Iteration 1, loss = 1.65690887\n",
            "Iteration 2, loss = 1.61371673\n",
            "Iteration 3, loss = 1.60204770\n",
            "Iteration 4, loss = 1.60126015\n",
            "Iteration 5, loss = 1.59887658\n",
            "Iteration 6, loss = 1.59217635\n",
            "Iteration 7, loss = 1.58352766\n",
            "Iteration 8, loss = 1.57454334\n",
            "Iteration 9, loss = 1.56830893\n",
            "Iteration 10, loss = 1.56261031\n",
            "Iteration 11, loss = 1.55558511\n",
            "Iteration 12, loss = 1.54863287\n",
            "Iteration 13, loss = 1.54112582\n",
            "Iteration 14, loss = 1.53181837\n",
            "Iteration 15, loss = 1.52222741\n",
            "Iteration 16, loss = 1.51269379\n",
            "Iteration 17, loss = 1.50320763\n",
            "Iteration 18, loss = 1.49249683\n",
            "Iteration 19, loss = 1.48081887\n",
            "Iteration 20, loss = 1.46869485\n",
            "Iteration 21, loss = 1.45604747\n",
            "Iteration 22, loss = 1.44275245\n",
            "Iteration 23, loss = 1.42928552\n",
            "Iteration 24, loss = 1.41432056\n",
            "Iteration 25, loss = 1.39968375\n",
            "Iteration 26, loss = 1.38526973\n",
            "Iteration 27, loss = 1.36972308\n",
            "Iteration 28, loss = 1.35610070\n",
            "Iteration 29, loss = 1.34065729\n",
            "Iteration 30, loss = 1.32740754\n",
            "Iteration 31, loss = 1.31331817\n",
            "Iteration 32, loss = 1.29984652\n",
            "Iteration 33, loss = 1.28709836\n",
            "Iteration 34, loss = 1.27577386\n",
            "Iteration 35, loss = 1.26357453\n",
            "Iteration 36, loss = 1.25329974\n",
            "Iteration 37, loss = 1.24349573\n",
            "Iteration 38, loss = 1.23407022\n",
            "Iteration 39, loss = 1.22587570\n",
            "Iteration 40, loss = 1.21731370\n",
            "Iteration 41, loss = 1.21035690\n",
            "Iteration 42, loss = 1.20359553\n",
            "Iteration 43, loss = 1.19730553\n",
            "Iteration 44, loss = 1.19123795\n",
            "Iteration 45, loss = 1.18572401\n",
            "Iteration 46, loss = 1.18086607\n",
            "Iteration 47, loss = 1.17621766\n",
            "Iteration 48, loss = 1.17139725\n",
            "Iteration 49, loss = 1.16767169\n",
            "Iteration 50, loss = 1.16360392\n",
            "Iteration 51, loss = 1.15982504\n",
            "Iteration 52, loss = 1.15712638\n",
            "Iteration 53, loss = 1.15418837\n",
            "Iteration 54, loss = 1.15119848\n",
            "Iteration 55, loss = 1.14931976\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 56, loss = 1.14532870\n",
            "Iteration 57, loss = 1.14290561\n",
            "Iteration 58, loss = 1.14125373\n",
            "Iteration 59, loss = 1.13885559\n",
            "Iteration 60, loss = 1.13635598\n",
            "Iteration 61, loss = 1.13458841\n",
            "Iteration 62, loss = 1.13206208\n",
            "Iteration 63, loss = 1.13063205\n",
            "Iteration 64, loss = 1.12950740\n",
            "Iteration 65, loss = 1.12727160\n",
            "Iteration 66, loss = 1.12581422\n",
            "Iteration 67, loss = 1.12432163\n",
            "Iteration 68, loss = 1.12321655\n",
            "Iteration 69, loss = 1.12104204\n",
            "Iteration 70, loss = 1.11990378\n",
            "Iteration 71, loss = 1.11922756\n",
            "Iteration 72, loss = 1.11729389\n",
            "Iteration 73, loss = 1.11612085\n",
            "Iteration 74, loss = 1.11647652\n",
            "Iteration 75, loss = 1.11344881\n",
            "Iteration 76, loss = 1.11257850\n",
            "Iteration 77, loss = 1.11144854\n",
            "Iteration 78, loss = 1.11034508\n",
            "Iteration 79, loss = 1.10904747\n",
            "Iteration 80, loss = 1.10836318\n",
            "Iteration 81, loss = 1.10707130\n",
            "Iteration 82, loss = 1.10558609\n",
            "Iteration 83, loss = 1.10482920\n",
            "Iteration 84, loss = 1.10414585\n",
            "Iteration 85, loss = 1.10394864\n",
            "Iteration 86, loss = 1.10176446\n",
            "Iteration 87, loss = 1.10115332\n",
            "Iteration 88, loss = 1.10054802\n",
            "Iteration 89, loss = 1.09906416\n",
            "Iteration 90, loss = 1.09859023\n",
            "Iteration 91, loss = 1.09753648\n",
            "Iteration 92, loss = 1.09535106\n",
            "Iteration 93, loss = 1.09434579\n",
            "Iteration 94, loss = 1.09433043\n",
            "Iteration 95, loss = 1.09272956\n",
            "Iteration 96, loss = 1.09108655\n",
            "Iteration 97, loss = 1.09152372\n",
            "Iteration 98, loss = 1.08998932\n",
            "Iteration 99, loss = 1.08907322\n",
            "Iteration 100, loss = 1.08809082\n",
            "Iteration 1, loss = 1.70482316\n",
            "Iteration 2, loss = 1.63385954\n",
            "Iteration 3, loss = 1.59786960\n",
            "Iteration 4, loss = 1.54747147\n",
            "Iteration 5, loss = 1.50354577\n",
            "Iteration 6, loss = 1.43957863\n",
            "Iteration 7, loss = 1.36047638\n",
            "Iteration 8, loss = 1.28551594\n",
            "Iteration 9, loss = 1.22217050\n",
            "Iteration 10, loss = 1.17330170\n",
            "Iteration 11, loss = 1.14738664\n",
            "Iteration 12, loss = 1.12887299\n",
            "Iteration 13, loss = 1.11915438\n",
            "Iteration 14, loss = 1.11523909\n",
            "Iteration 15, loss = 1.10955948\n",
            "Iteration 16, loss = 1.10373950\n",
            "Iteration 17, loss = 1.11046958\n",
            "Iteration 18, loss = 1.11005582\n",
            "Iteration 19, loss = 1.14766567\n",
            "Iteration 20, loss = 1.13697871\n",
            "Iteration 21, loss = 1.10208753\n",
            "Iteration 22, loss = 1.12292405\n",
            "Iteration 23, loss = 1.10760280\n",
            "Iteration 24, loss = 1.08947615\n",
            "Iteration 25, loss = 1.08817520\n",
            "Iteration 26, loss = 1.08614484\n",
            "Iteration 27, loss = 1.07495641\n",
            "Iteration 28, loss = 1.07154447\n",
            "Iteration 29, loss = 1.07023188\n",
            "Iteration 30, loss = 1.06602950\n",
            "Iteration 31, loss = 1.05705967\n",
            "Iteration 32, loss = 1.04403693\n",
            "Iteration 33, loss = 1.04742214\n",
            "Iteration 34, loss = 1.05746706\n",
            "Iteration 35, loss = 1.04324565\n",
            "Iteration 36, loss = 1.02756917\n",
            "Iteration 37, loss = 1.02508564\n",
            "Iteration 38, loss = 1.01480477\n",
            "Iteration 39, loss = 1.02749132\n",
            "Iteration 40, loss = 0.99028905\n",
            "Iteration 41, loss = 0.99476513\n",
            "Iteration 42, loss = 0.98141930\n",
            "Iteration 43, loss = 0.98280253\n",
            "Iteration 44, loss = 1.02742170\n",
            "Iteration 45, loss = 0.96430542\n",
            "Iteration 46, loss = 0.96112509\n",
            "Iteration 47, loss = 0.95538741\n",
            "Iteration 48, loss = 0.94840913\n",
            "Iteration 49, loss = 0.92519227\n",
            "Iteration 50, loss = 0.93349627\n",
            "Iteration 51, loss = 0.90193142\n",
            "Iteration 52, loss = 0.91379696\n",
            "Iteration 53, loss = 0.90429689\n",
            "Iteration 54, loss = 0.90860087\n",
            "Iteration 55, loss = 0.96333685\n",
            "Iteration 56, loss = 0.96094259\n",
            "Iteration 57, loss = 0.93396846\n",
            "Iteration 58, loss = 0.91976763\n",
            "Iteration 59, loss = 0.95067036\n",
            "Iteration 60, loss = 0.93831450\n",
            "Iteration 61, loss = 0.91324702\n",
            "Iteration 62, loss = 0.87851101\n",
            "Iteration 63, loss = 0.86524120\n",
            "Iteration 64, loss = 0.85357556\n",
            "Iteration 65, loss = 0.84417769\n",
            "Iteration 66, loss = 0.84254463\n",
            "Iteration 67, loss = 0.84499772\n",
            "Iteration 68, loss = 0.83181194\n",
            "Iteration 69, loss = 0.83183972\n",
            "Iteration 70, loss = 0.82761112\n",
            "Iteration 71, loss = 0.84179284\n",
            "Iteration 72, loss = 0.83585757\n",
            "Iteration 73, loss = 0.82455758\n",
            "Iteration 74, loss = 0.82448735\n",
            "Iteration 75, loss = 0.83332369\n",
            "Iteration 76, loss = 0.84778583\n",
            "Iteration 77, loss = 0.82036063\n",
            "Iteration 78, loss = 0.85498177\n",
            "Iteration 79, loss = 0.78911625\n",
            "Iteration 80, loss = 0.79381666\n",
            "Iteration 81, loss = 0.80780154\n",
            "Iteration 82, loss = 0.80418977\n",
            "Iteration 83, loss = 0.81558728\n",
            "Iteration 84, loss = 0.83890888\n",
            "Iteration 85, loss = 0.82977931\n",
            "Iteration 86, loss = 0.87572836\n",
            "Iteration 87, loss = 0.87144210\n",
            "Iteration 88, loss = 0.99252961\n",
            "Iteration 89, loss = 0.90620977\n",
            "Iteration 90, loss = 0.79859214\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.60595672\n",
            "Iteration 2, loss = 1.60401230\n",
            "Iteration 3, loss = 1.60246256\n",
            "Iteration 4, loss = 1.60111339\n",
            "Iteration 5, loss = 1.60062400\n",
            "Iteration 6, loss = 1.59995958\n",
            "Iteration 7, loss = 1.59962798\n",
            "Iteration 8, loss = 1.59931189\n",
            "Iteration 9, loss = 1.59946269\n",
            "Iteration 10, loss = 1.59914400\n",
            "Iteration 11, loss = 1.59918840\n",
            "Iteration 12, loss = 1.59906531\n",
            "Iteration 13, loss = 1.59915854\n",
            "Iteration 14, loss = 1.59898972\n",
            "Iteration 15, loss = 1.59889001\n",
            "Iteration 16, loss = 1.59876513\n",
            "Iteration 17, loss = 1.59866699\n",
            "Iteration 18, loss = 1.59856721\n",
            "Iteration 19, loss = 1.59847729\n",
            "Iteration 20, loss = 1.59839351\n",
            "Iteration 21, loss = 1.59830466\n",
            "Iteration 22, loss = 1.59820036\n",
            "Iteration 23, loss = 1.59817030\n",
            "Iteration 24, loss = 1.59803861\n",
            "Iteration 25, loss = 1.59792452\n",
            "Iteration 26, loss = 1.59796139\n",
            "Iteration 27, loss = 1.59776235\n",
            "Iteration 28, loss = 1.59776794\n",
            "Iteration 29, loss = 1.59759784\n",
            "Iteration 30, loss = 1.59762999\n",
            "Iteration 31, loss = 1.59752588\n",
            "Iteration 32, loss = 1.59741769\n",
            "Iteration 33, loss = 1.59729144\n",
            "Iteration 34, loss = 1.59737458\n",
            "Iteration 35, loss = 1.59721636\n",
            "Iteration 36, loss = 1.59707012\n",
            "Iteration 37, loss = 1.59697819\n",
            "Iteration 38, loss = 1.59689040\n",
            "Iteration 39, loss = 1.59686798\n",
            "Iteration 40, loss = 1.59676099\n",
            "Iteration 41, loss = 1.59670402\n",
            "Iteration 42, loss = 1.59657972\n",
            "Iteration 43, loss = 1.59653813\n",
            "Iteration 44, loss = 1.59643012\n",
            "Iteration 45, loss = 1.59631611\n",
            "Iteration 46, loss = 1.59624124\n",
            "Iteration 47, loss = 1.59618578\n",
            "Iteration 48, loss = 1.59605744\n",
            "Iteration 49, loss = 1.59599881\n",
            "Iteration 50, loss = 1.59594332\n",
            "Iteration 51, loss = 1.59580042\n",
            "Iteration 52, loss = 1.59578860\n",
            "Iteration 53, loss = 1.59568953\n",
            "Iteration 54, loss = 1.59564564\n",
            "Iteration 55, loss = 1.59557786\n",
            "Iteration 56, loss = 1.59544990\n",
            "Iteration 57, loss = 1.59529933\n",
            "Iteration 58, loss = 1.59526893\n",
            "Iteration 59, loss = 1.59518416\n",
            "Iteration 60, loss = 1.59508442\n",
            "Iteration 61, loss = 1.59497009\n",
            "Iteration 62, loss = 1.59488638\n",
            "Iteration 63, loss = 1.59482233\n",
            "Iteration 64, loss = 1.59473242\n",
            "Iteration 65, loss = 1.59460727\n",
            "Iteration 66, loss = 1.59455092\n",
            "Iteration 67, loss = 1.59442668\n",
            "Iteration 68, loss = 1.59434491\n",
            "Iteration 69, loss = 1.59426234\n",
            "Iteration 70, loss = 1.59419518\n",
            "Iteration 71, loss = 1.59407978\n",
            "Iteration 72, loss = 1.59396664\n",
            "Iteration 73, loss = 1.59387280\n",
            "Iteration 74, loss = 1.59383622\n",
            "Iteration 75, loss = 1.59371315\n",
            "Iteration 76, loss = 1.59361903\n",
            "Iteration 77, loss = 1.59358868\n",
            "Iteration 78, loss = 1.59344108\n",
            "Iteration 79, loss = 1.59336641\n",
            "Iteration 80, loss = 1.59333533\n",
            "Iteration 81, loss = 1.59314430\n",
            "Iteration 82, loss = 1.59307251\n",
            "Iteration 83, loss = 1.59297263\n",
            "Iteration 84, loss = 1.59288256\n",
            "Iteration 85, loss = 1.59280924\n",
            "Iteration 86, loss = 1.59267410\n",
            "Iteration 87, loss = 1.59258901\n",
            "Iteration 88, loss = 1.59249533\n",
            "Iteration 89, loss = 1.59244685\n",
            "Iteration 90, loss = 1.59225838\n",
            "Iteration 91, loss = 1.59219094\n",
            "Iteration 92, loss = 1.59203639\n",
            "Iteration 93, loss = 1.59193932\n",
            "Iteration 94, loss = 1.59184543\n",
            "Iteration 95, loss = 1.59174743\n",
            "Iteration 96, loss = 1.59162039\n",
            "Iteration 97, loss = 1.59157595\n",
            "Iteration 98, loss = 1.59144436\n",
            "Iteration 99, loss = 1.59140347\n",
            "Iteration 100, loss = 1.59123098\n",
            "Iteration 1, loss = 1.60297997\n",
            "Iteration 2, loss = 1.60179970\n",
            "Iteration 3, loss = 1.60035953\n",
            "Iteration 4, loss = 1.60121213\n",
            "Iteration 5, loss = 1.59832526\n",
            "Iteration 6, loss = 1.59668267\n",
            "Iteration 7, loss = 1.59527785\n",
            "Iteration 8, loss = 1.59480783\n",
            "Iteration 9, loss = 1.59502762\n",
            "Iteration 10, loss = 1.59371977\n",
            "Iteration 11, loss = 1.59231814\n",
            "Iteration 12, loss = 1.59109225\n",
            "Iteration 13, loss = 1.59116447\n",
            "Iteration 14, loss = 1.58916569\n",
            "Iteration 15, loss = 1.58808082\n",
            "Iteration 16, loss = 1.58689182\n",
            "Iteration 17, loss = 1.58620281\n",
            "Iteration 18, loss = 1.58452474\n",
            "Iteration 19, loss = 1.58331348\n",
            "Iteration 20, loss = 1.58174440\n",
            "Iteration 21, loss = 1.58020947\n",
            "Iteration 22, loss = 1.57854200\n",
            "Iteration 23, loss = 1.57703206\n",
            "Iteration 24, loss = 1.57496662\n",
            "Iteration 25, loss = 1.57273246\n",
            "Iteration 26, loss = 1.57137554\n",
            "Iteration 27, loss = 1.56844781\n",
            "Iteration 28, loss = 1.56724792\n",
            "Iteration 29, loss = 1.56354294\n",
            "Iteration 30, loss = 1.56160711\n",
            "Iteration 31, loss = 1.55870158\n",
            "Iteration 32, loss = 1.55536495\n",
            "Iteration 33, loss = 1.55158291\n",
            "Iteration 34, loss = 1.54973848\n",
            "Iteration 35, loss = 1.54507890\n",
            "Iteration 36, loss = 1.54076335\n",
            "Iteration 37, loss = 1.53669380\n",
            "Iteration 38, loss = 1.53232486\n",
            "Iteration 39, loss = 1.52813373\n",
            "Iteration 40, loss = 1.52318744\n",
            "Iteration 41, loss = 1.51824055\n",
            "Iteration 42, loss = 1.51312569\n",
            "Iteration 43, loss = 1.50785287\n",
            "Iteration 44, loss = 1.50236530\n",
            "Iteration 45, loss = 1.49641415\n",
            "Iteration 46, loss = 1.49005270\n",
            "Iteration 47, loss = 1.48412155\n",
            "Iteration 48, loss = 1.47723123\n",
            "Iteration 49, loss = 1.47100332\n",
            "Iteration 50, loss = 1.46414359\n",
            "Iteration 51, loss = 1.45676838\n",
            "Iteration 52, loss = 1.45038983\n",
            "Iteration 53, loss = 1.44326714\n",
            "Iteration 54, loss = 1.43609286\n",
            "Iteration 55, loss = 1.42864862\n",
            "Iteration 56, loss = 1.42034640\n",
            "Iteration 57, loss = 1.41257144\n",
            "Iteration 58, loss = 1.40540524\n",
            "Iteration 59, loss = 1.39834604\n",
            "Iteration 60, loss = 1.39022097\n",
            "Iteration 61, loss = 1.38250829\n",
            "Iteration 62, loss = 1.37479638\n",
            "Iteration 63, loss = 1.36759868\n",
            "Iteration 64, loss = 1.36035864\n",
            "Iteration 65, loss = 1.35268987\n",
            "Iteration 66, loss = 1.34590587\n",
            "Iteration 67, loss = 1.33889874\n",
            "Iteration 68, loss = 1.33210582\n",
            "Iteration 69, loss = 1.32465122\n",
            "Iteration 70, loss = 1.31838624\n",
            "Iteration 71, loss = 1.31191382\n",
            "Iteration 72, loss = 1.30550474\n",
            "Iteration 73, loss = 1.29947940\n",
            "Iteration 74, loss = 1.29411628\n",
            "Iteration 75, loss = 1.28800856\n",
            "Iteration 76, loss = 1.28254322\n",
            "Iteration 77, loss = 1.27746521\n",
            "Iteration 78, loss = 1.27188100\n",
            "Iteration 79, loss = 1.26699785\n",
            "Iteration 80, loss = 1.26265755\n",
            "Iteration 81, loss = 1.25745619\n",
            "Iteration 82, loss = 1.25291861\n",
            "Iteration 83, loss = 1.24865227\n",
            "Iteration 84, loss = 1.24464709\n",
            "Iteration 85, loss = 1.24111324\n",
            "Iteration 86, loss = 1.23648165\n",
            "Iteration 87, loss = 1.23305245\n",
            "Iteration 88, loss = 1.22951102\n",
            "Iteration 89, loss = 1.22609089\n",
            "Iteration 90, loss = 1.22255334\n",
            "Iteration 91, loss = 1.21953217\n",
            "Iteration 92, loss = 1.21611901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 93, loss = 1.21294411\n",
            "Iteration 94, loss = 1.21048361\n",
            "Iteration 95, loss = 1.20779727\n",
            "Iteration 96, loss = 1.20480401\n",
            "Iteration 97, loss = 1.20297140\n",
            "Iteration 98, loss = 1.19996874\n",
            "Iteration 99, loss = 1.19842971\n",
            "Iteration 100, loss = 1.19544719\n",
            "Iteration 1, loss = 1.83519290\n",
            "Iteration 2, loss = 1.65116704\n",
            "Iteration 3, loss = 1.79184994\n",
            "Iteration 4, loss = 1.65533353\n",
            "Iteration 5, loss = 1.60631647\n",
            "Iteration 6, loss = 1.64891531\n",
            "Iteration 7, loss = 1.63127194\n",
            "Iteration 8, loss = 1.59227964\n",
            "Iteration 9, loss = 1.57475834\n",
            "Iteration 10, loss = 1.58364490\n",
            "Iteration 11, loss = 1.58007599\n",
            "Iteration 12, loss = 1.56138498\n",
            "Iteration 13, loss = 1.54047038\n",
            "Iteration 14, loss = 1.52190786\n",
            "Iteration 15, loss = 1.50517020\n",
            "Iteration 16, loss = 1.48086907\n",
            "Iteration 17, loss = 1.44625024\n",
            "Iteration 18, loss = 1.40661344\n",
            "Iteration 19, loss = 1.37021951\n",
            "Iteration 20, loss = 1.33608205\n",
            "Iteration 21, loss = 1.29975201\n",
            "Iteration 22, loss = 1.26449862\n",
            "Iteration 23, loss = 1.23853212\n",
            "Iteration 24, loss = 1.21487845\n",
            "Iteration 25, loss = 1.19358313\n",
            "Iteration 26, loss = 1.18193248\n",
            "Iteration 27, loss = 1.16667001\n",
            "Iteration 28, loss = 1.16186346\n",
            "Iteration 29, loss = 1.15007345\n",
            "Iteration 30, loss = 1.14583952\n",
            "Iteration 31, loss = 1.14008209\n",
            "Iteration 32, loss = 1.13210119\n",
            "Iteration 33, loss = 1.13079773\n",
            "Iteration 34, loss = 1.13315093\n",
            "Iteration 35, loss = 1.13029807\n",
            "Iteration 36, loss = 1.12904322\n",
            "Iteration 37, loss = 1.12357248\n",
            "Iteration 38, loss = 1.12290741\n",
            "Iteration 39, loss = 1.12590465\n",
            "Iteration 40, loss = 1.11600343\n",
            "Iteration 41, loss = 1.11553594\n",
            "Iteration 42, loss = 1.11548601\n",
            "Iteration 43, loss = 1.11675530\n",
            "Iteration 44, loss = 1.11473908\n",
            "Iteration 45, loss = 1.11238319\n",
            "Iteration 46, loss = 1.11233814\n",
            "Iteration 47, loss = 1.11379663\n",
            "Iteration 48, loss = 1.11178294\n",
            "Iteration 49, loss = 1.10989112\n",
            "Iteration 50, loss = 1.10811798\n",
            "Iteration 51, loss = 1.11051615\n",
            "Iteration 52, loss = 1.10708386\n",
            "Iteration 53, loss = 1.11528871\n",
            "Iteration 54, loss = 1.10887913\n",
            "Iteration 55, loss = 1.12291560\n",
            "Iteration 56, loss = 1.10996119\n",
            "Iteration 57, loss = 1.11849202\n",
            "Iteration 58, loss = 1.10508496\n",
            "Iteration 59, loss = 1.11571090\n",
            "Iteration 60, loss = 1.10339815\n",
            "Iteration 61, loss = 1.11053594\n",
            "Iteration 62, loss = 1.10499019\n",
            "Iteration 63, loss = 1.10818165\n",
            "Iteration 64, loss = 1.10145495\n",
            "Iteration 65, loss = 1.10948496\n",
            "Iteration 66, loss = 1.10392621\n",
            "Iteration 67, loss = 1.10881819\n",
            "Iteration 68, loss = 1.10052558\n",
            "Iteration 69, loss = 1.10727603\n",
            "Iteration 70, loss = 1.10183141\n",
            "Iteration 71, loss = 1.10630782\n",
            "Iteration 72, loss = 1.09876572\n",
            "Iteration 73, loss = 1.09954258\n",
            "Iteration 74, loss = 1.10145387\n",
            "Iteration 75, loss = 1.10068517\n",
            "Iteration 76, loss = 1.09984892\n",
            "Iteration 77, loss = 1.09827612\n",
            "Iteration 78, loss = 1.10242257\n",
            "Iteration 79, loss = 1.09695986\n",
            "Iteration 80, loss = 1.10027048\n",
            "Iteration 81, loss = 1.09952079\n",
            "Iteration 82, loss = 1.09609388\n",
            "Iteration 83, loss = 1.09123323\n",
            "Iteration 84, loss = 1.09556383\n",
            "Iteration 85, loss = 1.09443011\n",
            "Iteration 86, loss = 1.09404370\n",
            "Iteration 87, loss = 1.09564303\n",
            "Iteration 88, loss = 1.09542886\n",
            "Iteration 89, loss = 1.09697729\n",
            "Iteration 90, loss = 1.08877946\n",
            "Iteration 91, loss = 1.08837103\n",
            "Iteration 92, loss = 1.08859109\n",
            "Iteration 93, loss = 1.09023279\n",
            "Iteration 94, loss = 1.08723132\n",
            "Iteration 95, loss = 1.07855956\n",
            "Iteration 96, loss = 1.07939716\n",
            "Iteration 97, loss = 1.07252120\n",
            "Iteration 98, loss = 1.07998315\n",
            "Iteration 99, loss = 1.07642268\n",
            "Iteration 100, loss = 1.07002478\n",
            "Iteration 1, loss = 1.66709671\n",
            "Iteration 2, loss = 1.66052215\n",
            "Iteration 3, loss = 1.65455861\n",
            "Iteration 4, loss = 1.64848960\n",
            "Iteration 5, loss = 1.64360767\n",
            "Iteration 6, loss = 1.63914179\n",
            "Iteration 7, loss = 1.63451568\n",
            "Iteration 8, loss = 1.63054728\n",
            "Iteration 9, loss = 1.62694052\n",
            "Iteration 10, loss = 1.62323328\n",
            "Iteration 11, loss = 1.62078409\n",
            "Iteration 12, loss = 1.61783468\n",
            "Iteration 13, loss = 1.61550573\n",
            "Iteration 14, loss = 1.61313369\n",
            "Iteration 15, loss = 1.61078528\n",
            "Iteration 16, loss = 1.60906545\n",
            "Iteration 17, loss = 1.60749541\n",
            "Iteration 18, loss = 1.60582827\n",
            "Iteration 19, loss = 1.60452975\n",
            "Iteration 20, loss = 1.60328180\n",
            "Iteration 21, loss = 1.60194256\n",
            "Iteration 22, loss = 1.60098959\n",
            "Iteration 23, loss = 1.59993029\n",
            "Iteration 24, loss = 1.59914873\n",
            "Iteration 25, loss = 1.59816458\n",
            "Iteration 26, loss = 1.59743794\n",
            "Iteration 27, loss = 1.59660228\n",
            "Iteration 28, loss = 1.59585273\n",
            "Iteration 29, loss = 1.59510480\n",
            "Iteration 30, loss = 1.59445145\n",
            "Iteration 31, loss = 1.59374003\n",
            "Iteration 32, loss = 1.59313073\n",
            "Iteration 33, loss = 1.59240249\n",
            "Iteration 34, loss = 1.59178897\n",
            "Iteration 35, loss = 1.59117041\n",
            "Iteration 36, loss = 1.59048588\n",
            "Iteration 37, loss = 1.58985062\n",
            "Iteration 38, loss = 1.58917182\n",
            "Iteration 39, loss = 1.58857579\n",
            "Iteration 40, loss = 1.58790676\n",
            "Iteration 41, loss = 1.58727222\n",
            "Iteration 42, loss = 1.58659298\n",
            "Iteration 43, loss = 1.58596611\n",
            "Iteration 44, loss = 1.58529333\n",
            "Iteration 45, loss = 1.58462776\n",
            "Iteration 46, loss = 1.58396493\n",
            "Iteration 47, loss = 1.58331940\n",
            "Iteration 48, loss = 1.58260705\n",
            "Iteration 49, loss = 1.58195858\n",
            "Iteration 50, loss = 1.58126984\n",
            "Iteration 51, loss = 1.58057576\n",
            "Iteration 52, loss = 1.57992774\n",
            "Iteration 53, loss = 1.57923383\n",
            "Iteration 54, loss = 1.57856161\n",
            "Iteration 55, loss = 1.57789212\n",
            "Iteration 56, loss = 1.57712749\n",
            "Iteration 57, loss = 1.57639853\n",
            "Iteration 58, loss = 1.57572178\n",
            "Iteration 59, loss = 1.57500763\n",
            "Iteration 60, loss = 1.57427323\n",
            "Iteration 61, loss = 1.57353407\n",
            "Iteration 62, loss = 1.57280790\n",
            "Iteration 63, loss = 1.57207540\n",
            "Iteration 64, loss = 1.57133215\n",
            "Iteration 65, loss = 1.57056464\n",
            "Iteration 66, loss = 1.56980336\n",
            "Iteration 67, loss = 1.56904904\n",
            "Iteration 68, loss = 1.56828079\n",
            "Iteration 69, loss = 1.56747422\n",
            "Iteration 70, loss = 1.56671936\n",
            "Iteration 71, loss = 1.56593407\n",
            "Iteration 72, loss = 1.56511137\n",
            "Iteration 73, loss = 1.56432841\n",
            "Iteration 74, loss = 1.56354213\n",
            "Iteration 75, loss = 1.56270585\n",
            "Iteration 76, loss = 1.56190094\n",
            "Iteration 77, loss = 1.56108615\n",
            "Iteration 78, loss = 1.56021074\n",
            "Iteration 79, loss = 1.55940455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 80, loss = 1.55853859\n",
            "Iteration 81, loss = 1.55764521\n",
            "Iteration 82, loss = 1.55681371\n",
            "Iteration 83, loss = 1.55590825\n",
            "Iteration 84, loss = 1.55503962\n",
            "Iteration 85, loss = 1.55417142\n",
            "Iteration 86, loss = 1.55323044\n",
            "Iteration 87, loss = 1.55237874\n",
            "Iteration 88, loss = 1.55143669\n",
            "Iteration 89, loss = 1.55052115\n",
            "Iteration 90, loss = 1.54957547\n",
            "Iteration 91, loss = 1.54864946\n",
            "Iteration 92, loss = 1.54768976\n",
            "Iteration 93, loss = 1.54672727\n",
            "Iteration 94, loss = 1.54577327\n",
            "Iteration 95, loss = 1.54479460\n",
            "Iteration 96, loss = 1.54378122\n",
            "Iteration 97, loss = 1.54283957\n",
            "Iteration 98, loss = 1.54184972\n",
            "Iteration 99, loss = 1.54085027\n",
            "Iteration 100, loss = 1.53981252\n",
            "Iteration 1, loss = 1.65407085\n",
            "Iteration 2, loss = 1.61322873\n",
            "Iteration 3, loss = 1.60142650\n",
            "Iteration 4, loss = 1.60005446\n",
            "Iteration 5, loss = 1.59799678\n",
            "Iteration 6, loss = 1.59238092\n",
            "Iteration 7, loss = 1.58430518\n",
            "Iteration 8, loss = 1.57551009\n",
            "Iteration 9, loss = 1.56924240\n",
            "Iteration 10, loss = 1.56351830\n",
            "Iteration 11, loss = 1.55665196\n",
            "Iteration 12, loss = 1.55006406\n",
            "Iteration 13, loss = 1.54285391\n",
            "Iteration 14, loss = 1.53380872\n",
            "Iteration 15, loss = 1.52443305\n",
            "Iteration 16, loss = 1.51501717\n",
            "Iteration 17, loss = 1.50569147\n",
            "Iteration 18, loss = 1.49514629\n",
            "Iteration 19, loss = 1.48368958\n",
            "Iteration 20, loss = 1.47176483\n",
            "Iteration 21, loss = 1.45916940\n",
            "Iteration 22, loss = 1.44593277\n",
            "Iteration 23, loss = 1.43253402\n",
            "Iteration 24, loss = 1.41757302\n",
            "Iteration 25, loss = 1.40295366\n",
            "Iteration 26, loss = 1.38843656\n",
            "Iteration 27, loss = 1.37285854\n",
            "Iteration 28, loss = 1.35897684\n",
            "Iteration 29, loss = 1.34341900\n",
            "Iteration 30, loss = 1.32996019\n",
            "Iteration 31, loss = 1.31560867\n",
            "Iteration 32, loss = 1.30177248\n",
            "Iteration 33, loss = 1.28875193\n",
            "Iteration 34, loss = 1.27711087\n",
            "Iteration 35, loss = 1.26470629\n",
            "Iteration 36, loss = 1.25408618\n",
            "Iteration 37, loss = 1.24393712\n",
            "Iteration 38, loss = 1.23433631\n",
            "Iteration 39, loss = 1.22594069\n",
            "Iteration 40, loss = 1.21714414\n",
            "Iteration 41, loss = 1.20993536\n",
            "Iteration 42, loss = 1.20294018\n",
            "Iteration 43, loss = 1.19651959\n",
            "Iteration 44, loss = 1.19033688\n",
            "Iteration 45, loss = 1.18467026\n",
            "Iteration 46, loss = 1.17972588\n",
            "Iteration 47, loss = 1.17493161\n",
            "Iteration 48, loss = 1.17007524\n",
            "Iteration 49, loss = 1.16624707\n",
            "Iteration 50, loss = 1.16215497\n",
            "Iteration 51, loss = 1.15833982\n",
            "Iteration 52, loss = 1.15548259\n",
            "Iteration 53, loss = 1.15249669\n",
            "Iteration 54, loss = 1.14937830\n",
            "Iteration 55, loss = 1.14757165\n",
            "Iteration 56, loss = 1.14368352\n",
            "Iteration 57, loss = 1.14124892\n",
            "Iteration 58, loss = 1.13946462\n",
            "Iteration 59, loss = 1.13708288\n",
            "Iteration 60, loss = 1.13463681\n",
            "Iteration 61, loss = 1.13284225\n",
            "Iteration 62, loss = 1.13036202\n",
            "Iteration 63, loss = 1.12887477\n",
            "Iteration 64, loss = 1.12781578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 65, loss = 1.12558824\n",
            "Iteration 66, loss = 1.12406517\n",
            "Iteration 67, loss = 1.12260256\n",
            "Iteration 68, loss = 1.12147054\n",
            "Iteration 69, loss = 1.11943617\n",
            "Iteration 70, loss = 1.11829424\n",
            "Iteration 71, loss = 1.11756563\n",
            "Iteration 72, loss = 1.11567302\n",
            "Iteration 73, loss = 1.11451600\n",
            "Iteration 74, loss = 1.11482513\n",
            "Iteration 75, loss = 1.11184462\n",
            "Iteration 76, loss = 1.11090067\n",
            "Iteration 77, loss = 1.10977670\n",
            "Iteration 78, loss = 1.10866913\n",
            "Iteration 79, loss = 1.10739493\n",
            "Iteration 80, loss = 1.10657348\n",
            "Iteration 81, loss = 1.10541335\n",
            "Iteration 82, loss = 1.10395830\n",
            "Iteration 83, loss = 1.10310143\n",
            "Iteration 84, loss = 1.10240923\n",
            "Iteration 85, loss = 1.10207653\n",
            "Iteration 86, loss = 1.09995814\n",
            "Iteration 87, loss = 1.09934453\n",
            "Iteration 88, loss = 1.09876211\n",
            "Iteration 89, loss = 1.09716853\n",
            "Iteration 90, loss = 1.09660893\n",
            "Iteration 91, loss = 1.09556031\n",
            "Iteration 92, loss = 1.09343398\n",
            "Iteration 93, loss = 1.09235973\n",
            "Iteration 94, loss = 1.09219935\n",
            "Iteration 95, loss = 1.09060006\n",
            "Iteration 96, loss = 1.08895148\n",
            "Iteration 97, loss = 1.08925830\n",
            "Iteration 98, loss = 1.08765534\n",
            "Iteration 99, loss = 1.08649639\n",
            "Iteration 100, loss = 1.08562151\n",
            "Iteration 1, loss = 1.69575138\n",
            "Iteration 2, loss = 1.63518870\n",
            "Iteration 3, loss = 1.59456855\n",
            "Iteration 4, loss = 1.54678463\n",
            "Iteration 5, loss = 1.49421396\n",
            "Iteration 6, loss = 1.42836134\n",
            "Iteration 7, loss = 1.34728828\n",
            "Iteration 8, loss = 1.27726837\n",
            "Iteration 9, loss = 1.21347821\n",
            "Iteration 10, loss = 1.16926726\n",
            "Iteration 11, loss = 1.14713368\n",
            "Iteration 12, loss = 1.12590122\n",
            "Iteration 13, loss = 1.11960586\n",
            "Iteration 14, loss = 1.11623205\n",
            "Iteration 15, loss = 1.11051637\n",
            "Iteration 16, loss = 1.10469690\n",
            "Iteration 17, loss = 1.11128850\n",
            "Iteration 18, loss = 1.10822511\n",
            "Iteration 19, loss = 1.14816417\n",
            "Iteration 20, loss = 1.13852679\n",
            "Iteration 21, loss = 1.10462688\n",
            "Iteration 22, loss = 1.11900982\n",
            "Iteration 23, loss = 1.11014662\n",
            "Iteration 24, loss = 1.09257965\n",
            "Iteration 25, loss = 1.09318840\n",
            "Iteration 26, loss = 1.09675441\n",
            "Iteration 27, loss = 1.07575331\n",
            "Iteration 28, loss = 1.08523018\n",
            "Iteration 29, loss = 1.08175797\n",
            "Iteration 30, loss = 1.06298934\n",
            "Iteration 31, loss = 1.07607205\n",
            "Iteration 32, loss = 1.06410178\n",
            "Iteration 33, loss = 1.06771225\n",
            "Iteration 34, loss = 1.08193881\n",
            "Iteration 35, loss = 1.02852012\n",
            "Iteration 36, loss = 1.06851016\n",
            "Iteration 37, loss = 1.05601745\n",
            "Iteration 38, loss = 1.02595512\n",
            "Iteration 39, loss = 1.03375935\n",
            "Iteration 40, loss = 1.00912471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 1.00841634\n",
            "Iteration 42, loss = 0.99220640\n",
            "Iteration 43, loss = 0.99730154\n",
            "Iteration 44, loss = 1.00804799\n",
            "Iteration 45, loss = 0.97168282\n",
            "Iteration 46, loss = 0.96371247\n",
            "Iteration 47, loss = 0.96172871\n",
            "Iteration 48, loss = 0.95301610\n",
            "Iteration 49, loss = 0.94002303\n",
            "Iteration 50, loss = 0.96657710\n",
            "Iteration 51, loss = 0.92261525\n",
            "Iteration 52, loss = 0.91824966\n",
            "Iteration 53, loss = 0.93428440\n",
            "Iteration 54, loss = 0.92868360\n",
            "Iteration 55, loss = 0.95174792\n",
            "Iteration 56, loss = 0.95051528\n",
            "Iteration 57, loss = 0.92649585\n",
            "Iteration 58, loss = 0.92566060\n",
            "Iteration 59, loss = 0.95603684\n",
            "Iteration 60, loss = 0.96120323\n",
            "Iteration 61, loss = 0.93475920\n",
            "Iteration 62, loss = 0.89484035\n",
            "Iteration 63, loss = 0.88330161\n",
            "Iteration 64, loss = 0.86560023\n",
            "Iteration 65, loss = 0.85390015\n",
            "Iteration 66, loss = 0.85104428\n",
            "Iteration 67, loss = 0.84773889\n",
            "Iteration 68, loss = 0.83518042\n",
            "Iteration 69, loss = 0.83386207\n",
            "Iteration 70, loss = 0.82705747\n",
            "Iteration 71, loss = 0.82465125\n",
            "Iteration 72, loss = 0.82658900\n",
            "Iteration 73, loss = 0.81832686\n",
            "Iteration 74, loss = 0.82533585\n",
            "Iteration 75, loss = 0.83349365\n",
            "Iteration 76, loss = 0.83643982\n",
            "Iteration 77, loss = 0.82771986\n",
            "Iteration 78, loss = 0.88916134\n",
            "Iteration 79, loss = 0.80780894\n",
            "Iteration 80, loss = 0.80444302\n",
            "Iteration 81, loss = 0.80777290\n",
            "Iteration 82, loss = 0.79299444\n",
            "Iteration 83, loss = 0.81623609\n",
            "Iteration 84, loss = 0.82493937\n",
            "Iteration 85, loss = 0.80047130\n",
            "Iteration 86, loss = 0.84537643\n",
            "Iteration 87, loss = 0.84179863\n",
            "Iteration 88, loss = 0.92627718\n",
            "Iteration 89, loss = 0.89929658\n",
            "Iteration 90, loss = 0.79492775\n",
            "Iteration 91, loss = 0.79775660\n",
            "Iteration 92, loss = 0.77972292\n",
            "Iteration 93, loss = 0.78033896\n",
            "Iteration 94, loss = 0.78683662\n",
            "Iteration 95, loss = 0.76828798\n",
            "Iteration 96, loss = 0.75829147\n",
            "Iteration 97, loss = 0.75203174\n",
            "Iteration 98, loss = 0.74545842\n",
            "Iteration 99, loss = 0.76630091\n",
            "Iteration 100, loss = 0.73521018\n",
            "Iteration 1, loss = 1.62515154\n",
            "Iteration 2, loss = 1.62358389\n",
            "Iteration 3, loss = 1.62217126\n",
            "Iteration 4, loss = 1.62078995\n",
            "Iteration 5, loss = 1.61939631\n",
            "Iteration 6, loss = 1.61830765\n",
            "Iteration 7, loss = 1.61699640\n",
            "Iteration 8, loss = 1.61583888\n",
            "Iteration 9, loss = 1.61471641\n",
            "Iteration 10, loss = 1.61362360\n",
            "Iteration 11, loss = 1.61281502\n",
            "Iteration 12, loss = 1.61180399\n",
            "Iteration 13, loss = 1.61099900\n",
            "Iteration 14, loss = 1.61001950\n",
            "Iteration 15, loss = 1.60907570\n",
            "Iteration 16, loss = 1.60831398\n",
            "Iteration 17, loss = 1.60761009\n",
            "Iteration 18, loss = 1.60680059\n",
            "Iteration 19, loss = 1.60615031\n",
            "Iteration 20, loss = 1.60545779\n",
            "Iteration 21, loss = 1.60473066\n",
            "Iteration 22, loss = 1.60416633\n",
            "Iteration 23, loss = 1.60354013\n",
            "Iteration 24, loss = 1.60300182\n",
            "Iteration 25, loss = 1.60238154\n",
            "Iteration 26, loss = 1.60188987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 27, loss = 1.60133235\n",
            "Iteration 28, loss = 1.60082352\n",
            "Iteration 29, loss = 1.60031508\n",
            "Iteration 30, loss = 1.59986801\n",
            "Iteration 31, loss = 1.59938262\n",
            "Iteration 32, loss = 1.59899248\n",
            "Iteration 33, loss = 1.59849958\n",
            "Iteration 34, loss = 1.59807877\n",
            "Iteration 35, loss = 1.59772749\n",
            "Iteration 36, loss = 1.59729174\n",
            "Iteration 37, loss = 1.59691165\n",
            "Iteration 38, loss = 1.59648054\n",
            "Iteration 39, loss = 1.59612828\n",
            "Iteration 40, loss = 1.59573559\n",
            "Iteration 41, loss = 1.59536295\n",
            "Iteration 42, loss = 1.59499917\n",
            "Iteration 43, loss = 1.59462392\n",
            "Iteration 44, loss = 1.59429032\n",
            "Iteration 45, loss = 1.59389605\n",
            "Iteration 46, loss = 1.59354612\n",
            "Iteration 47, loss = 1.59319239\n",
            "Iteration 48, loss = 1.59282906\n",
            "Iteration 49, loss = 1.59247102\n",
            "Iteration 50, loss = 1.59213180\n",
            "Iteration 51, loss = 1.59174643\n",
            "Iteration 52, loss = 1.59139084\n",
            "Iteration 53, loss = 1.59107154\n",
            "Iteration 54, loss = 1.59069945\n",
            "Iteration 55, loss = 1.59037123\n",
            "Iteration 56, loss = 1.58999437\n",
            "Iteration 57, loss = 1.58960529\n",
            "Iteration 58, loss = 1.58926549\n",
            "Iteration 59, loss = 1.58887340\n",
            "Iteration 60, loss = 1.58853108\n",
            "Iteration 61, loss = 1.58814419\n",
            "Iteration 62, loss = 1.58777256\n",
            "Iteration 63, loss = 1.58741379\n",
            "Iteration 64, loss = 1.58702297\n",
            "Iteration 65, loss = 1.58664427\n",
            "Iteration 66, loss = 1.58626693\n",
            "Iteration 67, loss = 1.58587944\n",
            "Iteration 68, loss = 1.58549063\n",
            "Iteration 69, loss = 1.58509235\n",
            "Iteration 70, loss = 1.58471391\n",
            "Iteration 71, loss = 1.58431621\n",
            "Iteration 72, loss = 1.58389803\n",
            "Iteration 73, loss = 1.58350200\n",
            "Iteration 74, loss = 1.58311715\n",
            "Iteration 75, loss = 1.58268201\n",
            "Iteration 76, loss = 1.58227623\n",
            "Iteration 77, loss = 1.58185980\n",
            "Iteration 78, loss = 1.58142295\n",
            "Iteration 79, loss = 1.58102166\n",
            "Iteration 80, loss = 1.58059415\n",
            "Iteration 81, loss = 1.58013529\n",
            "Iteration 82, loss = 1.57971851\n",
            "Iteration 83, loss = 1.57926201\n",
            "Iteration 84, loss = 1.57882763\n",
            "Iteration 85, loss = 1.57838392\n",
            "Iteration 86, loss = 1.57791209\n",
            "Iteration 87, loss = 1.57748901\n",
            "Iteration 88, loss = 1.57700916\n",
            "Iteration 89, loss = 1.57653991\n",
            "Iteration 90, loss = 1.57606077\n",
            "Iteration 91, loss = 1.57559672\n",
            "Iteration 92, loss = 1.57510439\n",
            "Iteration 93, loss = 1.57461446\n",
            "Iteration 94, loss = 1.57413062\n",
            "Iteration 95, loss = 1.57363085\n",
            "Iteration 96, loss = 1.57311110\n",
            "Iteration 97, loss = 1.57263312\n",
            "Iteration 98, loss = 1.57212369\n",
            "Iteration 99, loss = 1.57161005\n",
            "Iteration 100, loss = 1.57107356\n",
            "Iteration 1, loss = 1.62306755\n",
            "Iteration 2, loss = 1.61062041\n",
            "Iteration 3, loss = 1.60289498\n",
            "Iteration 4, loss = 1.59765936\n",
            "Iteration 5, loss = 1.59401737\n",
            "Iteration 6, loss = 1.59187380\n",
            "Iteration 7, loss = 1.58909354\n",
            "Iteration 8, loss = 1.58625684\n",
            "Iteration 9, loss = 1.58293228\n",
            "Iteration 10, loss = 1.57876979\n",
            "Iteration 11, loss = 1.57422756\n",
            "Iteration 12, loss = 1.56992817\n",
            "Iteration 13, loss = 1.56582823\n",
            "Iteration 14, loss = 1.56078383\n",
            "Iteration 15, loss = 1.55612318\n",
            "Iteration 16, loss = 1.55036212\n",
            "Iteration 17, loss = 1.54442983\n",
            "Iteration 18, loss = 1.53781432\n",
            "Iteration 19, loss = 1.53068518\n",
            "Iteration 20, loss = 1.52318863\n",
            "Iteration 21, loss = 1.51473520\n",
            "Iteration 22, loss = 1.50610419\n",
            "Iteration 23, loss = 1.49700506\n",
            "Iteration 24, loss = 1.48668609\n",
            "Iteration 25, loss = 1.47626592\n",
            "Iteration 26, loss = 1.46542248\n",
            "Iteration 27, loss = 1.45325818\n",
            "Iteration 28, loss = 1.44186702\n",
            "Iteration 29, loss = 1.42875842\n",
            "Iteration 30, loss = 1.41678700\n",
            "Iteration 31, loss = 1.40369371\n",
            "Iteration 32, loss = 1.39016278\n",
            "Iteration 33, loss = 1.37685253\n",
            "Iteration 34, loss = 1.36442449\n",
            "Iteration 35, loss = 1.35107908\n",
            "Iteration 36, loss = 1.33825454\n",
            "Iteration 37, loss = 1.32597225\n",
            "Iteration 38, loss = 1.31381261\n",
            "Iteration 39, loss = 1.30250793\n",
            "Iteration 40, loss = 1.29103258\n",
            "Iteration 41, loss = 1.28069990\n",
            "Iteration 42, loss = 1.27054248\n",
            "Iteration 43, loss = 1.26126096\n",
            "Iteration 44, loss = 1.25207532\n",
            "Iteration 45, loss = 1.24358943\n",
            "Iteration 46, loss = 1.23574354\n",
            "Iteration 47, loss = 1.22839098\n",
            "Iteration 48, loss = 1.22125735\n",
            "Iteration 49, loss = 1.21526127\n",
            "Iteration 50, loss = 1.20892793\n",
            "Iteration 51, loss = 1.20314851\n",
            "Iteration 52, loss = 1.19839191\n",
            "Iteration 53, loss = 1.19351743\n",
            "Iteration 54, loss = 1.18882911\n",
            "Iteration 55, loss = 1.18526318\n",
            "Iteration 56, loss = 1.18045797\n",
            "Iteration 57, loss = 1.17687468\n",
            "Iteration 58, loss = 1.17365113\n",
            "Iteration 59, loss = 1.17042202\n",
            "Iteration 60, loss = 1.16699199\n",
            "Iteration 61, loss = 1.16425239\n",
            "Iteration 62, loss = 1.16116829\n",
            "Iteration 63, loss = 1.15876494\n",
            "Iteration 64, loss = 1.15695914\n",
            "Iteration 65, loss = 1.15414194\n",
            "Iteration 66, loss = 1.15218593\n",
            "Iteration 67, loss = 1.15012952\n",
            "Iteration 68, loss = 1.14845788\n",
            "Iteration 69, loss = 1.14619025\n",
            "Iteration 70, loss = 1.14462805\n",
            "Iteration 71, loss = 1.14323913\n",
            "Iteration 72, loss = 1.14128988\n",
            "Iteration 73, loss = 1.13962756\n",
            "Iteration 74, loss = 1.13908586\n",
            "Iteration 75, loss = 1.13701234\n",
            "Iteration 76, loss = 1.13579528\n",
            "Iteration 77, loss = 1.13448175\n",
            "Iteration 78, loss = 1.13302058\n",
            "Iteration 79, loss = 1.13211137\n",
            "Iteration 80, loss = 1.13114419\n",
            "Iteration 81, loss = 1.13003222\n",
            "Iteration 82, loss = 1.12872013\n",
            "Iteration 83, loss = 1.12786074\n",
            "Iteration 84, loss = 1.12703280\n",
            "Iteration 85, loss = 1.12647503\n",
            "Iteration 86, loss = 1.12494965\n",
            "Iteration 87, loss = 1.12438440\n",
            "Iteration 88, loss = 1.12386201\n",
            "Iteration 89, loss = 1.12261776\n",
            "Iteration 90, loss = 1.12194570\n",
            "Iteration 91, loss = 1.12145744\n",
            "Iteration 92, loss = 1.12028026\n",
            "Iteration 93, loss = 1.11945130\n",
            "Iteration 94, loss = 1.11946582\n",
            "Iteration 95, loss = 1.11859389\n",
            "Iteration 96, loss = 1.11737298\n",
            "Iteration 97, loss = 1.11797290\n",
            "Iteration 98, loss = 1.11698245\n",
            "Iteration 99, loss = 1.11633777\n",
            "Iteration 100, loss = 1.11583147\n",
            "Iteration 1, loss = 1.63906929\n",
            "Iteration 2, loss = 1.59030593\n",
            "Iteration 3, loss = 1.57388857\n",
            "Iteration 4, loss = 1.53122212\n",
            "Iteration 5, loss = 1.48006051\n",
            "Iteration 6, loss = 1.42788931\n",
            "Iteration 7, loss = 1.36217175\n",
            "Iteration 8, loss = 1.29996045\n",
            "Iteration 9, loss = 1.25183357\n",
            "Iteration 10, loss = 1.20336127\n",
            "Iteration 11, loss = 1.17676544\n",
            "Iteration 12, loss = 1.15368904\n",
            "Iteration 13, loss = 1.14223274\n",
            "Iteration 14, loss = 1.13184199\n",
            "Iteration 15, loss = 1.12567478\n",
            "Iteration 16, loss = 1.12149337\n",
            "Iteration 17, loss = 1.11511663\n",
            "Iteration 18, loss = 1.11899054\n",
            "Iteration 19, loss = 1.14180188\n",
            "Iteration 20, loss = 1.12231414\n",
            "Iteration 21, loss = 1.12364037\n",
            "Iteration 22, loss = 1.12410819\n",
            "Iteration 23, loss = 1.11454654\n",
            "Iteration 24, loss = 1.11794391\n",
            "Iteration 25, loss = 1.11147995\n",
            "Iteration 26, loss = 1.11264813\n",
            "Iteration 27, loss = 1.11655771\n",
            "Iteration 28, loss = 1.11081341\n",
            "Iteration 29, loss = 1.11346594\n",
            "Iteration 30, loss = 1.12221172\n",
            "Iteration 31, loss = 1.10911094\n",
            "Iteration 32, loss = 1.11197124\n",
            "Iteration 33, loss = 1.10670328\n",
            "Iteration 34, loss = 1.11149660\n",
            "Iteration 35, loss = 1.11212076\n",
            "Iteration 36, loss = 1.11084245\n",
            "Iteration 37, loss = 1.10832533\n",
            "Iteration 38, loss = 1.10358902\n",
            "Iteration 39, loss = 1.10740886\n",
            "Iteration 40, loss = 1.09808205\n",
            "Iteration 41, loss = 1.09609734\n",
            "Iteration 42, loss = 1.09772575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 43, loss = 1.09688037\n",
            "Iteration 44, loss = 1.09206471\n",
            "Iteration 45, loss = 1.08994377\n",
            "Iteration 46, loss = 1.08552814\n",
            "Iteration 47, loss = 1.08775492\n",
            "Iteration 48, loss = 1.08527681\n",
            "Iteration 49, loss = 1.08285433\n",
            "Iteration 50, loss = 1.07716801\n",
            "Iteration 51, loss = 1.07543521\n",
            "Iteration 52, loss = 1.07162292\n",
            "Iteration 53, loss = 1.07373561\n",
            "Iteration 54, loss = 1.06654979\n",
            "Iteration 55, loss = 1.07485920\n",
            "Iteration 56, loss = 1.06389754\n",
            "Iteration 57, loss = 1.06358774\n",
            "Iteration 58, loss = 1.05852944\n",
            "Iteration 59, loss = 1.05914831\n",
            "Iteration 60, loss = 1.04944606\n",
            "Iteration 61, loss = 1.04374167\n",
            "Iteration 62, loss = 1.04805740\n",
            "Iteration 63, loss = 1.03019517\n",
            "Iteration 64, loss = 1.03269323\n",
            "Iteration 65, loss = 1.02473959\n",
            "Iteration 66, loss = 1.01815289\n",
            "Iteration 67, loss = 1.00630433\n",
            "Iteration 68, loss = 1.00618324\n",
            "Iteration 69, loss = 1.00002641\n",
            "Iteration 70, loss = 0.99379477\n",
            "Iteration 71, loss = 0.98792677\n",
            "Iteration 72, loss = 0.97826452\n",
            "Iteration 73, loss = 0.97600643\n",
            "Iteration 74, loss = 0.97678058\n",
            "Iteration 75, loss = 0.96678425\n",
            "Iteration 76, loss = 0.96475656\n",
            "Iteration 77, loss = 0.94800112\n",
            "Iteration 78, loss = 0.95346990\n",
            "Iteration 79, loss = 0.96869879\n",
            "Iteration 80, loss = 0.96032599\n",
            "Iteration 81, loss = 0.93558891\n",
            "Iteration 82, loss = 0.92155891\n",
            "Iteration 83, loss = 0.92621430\n",
            "Iteration 84, loss = 0.91206708\n",
            "Iteration 85, loss = 0.90684405\n",
            "Iteration 86, loss = 0.91103810\n",
            "Iteration 87, loss = 0.91391728\n",
            "Iteration 88, loss = 0.92708990\n",
            "Iteration 89, loss = 0.89403107\n",
            "Iteration 90, loss = 0.88296139\n",
            "Iteration 91, loss = 0.88747021\n",
            "Iteration 92, loss = 0.87743829\n",
            "Iteration 93, loss = 0.88322831\n",
            "Iteration 94, loss = 0.87052724\n",
            "Iteration 95, loss = 0.86084571\n",
            "Iteration 96, loss = 0.85957031\n",
            "Iteration 97, loss = 0.85350341\n",
            "Iteration 98, loss = 0.85576917\n",
            "Iteration 99, loss = 0.85043408\n",
            "Iteration 100, loss = 0.84794071\n",
            "Iteration 1, loss = 1.74355010\n",
            "Iteration 2, loss = 1.73407913\n",
            "Iteration 3, loss = 1.72476064\n",
            "Iteration 4, loss = 1.71583757\n",
            "Iteration 5, loss = 1.70703195\n",
            "Iteration 6, loss = 1.69894273\n",
            "Iteration 7, loss = 1.69131326\n",
            "Iteration 8, loss = 1.68347761\n",
            "Iteration 9, loss = 1.67592094\n",
            "Iteration 10, loss = 1.66943811\n",
            "Iteration 11, loss = 1.66263314\n",
            "Iteration 12, loss = 1.65599597\n",
            "Iteration 13, loss = 1.65064662\n",
            "Iteration 14, loss = 1.64484977\n",
            "Iteration 15, loss = 1.63936076\n",
            "Iteration 16, loss = 1.63463831\n",
            "Iteration 17, loss = 1.62988575\n",
            "Iteration 18, loss = 1.62540482\n",
            "Iteration 19, loss = 1.62110167\n",
            "Iteration 20, loss = 1.61755337\n",
            "Iteration 21, loss = 1.61366360\n",
            "Iteration 22, loss = 1.61031981\n",
            "Iteration 23, loss = 1.60735952\n",
            "Iteration 24, loss = 1.60419481\n",
            "Iteration 25, loss = 1.60165770\n",
            "Iteration 26, loss = 1.59920473\n",
            "Iteration 27, loss = 1.59704153\n",
            "Iteration 28, loss = 1.59502494\n",
            "Iteration 29, loss = 1.59282736\n",
            "Iteration 30, loss = 1.59096645\n",
            "Iteration 31, loss = 1.58952268\n",
            "Iteration 32, loss = 1.58800719\n",
            "Iteration 33, loss = 1.58633250\n",
            "Iteration 34, loss = 1.58542598\n",
            "Iteration 35, loss = 1.58394604\n",
            "Iteration 36, loss = 1.58279199\n",
            "Iteration 37, loss = 1.58174345\n",
            "Iteration 38, loss = 1.58085930\n",
            "Iteration 39, loss = 1.57974162\n",
            "Iteration 40, loss = 1.57882026\n",
            "Iteration 41, loss = 1.57819868\n",
            "Iteration 42, loss = 1.57736992\n",
            "Iteration 43, loss = 1.57643029\n",
            "Iteration 44, loss = 1.57575516\n",
            "Iteration 45, loss = 1.57498531\n",
            "Iteration 46, loss = 1.57433700\n",
            "Iteration 47, loss = 1.57367232\n",
            "Iteration 48, loss = 1.57283504\n",
            "Iteration 49, loss = 1.57224637\n",
            "Iteration 50, loss = 1.57151534\n",
            "Iteration 51, loss = 1.57096623\n",
            "Iteration 52, loss = 1.57020504\n",
            "Iteration 53, loss = 1.56961130\n",
            "Iteration 54, loss = 1.56890542\n",
            "Iteration 55, loss = 1.56826224\n",
            "Iteration 56, loss = 1.56757621\n",
            "Iteration 57, loss = 1.56695534\n",
            "Iteration 58, loss = 1.56631692\n",
            "Iteration 59, loss = 1.56558740\n",
            "Iteration 60, loss = 1.56493510\n",
            "Iteration 61, loss = 1.56426152\n",
            "Iteration 62, loss = 1.56360446\n",
            "Iteration 63, loss = 1.56290136\n",
            "Iteration 64, loss = 1.56222752\n",
            "Iteration 65, loss = 1.56156000\n",
            "Iteration 66, loss = 1.56084668\n",
            "Iteration 67, loss = 1.56015863\n",
            "Iteration 68, loss = 1.55947267\n",
            "Iteration 69, loss = 1.55876703\n",
            "Iteration 70, loss = 1.55804732\n",
            "Iteration 71, loss = 1.55735243\n",
            "Iteration 72, loss = 1.55667411\n",
            "Iteration 73, loss = 1.55588713\n",
            "Iteration 74, loss = 1.55519680\n",
            "Iteration 75, loss = 1.55444562\n",
            "Iteration 76, loss = 1.55369619\n",
            "Iteration 77, loss = 1.55301411\n",
            "Iteration 78, loss = 1.55224118\n",
            "Iteration 79, loss = 1.55146879\n",
            "Iteration 80, loss = 1.55078146\n",
            "Iteration 81, loss = 1.54994029\n",
            "Iteration 82, loss = 1.54923033\n",
            "Iteration 83, loss = 1.54839906\n",
            "Iteration 84, loss = 1.54763199\n",
            "Iteration 85, loss = 1.54686201\n",
            "Iteration 86, loss = 1.54606635\n",
            "Iteration 87, loss = 1.54527931\n",
            "Iteration 88, loss = 1.54445577\n",
            "Iteration 89, loss = 1.54366598\n",
            "Iteration 90, loss = 1.54284665\n",
            "Iteration 91, loss = 1.54200127\n",
            "Iteration 92, loss = 1.54120553\n",
            "Iteration 93, loss = 1.54035365\n",
            "Iteration 94, loss = 1.53957228\n",
            "Iteration 95, loss = 1.53866866\n",
            "Iteration 96, loss = 1.53785717\n",
            "Iteration 97, loss = 1.53696457\n",
            "Iteration 98, loss = 1.53609089\n",
            "Iteration 99, loss = 1.53525320\n",
            "Iteration 100, loss = 1.53438085\n",
            "Iteration 1, loss = 1.72252527\n",
            "Iteration 2, loss = 1.64756166\n",
            "Iteration 3, loss = 1.60200980\n",
            "Iteration 4, loss = 1.58322141\n",
            "Iteration 5, loss = 1.58097532\n",
            "Iteration 6, loss = 1.58496430\n",
            "Iteration 7, loss = 1.58355290\n",
            "Iteration 8, loss = 1.57559466\n",
            "Iteration 9, loss = 1.56575937\n",
            "Iteration 10, loss = 1.55305895\n",
            "Iteration 11, loss = 1.54441486\n",
            "Iteration 12, loss = 1.53757280\n",
            "Iteration 13, loss = 1.52965685\n",
            "Iteration 14, loss = 1.52262313\n",
            "Iteration 15, loss = 1.51464090\n",
            "Iteration 16, loss = 1.50648358\n",
            "Iteration 17, loss = 1.49689037\n",
            "Iteration 18, loss = 1.48654378\n",
            "Iteration 19, loss = 1.47543623\n",
            "Iteration 20, loss = 1.46459609\n",
            "Iteration 21, loss = 1.45296865\n",
            "Iteration 22, loss = 1.44190789\n",
            "Iteration 23, loss = 1.43004817\n",
            "Iteration 24, loss = 1.41826027\n",
            "Iteration 25, loss = 1.40510503\n",
            "Iteration 26, loss = 1.39275043\n",
            "Iteration 27, loss = 1.38021342\n",
            "Iteration 28, loss = 1.36713319\n",
            "Iteration 29, loss = 1.35393165\n",
            "Iteration 30, loss = 1.34200551\n",
            "Iteration 31, loss = 1.32935629\n",
            "Iteration 32, loss = 1.31796602\n",
            "Iteration 33, loss = 1.30569441\n",
            "Iteration 34, loss = 1.29526145\n",
            "Iteration 35, loss = 1.28380439\n",
            "Iteration 36, loss = 1.27361390\n",
            "Iteration 37, loss = 1.26380213\n",
            "Iteration 38, loss = 1.25473363\n",
            "Iteration 39, loss = 1.24586061\n",
            "Iteration 40, loss = 1.23828690\n",
            "Iteration 41, loss = 1.23075005\n",
            "Iteration 42, loss = 1.22332105\n",
            "Iteration 43, loss = 1.21639847\n",
            "Iteration 44, loss = 1.21025028\n",
            "Iteration 45, loss = 1.20440931\n",
            "Iteration 46, loss = 1.19892141\n",
            "Iteration 47, loss = 1.19480808\n",
            "Iteration 48, loss = 1.18977563\n",
            "Iteration 49, loss = 1.18488844\n",
            "Iteration 50, loss = 1.18086084\n",
            "Iteration 51, loss = 1.17664112\n",
            "Iteration 52, loss = 1.17325024\n",
            "Iteration 53, loss = 1.16964354\n",
            "Iteration 54, loss = 1.16657532\n",
            "Iteration 55, loss = 1.16313507\n",
            "Iteration 56, loss = 1.16037781\n",
            "Iteration 57, loss = 1.15784077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 1.15538416\n",
            "Iteration 59, loss = 1.15286516\n",
            "Iteration 60, loss = 1.15046504\n",
            "Iteration 61, loss = 1.14796365\n",
            "Iteration 62, loss = 1.14579195\n",
            "Iteration 63, loss = 1.14356495\n",
            "Iteration 64, loss = 1.14180710\n",
            "Iteration 65, loss = 1.14039788\n",
            "Iteration 66, loss = 1.13797549\n",
            "Iteration 67, loss = 1.13607500\n",
            "Iteration 68, loss = 1.13475007\n",
            "Iteration 69, loss = 1.13324854\n",
            "Iteration 70, loss = 1.13141023\n",
            "Iteration 71, loss = 1.12959728\n",
            "Iteration 72, loss = 1.12880644\n",
            "Iteration 73, loss = 1.12638509\n",
            "Iteration 74, loss = 1.12575813\n",
            "Iteration 75, loss = 1.12434098\n",
            "Iteration 76, loss = 1.12260604\n",
            "Iteration 77, loss = 1.12207231\n",
            "Iteration 78, loss = 1.12115873\n",
            "Iteration 79, loss = 1.11857078\n",
            "Iteration 80, loss = 1.11906901\n",
            "Iteration 81, loss = 1.11630177\n",
            "Iteration 82, loss = 1.11619997\n",
            "Iteration 83, loss = 1.11371741\n",
            "Iteration 84, loss = 1.11297417\n",
            "Iteration 85, loss = 1.11158945\n",
            "Iteration 86, loss = 1.11020974\n",
            "Iteration 87, loss = 1.10961139\n",
            "Iteration 88, loss = 1.10783747\n",
            "Iteration 89, loss = 1.10714531\n",
            "Iteration 90, loss = 1.10578136\n",
            "Iteration 91, loss = 1.10443572\n",
            "Iteration 92, loss = 1.10365542\n",
            "Iteration 93, loss = 1.10236447\n",
            "Iteration 94, loss = 1.10165653\n",
            "Iteration 95, loss = 1.10058445\n",
            "Iteration 96, loss = 1.10040579\n",
            "Iteration 97, loss = 1.09804916\n",
            "Iteration 98, loss = 1.09668985\n",
            "Iteration 99, loss = 1.09616864\n",
            "Iteration 100, loss = 1.09490278\n",
            "Iteration 1, loss = 1.68833247\n",
            "Iteration 2, loss = 1.60555645\n",
            "Iteration 3, loss = 1.53884868\n",
            "Iteration 4, loss = 1.46860282\n",
            "Iteration 5, loss = 1.38527106\n",
            "Iteration 6, loss = 1.31954027\n",
            "Iteration 7, loss = 1.24923145\n",
            "Iteration 8, loss = 1.20402447\n",
            "Iteration 9, loss = 1.17736146\n",
            "Iteration 10, loss = 1.14846425\n",
            "Iteration 11, loss = 1.14514992\n",
            "Iteration 12, loss = 1.13148292\n",
            "Iteration 13, loss = 1.11988561\n",
            "Iteration 14, loss = 1.11388555\n",
            "Iteration 15, loss = 1.11175446\n",
            "Iteration 16, loss = 1.11090885\n",
            "Iteration 17, loss = 1.11369227\n",
            "Iteration 18, loss = 1.14507390\n",
            "Iteration 19, loss = 1.13668664\n",
            "Iteration 20, loss = 1.11256174\n",
            "Iteration 21, loss = 1.12689176\n",
            "Iteration 22, loss = 1.12010092\n",
            "Iteration 23, loss = 1.10341219\n",
            "Iteration 24, loss = 1.10543186\n",
            "Iteration 25, loss = 1.10467200\n",
            "Iteration 26, loss = 1.08849574\n",
            "Iteration 27, loss = 1.09107729\n",
            "Iteration 28, loss = 1.08512942\n",
            "Iteration 29, loss = 1.08660213\n",
            "Iteration 30, loss = 1.07316708\n",
            "Iteration 31, loss = 1.05654339\n",
            "Iteration 32, loss = 1.05176731\n",
            "Iteration 33, loss = 1.04602292\n",
            "Iteration 34, loss = 1.04839072\n",
            "Iteration 35, loss = 1.04293531\n",
            "Iteration 36, loss = 1.03095402\n",
            "Iteration 37, loss = 1.01849736\n",
            "Iteration 38, loss = 1.03686406\n",
            "Iteration 39, loss = 1.03054750\n",
            "Iteration 40, loss = 1.00366559\n",
            "Iteration 41, loss = 1.00690715\n",
            "Iteration 42, loss = 1.04141218\n",
            "Iteration 43, loss = 1.04366139\n",
            "Iteration 44, loss = 1.03693978\n",
            "Iteration 45, loss = 1.01658787\n",
            "Iteration 46, loss = 0.99014676\n",
            "Iteration 47, loss = 0.98558754\n",
            "Iteration 48, loss = 0.99623159\n",
            "Iteration 49, loss = 1.00887584\n",
            "Iteration 50, loss = 0.97054030\n",
            "Iteration 51, loss = 0.98024191\n",
            "Iteration 52, loss = 0.94745539\n",
            "Iteration 53, loss = 0.95276526\n",
            "Iteration 54, loss = 0.94231104\n",
            "Iteration 55, loss = 0.92052565\n",
            "Iteration 56, loss = 0.91116742\n",
            "Iteration 57, loss = 0.90534241\n",
            "Iteration 58, loss = 0.90511458\n",
            "Iteration 59, loss = 0.91084160\n",
            "Iteration 60, loss = 0.92584045\n",
            "Iteration 61, loss = 0.92172250\n",
            "Iteration 62, loss = 0.90074832\n",
            "Iteration 63, loss = 0.88648473\n",
            "Iteration 64, loss = 0.88513583\n",
            "Iteration 65, loss = 0.88161870\n",
            "Iteration 66, loss = 0.88370512\n",
            "Iteration 67, loss = 0.87982768\n",
            "Iteration 68, loss = 0.87068759\n",
            "Iteration 69, loss = 0.89160586\n",
            "Iteration 70, loss = 0.90054890\n",
            "Iteration 71, loss = 0.85196084\n",
            "Iteration 72, loss = 0.88757345\n",
            "Iteration 73, loss = 0.89567272\n",
            "Iteration 74, loss = 0.90060320\n",
            "Iteration 75, loss = 0.87179788\n",
            "Iteration 76, loss = 0.86213295\n",
            "Iteration 77, loss = 0.87643550\n",
            "Iteration 78, loss = 0.84932822\n",
            "Iteration 79, loss = 0.82978788\n",
            "Iteration 80, loss = 0.84922531\n",
            "Iteration 81, loss = 0.83432557\n",
            "Iteration 82, loss = 0.84783514\n",
            "Iteration 83, loss = 0.81606657\n",
            "Iteration 84, loss = 0.82552305\n",
            "Iteration 85, loss = 0.82406383\n",
            "Iteration 86, loss = 0.81425653\n",
            "Iteration 87, loss = 0.84949792\n",
            "Iteration 88, loss = 0.81735941\n",
            "Iteration 89, loss = 0.81805914\n",
            "Iteration 90, loss = 0.82120245\n",
            "Iteration 91, loss = 0.80053879\n",
            "Iteration 92, loss = 0.80143063\n",
            "Iteration 93, loss = 0.79211062\n",
            "Iteration 94, loss = 0.79796121\n",
            "Iteration 95, loss = 0.80146016\n",
            "Iteration 96, loss = 0.82448715\n",
            "Iteration 97, loss = 0.87546522\n",
            "Iteration 98, loss = 0.82980187\n",
            "Iteration 99, loss = 0.83968136\n",
            "Iteration 100, loss = 0.84175575\n",
            "Iteration 1, loss = 1.67610678\n",
            "Iteration 2, loss = 1.66885316\n",
            "Iteration 3, loss = 1.66191590\n",
            "Iteration 4, loss = 1.65482944\n",
            "Iteration 5, loss = 1.64833251\n",
            "Iteration 6, loss = 1.64292907\n",
            "Iteration 7, loss = 1.63732172\n",
            "Iteration 8, loss = 1.63193109\n",
            "Iteration 9, loss = 1.62768820\n",
            "Iteration 10, loss = 1.62350391\n",
            "Iteration 11, loss = 1.61944695\n",
            "Iteration 12, loss = 1.61605123\n",
            "Iteration 13, loss = 1.61314643\n",
            "Iteration 14, loss = 1.61079133\n",
            "Iteration 15, loss = 1.60806680\n",
            "Iteration 16, loss = 1.60616242\n",
            "Iteration 17, loss = 1.60436218\n",
            "Iteration 18, loss = 1.60307011\n",
            "Iteration 19, loss = 1.60153754\n",
            "Iteration 20, loss = 1.60099788\n",
            "Iteration 21, loss = 1.59992272\n",
            "Iteration 22, loss = 1.59940472\n",
            "Iteration 23, loss = 1.59870044\n",
            "Iteration 24, loss = 1.59824209\n",
            "Iteration 25, loss = 1.59791670\n",
            "Iteration 26, loss = 1.59760403\n",
            "Iteration 27, loss = 1.59741087\n",
            "Iteration 28, loss = 1.59729002\n",
            "Iteration 29, loss = 1.59710890\n",
            "Iteration 30, loss = 1.59694873\n",
            "Iteration 31, loss = 1.59692426\n",
            "Iteration 32, loss = 1.59681599\n",
            "Iteration 33, loss = 1.59669252\n",
            "Iteration 34, loss = 1.59674145\n",
            "Iteration 35, loss = 1.59657403\n",
            "Iteration 36, loss = 1.59652159\n",
            "Iteration 37, loss = 1.59641636\n",
            "Iteration 38, loss = 1.59637012\n",
            "Iteration 39, loss = 1.59628188\n",
            "Iteration 40, loss = 1.59624789\n",
            "Iteration 41, loss = 1.59618342\n",
            "Iteration 42, loss = 1.59607478\n",
            "Iteration 43, loss = 1.59602092\n",
            "Iteration 44, loss = 1.59592003\n",
            "Iteration 45, loss = 1.59584403\n",
            "Iteration 46, loss = 1.59576688\n",
            "Iteration 47, loss = 1.59570655\n",
            "Iteration 48, loss = 1.59564398\n",
            "Iteration 49, loss = 1.59553979\n",
            "Iteration 50, loss = 1.59546623\n",
            "Iteration 51, loss = 1.59543987\n",
            "Iteration 52, loss = 1.59532905\n",
            "Iteration 53, loss = 1.59527293\n",
            "Iteration 54, loss = 1.59517971\n",
            "Iteration 55, loss = 1.59510036\n",
            "Iteration 56, loss = 1.59501731\n",
            "Iteration 57, loss = 1.59496287\n",
            "Iteration 58, loss = 1.59492514\n",
            "Iteration 59, loss = 1.59484469\n",
            "Iteration 60, loss = 1.59474014\n",
            "Iteration 61, loss = 1.59465858\n",
            "Iteration 62, loss = 1.59458185\n",
            "Iteration 63, loss = 1.59449580\n",
            "Iteration 64, loss = 1.59442312\n",
            "Iteration 65, loss = 1.59436953\n",
            "Iteration 66, loss = 1.59429110\n",
            "Iteration 67, loss = 1.59419877\n",
            "Iteration 68, loss = 1.59411821\n",
            "Iteration 69, loss = 1.59405163\n",
            "Iteration 70, loss = 1.59398235\n",
            "Iteration 71, loss = 1.59389254\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.65706272\n",
            "Iteration 2, loss = 1.61025851\n",
            "Iteration 3, loss = 1.60107823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 1.60664603\n",
            "Iteration 5, loss = 1.61081638\n",
            "Iteration 6, loss = 1.60595044\n",
            "Iteration 7, loss = 1.59922436\n",
            "Iteration 8, loss = 1.59621966\n",
            "Iteration 9, loss = 1.59604464\n",
            "Iteration 10, loss = 1.59568078\n",
            "Iteration 11, loss = 1.59618569\n",
            "Iteration 12, loss = 1.59523985\n",
            "Iteration 13, loss = 1.59270576\n",
            "Iteration 14, loss = 1.59181964\n",
            "Iteration 15, loss = 1.58964969\n",
            "Iteration 16, loss = 1.58973177\n",
            "Iteration 17, loss = 1.58894236\n",
            "Iteration 18, loss = 1.58812901\n",
            "Iteration 19, loss = 1.58645473\n",
            "Iteration 20, loss = 1.58461276\n",
            "Iteration 21, loss = 1.58356722\n",
            "Iteration 22, loss = 1.58291674\n",
            "Iteration 23, loss = 1.58218197\n",
            "Iteration 24, loss = 1.58042024\n",
            "Iteration 25, loss = 1.57814670\n",
            "Iteration 26, loss = 1.57653715\n",
            "Iteration 27, loss = 1.57539288\n",
            "Iteration 28, loss = 1.57373482\n",
            "Iteration 29, loss = 1.57153895\n",
            "Iteration 30, loss = 1.56999106\n",
            "Iteration 31, loss = 1.56727900\n",
            "Iteration 32, loss = 1.56550542\n",
            "Iteration 33, loss = 1.56295538\n",
            "Iteration 34, loss = 1.56123848\n",
            "Iteration 35, loss = 1.55777715\n",
            "Iteration 36, loss = 1.55497190\n",
            "Iteration 37, loss = 1.55195326\n",
            "Iteration 38, loss = 1.54921200\n",
            "Iteration 39, loss = 1.54562682\n",
            "Iteration 40, loss = 1.54282552\n",
            "Iteration 41, loss = 1.53924548\n",
            "Iteration 42, loss = 1.53507776\n",
            "Iteration 43, loss = 1.53090518\n",
            "Iteration 44, loss = 1.52653784\n",
            "Iteration 45, loss = 1.52213400\n",
            "Iteration 46, loss = 1.51751842\n",
            "Iteration 47, loss = 1.51313335\n",
            "Iteration 48, loss = 1.50781406\n",
            "Iteration 49, loss = 1.50250189\n",
            "Iteration 50, loss = 1.49678772\n",
            "Iteration 51, loss = 1.49181698\n",
            "Iteration 52, loss = 1.48549366\n",
            "Iteration 53, loss = 1.47977987\n",
            "Iteration 54, loss = 1.47331431\n",
            "Iteration 55, loss = 1.46680089\n",
            "Iteration 56, loss = 1.46026224\n",
            "Iteration 57, loss = 1.45389208\n",
            "Iteration 58, loss = 1.44764993\n",
            "Iteration 59, loss = 1.44044200\n",
            "Iteration 60, loss = 1.43323087\n",
            "Iteration 61, loss = 1.42612187\n",
            "Iteration 62, loss = 1.41910568\n",
            "Iteration 63, loss = 1.41163113\n",
            "Iteration 64, loss = 1.40453771\n",
            "Iteration 65, loss = 1.39750623\n",
            "Iteration 66, loss = 1.38984568\n",
            "Iteration 67, loss = 1.38244167\n",
            "Iteration 68, loss = 1.37531680\n",
            "Iteration 69, loss = 1.36812749\n",
            "Iteration 70, loss = 1.36112743\n",
            "Iteration 71, loss = 1.35372632\n",
            "Iteration 72, loss = 1.34721475\n",
            "Iteration 73, loss = 1.33953948\n",
            "Iteration 74, loss = 1.33326891\n",
            "Iteration 75, loss = 1.32646543\n",
            "Iteration 76, loss = 1.31975550\n",
            "Iteration 77, loss = 1.31434758\n",
            "Iteration 78, loss = 1.30753716\n",
            "Iteration 79, loss = 1.30094111\n",
            "Iteration 80, loss = 1.29607263\n",
            "Iteration 81, loss = 1.28947266\n",
            "Iteration 82, loss = 1.28490267\n",
            "Iteration 83, loss = 1.27846024\n",
            "Iteration 84, loss = 1.27358245\n",
            "Iteration 85, loss = 1.26841390\n",
            "Iteration 86, loss = 1.26344697\n",
            "Iteration 87, loss = 1.25906692\n",
            "Iteration 88, loss = 1.25417116\n",
            "Iteration 89, loss = 1.24988441\n",
            "Iteration 90, loss = 1.24571152\n",
            "Iteration 91, loss = 1.24128992\n",
            "Iteration 92, loss = 1.23777338\n",
            "Iteration 93, loss = 1.23372607\n",
            "Iteration 94, loss = 1.23042238\n",
            "Iteration 95, loss = 1.22689145\n",
            "Iteration 96, loss = 1.22422566\n",
            "Iteration 97, loss = 1.22021058\n",
            "Iteration 98, loss = 1.21694630\n",
            "Iteration 99, loss = 1.21439072\n",
            "Iteration 100, loss = 1.21123437\n",
            "Iteration 1, loss = 1.70497077\n",
            "Iteration 2, loss = 1.63424863\n",
            "Iteration 3, loss = 1.67139234\n",
            "Iteration 4, loss = 1.59591594\n",
            "Iteration 5, loss = 1.60141809\n",
            "Iteration 6, loss = 1.60928454\n",
            "Iteration 7, loss = 1.58015171\n",
            "Iteration 8, loss = 1.57343252\n",
            "Iteration 9, loss = 1.57043290\n",
            "Iteration 10, loss = 1.54638544\n",
            "Iteration 11, loss = 1.52193352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 1.49973822\n",
            "Iteration 13, loss = 1.47374920\n",
            "Iteration 14, loss = 1.43559441\n",
            "Iteration 15, loss = 1.39425638\n",
            "Iteration 16, loss = 1.35561715\n",
            "Iteration 17, loss = 1.31632496\n",
            "Iteration 18, loss = 1.27865142\n",
            "Iteration 19, loss = 1.24501341\n",
            "Iteration 20, loss = 1.21892971\n",
            "Iteration 21, loss = 1.19459956\n",
            "Iteration 22, loss = 1.17871104\n",
            "Iteration 23, loss = 1.17383516\n",
            "Iteration 24, loss = 1.15881722\n",
            "Iteration 25, loss = 1.14856267\n",
            "Iteration 26, loss = 1.14193084\n",
            "Iteration 27, loss = 1.13599948\n",
            "Iteration 28, loss = 1.13405936\n",
            "Iteration 29, loss = 1.13078395\n",
            "Iteration 30, loss = 1.13535761\n",
            "Iteration 31, loss = 1.12226930\n",
            "Iteration 32, loss = 1.12683466\n",
            "Iteration 33, loss = 1.12218532\n",
            "Iteration 34, loss = 1.12316687\n",
            "Iteration 35, loss = 1.11735345\n",
            "Iteration 36, loss = 1.11164214\n",
            "Iteration 37, loss = 1.11248460\n",
            "Iteration 38, loss = 1.11682742\n",
            "Iteration 39, loss = 1.11058464\n",
            "Iteration 40, loss = 1.11694102\n",
            "Iteration 41, loss = 1.12606845\n",
            "Iteration 42, loss = 1.12909454\n",
            "Iteration 43, loss = 1.12093035\n",
            "Iteration 44, loss = 1.12319888\n",
            "Iteration 45, loss = 1.11170082\n",
            "Iteration 46, loss = 1.11315762\n",
            "Iteration 47, loss = 1.11661206\n",
            "Iteration 48, loss = 1.12031997\n",
            "Iteration 49, loss = 1.11505765\n",
            "Iteration 50, loss = 1.10772285\n",
            "Iteration 51, loss = 1.10932253\n",
            "Iteration 52, loss = 1.11709469\n",
            "Iteration 53, loss = 1.11279336\n",
            "Iteration 54, loss = 1.10944364\n",
            "Iteration 55, loss = 1.10614956\n",
            "Iteration 56, loss = 1.10706056\n",
            "Iteration 57, loss = 1.10819558\n",
            "Iteration 58, loss = 1.10922286\n",
            "Iteration 59, loss = 1.10666224\n",
            "Iteration 60, loss = 1.10469968\n",
            "Iteration 61, loss = 1.10279075\n",
            "Iteration 62, loss = 1.10093632\n",
            "Iteration 63, loss = 1.09995396\n",
            "Iteration 64, loss = 1.10179241\n",
            "Iteration 65, loss = 1.10558189\n",
            "Iteration 66, loss = 1.09583401\n",
            "Iteration 67, loss = 1.10101038\n",
            "Iteration 68, loss = 1.09723129\n",
            "Iteration 69, loss = 1.09438466\n",
            "Iteration 70, loss = 1.10035681\n",
            "Iteration 71, loss = 1.09453776\n",
            "Iteration 72, loss = 1.09602856\n",
            "Iteration 73, loss = 1.08842485\n",
            "Iteration 74, loss = 1.09504034\n",
            "Iteration 75, loss = 1.08878064\n",
            "Iteration 76, loss = 1.08662173\n",
            "Iteration 77, loss = 1.09218342\n",
            "Iteration 78, loss = 1.09486538\n",
            "Iteration 79, loss = 1.08018616\n",
            "Iteration 80, loss = 1.10014668\n",
            "Iteration 81, loss = 1.07741164\n",
            "Iteration 82, loss = 1.10239079\n",
            "Iteration 83, loss = 1.08085795\n",
            "Iteration 84, loss = 1.09547898\n",
            "Iteration 85, loss = 1.08700498\n",
            "Iteration 86, loss = 1.07973218\n",
            "Iteration 87, loss = 1.09261429\n",
            "Iteration 88, loss = 1.07490002\n",
            "Iteration 89, loss = 1.07750727\n",
            "Iteration 90, loss = 1.07405241\n",
            "Iteration 91, loss = 1.07048279\n",
            "Iteration 92, loss = 1.06659295\n",
            "Iteration 93, loss = 1.06521079\n",
            "Iteration 94, loss = 1.04910732\n",
            "Iteration 95, loss = 1.05911797\n",
            "Iteration 96, loss = 1.06593561\n",
            "Iteration 97, loss = 1.06115100\n",
            "Iteration 98, loss = 1.05479430\n",
            "Iteration 99, loss = 1.05089096\n",
            "Iteration 100, loss = 1.03742831\n",
            "Iteration 1, loss = 1.74052277\n",
            "Iteration 2, loss = 1.73161771\n",
            "Iteration 3, loss = 1.72282788\n",
            "Iteration 4, loss = 1.71440883\n",
            "Iteration 5, loss = 1.70608149\n",
            "Iteration 6, loss = 1.69841992\n",
            "Iteration 7, loss = 1.69117966\n",
            "Iteration 8, loss = 1.68372874\n",
            "Iteration 9, loss = 1.67653859\n",
            "Iteration 10, loss = 1.67033395\n",
            "Iteration 11, loss = 1.66382294\n",
            "Iteration 12, loss = 1.65745403\n",
            "Iteration 13, loss = 1.65228956\n",
            "Iteration 14, loss = 1.64669532\n",
            "Iteration 15, loss = 1.64137929\n",
            "Iteration 16, loss = 1.63677945\n",
            "Iteration 17, loss = 1.63214431\n",
            "Iteration 18, loss = 1.62775593\n",
            "Iteration 19, loss = 1.62352759\n",
            "Iteration 20, loss = 1.62001375\n",
            "Iteration 21, loss = 1.61616297\n",
            "Iteration 22, loss = 1.61283007\n",
            "Iteration 23, loss = 1.60985385\n",
            "Iteration 24, loss = 1.60667892\n",
            "Iteration 25, loss = 1.60410418\n",
            "Iteration 26, loss = 1.60160514\n",
            "Iteration 27, loss = 1.59938379\n",
            "Iteration 28, loss = 1.59730512\n",
            "Iteration 29, loss = 1.59504834\n",
            "Iteration 30, loss = 1.59311279\n",
            "Iteration 31, loss = 1.59159705\n",
            "Iteration 32, loss = 1.59000129\n",
            "Iteration 33, loss = 1.58825673\n",
            "Iteration 34, loss = 1.58727489\n",
            "Iteration 35, loss = 1.58572467\n",
            "Iteration 36, loss = 1.58450765\n",
            "Iteration 37, loss = 1.58339026\n",
            "Iteration 38, loss = 1.58245111\n",
            "Iteration 39, loss = 1.58127981\n",
            "Iteration 40, loss = 1.58029549\n",
            "Iteration 41, loss = 1.57965456\n",
            "Iteration 42, loss = 1.57878751\n",
            "Iteration 43, loss = 1.57781371\n",
            "Iteration 44, loss = 1.57711671\n",
            "Iteration 45, loss = 1.57631559\n",
            "Iteration 46, loss = 1.57566286\n",
            "Iteration 47, loss = 1.57499660\n",
            "Iteration 48, loss = 1.57413408\n",
            "Iteration 49, loss = 1.57356160\n",
            "Iteration 50, loss = 1.57282175\n",
            "Iteration 51, loss = 1.57228263\n",
            "Iteration 52, loss = 1.57152603\n",
            "Iteration 53, loss = 1.57095269\n",
            "Iteration 54, loss = 1.57025375\n",
            "Iteration 55, loss = 1.56963056\n",
            "Iteration 56, loss = 1.56896101\n",
            "Iteration 57, loss = 1.56836546\n",
            "Iteration 58, loss = 1.56774787\n",
            "Iteration 59, loss = 1.56703239\n",
            "Iteration 60, loss = 1.56641100\n",
            "Iteration 61, loss = 1.56575964\n",
            "Iteration 62, loss = 1.56513209\n",
            "Iteration 63, loss = 1.56445209\n",
            "Iteration 64, loss = 1.56380735\n",
            "Iteration 65, loss = 1.56316742\n",
            "Iteration 66, loss = 1.56248218\n",
            "Iteration 67, loss = 1.56182249\n",
            "Iteration 68, loss = 1.56116831\n",
            "Iteration 69, loss = 1.56049180\n",
            "Iteration 70, loss = 1.55979834\n",
            "Iteration 71, loss = 1.55913900\n",
            "Iteration 72, loss = 1.55848681\n",
            "Iteration 73, loss = 1.55773473\n",
            "Iteration 74, loss = 1.55707563\n",
            "Iteration 75, loss = 1.55635474\n",
            "Iteration 76, loss = 1.55563905\n",
            "Iteration 77, loss = 1.55498208\n",
            "Iteration 78, loss = 1.55424595\n",
            "Iteration 79, loss = 1.55350758\n",
            "Iteration 80, loss = 1.55284899\n",
            "Iteration 81, loss = 1.55204453\n",
            "Iteration 82, loss = 1.55136572\n",
            "Iteration 83, loss = 1.55057045\n",
            "Iteration 84, loss = 1.54983469\n",
            "Iteration 85, loss = 1.54909890\n",
            "Iteration 86, loss = 1.54833732\n",
            "Iteration 87, loss = 1.54758249\n",
            "Iteration 88, loss = 1.54679480\n",
            "Iteration 89, loss = 1.54603862\n",
            "Iteration 90, loss = 1.54525399\n",
            "Iteration 91, loss = 1.54444455\n",
            "Iteration 92, loss = 1.54368196\n",
            "Iteration 93, loss = 1.54286626\n",
            "Iteration 94, loss = 1.54211599\n",
            "Iteration 95, loss = 1.54125008\n",
            "Iteration 96, loss = 1.54047153\n",
            "Iteration 97, loss = 1.53961676\n",
            "Iteration 98, loss = 1.53877866\n",
            "Iteration 99, loss = 1.53797414\n",
            "Iteration 100, loss = 1.53713823\n",
            "Iteration 1, loss = 1.72079722\n",
            "Iteration 2, loss = 1.64939156\n",
            "Iteration 3, loss = 1.60447814\n",
            "Iteration 4, loss = 1.58417483\n",
            "Iteration 5, loss = 1.57981980\n",
            "Iteration 6, loss = 1.58341794\n",
            "Iteration 7, loss = 1.58345352\n",
            "Iteration 8, loss = 1.57697937\n",
            "Iteration 9, loss = 1.56785664\n",
            "Iteration 10, loss = 1.55541201\n",
            "Iteration 11, loss = 1.54644937\n",
            "Iteration 12, loss = 1.53917037\n",
            "Iteration 13, loss = 1.53077979\n",
            "Iteration 14, loss = 1.52373344\n",
            "Iteration 15, loss = 1.51600450\n",
            "Iteration 16, loss = 1.50813039\n",
            "Iteration 17, loss = 1.49888489\n",
            "Iteration 18, loss = 1.48879378\n",
            "Iteration 19, loss = 1.47774058\n",
            "Iteration 20, loss = 1.46685602\n",
            "Iteration 21, loss = 1.45512912\n",
            "Iteration 22, loss = 1.44398511\n",
            "Iteration 23, loss = 1.43213537\n",
            "Iteration 24, loss = 1.42038882\n",
            "Iteration 25, loss = 1.40729456\n",
            "Iteration 26, loss = 1.39498577\n",
            "Iteration 27, loss = 1.38246168\n",
            "Iteration 28, loss = 1.36930192\n",
            "Iteration 29, loss = 1.35603862\n",
            "Iteration 30, loss = 1.34395465\n",
            "Iteration 31, loss = 1.33115346\n",
            "Iteration 32, loss = 1.31964503\n",
            "Iteration 33, loss = 1.30725372\n",
            "Iteration 34, loss = 1.29660426\n",
            "Iteration 35, loss = 1.28507385\n",
            "Iteration 36, loss = 1.27474991\n",
            "Iteration 37, loss = 1.26478156\n",
            "Iteration 38, loss = 1.25554321\n",
            "Iteration 39, loss = 1.24652898\n",
            "Iteration 40, loss = 1.23873352\n",
            "Iteration 41, loss = 1.23108924\n",
            "Iteration 42, loss = 1.22362540\n",
            "Iteration 43, loss = 1.21658395\n",
            "Iteration 44, loss = 1.21037513\n",
            "Iteration 45, loss = 1.20445270\n",
            "Iteration 46, loss = 1.19891939\n",
            "Iteration 47, loss = 1.19468724\n",
            "Iteration 48, loss = 1.18960408\n",
            "Iteration 49, loss = 1.18470368\n",
            "Iteration 50, loss = 1.18065384\n",
            "Iteration 51, loss = 1.17640637\n",
            "Iteration 52, loss = 1.17296549\n",
            "Iteration 53, loss = 1.16937076\n",
            "Iteration 54, loss = 1.16631115\n",
            "Iteration 55, loss = 1.16289269\n",
            "Iteration 56, loss = 1.16011690\n",
            "Iteration 57, loss = 1.15757216\n",
            "Iteration 58, loss = 1.15508917\n",
            "Iteration 59, loss = 1.15263866\n",
            "Iteration 60, loss = 1.15020969\n",
            "Iteration 61, loss = 1.14775983\n",
            "Iteration 62, loss = 1.14560130\n",
            "Iteration 63, loss = 1.14341646\n",
            "Iteration 64, loss = 1.14168571\n",
            "Iteration 65, loss = 1.14026525\n",
            "Iteration 66, loss = 1.13792278\n",
            "Iteration 67, loss = 1.13605740\n",
            "Iteration 68, loss = 1.13477831\n",
            "Iteration 69, loss = 1.13326795\n",
            "Iteration 70, loss = 1.13141616\n",
            "Iteration 71, loss = 1.12970560\n",
            "Iteration 72, loss = 1.12889375\n",
            "Iteration 73, loss = 1.12656775\n",
            "Iteration 74, loss = 1.12589786\n",
            "Iteration 75, loss = 1.12463611\n",
            "Iteration 76, loss = 1.12292415\n",
            "Iteration 77, loss = 1.12224546\n",
            "Iteration 78, loss = 1.12147153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 79, loss = 1.11897846\n",
            "Iteration 80, loss = 1.11942054\n",
            "Iteration 81, loss = 1.11677580\n",
            "Iteration 82, loss = 1.11663161\n",
            "Iteration 83, loss = 1.11431668\n",
            "Iteration 84, loss = 1.11352370\n",
            "Iteration 85, loss = 1.11222102\n",
            "Iteration 86, loss = 1.11093160\n",
            "Iteration 87, loss = 1.11031334\n",
            "Iteration 88, loss = 1.10861985\n",
            "Iteration 89, loss = 1.10796468\n",
            "Iteration 90, loss = 1.10664615\n",
            "Iteration 91, loss = 1.10539362\n",
            "Iteration 92, loss = 1.10455699\n",
            "Iteration 93, loss = 1.10336437\n",
            "Iteration 94, loss = 1.10267547\n",
            "Iteration 95, loss = 1.10169617\n",
            "Iteration 96, loss = 1.10136105\n",
            "Iteration 97, loss = 1.09921732\n",
            "Iteration 98, loss = 1.09791946\n",
            "Iteration 99, loss = 1.09736978\n",
            "Iteration 100, loss = 1.09618477\n",
            "Iteration 1, loss = 1.68011677\n",
            "Iteration 2, loss = 1.60679715\n",
            "Iteration 3, loss = 1.53220714\n",
            "Iteration 4, loss = 1.46465657\n",
            "Iteration 5, loss = 1.37853026\n",
            "Iteration 6, loss = 1.30809911\n",
            "Iteration 7, loss = 1.23969189\n",
            "Iteration 8, loss = 1.19575831\n",
            "Iteration 9, loss = 1.16754552\n",
            "Iteration 10, loss = 1.14498445\n",
            "Iteration 11, loss = 1.14119675\n",
            "Iteration 12, loss = 1.13159846\n",
            "Iteration 13, loss = 1.12173507\n",
            "Iteration 14, loss = 1.11540598\n",
            "Iteration 15, loss = 1.11264914\n",
            "Iteration 16, loss = 1.11312803\n",
            "Iteration 17, loss = 1.11558933\n",
            "Iteration 18, loss = 1.15057430\n",
            "Iteration 19, loss = 1.13613040\n",
            "Iteration 20, loss = 1.11507294\n",
            "Iteration 21, loss = 1.12855049\n",
            "Iteration 22, loss = 1.12085880\n",
            "Iteration 23, loss = 1.10163289\n",
            "Iteration 24, loss = 1.11252112\n",
            "Iteration 25, loss = 1.10193956\n",
            "Iteration 26, loss = 1.09366471\n",
            "Iteration 27, loss = 1.09184775\n",
            "Iteration 28, loss = 1.08724980\n",
            "Iteration 29, loss = 1.08639336\n",
            "Iteration 30, loss = 1.06775387\n",
            "Iteration 31, loss = 1.06095138\n",
            "Iteration 32, loss = 1.05934575\n",
            "Iteration 33, loss = 1.04627399\n",
            "Iteration 34, loss = 1.04563943\n",
            "Iteration 35, loss = 1.04384557\n",
            "Iteration 36, loss = 1.03428534\n",
            "Iteration 37, loss = 1.02289747\n",
            "Iteration 38, loss = 1.03736286\n",
            "Iteration 39, loss = 1.03213862\n",
            "Iteration 40, loss = 1.00261460\n",
            "Iteration 41, loss = 1.01425420\n",
            "Iteration 42, loss = 1.04687247\n",
            "Iteration 43, loss = 1.04537744\n",
            "Iteration 44, loss = 1.03548806\n",
            "Iteration 45, loss = 1.01538883\n",
            "Iteration 46, loss = 0.99163690\n",
            "Iteration 47, loss = 0.98973762\n",
            "Iteration 48, loss = 0.99603762\n",
            "Iteration 49, loss = 1.00543272\n",
            "Iteration 50, loss = 0.96888407\n",
            "Iteration 51, loss = 0.97958567\n",
            "Iteration 52, loss = 0.94751117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.95083788\n",
            "Iteration 54, loss = 0.94410650\n",
            "Iteration 55, loss = 0.92336717\n",
            "Iteration 56, loss = 0.91235056\n",
            "Iteration 57, loss = 0.90584092\n",
            "Iteration 58, loss = 0.90119352\n",
            "Iteration 59, loss = 0.90891421\n",
            "Iteration 60, loss = 0.92442223\n",
            "Iteration 61, loss = 0.93010358\n",
            "Iteration 62, loss = 0.90550775\n",
            "Iteration 63, loss = 0.89100410\n",
            "Iteration 64, loss = 0.89171827\n",
            "Iteration 65, loss = 0.88645679\n",
            "Iteration 66, loss = 0.88603155\n",
            "Iteration 67, loss = 0.88288827\n",
            "Iteration 68, loss = 0.87236760\n",
            "Iteration 69, loss = 0.89307338\n",
            "Iteration 70, loss = 0.89510469\n",
            "Iteration 71, loss = 0.84974197\n",
            "Iteration 72, loss = 0.88000033\n",
            "Iteration 73, loss = 0.88826284\n",
            "Iteration 74, loss = 0.87940850\n",
            "Iteration 75, loss = 0.85628335\n",
            "Iteration 76, loss = 0.84627495\n",
            "Iteration 77, loss = 0.86260687\n",
            "Iteration 78, loss = 0.84021749\n",
            "Iteration 79, loss = 0.82612661\n",
            "Iteration 80, loss = 0.84628881\n",
            "Iteration 81, loss = 0.83064025\n",
            "Iteration 82, loss = 0.86676089\n",
            "Iteration 83, loss = 0.81315825\n",
            "Iteration 84, loss = 0.84509226\n",
            "Iteration 85, loss = 0.82290548\n",
            "Iteration 86, loss = 0.83782914\n",
            "Iteration 87, loss = 0.87212827\n",
            "Iteration 88, loss = 0.82642645\n",
            "Iteration 89, loss = 0.81725061\n",
            "Iteration 90, loss = 0.81766898\n",
            "Iteration 91, loss = 0.82150724\n",
            "Iteration 92, loss = 0.80108990\n",
            "Iteration 93, loss = 0.79931417\n",
            "Iteration 94, loss = 0.82117871\n",
            "Iteration 95, loss = 0.84350859\n",
            "Iteration 96, loss = 0.85933085\n",
            "Iteration 97, loss = 0.87486408\n",
            "Iteration 98, loss = 0.88016288\n",
            "Iteration 99, loss = 0.91482377\n",
            "Iteration 100, loss = 0.96420139\n",
            "Iteration 1, loss = 1.69653019\n",
            "Iteration 2, loss = 1.69193759\n",
            "Iteration 3, loss = 1.68737366\n",
            "Iteration 4, loss = 1.68286902\n",
            "Iteration 5, loss = 1.67843340\n",
            "Iteration 6, loss = 1.67433846\n",
            "Iteration 7, loss = 1.67040299\n",
            "Iteration 8, loss = 1.66626494\n",
            "Iteration 9, loss = 1.66232821\n",
            "Iteration 10, loss = 1.65877376\n",
            "Iteration 11, loss = 1.65504622\n",
            "Iteration 12, loss = 1.65135674\n",
            "Iteration 13, loss = 1.64816442\n",
            "Iteration 14, loss = 1.64484999\n",
            "Iteration 15, loss = 1.64152599\n",
            "Iteration 16, loss = 1.63852170\n",
            "Iteration 17, loss = 1.63549899\n",
            "Iteration 18, loss = 1.63270457\n",
            "Iteration 19, loss = 1.62973075\n",
            "Iteration 20, loss = 1.62730192\n",
            "Iteration 21, loss = 1.62453988\n",
            "Iteration 22, loss = 1.62211076\n",
            "Iteration 23, loss = 1.61965261\n",
            "Iteration 24, loss = 1.61735280\n",
            "Iteration 25, loss = 1.61516126\n",
            "Iteration 26, loss = 1.61303371\n",
            "Iteration 27, loss = 1.61105619\n",
            "Iteration 28, loss = 1.60913144\n",
            "Iteration 29, loss = 1.60709343\n",
            "Iteration 30, loss = 1.60513905\n",
            "Iteration 31, loss = 1.60356445\n",
            "Iteration 32, loss = 1.60184248\n",
            "Iteration 33, loss = 1.60003979\n",
            "Iteration 34, loss = 1.59877554\n",
            "Iteration 35, loss = 1.59706423\n",
            "Iteration 36, loss = 1.59563214\n",
            "Iteration 37, loss = 1.59422959\n",
            "Iteration 38, loss = 1.59301531\n",
            "Iteration 39, loss = 1.59157653\n",
            "Iteration 40, loss = 1.59024799\n",
            "Iteration 41, loss = 1.58932501\n",
            "Iteration 42, loss = 1.58824718\n",
            "Iteration 43, loss = 1.58696619\n",
            "Iteration 44, loss = 1.58605028\n",
            "Iteration 45, loss = 1.58495184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 46, loss = 1.58408847\n",
            "Iteration 47, loss = 1.58325796\n",
            "Iteration 48, loss = 1.58213266\n",
            "Iteration 49, loss = 1.58145809\n",
            "Iteration 50, loss = 1.58054622\n",
            "Iteration 51, loss = 1.57986749\n",
            "Iteration 52, loss = 1.57900793\n",
            "Iteration 53, loss = 1.57836796\n",
            "Iteration 54, loss = 1.57756337\n",
            "Iteration 55, loss = 1.57692587\n",
            "Iteration 56, loss = 1.57621027\n",
            "Iteration 57, loss = 1.57563141\n",
            "Iteration 58, loss = 1.57502413\n",
            "Iteration 59, loss = 1.57428526\n",
            "Iteration 60, loss = 1.57372754\n",
            "Iteration 61, loss = 1.57310595\n",
            "Iteration 62, loss = 1.57256511\n",
            "Iteration 63, loss = 1.57193145\n",
            "Iteration 64, loss = 1.57139116\n",
            "Iteration 65, loss = 1.57085165\n",
            "Iteration 66, loss = 1.57026994\n",
            "Iteration 67, loss = 1.56970462\n",
            "Iteration 68, loss = 1.56919364\n",
            "Iteration 69, loss = 1.56865151\n",
            "Iteration 70, loss = 1.56806039\n",
            "Iteration 71, loss = 1.56759502\n",
            "Iteration 72, loss = 1.56702971\n",
            "Iteration 73, loss = 1.56648976\n",
            "Iteration 74, loss = 1.56598820\n",
            "Iteration 75, loss = 1.56542088\n",
            "Iteration 76, loss = 1.56488364\n",
            "Iteration 77, loss = 1.56435126\n",
            "Iteration 78, loss = 1.56384367\n",
            "Iteration 79, loss = 1.56329056\n",
            "Iteration 80, loss = 1.56278237\n",
            "Iteration 81, loss = 1.56218537\n",
            "Iteration 82, loss = 1.56167498\n",
            "Iteration 83, loss = 1.56110790\n",
            "Iteration 84, loss = 1.56054859\n",
            "Iteration 85, loss = 1.56003664\n",
            "Iteration 86, loss = 1.55946867\n",
            "Iteration 87, loss = 1.55890494\n",
            "Iteration 88, loss = 1.55833850\n",
            "Iteration 89, loss = 1.55778689\n",
            "Iteration 90, loss = 1.55720439\n",
            "Iteration 91, loss = 1.55662250\n",
            "Iteration 92, loss = 1.55605514\n",
            "Iteration 93, loss = 1.55546712\n",
            "Iteration 94, loss = 1.55492246\n",
            "Iteration 95, loss = 1.55428891\n",
            "Iteration 96, loss = 1.55369844\n",
            "Iteration 97, loss = 1.55310670\n",
            "Iteration 98, loss = 1.55249123\n",
            "Iteration 99, loss = 1.55191070\n",
            "Iteration 100, loss = 1.55130145\n",
            "Iteration 1, loss = 1.68506042\n",
            "Iteration 2, loss = 1.64654518\n",
            "Iteration 3, loss = 1.61731871\n",
            "Iteration 4, loss = 1.59678499\n",
            "Iteration 5, loss = 1.58303568\n",
            "Iteration 6, loss = 1.57816713\n",
            "Iteration 7, loss = 1.57741666\n",
            "Iteration 8, loss = 1.57603399\n",
            "Iteration 9, loss = 1.57409477\n",
            "Iteration 10, loss = 1.56963581\n",
            "Iteration 11, loss = 1.56408076\n",
            "Iteration 12, loss = 1.55793122\n",
            "Iteration 13, loss = 1.55042768\n",
            "Iteration 14, loss = 1.54394121\n",
            "Iteration 15, loss = 1.53790895\n",
            "Iteration 16, loss = 1.53165283\n",
            "Iteration 17, loss = 1.52540835\n",
            "Iteration 18, loss = 1.51908039\n",
            "Iteration 19, loss = 1.51175737\n",
            "Iteration 20, loss = 1.50403367\n",
            "Iteration 21, loss = 1.49573961\n",
            "Iteration 22, loss = 1.48731014\n",
            "Iteration 23, loss = 1.47851427\n",
            "Iteration 24, loss = 1.46906964\n",
            "Iteration 25, loss = 1.45912750\n",
            "Iteration 26, loss = 1.44955479\n",
            "Iteration 27, loss = 1.43977771\n",
            "Iteration 28, loss = 1.42936303\n",
            "Iteration 29, loss = 1.41835260\n",
            "Iteration 30, loss = 1.40785482\n",
            "Iteration 31, loss = 1.39689277\n",
            "Iteration 32, loss = 1.38643890\n",
            "Iteration 33, loss = 1.37501672\n",
            "Iteration 34, loss = 1.36455515\n",
            "Iteration 35, loss = 1.35336370\n",
            "Iteration 36, loss = 1.34268565\n",
            "Iteration 37, loss = 1.33217807\n",
            "Iteration 38, loss = 1.32201098\n",
            "Iteration 39, loss = 1.31180189\n",
            "Iteration 40, loss = 1.30247260\n",
            "Iteration 41, loss = 1.29321598\n",
            "Iteration 42, loss = 1.28401780\n",
            "Iteration 43, loss = 1.27511053\n",
            "Iteration 44, loss = 1.26689029\n",
            "Iteration 45, loss = 1.25909860\n",
            "Iteration 46, loss = 1.25165735\n",
            "Iteration 47, loss = 1.24502055\n",
            "Iteration 48, loss = 1.23795507\n",
            "Iteration 49, loss = 1.23137283\n",
            "Iteration 50, loss = 1.22548177\n",
            "Iteration 51, loss = 1.21968122\n",
            "Iteration 52, loss = 1.21432754\n",
            "Iteration 53, loss = 1.20921164\n",
            "Iteration 54, loss = 1.20458954\n",
            "Iteration 55, loss = 1.19976691\n",
            "Iteration 56, loss = 1.19558897\n",
            "Iteration 57, loss = 1.19176903\n",
            "Iteration 58, loss = 1.18807894\n",
            "Iteration 59, loss = 1.18456381\n",
            "Iteration 60, loss = 1.18087712\n",
            "Iteration 61, loss = 1.17765630\n",
            "Iteration 62, loss = 1.17456779\n",
            "Iteration 63, loss = 1.17159417\n",
            "Iteration 64, loss = 1.16907363\n",
            "Iteration 65, loss = 1.16673842\n",
            "Iteration 66, loss = 1.16389544\n",
            "Iteration 67, loss = 1.16150929\n",
            "Iteration 68, loss = 1.15955167\n",
            "Iteration 69, loss = 1.15741286\n",
            "Iteration 70, loss = 1.15518347\n",
            "Iteration 71, loss = 1.15307929\n",
            "Iteration 72, loss = 1.15167393\n",
            "Iteration 73, loss = 1.14925369\n",
            "Iteration 74, loss = 1.14802235\n",
            "Iteration 75, loss = 1.14656073\n",
            "Iteration 76, loss = 1.14466543\n",
            "Iteration 77, loss = 1.14354762\n",
            "Iteration 78, loss = 1.14236003\n",
            "Iteration 79, loss = 1.14020249\n",
            "Iteration 80, loss = 1.13993950\n",
            "Iteration 81, loss = 1.13766716\n",
            "Iteration 82, loss = 1.13717235\n",
            "Iteration 83, loss = 1.13514430\n",
            "Iteration 84, loss = 1.13414760\n",
            "Iteration 85, loss = 1.13301520\n",
            "Iteration 86, loss = 1.13191029\n",
            "Iteration 87, loss = 1.13105828\n",
            "Iteration 88, loss = 1.12969187\n",
            "Iteration 89, loss = 1.12905632\n",
            "Iteration 90, loss = 1.12796315\n",
            "Iteration 91, loss = 1.12686328\n",
            "Iteration 92, loss = 1.12609081\n",
            "Iteration 93, loss = 1.12520632\n",
            "Iteration 94, loss = 1.12451461\n",
            "Iteration 95, loss = 1.12381581\n",
            "Iteration 96, loss = 1.12301719\n",
            "Iteration 97, loss = 1.12220229\n",
            "Iteration 98, loss = 1.12124818\n",
            "Iteration 99, loss = 1.12077321\n",
            "Iteration 100, loss = 1.11998289\n",
            "Iteration 1, loss = 1.63036103\n",
            "Iteration 2, loss = 1.62891530\n",
            "Iteration 3, loss = 1.55149075\n",
            "Iteration 4, loss = 1.50922382\n",
            "Iteration 5, loss = 1.45629316\n",
            "Iteration 6, loss = 1.38432723\n",
            "Iteration 7, loss = 1.32349131\n",
            "Iteration 8, loss = 1.27083743\n",
            "Iteration 9, loss = 1.22203364\n",
            "Iteration 10, loss = 1.18065211\n",
            "Iteration 11, loss = 1.16675798\n",
            "Iteration 12, loss = 1.14567547\n",
            "Iteration 13, loss = 1.13346683\n",
            "Iteration 14, loss = 1.12630705\n",
            "Iteration 15, loss = 1.11965128\n",
            "Iteration 16, loss = 1.11502916\n",
            "Iteration 17, loss = 1.11465355\n",
            "Iteration 18, loss = 1.12728472\n",
            "Iteration 19, loss = 1.11398952\n",
            "Iteration 20, loss = 1.10966799\n",
            "Iteration 21, loss = 1.10994809\n",
            "Iteration 22, loss = 1.10994234\n",
            "Iteration 23, loss = 1.10397221\n",
            "Iteration 24, loss = 1.10446642\n",
            "Iteration 25, loss = 1.10020357\n",
            "Iteration 26, loss = 1.09518210\n",
            "Iteration 27, loss = 1.09397340\n",
            "Iteration 28, loss = 1.10149374\n",
            "Iteration 29, loss = 1.10415508\n",
            "Iteration 30, loss = 1.10135122\n",
            "Iteration 31, loss = 1.09232616\n",
            "Iteration 32, loss = 1.08666670\n",
            "Iteration 33, loss = 1.08530705\n",
            "Iteration 34, loss = 1.09174315\n",
            "Iteration 35, loss = 1.08862499\n",
            "Iteration 36, loss = 1.08210545\n",
            "Iteration 37, loss = 1.08370025\n",
            "Iteration 38, loss = 1.08454125\n",
            "Iteration 39, loss = 1.07824766\n",
            "Iteration 40, loss = 1.07825686\n",
            "Iteration 41, loss = 1.08500259\n",
            "Iteration 42, loss = 1.08750117\n",
            "Iteration 43, loss = 1.07693472\n",
            "Iteration 44, loss = 1.07713108\n",
            "Iteration 45, loss = 1.06567927\n",
            "Iteration 46, loss = 1.06284158\n",
            "Iteration 47, loss = 1.06462726\n",
            "Iteration 48, loss = 1.05860080\n",
            "Iteration 49, loss = 1.05442442\n",
            "Iteration 50, loss = 1.05084452\n",
            "Iteration 51, loss = 1.04573855\n",
            "Iteration 52, loss = 1.04651971\n",
            "Iteration 53, loss = 1.04562917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 54, loss = 1.03765420\n",
            "Iteration 55, loss = 1.03959277\n",
            "Iteration 56, loss = 1.02947875\n",
            "Iteration 57, loss = 1.02510140\n",
            "Iteration 58, loss = 1.02516466\n",
            "Iteration 59, loss = 1.03325130\n",
            "Iteration 60, loss = 1.03520495\n",
            "Iteration 61, loss = 1.01757893\n",
            "Iteration 62, loss = 1.01655227\n",
            "Iteration 63, loss = 1.00722636\n",
            "Iteration 64, loss = 1.00300124\n",
            "Iteration 65, loss = 0.99667350\n",
            "Iteration 66, loss = 0.99478188\n",
            "Iteration 67, loss = 0.98957183\n",
            "Iteration 68, loss = 0.98853036\n",
            "Iteration 69, loss = 0.97119491\n",
            "Iteration 70, loss = 0.97803702\n",
            "Iteration 71, loss = 0.98047589\n",
            "Iteration 72, loss = 0.98359870\n",
            "Iteration 73, loss = 0.98326099\n",
            "Iteration 74, loss = 0.94710346\n",
            "Iteration 75, loss = 0.96882666\n",
            "Iteration 76, loss = 0.96555113\n",
            "Iteration 77, loss = 0.98453139\n",
            "Iteration 78, loss = 0.97793730\n",
            "Iteration 79, loss = 0.94646480\n",
            "Iteration 80, loss = 0.96291718\n",
            "Iteration 81, loss = 0.95184388\n",
            "Iteration 82, loss = 0.94912350\n",
            "Iteration 83, loss = 0.91970852\n",
            "Iteration 84, loss = 0.94716388\n",
            "Iteration 85, loss = 0.92272517\n",
            "Iteration 86, loss = 0.91494648\n",
            "Iteration 87, loss = 0.91602206\n",
            "Iteration 88, loss = 0.91265298\n",
            "Iteration 89, loss = 0.90556170\n",
            "Iteration 90, loss = 0.89439449\n",
            "Iteration 91, loss = 0.89147433\n",
            "Iteration 92, loss = 0.88198793\n",
            "Iteration 93, loss = 0.88658220\n",
            "Iteration 94, loss = 0.87473181\n",
            "Iteration 95, loss = 0.88435847\n",
            "Iteration 96, loss = 0.89717354\n",
            "Iteration 97, loss = 0.91365313\n",
            "Iteration 98, loss = 0.91813699\n",
            "Iteration 99, loss = 0.93450074\n",
            "Iteration 100, loss = 0.92516762\n",
            "Iteration 1, loss = 1.72761445\n",
            "Iteration 2, loss = 1.71616570\n",
            "Iteration 3, loss = 1.70527831\n",
            "Iteration 4, loss = 1.69564106\n",
            "Iteration 5, loss = 1.68611919\n",
            "Iteration 6, loss = 1.67715385\n",
            "Iteration 7, loss = 1.66850569\n",
            "Iteration 8, loss = 1.66129715\n",
            "Iteration 9, loss = 1.65363440\n",
            "Iteration 10, loss = 1.64721007\n",
            "Iteration 11, loss = 1.64110466\n",
            "Iteration 12, loss = 1.63532376\n",
            "Iteration 13, loss = 1.63049786\n",
            "Iteration 14, loss = 1.62507160\n",
            "Iteration 15, loss = 1.62075000\n",
            "Iteration 16, loss = 1.61668337\n",
            "Iteration 17, loss = 1.61305032\n",
            "Iteration 18, loss = 1.61002318\n",
            "Iteration 19, loss = 1.60663262\n",
            "Iteration 20, loss = 1.60350043\n",
            "Iteration 21, loss = 1.60107469\n",
            "Iteration 22, loss = 1.59898299\n",
            "Iteration 23, loss = 1.59687101\n",
            "Iteration 24, loss = 1.59496726\n",
            "Iteration 25, loss = 1.59299773\n",
            "Iteration 26, loss = 1.59109858\n",
            "Iteration 27, loss = 1.58962025\n",
            "Iteration 28, loss = 1.58818345\n",
            "Iteration 29, loss = 1.58698256\n",
            "Iteration 30, loss = 1.58561958\n",
            "Iteration 31, loss = 1.58441194\n",
            "Iteration 32, loss = 1.58336890\n",
            "Iteration 33, loss = 1.58222653\n",
            "Iteration 34, loss = 1.58140974\n",
            "Iteration 35, loss = 1.58050349\n",
            "Iteration 36, loss = 1.57957142\n",
            "Iteration 37, loss = 1.57873928\n",
            "Iteration 38, loss = 1.57801113\n",
            "Iteration 39, loss = 1.57714411\n",
            "Iteration 40, loss = 1.57637571\n",
            "Iteration 41, loss = 1.57561621\n",
            "Iteration 42, loss = 1.57485173\n",
            "Iteration 43, loss = 1.57420107\n",
            "Iteration 44, loss = 1.57346665\n",
            "Iteration 45, loss = 1.57276939\n",
            "Iteration 46, loss = 1.57209641\n",
            "Iteration 47, loss = 1.57145978\n",
            "Iteration 48, loss = 1.57073465\n",
            "Iteration 49, loss = 1.57003877\n",
            "Iteration 50, loss = 1.56940952\n",
            "Iteration 51, loss = 1.56866793\n",
            "Iteration 52, loss = 1.56804404\n",
            "Iteration 53, loss = 1.56738538\n",
            "Iteration 54, loss = 1.56664073\n",
            "Iteration 55, loss = 1.56594438\n",
            "Iteration 56, loss = 1.56527299\n",
            "Iteration 57, loss = 1.56457999\n",
            "Iteration 58, loss = 1.56388872\n",
            "Iteration 59, loss = 1.56320531\n",
            "Iteration 60, loss = 1.56253992\n",
            "Iteration 61, loss = 1.56180057\n",
            "Iteration 62, loss = 1.56112432\n",
            "Iteration 63, loss = 1.56044025\n",
            "Iteration 64, loss = 1.55970395\n",
            "Iteration 65, loss = 1.55896527\n",
            "Iteration 66, loss = 1.55824426\n",
            "Iteration 67, loss = 1.55754216\n",
            "Iteration 68, loss = 1.55678173\n",
            "Iteration 69, loss = 1.55606660\n",
            "Iteration 70, loss = 1.55535407\n",
            "Iteration 71, loss = 1.55458920\n",
            "Iteration 72, loss = 1.55383909\n",
            "Iteration 73, loss = 1.55308913\n",
            "Iteration 74, loss = 1.55235780\n",
            "Iteration 75, loss = 1.55157638\n",
            "Iteration 76, loss = 1.55080666\n",
            "Iteration 77, loss = 1.55001559\n",
            "Iteration 78, loss = 1.54926869\n",
            "Iteration 79, loss = 1.54845311\n",
            "Iteration 80, loss = 1.54767938\n",
            "Iteration 81, loss = 1.54688921\n",
            "Iteration 82, loss = 1.54609898\n",
            "Iteration 83, loss = 1.54528347\n",
            "Iteration 84, loss = 1.54443967\n",
            "Iteration 85, loss = 1.54366845\n",
            "Iteration 86, loss = 1.54282502\n",
            "Iteration 87, loss = 1.54198775\n",
            "Iteration 88, loss = 1.54114693\n",
            "Iteration 89, loss = 1.54030062\n",
            "Iteration 90, loss = 1.53944902\n",
            "Iteration 91, loss = 1.53854639\n",
            "Iteration 92, loss = 1.53770356\n",
            "Iteration 93, loss = 1.53685376\n",
            "Iteration 94, loss = 1.53594507\n",
            "Iteration 95, loss = 1.53508433\n",
            "Iteration 96, loss = 1.53416243\n",
            "Iteration 97, loss = 1.53328492\n",
            "Iteration 98, loss = 1.53235585\n",
            "Iteration 99, loss = 1.53148858\n",
            "Iteration 100, loss = 1.53048366\n",
            "Iteration 1, loss = 1.70489708\n",
            "Iteration 2, loss = 1.62699904\n",
            "Iteration 3, loss = 1.59248318\n",
            "Iteration 4, loss = 1.58714978\n",
            "Iteration 5, loss = 1.58911984\n",
            "Iteration 6, loss = 1.58835671\n",
            "Iteration 7, loss = 1.58546984\n",
            "Iteration 8, loss = 1.57731896\n",
            "Iteration 9, loss = 1.56850824\n",
            "Iteration 10, loss = 1.55848854\n",
            "Iteration 11, loss = 1.55019922\n",
            "Iteration 12, loss = 1.54520965\n",
            "Iteration 13, loss = 1.53896321\n",
            "Iteration 14, loss = 1.53425545\n",
            "Iteration 15, loss = 1.52711713\n",
            "Iteration 16, loss = 1.51882583\n",
            "Iteration 17, loss = 1.50991151\n",
            "Iteration 18, loss = 1.50085647\n",
            "Iteration 19, loss = 1.49096772\n",
            "Iteration 20, loss = 1.48142859\n",
            "Iteration 21, loss = 1.47113608\n",
            "Iteration 22, loss = 1.46016164\n",
            "Iteration 23, loss = 1.44871872\n",
            "Iteration 24, loss = 1.43639289\n",
            "Iteration 25, loss = 1.42405107\n",
            "Iteration 26, loss = 1.41197897\n",
            "Iteration 27, loss = 1.39838446\n",
            "Iteration 28, loss = 1.38548397\n",
            "Iteration 29, loss = 1.37231664\n",
            "Iteration 30, loss = 1.35940389\n",
            "Iteration 31, loss = 1.34572262\n",
            "Iteration 32, loss = 1.33243734\n",
            "Iteration 33, loss = 1.31992331\n",
            "Iteration 34, loss = 1.30744487\n",
            "Iteration 35, loss = 1.29536220\n",
            "Iteration 36, loss = 1.28408980\n",
            "Iteration 37, loss = 1.27313900\n",
            "Iteration 38, loss = 1.26292184\n",
            "Iteration 39, loss = 1.25289182\n",
            "Iteration 40, loss = 1.24350106\n",
            "Iteration 41, loss = 1.23508117\n",
            "Iteration 42, loss = 1.22706999\n",
            "Iteration 43, loss = 1.21919159\n",
            "Iteration 44, loss = 1.21215344\n",
            "Iteration 45, loss = 1.20522104\n",
            "Iteration 46, loss = 1.19889041\n",
            "Iteration 47, loss = 1.19345350\n",
            "Iteration 48, loss = 1.18801033\n",
            "Iteration 49, loss = 1.18285855\n",
            "Iteration 50, loss = 1.17880023\n",
            "Iteration 51, loss = 1.17365682\n",
            "Iteration 52, loss = 1.16964660\n",
            "Iteration 53, loss = 1.16626345\n",
            "Iteration 54, loss = 1.16289539\n",
            "Iteration 55, loss = 1.15889499\n",
            "Iteration 56, loss = 1.15589529\n",
            "Iteration 57, loss = 1.15303110\n",
            "Iteration 58, loss = 1.14994585\n",
            "Iteration 59, loss = 1.14733381\n",
            "Iteration 60, loss = 1.14535978\n",
            "Iteration 61, loss = 1.14286046\n",
            "Iteration 62, loss = 1.14085853\n",
            "Iteration 63, loss = 1.13974905\n",
            "Iteration 64, loss = 1.13675812\n",
            "Iteration 65, loss = 1.13444068\n",
            "Iteration 66, loss = 1.13276867\n",
            "Iteration 67, loss = 1.13096959\n",
            "Iteration 68, loss = 1.12972454\n",
            "Iteration 69, loss = 1.12828722\n",
            "Iteration 70, loss = 1.12674360\n",
            "Iteration 71, loss = 1.12464756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 72, loss = 1.12414534\n",
            "Iteration 73, loss = 1.12294986\n",
            "Iteration 74, loss = 1.12185819\n",
            "Iteration 75, loss = 1.12015176\n",
            "Iteration 76, loss = 1.11950962\n",
            "Iteration 77, loss = 1.11817622\n",
            "Iteration 78, loss = 1.11714310\n",
            "Iteration 79, loss = 1.11581047\n",
            "Iteration 80, loss = 1.11461569\n",
            "Iteration 81, loss = 1.11397133\n",
            "Iteration 82, loss = 1.11340171\n",
            "Iteration 83, loss = 1.11281445\n",
            "Iteration 84, loss = 1.11102160\n",
            "Iteration 85, loss = 1.11100955\n",
            "Iteration 86, loss = 1.10966055\n",
            "Iteration 87, loss = 1.10882883\n",
            "Iteration 88, loss = 1.10836060\n",
            "Iteration 89, loss = 1.10761319\n",
            "Iteration 90, loss = 1.10644770\n",
            "Iteration 91, loss = 1.10719752\n",
            "Iteration 92, loss = 1.10568070\n",
            "Iteration 93, loss = 1.10475196\n",
            "Iteration 94, loss = 1.10429786\n",
            "Iteration 95, loss = 1.10425796\n",
            "Iteration 96, loss = 1.10259257\n",
            "Iteration 97, loss = 1.10277420\n",
            "Iteration 98, loss = 1.10156473\n",
            "Iteration 99, loss = 1.10259185\n",
            "Iteration 100, loss = 1.10012617\n",
            "Iteration 1, loss = 1.72234726\n",
            "Iteration 2, loss = 1.67745073\n",
            "Iteration 3, loss = 1.60332887\n",
            "Iteration 4, loss = 1.60144698\n",
            "Iteration 5, loss = 1.54870347\n",
            "Iteration 6, loss = 1.51780299\n",
            "Iteration 7, loss = 1.47397737\n",
            "Iteration 8, loss = 1.39778145\n",
            "Iteration 9, loss = 1.34016104\n",
            "Iteration 10, loss = 1.27882691\n",
            "Iteration 11, loss = 1.21318443\n",
            "Iteration 12, loss = 1.17461393\n",
            "Iteration 13, loss = 1.16285045\n",
            "Iteration 14, loss = 1.14119135\n",
            "Iteration 15, loss = 1.11920977\n",
            "Iteration 16, loss = 1.12783281\n",
            "Iteration 17, loss = 1.11340671\n",
            "Iteration 18, loss = 1.11425071\n",
            "Iteration 19, loss = 1.10957815\n",
            "Iteration 20, loss = 1.11312975\n",
            "Iteration 21, loss = 1.10562518\n",
            "Iteration 22, loss = 1.11272123\n",
            "Iteration 23, loss = 1.11023582\n",
            "Iteration 24, loss = 1.09993278\n",
            "Iteration 25, loss = 1.09761537\n",
            "Iteration 26, loss = 1.11469354\n",
            "Iteration 27, loss = 1.10512321\n",
            "Iteration 28, loss = 1.09263263\n",
            "Iteration 29, loss = 1.09311396\n",
            "Iteration 30, loss = 1.08749974\n",
            "Iteration 31, loss = 1.07571548\n",
            "Iteration 32, loss = 1.07218754\n",
            "Iteration 33, loss = 1.05687338\n",
            "Iteration 34, loss = 1.06019733\n",
            "Iteration 35, loss = 1.04558976\n",
            "Iteration 36, loss = 1.04351623\n",
            "Iteration 37, loss = 1.03172072\n",
            "Iteration 38, loss = 1.03384257\n",
            "Iteration 39, loss = 1.04493040\n",
            "Iteration 40, loss = 1.07358071\n",
            "Iteration 41, loss = 1.04024417\n",
            "Iteration 42, loss = 1.02898829\n",
            "Iteration 43, loss = 0.99477396\n",
            "Iteration 44, loss = 0.97711490\n",
            "Iteration 45, loss = 0.99382503\n",
            "Iteration 46, loss = 0.97681049\n",
            "Iteration 47, loss = 0.97361157\n",
            "Iteration 48, loss = 0.96698742\n",
            "Iteration 49, loss = 0.93895898\n",
            "Iteration 50, loss = 0.96731830\n",
            "Iteration 51, loss = 0.93933408\n",
            "Iteration 52, loss = 0.96841309\n",
            "Iteration 53, loss = 0.99014104\n",
            "Iteration 54, loss = 0.93703859\n",
            "Iteration 55, loss = 0.91521243\n",
            "Iteration 56, loss = 0.91424148\n",
            "Iteration 57, loss = 0.88642223\n",
            "Iteration 58, loss = 0.89425091\n",
            "Iteration 59, loss = 0.87779747\n",
            "Iteration 60, loss = 0.87634163\n",
            "Iteration 61, loss = 0.87050237\n",
            "Iteration 62, loss = 0.88298409\n",
            "Iteration 63, loss = 0.90842795\n",
            "Iteration 64, loss = 0.86542686\n",
            "Iteration 65, loss = 0.95732952\n",
            "Iteration 66, loss = 0.93291765\n",
            "Iteration 67, loss = 0.96958073\n",
            "Iteration 68, loss = 0.96881543\n",
            "Iteration 69, loss = 0.99071760\n",
            "Iteration 70, loss = 1.00254169\n",
            "Iteration 71, loss = 1.01973667\n",
            "Iteration 72, loss = 0.92781948\n",
            "Iteration 73, loss = 0.91048577\n",
            "Iteration 74, loss = 0.90342474\n",
            "Iteration 75, loss = 0.90017959\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.69854030\n",
            "Iteration 2, loss = 1.69205697\n",
            "Iteration 3, loss = 1.68547570\n",
            "Iteration 4, loss = 1.67967695\n",
            "Iteration 5, loss = 1.67438928\n",
            "Iteration 6, loss = 1.66873480\n",
            "Iteration 7, loss = 1.66386043\n",
            "Iteration 8, loss = 1.65910386\n",
            "Iteration 9, loss = 1.65439618\n",
            "Iteration 10, loss = 1.65005189\n",
            "Iteration 11, loss = 1.64608837\n",
            "Iteration 12, loss = 1.64206130\n",
            "Iteration 13, loss = 1.63864123\n",
            "Iteration 14, loss = 1.63480449\n",
            "Iteration 15, loss = 1.63192545\n",
            "Iteration 16, loss = 1.62843554\n",
            "Iteration 17, loss = 1.62534577\n",
            "Iteration 18, loss = 1.62281903\n",
            "Iteration 19, loss = 1.62024109\n",
            "Iteration 20, loss = 1.61785562\n",
            "Iteration 21, loss = 1.61570858\n",
            "Iteration 22, loss = 1.61367009\n",
            "Iteration 23, loss = 1.61179448\n",
            "Iteration 24, loss = 1.61011265\n",
            "Iteration 25, loss = 1.60849371\n",
            "Iteration 26, loss = 1.60701140\n",
            "Iteration 27, loss = 1.60556011\n",
            "Iteration 28, loss = 1.60417414\n",
            "Iteration 29, loss = 1.60320800\n",
            "Iteration 30, loss = 1.60236200\n",
            "Iteration 31, loss = 1.60115428\n",
            "Iteration 32, loss = 1.60046560\n",
            "Iteration 33, loss = 1.59970529\n",
            "Iteration 34, loss = 1.59921749\n",
            "Iteration 35, loss = 1.59862601\n",
            "Iteration 36, loss = 1.59803347\n",
            "Iteration 37, loss = 1.59774540\n",
            "Iteration 38, loss = 1.59741778\n",
            "Iteration 39, loss = 1.59691775\n",
            "Iteration 40, loss = 1.59655993\n",
            "Iteration 41, loss = 1.59633031\n",
            "Iteration 42, loss = 1.59602275\n",
            "Iteration 43, loss = 1.59592079\n",
            "Iteration 44, loss = 1.59567426\n",
            "Iteration 45, loss = 1.59545827\n",
            "Iteration 46, loss = 1.59533731\n",
            "Iteration 47, loss = 1.59521181\n",
            "Iteration 48, loss = 1.59507993\n",
            "Iteration 49, loss = 1.59493110\n",
            "Iteration 50, loss = 1.59484686\n",
            "Iteration 51, loss = 1.59467033\n",
            "Iteration 52, loss = 1.59464995\n",
            "Iteration 53, loss = 1.59457052\n",
            "Iteration 54, loss = 1.59438169\n",
            "Iteration 55, loss = 1.59425760\n",
            "Iteration 56, loss = 1.59420192\n",
            "Iteration 57, loss = 1.59406849\n",
            "Iteration 58, loss = 1.59397489\n",
            "Iteration 59, loss = 1.59392363\n",
            "Iteration 60, loss = 1.59383110\n",
            "Iteration 61, loss = 1.59371575\n",
            "Iteration 62, loss = 1.59366033\n",
            "Iteration 63, loss = 1.59358023\n",
            "Iteration 64, loss = 1.59345946\n",
            "Iteration 65, loss = 1.59335501\n",
            "Iteration 66, loss = 1.59322258\n",
            "Iteration 67, loss = 1.59315339\n",
            "Iteration 68, loss = 1.59303136\n",
            "Iteration 69, loss = 1.59295033\n",
            "Iteration 70, loss = 1.59286679\n",
            "Iteration 71, loss = 1.59276753\n",
            "Iteration 72, loss = 1.59264914\n",
            "Iteration 73, loss = 1.59256044\n",
            "Iteration 74, loss = 1.59247827\n",
            "Iteration 75, loss = 1.59237757\n",
            "Iteration 76, loss = 1.59228026\n",
            "Iteration 77, loss = 1.59215579\n",
            "Iteration 78, loss = 1.59208150\n",
            "Iteration 79, loss = 1.59198966\n",
            "Iteration 80, loss = 1.59185526\n",
            "Iteration 81, loss = 1.59175286\n",
            "Iteration 82, loss = 1.59171970\n",
            "Iteration 83, loss = 1.59156621\n",
            "Iteration 84, loss = 1.59144524\n",
            "Iteration 85, loss = 1.59139015\n",
            "Iteration 86, loss = 1.59126219\n",
            "Iteration 87, loss = 1.59114390\n",
            "Iteration 88, loss = 1.59105473\n",
            "Iteration 89, loss = 1.59092695\n",
            "Iteration 90, loss = 1.59084395\n",
            "Iteration 91, loss = 1.59069446\n",
            "Iteration 92, loss = 1.59060062\n",
            "Iteration 93, loss = 1.59053433\n",
            "Iteration 94, loss = 1.59037589\n",
            "Iteration 95, loss = 1.59031332\n",
            "Iteration 96, loss = 1.59015813\n",
            "Iteration 97, loss = 1.59007811\n",
            "Iteration 98, loss = 1.58995329\n",
            "Iteration 99, loss = 1.58987088\n",
            "Iteration 100, loss = 1.58970388\n",
            "Iteration 1, loss = 1.68520427\n",
            "Iteration 2, loss = 1.63963333\n",
            "Iteration 3, loss = 1.60817003\n",
            "Iteration 4, loss = 1.59850879\n",
            "Iteration 5, loss = 1.59970869\n",
            "Iteration 6, loss = 1.60458759\n",
            "Iteration 7, loss = 1.60727112\n",
            "Iteration 8, loss = 1.60457282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss = 1.59956153\n",
            "Iteration 10, loss = 1.59420821\n",
            "Iteration 11, loss = 1.59098064\n",
            "Iteration 12, loss = 1.59134008\n",
            "Iteration 13, loss = 1.59106185\n",
            "Iteration 14, loss = 1.59061505\n",
            "Iteration 15, loss = 1.58943021\n",
            "Iteration 16, loss = 1.58725874\n",
            "Iteration 17, loss = 1.58512718\n",
            "Iteration 18, loss = 1.58400727\n",
            "Iteration 19, loss = 1.58209399\n",
            "Iteration 20, loss = 1.58127189\n",
            "Iteration 21, loss = 1.58002271\n",
            "Iteration 22, loss = 1.57848092\n",
            "Iteration 23, loss = 1.57701456\n",
            "Iteration 24, loss = 1.57538021\n",
            "Iteration 25, loss = 1.57310696\n",
            "Iteration 26, loss = 1.57148432\n",
            "Iteration 27, loss = 1.56917909\n",
            "Iteration 28, loss = 1.56746522\n",
            "Iteration 29, loss = 1.56567505\n",
            "Iteration 30, loss = 1.56341361\n",
            "Iteration 31, loss = 1.56071187\n",
            "Iteration 32, loss = 1.55800984\n",
            "Iteration 33, loss = 1.55541948\n",
            "Iteration 34, loss = 1.55248356\n",
            "Iteration 35, loss = 1.54945883\n",
            "Iteration 36, loss = 1.54658876\n",
            "Iteration 37, loss = 1.54332763\n",
            "Iteration 38, loss = 1.53995120\n",
            "Iteration 39, loss = 1.53616440\n",
            "Iteration 40, loss = 1.53214478\n",
            "Iteration 41, loss = 1.52858973\n",
            "Iteration 42, loss = 1.52406782\n",
            "Iteration 43, loss = 1.51963498\n",
            "Iteration 44, loss = 1.51567071\n",
            "Iteration 45, loss = 1.51022962\n",
            "Iteration 46, loss = 1.50542280\n",
            "Iteration 47, loss = 1.50085268\n",
            "Iteration 48, loss = 1.49529188\n",
            "Iteration 49, loss = 1.48981459\n",
            "Iteration 50, loss = 1.48452834\n",
            "Iteration 51, loss = 1.47843709\n",
            "Iteration 52, loss = 1.47285547\n",
            "Iteration 53, loss = 1.46689983\n",
            "Iteration 54, loss = 1.46065235\n",
            "Iteration 55, loss = 1.45395716\n",
            "Iteration 56, loss = 1.44765603\n",
            "Iteration 57, loss = 1.44090985\n",
            "Iteration 58, loss = 1.43417370\n",
            "Iteration 59, loss = 1.42760633\n",
            "Iteration 60, loss = 1.42130604\n",
            "Iteration 61, loss = 1.41405721\n",
            "Iteration 62, loss = 1.40772015\n",
            "Iteration 63, loss = 1.40120935\n",
            "Iteration 64, loss = 1.39381462\n",
            "Iteration 65, loss = 1.38664166\n",
            "Iteration 66, loss = 1.37972138\n",
            "Iteration 67, loss = 1.37309143\n",
            "Iteration 68, loss = 1.36606255\n",
            "Iteration 69, loss = 1.35959481\n",
            "Iteration 70, loss = 1.35320866\n",
            "Iteration 71, loss = 1.34640580\n",
            "Iteration 72, loss = 1.34008796\n",
            "Iteration 73, loss = 1.33372470\n",
            "Iteration 74, loss = 1.32785885\n",
            "Iteration 75, loss = 1.32172791\n",
            "Iteration 76, loss = 1.31584787\n",
            "Iteration 77, loss = 1.30994089\n",
            "Iteration 78, loss = 1.30481792\n",
            "Iteration 79, loss = 1.29869457\n",
            "Iteration 80, loss = 1.29327396\n",
            "Iteration 81, loss = 1.28836357\n",
            "Iteration 82, loss = 1.28401855\n",
            "Iteration 83, loss = 1.27866371\n",
            "Iteration 84, loss = 1.27350705\n",
            "Iteration 85, loss = 1.27012703\n",
            "Iteration 86, loss = 1.26518949\n",
            "Iteration 87, loss = 1.26083930\n",
            "Iteration 88, loss = 1.25662683\n",
            "Iteration 89, loss = 1.25251475\n",
            "Iteration 90, loss = 1.24889055\n",
            "Iteration 91, loss = 1.24499015\n",
            "Iteration 92, loss = 1.24114262\n",
            "Iteration 93, loss = 1.23834794\n",
            "Iteration 94, loss = 1.23430312\n",
            "Iteration 95, loss = 1.23177178\n",
            "Iteration 96, loss = 1.22811837\n",
            "Iteration 97, loss = 1.22525276\n",
            "Iteration 98, loss = 1.22204315\n",
            "Iteration 99, loss = 1.22014669\n",
            "Iteration 100, loss = 1.21646183\n",
            "Iteration 1, loss = 1.73886480\n",
            "Iteration 2, loss = 1.66969211\n",
            "Iteration 3, loss = 1.66371433\n",
            "Iteration 4, loss = 1.64982567\n",
            "Iteration 5, loss = 1.61086313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.59461969\n",
            "Iteration 7, loss = 1.59908890\n",
            "Iteration 8, loss = 1.58494879\n",
            "Iteration 9, loss = 1.56820433\n",
            "Iteration 10, loss = 1.55513914\n",
            "Iteration 11, loss = 1.53918042\n",
            "Iteration 12, loss = 1.52021911\n",
            "Iteration 13, loss = 1.49533912\n",
            "Iteration 14, loss = 1.46760678\n",
            "Iteration 15, loss = 1.43405961\n",
            "Iteration 16, loss = 1.39150610\n",
            "Iteration 17, loss = 1.35454617\n",
            "Iteration 18, loss = 1.31446113\n",
            "Iteration 19, loss = 1.27355805\n",
            "Iteration 20, loss = 1.24941957\n",
            "Iteration 21, loss = 1.22030154\n",
            "Iteration 22, loss = 1.19952989\n",
            "Iteration 23, loss = 1.18425873\n",
            "Iteration 24, loss = 1.17088486\n",
            "Iteration 25, loss = 1.16438656\n",
            "Iteration 26, loss = 1.15484161\n",
            "Iteration 27, loss = 1.14487324\n",
            "Iteration 28, loss = 1.13900117\n",
            "Iteration 29, loss = 1.13495713\n",
            "Iteration 30, loss = 1.13136370\n",
            "Iteration 31, loss = 1.13662700\n",
            "Iteration 32, loss = 1.12989647\n",
            "Iteration 33, loss = 1.12260816\n",
            "Iteration 34, loss = 1.13085073\n",
            "Iteration 35, loss = 1.11981885\n",
            "Iteration 36, loss = 1.12865832\n",
            "Iteration 37, loss = 1.12003085\n",
            "Iteration 38, loss = 1.12454584\n",
            "Iteration 39, loss = 1.12004464\n",
            "Iteration 40, loss = 1.12511613\n",
            "Iteration 41, loss = 1.11157201\n",
            "Iteration 42, loss = 1.11785257\n",
            "Iteration 43, loss = 1.11145693\n",
            "Iteration 44, loss = 1.11602668\n",
            "Iteration 45, loss = 1.11125954\n",
            "Iteration 46, loss = 1.11359644\n",
            "Iteration 47, loss = 1.11391119\n",
            "Iteration 48, loss = 1.10945561\n",
            "Iteration 49, loss = 1.11117189\n",
            "Iteration 50, loss = 1.11382071\n",
            "Iteration 51, loss = 1.10965109\n",
            "Iteration 52, loss = 1.11170055\n",
            "Iteration 53, loss = 1.11150440\n",
            "Iteration 54, loss = 1.11680691\n",
            "Iteration 55, loss = 1.11059384\n",
            "Iteration 56, loss = 1.10836484\n",
            "Iteration 57, loss = 1.10904141\n",
            "Iteration 58, loss = 1.10551522\n",
            "Iteration 59, loss = 1.10375762\n",
            "Iteration 60, loss = 1.11547957\n",
            "Iteration 61, loss = 1.11164382\n",
            "Iteration 62, loss = 1.11126256\n",
            "Iteration 63, loss = 1.12014506\n",
            "Iteration 64, loss = 1.11091534\n",
            "Iteration 65, loss = 1.10818707\n",
            "Iteration 66, loss = 1.10388658\n",
            "Iteration 67, loss = 1.10463863\n",
            "Iteration 68, loss = 1.09992290\n",
            "Iteration 69, loss = 1.10226067\n",
            "Iteration 70, loss = 1.10260012\n",
            "Iteration 71, loss = 1.09920503\n",
            "Iteration 72, loss = 1.10201543\n",
            "Iteration 73, loss = 1.10195361\n",
            "Iteration 74, loss = 1.10014543\n",
            "Iteration 75, loss = 1.10190508\n",
            "Iteration 76, loss = 1.09953062\n",
            "Iteration 77, loss = 1.09789490\n",
            "Iteration 78, loss = 1.10108367\n",
            "Iteration 79, loss = 1.10406502\n",
            "Iteration 80, loss = 1.09924172\n",
            "Iteration 81, loss = 1.10258286\n",
            "Iteration 82, loss = 1.09942687\n",
            "Iteration 83, loss = 1.10074287\n",
            "Iteration 84, loss = 1.09309047\n",
            "Iteration 85, loss = 1.09496712\n",
            "Iteration 86, loss = 1.10014463\n",
            "Iteration 87, loss = 1.09365515\n",
            "Iteration 88, loss = 1.09236762\n",
            "Iteration 89, loss = 1.08736720\n",
            "Iteration 90, loss = 1.08971473\n",
            "Iteration 91, loss = 1.08420246\n",
            "Iteration 92, loss = 1.08710693\n",
            "Iteration 93, loss = 1.09258500\n",
            "Iteration 94, loss = 1.08272270\n",
            "Iteration 95, loss = 1.08633645\n",
            "Iteration 96, loss = 1.08259914\n",
            "Iteration 97, loss = 1.08480258\n",
            "Iteration 98, loss = 1.08141440\n",
            "Iteration 99, loss = 1.08649478\n",
            "Iteration 100, loss = 1.07666629\n",
            "Iteration 1, loss = 1.71582654\n",
            "Iteration 2, loss = 1.70567605\n",
            "Iteration 3, loss = 1.69598671\n",
            "Iteration 4, loss = 1.68744661\n",
            "Iteration 5, loss = 1.67896198\n",
            "Iteration 6, loss = 1.67095311\n",
            "Iteration 7, loss = 1.66319807\n",
            "Iteration 8, loss = 1.65675385\n",
            "Iteration 9, loss = 1.64983134\n",
            "Iteration 10, loss = 1.64404066\n",
            "Iteration 11, loss = 1.63851042\n",
            "Iteration 12, loss = 1.63324336\n",
            "Iteration 13, loss = 1.62886163\n",
            "Iteration 14, loss = 1.62386767\n",
            "Iteration 15, loss = 1.61990787\n",
            "Iteration 16, loss = 1.61614325\n",
            "Iteration 17, loss = 1.61277162\n",
            "Iteration 18, loss = 1.60996247\n",
            "Iteration 19, loss = 1.60679178\n",
            "Iteration 20, loss = 1.60385183\n",
            "Iteration 21, loss = 1.60155948\n",
            "Iteration 22, loss = 1.59958172\n",
            "Iteration 23, loss = 1.59756365\n",
            "Iteration 24, loss = 1.59574512\n",
            "Iteration 25, loss = 1.59385433\n",
            "Iteration 26, loss = 1.59202627\n",
            "Iteration 27, loss = 1.59059533\n",
            "Iteration 28, loss = 1.58919436\n",
            "Iteration 29, loss = 1.58802501\n",
            "Iteration 30, loss = 1.58670100\n",
            "Iteration 31, loss = 1.58550931\n",
            "Iteration 32, loss = 1.58449387\n",
            "Iteration 33, loss = 1.58337023\n",
            "Iteration 34, loss = 1.58256927\n",
            "Iteration 35, loss = 1.58168092\n",
            "Iteration 36, loss = 1.58076238\n",
            "Iteration 37, loss = 1.57995210\n",
            "Iteration 38, loss = 1.57923908\n",
            "Iteration 39, loss = 1.57838943\n",
            "Iteration 40, loss = 1.57763573\n",
            "Iteration 41, loss = 1.57689312\n",
            "Iteration 42, loss = 1.57614941\n",
            "Iteration 43, loss = 1.57552120\n",
            "Iteration 44, loss = 1.57480563\n",
            "Iteration 45, loss = 1.57413171\n",
            "Iteration 46, loss = 1.57348212\n",
            "Iteration 47, loss = 1.57286854\n",
            "Iteration 48, loss = 1.57216861\n",
            "Iteration 49, loss = 1.57149675\n",
            "Iteration 50, loss = 1.57089371\n",
            "Iteration 51, loss = 1.57017749\n",
            "Iteration 52, loss = 1.56958143\n",
            "Iteration 53, loss = 1.56895013\n",
            "Iteration 54, loss = 1.56822902\n",
            "Iteration 55, loss = 1.56756260\n",
            "Iteration 56, loss = 1.56692187\n",
            "Iteration 57, loss = 1.56625571\n",
            "Iteration 58, loss = 1.56559347\n",
            "Iteration 59, loss = 1.56494041\n",
            "Iteration 60, loss = 1.56430099\n",
            "Iteration 61, loss = 1.56359322\n",
            "Iteration 62, loss = 1.56294483\n",
            "Iteration 63, loss = 1.56228951\n",
            "Iteration 64, loss = 1.56158250\n",
            "Iteration 65, loss = 1.56087519\n",
            "Iteration 66, loss = 1.56018299\n",
            "Iteration 67, loss = 1.55950986\n",
            "Iteration 68, loss = 1.55878037\n",
            "Iteration 69, loss = 1.55809428\n",
            "Iteration 70, loss = 1.55741089\n",
            "Iteration 71, loss = 1.55667640\n",
            "Iteration 72, loss = 1.55595595\n",
            "Iteration 73, loss = 1.55523590\n",
            "Iteration 74, loss = 1.55453390\n",
            "Iteration 75, loss = 1.55378277\n",
            "Iteration 76, loss = 1.55304277\n",
            "Iteration 77, loss = 1.55228231\n",
            "Iteration 78, loss = 1.55156458\n",
            "Iteration 79, loss = 1.55077911\n",
            "Iteration 80, loss = 1.55003483\n",
            "Iteration 81, loss = 1.54927437\n",
            "Iteration 82, loss = 1.54851424\n",
            "Iteration 83, loss = 1.54772804\n",
            "Iteration 84, loss = 1.54691471\n",
            "Iteration 85, loss = 1.54617144\n",
            "Iteration 86, loss = 1.54535891\n",
            "Iteration 87, loss = 1.54455019\n",
            "Iteration 88, loss = 1.54373977\n",
            "Iteration 89, loss = 1.54292259\n",
            "Iteration 90, loss = 1.54210038\n",
            "Iteration 91, loss = 1.54122821\n",
            "Iteration 92, loss = 1.54041374\n",
            "Iteration 93, loss = 1.53959242\n",
            "Iteration 94, loss = 1.53871400\n",
            "Iteration 95, loss = 1.53788141\n",
            "Iteration 96, loss = 1.53698850\n",
            "Iteration 97, loss = 1.53613927\n",
            "Iteration 98, loss = 1.53523993\n",
            "Iteration 99, loss = 1.53439846\n",
            "Iteration 100, loss = 1.53342493\n",
            "Iteration 1, loss = 1.69557628\n",
            "Iteration 2, loss = 1.62557799\n",
            "Iteration 3, loss = 1.59325730\n",
            "Iteration 4, loss = 1.58744263\n",
            "Iteration 5, loss = 1.58852349\n",
            "Iteration 6, loss = 1.58747773\n",
            "Iteration 7, loss = 1.58496947\n",
            "Iteration 8, loss = 1.57771112\n",
            "Iteration 9, loss = 1.56952596\n",
            "Iteration 10, loss = 1.55993901\n",
            "Iteration 11, loss = 1.55172534\n",
            "Iteration 12, loss = 1.54661381\n",
            "Iteration 13, loss = 1.54024015\n",
            "Iteration 14, loss = 1.53559243\n",
            "Iteration 15, loss = 1.52858747\n",
            "Iteration 16, loss = 1.52052396\n",
            "Iteration 17, loss = 1.51174761\n",
            "Iteration 18, loss = 1.50267414\n",
            "Iteration 19, loss = 1.49274350\n",
            "Iteration 20, loss = 1.48309045\n",
            "Iteration 21, loss = 1.47268519\n",
            "Iteration 22, loss = 1.46162010\n",
            "Iteration 23, loss = 1.45013284\n",
            "Iteration 24, loss = 1.43775091\n",
            "Iteration 25, loss = 1.42528083\n",
            "Iteration 26, loss = 1.41300294\n",
            "Iteration 27, loss = 1.39916123\n",
            "Iteration 28, loss = 1.38601293\n",
            "Iteration 29, loss = 1.37259611\n",
            "Iteration 30, loss = 1.35946234\n",
            "Iteration 31, loss = 1.34568390\n",
            "Iteration 32, loss = 1.33225555\n",
            "Iteration 33, loss = 1.31948660\n",
            "Iteration 34, loss = 1.30672918\n",
            "Iteration 35, loss = 1.29441437\n",
            "Iteration 36, loss = 1.28289598\n",
            "Iteration 37, loss = 1.27189060\n",
            "Iteration 38, loss = 1.26157958\n",
            "Iteration 39, loss = 1.25139566\n",
            "Iteration 40, loss = 1.24190009\n",
            "Iteration 41, loss = 1.23333658\n",
            "Iteration 42, loss = 1.22530520\n",
            "Iteration 43, loss = 1.21736117\n",
            "Iteration 44, loss = 1.21032529\n",
            "Iteration 45, loss = 1.20340845\n",
            "Iteration 46, loss = 1.19706217\n",
            "Iteration 47, loss = 1.19155443\n",
            "Iteration 48, loss = 1.18617649\n",
            "Iteration 49, loss = 1.18106349\n",
            "Iteration 50, loss = 1.17693431\n",
            "Iteration 51, loss = 1.17193265\n",
            "Iteration 52, loss = 1.16787535\n",
            "Iteration 53, loss = 1.16436827\n",
            "Iteration 54, loss = 1.16117558\n",
            "Iteration 55, loss = 1.15719205\n",
            "Iteration 56, loss = 1.15411672\n",
            "Iteration 57, loss = 1.15143856\n",
            "Iteration 58, loss = 1.14841220\n",
            "Iteration 59, loss = 1.14572890\n",
            "Iteration 60, loss = 1.14362869\n",
            "Iteration 61, loss = 1.14121851\n",
            "Iteration 62, loss = 1.13920228\n",
            "Iteration 63, loss = 1.13802114\n",
            "Iteration 64, loss = 1.13520920\n",
            "Iteration 65, loss = 1.13299830\n",
            "Iteration 66, loss = 1.13130231\n",
            "Iteration 67, loss = 1.12947549\n",
            "Iteration 68, loss = 1.12830099\n",
            "Iteration 69, loss = 1.12685451\n",
            "Iteration 70, loss = 1.12532216\n",
            "Iteration 71, loss = 1.12327197\n",
            "Iteration 72, loss = 1.12274285\n",
            "Iteration 73, loss = 1.12151767\n",
            "Iteration 74, loss = 1.12027403\n",
            "Iteration 75, loss = 1.11871542\n",
            "Iteration 76, loss = 1.11805443\n",
            "Iteration 77, loss = 1.11675126\n",
            "Iteration 78, loss = 1.11555970\n",
            "Iteration 79, loss = 1.11430423\n",
            "Iteration 80, loss = 1.11311056\n",
            "Iteration 81, loss = 1.11249673\n",
            "Iteration 82, loss = 1.11171816\n",
            "Iteration 83, loss = 1.11115632\n",
            "Iteration 84, loss = 1.10947405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 1.10923369\n",
            "Iteration 86, loss = 1.10791726\n",
            "Iteration 87, loss = 1.10711722\n",
            "Iteration 88, loss = 1.10656724\n",
            "Iteration 89, loss = 1.10584533\n",
            "Iteration 90, loss = 1.10460231\n",
            "Iteration 91, loss = 1.10541915\n",
            "Iteration 92, loss = 1.10380354\n",
            "Iteration 93, loss = 1.10265921\n",
            "Iteration 94, loss = 1.10235928\n",
            "Iteration 95, loss = 1.10214671\n",
            "Iteration 96, loss = 1.10045050\n",
            "Iteration 97, loss = 1.10057182\n",
            "Iteration 98, loss = 1.09936470\n",
            "Iteration 99, loss = 1.10026829\n",
            "Iteration 100, loss = 1.09775241\n",
            "Iteration 1, loss = 1.70557385\n",
            "Iteration 2, loss = 1.66985308\n",
            "Iteration 3, loss = 1.59686146\n",
            "Iteration 4, loss = 1.59139795\n",
            "Iteration 5, loss = 1.54009932\n",
            "Iteration 6, loss = 1.50417641\n",
            "Iteration 7, loss = 1.45025986\n",
            "Iteration 8, loss = 1.37270337\n",
            "Iteration 9, loss = 1.31057196\n",
            "Iteration 10, loss = 1.25075397\n",
            "Iteration 11, loss = 1.19101572\n",
            "Iteration 12, loss = 1.15883221\n",
            "Iteration 13, loss = 1.14894417\n",
            "Iteration 14, loss = 1.13404784\n",
            "Iteration 15, loss = 1.11163597\n",
            "Iteration 16, loss = 1.11706841\n",
            "Iteration 17, loss = 1.11116689\n",
            "Iteration 18, loss = 1.11352874\n",
            "Iteration 19, loss = 1.10149424\n",
            "Iteration 20, loss = 1.11201863\n",
            "Iteration 21, loss = 1.10890666\n",
            "Iteration 22, loss = 1.11333882\n",
            "Iteration 23, loss = 1.10303105\n",
            "Iteration 24, loss = 1.09486171\n",
            "Iteration 25, loss = 1.09445355\n",
            "Iteration 26, loss = 1.10854775\n",
            "Iteration 27, loss = 1.09712897\n",
            "Iteration 28, loss = 1.08531199\n",
            "Iteration 29, loss = 1.08329546\n",
            "Iteration 30, loss = 1.08121727\n",
            "Iteration 31, loss = 1.06473377\n",
            "Iteration 32, loss = 1.06150986\n",
            "Iteration 33, loss = 1.05106810\n",
            "Iteration 34, loss = 1.04410626\n",
            "Iteration 35, loss = 1.03226641\n",
            "Iteration 36, loss = 1.03070572\n",
            "Iteration 37, loss = 1.01728930\n",
            "Iteration 38, loss = 1.01845189\n",
            "Iteration 39, loss = 1.02247784\n",
            "Iteration 40, loss = 1.06987149\n",
            "Iteration 41, loss = 1.04001331\n",
            "Iteration 42, loss = 1.03395687\n",
            "Iteration 43, loss = 0.99356383\n",
            "Iteration 44, loss = 0.96346176\n",
            "Iteration 45, loss = 0.97759776\n",
            "Iteration 46, loss = 0.96103479\n",
            "Iteration 47, loss = 0.95967817\n",
            "Iteration 48, loss = 0.94877427\n",
            "Iteration 49, loss = 0.92578498\n",
            "Iteration 50, loss = 0.94295375\n",
            "Iteration 51, loss = 0.92287980\n",
            "Iteration 52, loss = 0.94540426\n",
            "Iteration 53, loss = 0.99021556\n",
            "Iteration 54, loss = 0.93168900\n",
            "Iteration 55, loss = 0.91163859\n",
            "Iteration 56, loss = 0.90853271\n",
            "Iteration 57, loss = 0.87924252\n",
            "Iteration 58, loss = 0.88407002\n",
            "Iteration 59, loss = 0.87027632\n",
            "Iteration 60, loss = 0.86878290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 61, loss = 0.86302071\n",
            "Iteration 62, loss = 0.87714604\n",
            "Iteration 63, loss = 0.89843911\n",
            "Iteration 64, loss = 0.86088201\n",
            "Iteration 65, loss = 0.97064624\n",
            "Iteration 66, loss = 0.90866463\n",
            "Iteration 67, loss = 0.94435996\n",
            "Iteration 68, loss = 0.93196476\n",
            "Iteration 69, loss = 0.95028362\n",
            "Iteration 70, loss = 0.95882898\n",
            "Iteration 71, loss = 0.97670216\n",
            "Iteration 72, loss = 0.90717354\n",
            "Iteration 73, loss = 0.88531763\n",
            "Iteration 74, loss = 0.88849251\n",
            "Iteration 75, loss = 0.89137585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.67416868\n",
            "Iteration 2, loss = 1.67111297\n",
            "Iteration 3, loss = 1.66794198\n",
            "Iteration 4, loss = 1.66526252\n",
            "Iteration 5, loss = 1.66266578\n",
            "Iteration 6, loss = 1.65984200\n",
            "Iteration 7, loss = 1.65721664\n",
            "Iteration 8, loss = 1.65489466\n",
            "Iteration 9, loss = 1.65229806\n",
            "Iteration 10, loss = 1.64999586\n",
            "Iteration 11, loss = 1.64774273\n",
            "Iteration 12, loss = 1.64544856\n",
            "Iteration 13, loss = 1.64347076\n",
            "Iteration 14, loss = 1.64111349\n",
            "Iteration 15, loss = 1.63921001\n",
            "Iteration 16, loss = 1.63715874\n",
            "Iteration 17, loss = 1.63520238\n",
            "Iteration 18, loss = 1.63344113\n",
            "Iteration 19, loss = 1.63154183\n",
            "Iteration 20, loss = 1.62970588\n",
            "Iteration 21, loss = 1.62799675\n",
            "Iteration 22, loss = 1.62647019\n",
            "Iteration 23, loss = 1.62480970\n",
            "Iteration 24, loss = 1.62326102\n",
            "Iteration 25, loss = 1.62163830\n",
            "Iteration 26, loss = 1.62001763\n",
            "Iteration 27, loss = 1.61855972\n",
            "Iteration 28, loss = 1.61704014\n",
            "Iteration 29, loss = 1.61569221\n",
            "Iteration 30, loss = 1.61426670\n",
            "Iteration 31, loss = 1.61275110\n",
            "Iteration 32, loss = 1.61147626\n",
            "Iteration 33, loss = 1.61004355\n",
            "Iteration 34, loss = 1.60884630\n",
            "Iteration 35, loss = 1.60756482\n",
            "Iteration 36, loss = 1.60620289\n",
            "Iteration 37, loss = 1.60503885\n",
            "Iteration 38, loss = 1.60384422\n",
            "Iteration 39, loss = 1.60254200\n",
            "Iteration 40, loss = 1.60128983\n",
            "Iteration 41, loss = 1.60008273\n",
            "Iteration 42, loss = 1.59889375\n",
            "Iteration 43, loss = 1.59784021\n",
            "Iteration 44, loss = 1.59663404\n",
            "Iteration 45, loss = 1.59549324\n",
            "Iteration 46, loss = 1.59444160\n",
            "Iteration 47, loss = 1.59334469\n",
            "Iteration 48, loss = 1.59226833\n",
            "Iteration 49, loss = 1.59116331\n",
            "Iteration 50, loss = 1.59015400\n",
            "Iteration 51, loss = 1.58901216\n",
            "Iteration 52, loss = 1.58805704\n",
            "Iteration 53, loss = 1.58706948\n",
            "Iteration 54, loss = 1.58586011\n",
            "Iteration 55, loss = 1.58489117\n",
            "Iteration 56, loss = 1.58398507\n",
            "Iteration 57, loss = 1.58290434\n",
            "Iteration 58, loss = 1.58194796\n",
            "Iteration 59, loss = 1.58104038\n",
            "Iteration 60, loss = 1.58002778\n",
            "Iteration 61, loss = 1.57906043\n",
            "Iteration 62, loss = 1.57811310\n",
            "Iteration 63, loss = 1.57727802\n",
            "Iteration 64, loss = 1.57619964\n",
            "Iteration 65, loss = 1.57536055\n",
            "Iteration 66, loss = 1.57438076\n",
            "Iteration 67, loss = 1.57350247\n",
            "Iteration 68, loss = 1.57257289\n",
            "Iteration 69, loss = 1.57170663\n",
            "Iteration 70, loss = 1.57073469\n",
            "Iteration 71, loss = 1.56989825\n",
            "Iteration 72, loss = 1.56903566\n",
            "Iteration 73, loss = 1.56811712\n",
            "Iteration 74, loss = 1.56723052\n",
            "Iteration 75, loss = 1.56637942\n",
            "Iteration 76, loss = 1.56552993\n",
            "Iteration 77, loss = 1.56461642\n",
            "Iteration 78, loss = 1.56379592\n",
            "Iteration 79, loss = 1.56294163\n",
            "Iteration 80, loss = 1.56208219\n",
            "Iteration 81, loss = 1.56123009\n",
            "Iteration 82, loss = 1.56034682\n",
            "Iteration 83, loss = 1.55948755\n",
            "Iteration 84, loss = 1.55864995\n",
            "Iteration 85, loss = 1.55787010\n",
            "Iteration 86, loss = 1.55695558\n",
            "Iteration 87, loss = 1.55616505\n",
            "Iteration 88, loss = 1.55525578\n",
            "Iteration 89, loss = 1.55446054\n",
            "Iteration 90, loss = 1.55363227\n",
            "Iteration 91, loss = 1.55275538\n",
            "Iteration 92, loss = 1.55192887\n",
            "Iteration 93, loss = 1.55105310\n",
            "Iteration 94, loss = 1.55023876\n",
            "Iteration 95, loss = 1.54939881\n",
            "Iteration 96, loss = 1.54856510\n",
            "Iteration 97, loss = 1.54772767\n",
            "Iteration 98, loss = 1.54684411\n",
            "Iteration 99, loss = 1.54605843\n",
            "Iteration 100, loss = 1.54514988\n",
            "Iteration 1, loss = 1.66753673\n",
            "Iteration 2, loss = 1.64238013\n",
            "Iteration 3, loss = 1.62141075\n",
            "Iteration 4, loss = 1.60878127\n",
            "Iteration 5, loss = 1.59799604\n",
            "Iteration 6, loss = 1.58775009\n",
            "Iteration 7, loss = 1.57923149\n",
            "Iteration 8, loss = 1.57208470\n",
            "Iteration 9, loss = 1.56461902\n",
            "Iteration 10, loss = 1.55833308\n",
            "Iteration 11, loss = 1.55247092\n",
            "Iteration 12, loss = 1.54664322\n",
            "Iteration 13, loss = 1.54073837\n",
            "Iteration 14, loss = 1.53476631\n",
            "Iteration 15, loss = 1.52815495\n",
            "Iteration 16, loss = 1.52088180\n",
            "Iteration 17, loss = 1.51312518\n",
            "Iteration 18, loss = 1.50449313\n",
            "Iteration 19, loss = 1.49557031\n",
            "Iteration 20, loss = 1.48672081\n",
            "Iteration 21, loss = 1.47651586\n",
            "Iteration 22, loss = 1.46636493\n",
            "Iteration 23, loss = 1.45589265\n",
            "Iteration 24, loss = 1.44516911\n",
            "Iteration 25, loss = 1.43426803\n",
            "Iteration 26, loss = 1.42370416\n",
            "Iteration 27, loss = 1.41194776\n",
            "Iteration 28, loss = 1.40065288\n",
            "Iteration 29, loss = 1.38919747\n",
            "Iteration 30, loss = 1.37788334\n",
            "Iteration 31, loss = 1.36620446\n",
            "Iteration 32, loss = 1.35494233\n",
            "Iteration 33, loss = 1.34408278\n",
            "Iteration 34, loss = 1.33288220\n",
            "Iteration 35, loss = 1.32214979\n",
            "Iteration 36, loss = 1.31190018\n",
            "Iteration 37, loss = 1.30208110\n",
            "Iteration 38, loss = 1.29258683\n",
            "Iteration 39, loss = 1.28333117\n",
            "Iteration 40, loss = 1.27455296\n",
            "Iteration 41, loss = 1.26649331\n",
            "Iteration 42, loss = 1.25868985\n",
            "Iteration 43, loss = 1.25102001\n",
            "Iteration 44, loss = 1.24406227\n",
            "Iteration 45, loss = 1.23714218\n",
            "Iteration 46, loss = 1.23078076\n",
            "Iteration 47, loss = 1.22514856\n",
            "Iteration 48, loss = 1.21932330\n",
            "Iteration 49, loss = 1.21394706\n",
            "Iteration 50, loss = 1.20929302\n",
            "Iteration 51, loss = 1.20428122\n",
            "Iteration 52, loss = 1.19979708\n",
            "Iteration 53, loss = 1.19577234\n",
            "Iteration 54, loss = 1.19213499\n",
            "Iteration 55, loss = 1.18785277\n",
            "Iteration 56, loss = 1.18420755\n",
            "Iteration 57, loss = 1.18101974\n",
            "Iteration 58, loss = 1.17771374\n",
            "Iteration 59, loss = 1.17460025\n",
            "Iteration 60, loss = 1.17192898\n",
            "Iteration 61, loss = 1.16912477\n",
            "Iteration 62, loss = 1.16668491\n",
            "Iteration 63, loss = 1.16463246\n",
            "Iteration 64, loss = 1.16162736\n",
            "Iteration 65, loss = 1.15935094\n",
            "Iteration 66, loss = 1.15711321\n",
            "Iteration 67, loss = 1.15491543\n",
            "Iteration 68, loss = 1.15322385\n",
            "Iteration 69, loss = 1.15132444\n",
            "Iteration 70, loss = 1.14954198\n",
            "Iteration 71, loss = 1.14733339\n",
            "Iteration 72, loss = 1.14619865\n",
            "Iteration 73, loss = 1.14454618\n",
            "Iteration 74, loss = 1.14297530\n",
            "Iteration 75, loss = 1.14133494\n",
            "Iteration 76, loss = 1.14012975\n",
            "Iteration 77, loss = 1.13866647\n",
            "Iteration 78, loss = 1.13718038\n",
            "Iteration 79, loss = 1.13569845\n",
            "Iteration 80, loss = 1.13438110\n",
            "Iteration 81, loss = 1.13350135\n",
            "Iteration 82, loss = 1.13229645\n",
            "Iteration 83, loss = 1.13120965\n",
            "Iteration 84, loss = 1.12980805\n",
            "Iteration 85, loss = 1.12911987\n",
            "Iteration 86, loss = 1.12779854\n",
            "Iteration 87, loss = 1.12687897\n",
            "Iteration 88, loss = 1.12593414\n",
            "Iteration 89, loss = 1.12514423\n",
            "Iteration 90, loss = 1.12407629\n",
            "Iteration 91, loss = 1.12428236\n",
            "Iteration 92, loss = 1.12267428\n",
            "Iteration 93, loss = 1.12175751\n",
            "Iteration 94, loss = 1.12112293\n",
            "Iteration 95, loss = 1.12073322\n",
            "Iteration 96, loss = 1.11916182\n",
            "Iteration 97, loss = 1.11909434\n",
            "Iteration 98, loss = 1.11823377\n",
            "Iteration 99, loss = 1.11877449\n",
            "Iteration 100, loss = 1.11653398\n",
            "Iteration 1, loss = 1.63949534\n",
            "Iteration 2, loss = 1.57714231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3, loss = 1.54359282\n",
            "Iteration 4, loss = 1.48996193\n",
            "Iteration 5, loss = 1.42909411\n",
            "Iteration 6, loss = 1.35656090\n",
            "Iteration 7, loss = 1.28986142\n",
            "Iteration 8, loss = 1.24136868\n",
            "Iteration 9, loss = 1.19663027\n",
            "Iteration 10, loss = 1.17246178\n",
            "Iteration 11, loss = 1.15006764\n",
            "Iteration 12, loss = 1.14367101\n",
            "Iteration 13, loss = 1.14137773\n",
            "Iteration 14, loss = 1.12498136\n",
            "Iteration 15, loss = 1.12681353\n",
            "Iteration 16, loss = 1.11963159\n",
            "Iteration 17, loss = 1.11722537\n",
            "Iteration 18, loss = 1.12129678\n",
            "Iteration 19, loss = 1.12275682\n",
            "Iteration 20, loss = 1.10796308\n",
            "Iteration 21, loss = 1.12930660\n",
            "Iteration 22, loss = 1.13386536\n",
            "Iteration 23, loss = 1.11849831\n",
            "Iteration 24, loss = 1.12084177\n",
            "Iteration 25, loss = 1.11588768\n",
            "Iteration 26, loss = 1.11291126\n",
            "Iteration 27, loss = 1.10230015\n",
            "Iteration 28, loss = 1.10477015\n",
            "Iteration 29, loss = 1.10563136\n",
            "Iteration 30, loss = 1.09863580\n",
            "Iteration 31, loss = 1.10197273\n",
            "Iteration 32, loss = 1.10726535\n",
            "Iteration 33, loss = 1.09760508\n",
            "Iteration 34, loss = 1.10600878\n",
            "Iteration 35, loss = 1.10063572\n",
            "Iteration 36, loss = 1.10512293\n",
            "Iteration 37, loss = 1.09973803\n",
            "Iteration 38, loss = 1.09298140\n",
            "Iteration 39, loss = 1.10150665\n",
            "Iteration 40, loss = 1.09349683\n",
            "Iteration 41, loss = 1.09367543\n",
            "Iteration 42, loss = 1.08948858\n",
            "Iteration 43, loss = 1.08928962\n",
            "Iteration 44, loss = 1.08836336\n",
            "Iteration 45, loss = 1.08125582\n",
            "Iteration 46, loss = 1.07986785\n",
            "Iteration 47, loss = 1.07892693\n",
            "Iteration 48, loss = 1.08357120\n",
            "Iteration 49, loss = 1.07548372\n",
            "Iteration 50, loss = 1.07973345\n",
            "Iteration 51, loss = 1.07458363\n",
            "Iteration 52, loss = 1.07434038\n",
            "Iteration 53, loss = 1.07159318\n",
            "Iteration 54, loss = 1.07268111\n",
            "Iteration 55, loss = 1.06424897\n",
            "Iteration 56, loss = 1.06182014\n",
            "Iteration 57, loss = 1.05589199\n",
            "Iteration 58, loss = 1.05564197\n",
            "Iteration 59, loss = 1.04467534\n",
            "Iteration 60, loss = 1.05047729\n",
            "Iteration 61, loss = 1.04546132\n",
            "Iteration 62, loss = 1.04011186\n",
            "Iteration 63, loss = 1.04006429\n",
            "Iteration 64, loss = 1.02560469\n",
            "Iteration 65, loss = 1.02419878\n",
            "Iteration 66, loss = 1.01560320\n",
            "Iteration 67, loss = 1.02019781\n",
            "Iteration 68, loss = 1.01198576\n",
            "Iteration 69, loss = 1.00932009\n",
            "Iteration 70, loss = 1.02228151\n",
            "Iteration 71, loss = 0.99549507\n",
            "Iteration 72, loss = 0.98474962\n",
            "Iteration 73, loss = 0.97562925\n",
            "Iteration 74, loss = 0.99072479\n",
            "Iteration 75, loss = 0.97965088\n",
            "Iteration 76, loss = 0.95858366\n",
            "Iteration 77, loss = 0.95740295\n",
            "Iteration 78, loss = 0.95387516\n",
            "Iteration 79, loss = 0.97053641\n",
            "Iteration 80, loss = 0.95298258\n",
            "Iteration 81, loss = 0.95491997\n",
            "Iteration 82, loss = 0.94678937\n",
            "Iteration 83, loss = 0.94236759\n",
            "Iteration 84, loss = 0.92288111\n",
            "Iteration 85, loss = 0.91939810\n",
            "Iteration 86, loss = 0.91714687\n",
            "Iteration 87, loss = 0.91815669\n",
            "Iteration 88, loss = 0.90547871\n",
            "Iteration 89, loss = 0.92884012\n",
            "Iteration 90, loss = 0.95293933\n",
            "Iteration 91, loss = 0.89718113\n",
            "Iteration 92, loss = 0.90558155\n",
            "Iteration 93, loss = 0.88233352\n",
            "Iteration 94, loss = 0.88880280\n",
            "Iteration 95, loss = 0.91093190\n",
            "Iteration 96, loss = 0.90275412\n",
            "Iteration 97, loss = 0.89318081\n",
            "Iteration 98, loss = 0.88409194\n",
            "Iteration 99, loss = 0.86469349\n",
            "Iteration 100, loss = 0.85757211\n",
            "Iteration 1, loss = 1.88666241\n",
            "Iteration 2, loss = 1.87371181\n",
            "Iteration 3, loss = 1.86119602\n",
            "Iteration 4, loss = 1.84903482\n",
            "Iteration 5, loss = 1.83749350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.82646210\n",
            "Iteration 7, loss = 1.81565064\n",
            "Iteration 8, loss = 1.80554855\n",
            "Iteration 9, loss = 1.79580079\n",
            "Iteration 10, loss = 1.78639299\n",
            "Iteration 11, loss = 1.77778556\n",
            "Iteration 12, loss = 1.76921405\n",
            "Iteration 13, loss = 1.76105410\n",
            "Iteration 14, loss = 1.75347990\n",
            "Iteration 15, loss = 1.74572463\n",
            "Iteration 16, loss = 1.73827978\n",
            "Iteration 17, loss = 1.73186688\n",
            "Iteration 18, loss = 1.72500580\n",
            "Iteration 19, loss = 1.71897185\n",
            "Iteration 20, loss = 1.71252687\n",
            "Iteration 21, loss = 1.70682484\n",
            "Iteration 22, loss = 1.70098090\n",
            "Iteration 23, loss = 1.69566864\n",
            "Iteration 24, loss = 1.69028132\n",
            "Iteration 25, loss = 1.68531553\n",
            "Iteration 26, loss = 1.68018579\n",
            "Iteration 27, loss = 1.67568066\n",
            "Iteration 28, loss = 1.67121935\n",
            "Iteration 29, loss = 1.66715599\n",
            "Iteration 30, loss = 1.66266030\n",
            "Iteration 31, loss = 1.65892433\n",
            "Iteration 32, loss = 1.65459802\n",
            "Iteration 33, loss = 1.65108187\n",
            "Iteration 34, loss = 1.64775182\n",
            "Iteration 35, loss = 1.64396351\n",
            "Iteration 36, loss = 1.64077594\n",
            "Iteration 37, loss = 1.63755836\n",
            "Iteration 38, loss = 1.63474036\n",
            "Iteration 39, loss = 1.63188324\n",
            "Iteration 40, loss = 1.62912625\n",
            "Iteration 41, loss = 1.62631886\n",
            "Iteration 42, loss = 1.62422954\n",
            "Iteration 43, loss = 1.62144374\n",
            "Iteration 44, loss = 1.61937010\n",
            "Iteration 45, loss = 1.61742334\n",
            "Iteration 46, loss = 1.61508293\n",
            "Iteration 47, loss = 1.61327804\n",
            "Iteration 48, loss = 1.61149051\n",
            "Iteration 49, loss = 1.60952014\n",
            "Iteration 50, loss = 1.60803270\n",
            "Iteration 51, loss = 1.60623697\n",
            "Iteration 52, loss = 1.60483442\n",
            "Iteration 53, loss = 1.60354327\n",
            "Iteration 54, loss = 1.60222879\n",
            "Iteration 55, loss = 1.60093064\n",
            "Iteration 56, loss = 1.59970147\n",
            "Iteration 57, loss = 1.59874785\n",
            "Iteration 58, loss = 1.59749797\n",
            "Iteration 59, loss = 1.59657282\n",
            "Iteration 60, loss = 1.59558213\n",
            "Iteration 61, loss = 1.59481685\n",
            "Iteration 62, loss = 1.59381574\n",
            "Iteration 63, loss = 1.59309953\n",
            "Iteration 64, loss = 1.59217549\n",
            "Iteration 65, loss = 1.59154041\n",
            "Iteration 66, loss = 1.59065799\n",
            "Iteration 67, loss = 1.59002272\n",
            "Iteration 68, loss = 1.58934461\n",
            "Iteration 69, loss = 1.58873718\n",
            "Iteration 70, loss = 1.58817235\n",
            "Iteration 71, loss = 1.58745111\n",
            "Iteration 72, loss = 1.58690754\n",
            "Iteration 73, loss = 1.58631708\n",
            "Iteration 74, loss = 1.58576313\n",
            "Iteration 75, loss = 1.58515871\n",
            "Iteration 76, loss = 1.58462256\n",
            "Iteration 77, loss = 1.58409293\n",
            "Iteration 78, loss = 1.58353926\n",
            "Iteration 79, loss = 1.58305031\n",
            "Iteration 80, loss = 1.58247078\n",
            "Iteration 81, loss = 1.58194670\n",
            "Iteration 82, loss = 1.58147996\n",
            "Iteration 83, loss = 1.58092324\n",
            "Iteration 84, loss = 1.58044528\n",
            "Iteration 85, loss = 1.57993231\n",
            "Iteration 86, loss = 1.57938487\n",
            "Iteration 87, loss = 1.57885876\n",
            "Iteration 88, loss = 1.57835238\n",
            "Iteration 89, loss = 1.57786963\n",
            "Iteration 90, loss = 1.57732340\n",
            "Iteration 91, loss = 1.57684960\n",
            "Iteration 92, loss = 1.57632525\n",
            "Iteration 93, loss = 1.57579881\n",
            "Iteration 94, loss = 1.57523806\n",
            "Iteration 95, loss = 1.57479522\n",
            "Iteration 96, loss = 1.57421003\n",
            "Iteration 97, loss = 1.57366635\n",
            "Iteration 98, loss = 1.57314225\n",
            "Iteration 99, loss = 1.57265086\n",
            "Iteration 100, loss = 1.57208340\n",
            "Iteration 1, loss = 1.85981954\n",
            "Iteration 2, loss = 1.75913562\n",
            "Iteration 3, loss = 1.69159180\n",
            "Iteration 4, loss = 1.64522404\n",
            "Iteration 5, loss = 1.61692139\n",
            "Iteration 6, loss = 1.60262685\n",
            "Iteration 7, loss = 1.59488793\n",
            "Iteration 8, loss = 1.59755330\n",
            "Iteration 9, loss = 1.59857435\n",
            "Iteration 10, loss = 1.59690954\n",
            "Iteration 11, loss = 1.59120447\n",
            "Iteration 12, loss = 1.58327168\n",
            "Iteration 13, loss = 1.57467285\n",
            "Iteration 14, loss = 1.56628573\n",
            "Iteration 15, loss = 1.56024412\n",
            "Iteration 16, loss = 1.55519216\n",
            "Iteration 17, loss = 1.54960813\n",
            "Iteration 18, loss = 1.54466434\n",
            "Iteration 19, loss = 1.53906446\n",
            "Iteration 20, loss = 1.53266891\n",
            "Iteration 21, loss = 1.52552636\n",
            "Iteration 22, loss = 1.51748294\n",
            "Iteration 23, loss = 1.50935477\n",
            "Iteration 24, loss = 1.50034724\n",
            "Iteration 25, loss = 1.49167328\n",
            "Iteration 26, loss = 1.48110710\n",
            "Iteration 27, loss = 1.47157806\n",
            "Iteration 28, loss = 1.46122004\n",
            "Iteration 29, loss = 1.45052277\n",
            "Iteration 30, loss = 1.43891930\n",
            "Iteration 31, loss = 1.42749422\n",
            "Iteration 32, loss = 1.41562728\n",
            "Iteration 33, loss = 1.40337845\n",
            "Iteration 34, loss = 1.39072311\n",
            "Iteration 35, loss = 1.37840820\n",
            "Iteration 36, loss = 1.36575324\n",
            "Iteration 37, loss = 1.35428429\n",
            "Iteration 38, loss = 1.34225674\n",
            "Iteration 39, loss = 1.33021642\n",
            "Iteration 40, loss = 1.31888138\n",
            "Iteration 41, loss = 1.30802923\n",
            "Iteration 42, loss = 1.29844900\n",
            "Iteration 43, loss = 1.28781069\n",
            "Iteration 44, loss = 1.27857975\n",
            "Iteration 45, loss = 1.26946515\n",
            "Iteration 46, loss = 1.26062998\n",
            "Iteration 47, loss = 1.25290355\n",
            "Iteration 48, loss = 1.24462783\n",
            "Iteration 49, loss = 1.23755628\n",
            "Iteration 50, loss = 1.23090171\n",
            "Iteration 51, loss = 1.22474437\n",
            "Iteration 52, loss = 1.21855318\n",
            "Iteration 53, loss = 1.21244643\n",
            "Iteration 54, loss = 1.20717263\n",
            "Iteration 55, loss = 1.20236132\n",
            "Iteration 56, loss = 1.19757936\n",
            "Iteration 57, loss = 1.19316837\n",
            "Iteration 58, loss = 1.18860886\n",
            "Iteration 59, loss = 1.18482011\n",
            "Iteration 60, loss = 1.18100865\n",
            "Iteration 61, loss = 1.17761700\n",
            "Iteration 62, loss = 1.17391697\n",
            "Iteration 63, loss = 1.17122331\n",
            "Iteration 64, loss = 1.16790279\n",
            "Iteration 65, loss = 1.16510790\n",
            "Iteration 66, loss = 1.16188450\n",
            "Iteration 67, loss = 1.15879143\n",
            "Iteration 68, loss = 1.15729070\n",
            "Iteration 69, loss = 1.15507763\n",
            "Iteration 70, loss = 1.15240443\n",
            "Iteration 71, loss = 1.14969872\n",
            "Iteration 72, loss = 1.14757611\n",
            "Iteration 73, loss = 1.14575916\n",
            "Iteration 74, loss = 1.14343051\n",
            "Iteration 75, loss = 1.14191069\n",
            "Iteration 76, loss = 1.13978957\n",
            "Iteration 77, loss = 1.13847989\n",
            "Iteration 78, loss = 1.13650780\n",
            "Iteration 79, loss = 1.13454394\n",
            "Iteration 80, loss = 1.13360030\n",
            "Iteration 81, loss = 1.13189334\n",
            "Iteration 82, loss = 1.13024399\n",
            "Iteration 83, loss = 1.12897401\n",
            "Iteration 84, loss = 1.12826204\n",
            "Iteration 85, loss = 1.12622461\n",
            "Iteration 86, loss = 1.12468153\n",
            "Iteration 87, loss = 1.12321208\n",
            "Iteration 88, loss = 1.12339414\n",
            "Iteration 89, loss = 1.12129907\n",
            "Iteration 90, loss = 1.11948113\n",
            "Iteration 91, loss = 1.11893815\n",
            "Iteration 92, loss = 1.11829577\n",
            "Iteration 93, loss = 1.11708861\n",
            "Iteration 94, loss = 1.11530950\n",
            "Iteration 95, loss = 1.11605524\n",
            "Iteration 96, loss = 1.11379438\n",
            "Iteration 97, loss = 1.11207261\n",
            "Iteration 98, loss = 1.11108689\n",
            "Iteration 99, loss = 1.11057561\n",
            "Iteration 100, loss = 1.10949982\n",
            "Iteration 1, loss = 1.77402455\n",
            "Iteration 2, loss = 1.64098533\n",
            "Iteration 3, loss = 1.57025661\n",
            "Iteration 4, loss = 1.56896655\n",
            "Iteration 5, loss = 1.47206011\n",
            "Iteration 6, loss = 1.42617948\n",
            "Iteration 7, loss = 1.36421847\n",
            "Iteration 8, loss = 1.27835188\n",
            "Iteration 9, loss = 1.24648170\n",
            "Iteration 10, loss = 1.21857811\n",
            "Iteration 11, loss = 1.17675785\n",
            "Iteration 12, loss = 1.16321964\n",
            "Iteration 13, loss = 1.15078841\n",
            "Iteration 14, loss = 1.13920767\n",
            "Iteration 15, loss = 1.12789045\n",
            "Iteration 16, loss = 1.12640533\n",
            "Iteration 17, loss = 1.12650101\n",
            "Iteration 18, loss = 1.10947195\n",
            "Iteration 19, loss = 1.11000524\n",
            "Iteration 20, loss = 1.10761096\n",
            "Iteration 21, loss = 1.10390523\n",
            "Iteration 22, loss = 1.10631564\n",
            "Iteration 23, loss = 1.09704563\n",
            "Iteration 24, loss = 1.09917689\n",
            "Iteration 25, loss = 1.08869542\n",
            "Iteration 26, loss = 1.09044019\n",
            "Iteration 27, loss = 1.08559739\n",
            "Iteration 28, loss = 1.07828033\n",
            "Iteration 29, loss = 1.07595749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 30, loss = 1.07324792\n",
            "Iteration 31, loss = 1.06910142\n",
            "Iteration 32, loss = 1.06517052\n",
            "Iteration 33, loss = 1.06948581\n",
            "Iteration 34, loss = 1.09203837\n",
            "Iteration 35, loss = 1.09214424\n",
            "Iteration 36, loss = 1.06976025\n",
            "Iteration 37, loss = 1.09985002\n",
            "Iteration 38, loss = 1.09114400\n",
            "Iteration 39, loss = 1.06532689\n",
            "Iteration 40, loss = 1.08684531\n",
            "Iteration 41, loss = 1.07334506\n",
            "Iteration 42, loss = 1.07316241\n",
            "Iteration 43, loss = 1.03713075\n",
            "Iteration 44, loss = 1.04196201\n",
            "Iteration 45, loss = 1.03493699\n",
            "Iteration 46, loss = 1.01003015\n",
            "Iteration 47, loss = 1.01830663\n",
            "Iteration 48, loss = 1.03060589\n",
            "Iteration 49, loss = 1.01522379\n",
            "Iteration 50, loss = 1.01095643\n",
            "Iteration 51, loss = 1.00035393\n",
            "Iteration 52, loss = 0.98585131\n",
            "Iteration 53, loss = 0.96873203\n",
            "Iteration 54, loss = 0.95892472\n",
            "Iteration 55, loss = 0.96028228\n",
            "Iteration 56, loss = 0.95717871\n",
            "Iteration 57, loss = 0.94760982\n",
            "Iteration 58, loss = 0.94175000\n",
            "Iteration 59, loss = 0.93373727\n",
            "Iteration 60, loss = 0.92552792\n",
            "Iteration 61, loss = 0.92542731\n",
            "Iteration 62, loss = 0.91702587\n",
            "Iteration 63, loss = 0.92345345\n",
            "Iteration 64, loss = 0.91283031\n",
            "Iteration 65, loss = 0.91204743\n",
            "Iteration 66, loss = 0.90904331\n",
            "Iteration 67, loss = 0.88997153\n",
            "Iteration 68, loss = 0.88709469\n",
            "Iteration 69, loss = 0.88819351\n",
            "Iteration 70, loss = 0.88852452\n",
            "Iteration 71, loss = 0.91443502\n",
            "Iteration 72, loss = 0.89108796\n",
            "Iteration 73, loss = 0.91475544\n",
            "Iteration 74, loss = 0.86736061\n",
            "Iteration 75, loss = 0.86067065\n",
            "Iteration 76, loss = 0.87628063\n",
            "Iteration 77, loss = 0.86000775\n",
            "Iteration 78, loss = 0.83689900\n",
            "Iteration 79, loss = 0.84142353\n",
            "Iteration 80, loss = 0.83122191\n",
            "Iteration 81, loss = 0.82745368\n",
            "Iteration 82, loss = 0.82165735\n",
            "Iteration 83, loss = 0.82008270\n",
            "Iteration 84, loss = 0.87328302\n",
            "Iteration 85, loss = 0.88242631\n",
            "Iteration 86, loss = 0.96974982\n",
            "Iteration 87, loss = 0.99535666\n",
            "Iteration 88, loss = 0.95215636\n",
            "Iteration 89, loss = 0.91222558\n",
            "Iteration 90, loss = 0.92046537\n",
            "Iteration 91, loss = 0.92538264\n",
            "Iteration 92, loss = 0.87936925\n",
            "Iteration 93, loss = 0.86259685\n",
            "Iteration 94, loss = 0.86005614\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.71955312\n",
            "Iteration 2, loss = 1.71189895\n",
            "Iteration 3, loss = 1.70460547\n",
            "Iteration 4, loss = 1.69775648\n",
            "Iteration 5, loss = 1.69117425\n",
            "Iteration 6, loss = 1.68524007\n",
            "Iteration 7, loss = 1.67906751\n",
            "Iteration 8, loss = 1.67393233\n",
            "Iteration 9, loss = 1.66884994\n",
            "Iteration 10, loss = 1.66377923\n",
            "Iteration 11, loss = 1.65911836\n",
            "Iteration 12, loss = 1.65538561\n",
            "Iteration 13, loss = 1.65071620\n",
            "Iteration 14, loss = 1.64734852\n",
            "Iteration 15, loss = 1.64382818\n",
            "Iteration 16, loss = 1.64040830\n",
            "Iteration 17, loss = 1.63725165\n",
            "Iteration 18, loss = 1.63454766\n",
            "Iteration 19, loss = 1.63199773\n",
            "Iteration 20, loss = 1.62928861\n",
            "Iteration 21, loss = 1.62696291\n",
            "Iteration 22, loss = 1.62444358\n",
            "Iteration 23, loss = 1.62242808\n",
            "Iteration 24, loss = 1.62024878\n",
            "Iteration 25, loss = 1.61865235\n",
            "Iteration 26, loss = 1.61659526\n",
            "Iteration 27, loss = 1.61512194\n",
            "Iteration 28, loss = 1.61350062\n",
            "Iteration 29, loss = 1.61203323\n",
            "Iteration 30, loss = 1.61073958\n",
            "Iteration 31, loss = 1.60960965\n",
            "Iteration 32, loss = 1.60847815\n",
            "Iteration 33, loss = 1.60747036\n",
            "Iteration 34, loss = 1.60658001\n",
            "Iteration 35, loss = 1.60559749\n",
            "Iteration 36, loss = 1.60478147\n",
            "Iteration 37, loss = 1.60416900\n",
            "Iteration 38, loss = 1.60352355\n",
            "Iteration 39, loss = 1.60274827\n",
            "Iteration 40, loss = 1.60226535\n",
            "Iteration 41, loss = 1.60183381\n",
            "Iteration 42, loss = 1.60162680\n",
            "Iteration 43, loss = 1.60108138\n",
            "Iteration 44, loss = 1.60068114\n",
            "Iteration 45, loss = 1.60047769\n",
            "Iteration 46, loss = 1.60008142\n",
            "Iteration 47, loss = 1.59987211\n",
            "Iteration 48, loss = 1.59960089\n",
            "Iteration 49, loss = 1.59938121\n",
            "Iteration 50, loss = 1.59918435\n",
            "Iteration 51, loss = 1.59910460\n",
            "Iteration 52, loss = 1.59890779\n",
            "Iteration 53, loss = 1.59868373\n",
            "Iteration 54, loss = 1.59860723\n",
            "Iteration 55, loss = 1.59848548\n",
            "Iteration 56, loss = 1.59837655\n",
            "Iteration 57, loss = 1.59829426\n",
            "Iteration 58, loss = 1.59816725\n",
            "Iteration 59, loss = 1.59810483\n",
            "Iteration 60, loss = 1.59804110\n",
            "Iteration 61, loss = 1.59801848\n",
            "Iteration 62, loss = 1.59790554\n",
            "Iteration 63, loss = 1.59791269\n",
            "Iteration 64, loss = 1.59774336\n",
            "Iteration 65, loss = 1.59773084\n",
            "Iteration 66, loss = 1.59768248\n",
            "Iteration 67, loss = 1.59756776\n",
            "Iteration 68, loss = 1.59753301\n",
            "Iteration 69, loss = 1.59745933\n",
            "Iteration 70, loss = 1.59744101\n",
            "Iteration 71, loss = 1.59736342\n",
            "Iteration 72, loss = 1.59730931\n",
            "Iteration 73, loss = 1.59725472\n",
            "Iteration 74, loss = 1.59720936\n",
            "Iteration 75, loss = 1.59716809\n",
            "Iteration 76, loss = 1.59711069\n",
            "Iteration 77, loss = 1.59705148\n",
            "Iteration 78, loss = 1.59699584\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.70560328\n",
            "Iteration 2, loss = 1.64949091\n",
            "Iteration 3, loss = 1.62021680\n",
            "Iteration 4, loss = 1.60482391\n",
            "Iteration 5, loss = 1.59917953\n",
            "Iteration 6, loss = 1.60264703\n",
            "Iteration 7, loss = 1.60835857\n",
            "Iteration 8, loss = 1.61120026\n",
            "Iteration 9, loss = 1.60855091\n",
            "Iteration 10, loss = 1.60411430\n",
            "Iteration 11, loss = 1.60117587\n",
            "Iteration 12, loss = 1.59693933\n",
            "Iteration 13, loss = 1.59773422\n",
            "Iteration 14, loss = 1.59680064\n",
            "Iteration 15, loss = 1.59626296\n",
            "Iteration 16, loss = 1.59663706\n",
            "Iteration 17, loss = 1.59582742\n",
            "Iteration 18, loss = 1.59488824\n",
            "Iteration 19, loss = 1.59454482\n",
            "Iteration 20, loss = 1.59297252\n",
            "Iteration 21, loss = 1.59225256\n",
            "Iteration 22, loss = 1.59074329\n",
            "Iteration 23, loss = 1.59057222\n",
            "Iteration 24, loss = 1.58961557\n",
            "Iteration 25, loss = 1.58919180\n",
            "Iteration 26, loss = 1.58882987\n",
            "Iteration 27, loss = 1.58734002\n",
            "Iteration 28, loss = 1.58661167\n",
            "Iteration 29, loss = 1.58589271\n",
            "Iteration 30, loss = 1.58431987\n",
            "Iteration 31, loss = 1.58388632\n",
            "Iteration 32, loss = 1.58284468\n",
            "Iteration 33, loss = 1.58167964\n",
            "Iteration 34, loss = 1.58048574\n",
            "Iteration 35, loss = 1.57894582\n",
            "Iteration 36, loss = 1.57724347\n",
            "Iteration 37, loss = 1.57672390\n",
            "Iteration 38, loss = 1.57474266\n",
            "Iteration 39, loss = 1.57302765\n",
            "Iteration 40, loss = 1.57141816\n",
            "Iteration 41, loss = 1.56955680\n",
            "Iteration 42, loss = 1.56838503\n",
            "Iteration 43, loss = 1.56577035\n",
            "Iteration 44, loss = 1.56393659\n",
            "Iteration 45, loss = 1.56189732\n",
            "Iteration 46, loss = 1.55874800\n",
            "Iteration 47, loss = 1.55679566\n",
            "Iteration 48, loss = 1.55367119\n",
            "Iteration 49, loss = 1.55090664\n",
            "Iteration 50, loss = 1.54781469\n",
            "Iteration 51, loss = 1.54576383\n",
            "Iteration 52, loss = 1.54201761\n",
            "Iteration 53, loss = 1.53802717\n",
            "Iteration 54, loss = 1.53466256\n",
            "Iteration 55, loss = 1.53103829\n",
            "Iteration 56, loss = 1.52738597\n",
            "Iteration 57, loss = 1.52323216\n",
            "Iteration 58, loss = 1.51900955\n",
            "Iteration 59, loss = 1.51449720\n",
            "Iteration 60, loss = 1.51020841\n",
            "Iteration 61, loss = 1.50587790\n",
            "Iteration 62, loss = 1.50060270\n",
            "Iteration 63, loss = 1.49600574\n",
            "Iteration 64, loss = 1.49047071\n",
            "Iteration 65, loss = 1.48569268\n",
            "Iteration 66, loss = 1.48023777\n",
            "Iteration 67, loss = 1.47389080\n",
            "Iteration 68, loss = 1.46807723\n",
            "Iteration 69, loss = 1.46251304\n",
            "Iteration 70, loss = 1.45695294\n",
            "Iteration 71, loss = 1.45053444\n",
            "Iteration 72, loss = 1.44423890\n",
            "Iteration 73, loss = 1.43776729\n",
            "Iteration 74, loss = 1.43144886\n",
            "Iteration 75, loss = 1.42522675\n",
            "Iteration 76, loss = 1.41844904\n",
            "Iteration 77, loss = 1.41174061\n",
            "Iteration 78, loss = 1.40500173\n",
            "Iteration 79, loss = 1.39927299\n",
            "Iteration 80, loss = 1.39182197\n",
            "Iteration 81, loss = 1.38499198\n",
            "Iteration 82, loss = 1.37891692\n",
            "Iteration 83, loss = 1.37249980\n",
            "Iteration 84, loss = 1.36642899\n",
            "Iteration 85, loss = 1.35902276\n",
            "Iteration 86, loss = 1.35243634\n",
            "Iteration 87, loss = 1.34595317\n",
            "Iteration 88, loss = 1.34022658\n",
            "Iteration 89, loss = 1.33415580\n",
            "Iteration 90, loss = 1.32786478\n",
            "Iteration 91, loss = 1.32239026\n",
            "Iteration 92, loss = 1.31625520\n",
            "Iteration 93, loss = 1.31136588\n",
            "Iteration 94, loss = 1.30487728\n",
            "Iteration 95, loss = 1.30059819\n",
            "Iteration 96, loss = 1.29407432\n",
            "Iteration 97, loss = 1.28888765\n",
            "Iteration 98, loss = 1.28393013\n",
            "Iteration 99, loss = 1.27950935\n",
            "Iteration 100, loss = 1.27483800\n",
            "Iteration 1, loss = 1.75472036\n",
            "Iteration 2, loss = 1.67648417\n",
            "Iteration 3, loss = 1.66967497\n",
            "Iteration 4, loss = 1.62892993\n",
            "Iteration 5, loss = 1.61623262\n",
            "Iteration 6, loss = 1.61187517\n",
            "Iteration 7, loss = 1.60144013\n",
            "Iteration 8, loss = 1.59358983\n",
            "Iteration 9, loss = 1.59029974\n",
            "Iteration 10, loss = 1.58366939\n",
            "Iteration 11, loss = 1.57791079\n",
            "Iteration 12, loss = 1.56659024\n",
            "Iteration 13, loss = 1.55431780\n",
            "Iteration 14, loss = 1.53963710\n",
            "Iteration 15, loss = 1.52217990\n",
            "Iteration 16, loss = 1.50110341\n",
            "Iteration 17, loss = 1.47171243\n",
            "Iteration 18, loss = 1.44064815\n",
            "Iteration 19, loss = 1.40445864\n",
            "Iteration 20, loss = 1.36259445\n",
            "Iteration 21, loss = 1.32379983\n",
            "Iteration 22, loss = 1.28700099\n",
            "Iteration 23, loss = 1.25437686\n",
            "Iteration 24, loss = 1.22539907\n",
            "Iteration 25, loss = 1.20644206\n",
            "Iteration 26, loss = 1.18718392\n",
            "Iteration 27, loss = 1.16902333\n",
            "Iteration 28, loss = 1.16282186\n",
            "Iteration 29, loss = 1.15056444\n",
            "Iteration 30, loss = 1.14377394\n",
            "Iteration 31, loss = 1.14258582\n",
            "Iteration 32, loss = 1.13293210\n",
            "Iteration 33, loss = 1.12903981\n",
            "Iteration 34, loss = 1.12719040\n",
            "Iteration 35, loss = 1.12063966\n",
            "Iteration 36, loss = 1.11946170\n",
            "Iteration 37, loss = 1.12212061\n",
            "Iteration 38, loss = 1.11485597\n",
            "Iteration 39, loss = 1.11913676\n",
            "Iteration 40, loss = 1.11347455\n",
            "Iteration 41, loss = 1.11232324\n",
            "Iteration 42, loss = 1.11517612\n",
            "Iteration 43, loss = 1.10814957\n",
            "Iteration 44, loss = 1.10737850\n",
            "Iteration 45, loss = 1.10824532\n",
            "Iteration 46, loss = 1.10347948\n",
            "Iteration 47, loss = 1.10744817\n",
            "Iteration 48, loss = 1.10513647\n",
            "Iteration 49, loss = 1.10269660\n",
            "Iteration 50, loss = 1.10257558\n",
            "Iteration 51, loss = 1.10632966\n",
            "Iteration 52, loss = 1.09846198\n",
            "Iteration 53, loss = 1.09854233\n",
            "Iteration 54, loss = 1.10235345\n",
            "Iteration 55, loss = 1.09963944\n",
            "Iteration 56, loss = 1.09462197\n",
            "Iteration 57, loss = 1.09776734\n",
            "Iteration 58, loss = 1.09545190\n",
            "Iteration 59, loss = 1.09356466\n",
            "Iteration 60, loss = 1.09029877\n",
            "Iteration 61, loss = 1.09357377\n",
            "Iteration 62, loss = 1.09517200\n",
            "Iteration 63, loss = 1.09192582\n",
            "Iteration 64, loss = 1.08929587\n",
            "Iteration 65, loss = 1.08774443\n",
            "Iteration 66, loss = 1.09109648\n",
            "Iteration 67, loss = 1.08314294\n",
            "Iteration 68, loss = 1.08051517\n",
            "Iteration 69, loss = 1.08311575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 1.08149188\n",
            "Iteration 71, loss = 1.08286656\n",
            "Iteration 72, loss = 1.07705693\n",
            "Iteration 73, loss = 1.07278350\n",
            "Iteration 74, loss = 1.06982204\n",
            "Iteration 75, loss = 1.07187501\n",
            "Iteration 76, loss = 1.07092333\n",
            "Iteration 77, loss = 1.06622461\n",
            "Iteration 78, loss = 1.06705937\n",
            "Iteration 79, loss = 1.06839071\n",
            "Iteration 80, loss = 1.05979781\n",
            "Iteration 81, loss = 1.05543664\n",
            "Iteration 82, loss = 1.05859195\n",
            "Iteration 83, loss = 1.05628769\n",
            "Iteration 84, loss = 1.06341220\n",
            "Iteration 85, loss = 1.06699461\n",
            "Iteration 86, loss = 1.05650271\n",
            "Iteration 87, loss = 1.05329395\n",
            "Iteration 88, loss = 1.04734519\n",
            "Iteration 89, loss = 1.03248385\n",
            "Iteration 90, loss = 1.04340787\n",
            "Iteration 91, loss = 1.03399721\n",
            "Iteration 92, loss = 1.03331007\n",
            "Iteration 93, loss = 1.03423352\n",
            "Iteration 94, loss = 1.01842288\n",
            "Iteration 95, loss = 1.02802169\n",
            "Iteration 96, loss = 1.02313058\n",
            "Iteration 97, loss = 1.01600552\n",
            "Iteration 98, loss = 1.01474876\n",
            "Iteration 99, loss = 1.00892226\n",
            "Iteration 100, loss = 1.00610070\n",
            "Iteration 1, loss = 1.87045442\n",
            "Iteration 2, loss = 1.85860772\n",
            "Iteration 3, loss = 1.84710184\n",
            "Iteration 4, loss = 1.83596646\n",
            "Iteration 5, loss = 1.82533014\n",
            "Iteration 6, loss = 1.81517553\n",
            "Iteration 7, loss = 1.80516147\n",
            "Iteration 8, loss = 1.79582316\n",
            "Iteration 9, loss = 1.78679677\n",
            "Iteration 10, loss = 1.77803115\n",
            "Iteration 11, loss = 1.77000976\n",
            "Iteration 12, loss = 1.76203270\n",
            "Iteration 13, loss = 1.75436480\n",
            "Iteration 14, loss = 1.74728798\n",
            "Iteration 15, loss = 1.74001057\n",
            "Iteration 16, loss = 1.73300341\n",
            "Iteration 17, loss = 1.72695407\n",
            "Iteration 18, loss = 1.72049387\n",
            "Iteration 19, loss = 1.71479463\n",
            "Iteration 20, loss = 1.70869214\n",
            "Iteration 21, loss = 1.70329319\n",
            "Iteration 22, loss = 1.69773980\n",
            "Iteration 23, loss = 1.69269851\n",
            "Iteration 24, loss = 1.68756974\n",
            "Iteration 25, loss = 1.68285228\n",
            "Iteration 26, loss = 1.67795390\n",
            "Iteration 27, loss = 1.67366535\n",
            "Iteration 28, loss = 1.66940631\n",
            "Iteration 29, loss = 1.66552463\n",
            "Iteration 30, loss = 1.66122916\n",
            "Iteration 31, loss = 1.65766120\n",
            "Iteration 32, loss = 1.65351970\n",
            "Iteration 33, loss = 1.65015640\n",
            "Iteration 34, loss = 1.64696933\n",
            "Iteration 35, loss = 1.64333373\n",
            "Iteration 36, loss = 1.64027752\n",
            "Iteration 37, loss = 1.63719037\n",
            "Iteration 38, loss = 1.63448831\n",
            "Iteration 39, loss = 1.63173712\n",
            "Iteration 40, loss = 1.62908462\n",
            "Iteration 41, loss = 1.62638535\n",
            "Iteration 42, loss = 1.62437440\n",
            "Iteration 43, loss = 1.62168769\n",
            "Iteration 44, loss = 1.61968437\n",
            "Iteration 45, loss = 1.61780384\n",
            "Iteration 46, loss = 1.61553861\n",
            "Iteration 47, loss = 1.61378721\n",
            "Iteration 48, loss = 1.61205351\n",
            "Iteration 49, loss = 1.61014241\n",
            "Iteration 50, loss = 1.60869181\n",
            "Iteration 51, loss = 1.60694480\n",
            "Iteration 52, loss = 1.60557406\n",
            "Iteration 53, loss = 1.60430966\n",
            "Iteration 54, loss = 1.60302233\n",
            "Iteration 55, loss = 1.60174754\n",
            "Iteration 56, loss = 1.60053978\n",
            "Iteration 57, loss = 1.59959908\n",
            "Iteration 58, loss = 1.59836990\n",
            "Iteration 59, loss = 1.59745543\n",
            "Iteration 60, loss = 1.59647646\n",
            "Iteration 61, loss = 1.59571664\n",
            "Iteration 62, loss = 1.59472822\n",
            "Iteration 63, loss = 1.59401517\n",
            "Iteration 64, loss = 1.59310401\n",
            "Iteration 65, loss = 1.59247198\n",
            "Iteration 66, loss = 1.59159807\n",
            "Iteration 67, loss = 1.59097038\n",
            "Iteration 68, loss = 1.59029843\n",
            "Iteration 69, loss = 1.58969980\n",
            "Iteration 70, loss = 1.58914238\n",
            "Iteration 71, loss = 1.58843058\n",
            "Iteration 72, loss = 1.58789911\n",
            "Iteration 73, loss = 1.58731762\n",
            "Iteration 74, loss = 1.58677432\n",
            "Iteration 75, loss = 1.58618218\n",
            "Iteration 76, loss = 1.58566248\n",
            "Iteration 77, loss = 1.58514717\n",
            "Iteration 78, loss = 1.58460834\n",
            "Iteration 79, loss = 1.58413168\n",
            "Iteration 80, loss = 1.58357461\n",
            "Iteration 81, loss = 1.58306705\n",
            "Iteration 82, loss = 1.58262231\n",
            "Iteration 83, loss = 1.58207802\n",
            "Iteration 84, loss = 1.58162211\n",
            "Iteration 85, loss = 1.58113311\n",
            "Iteration 86, loss = 1.58060643\n",
            "Iteration 87, loss = 1.58010246\n",
            "Iteration 88, loss = 1.57961889\n",
            "Iteration 89, loss = 1.57915907\n",
            "Iteration 90, loss = 1.57863276\n",
            "Iteration 91, loss = 1.57818275\n",
            "Iteration 92, loss = 1.57768386\n",
            "Iteration 93, loss = 1.57717613\n",
            "Iteration 94, loss = 1.57664357\n",
            "Iteration 95, loss = 1.57621958\n",
            "Iteration 96, loss = 1.57566441\n",
            "Iteration 97, loss = 1.57514644\n",
            "Iteration 98, loss = 1.57464611\n",
            "Iteration 99, loss = 1.57417710\n",
            "Iteration 100, loss = 1.57363512\n",
            "Iteration 1, loss = 1.84567191\n",
            "Iteration 2, loss = 1.75252760\n",
            "Iteration 3, loss = 1.68887071\n",
            "Iteration 4, loss = 1.64439315\n",
            "Iteration 5, loss = 1.61688891\n",
            "Iteration 6, loss = 1.60264897\n",
            "Iteration 7, loss = 1.59472748\n",
            "Iteration 8, loss = 1.59678385\n",
            "Iteration 9, loss = 1.59768118\n",
            "Iteration 10, loss = 1.59658076\n",
            "Iteration 11, loss = 1.59186253\n",
            "Iteration 12, loss = 1.58476188\n",
            "Iteration 13, loss = 1.57657281\n",
            "Iteration 14, loss = 1.56824222\n",
            "Iteration 15, loss = 1.56196916\n",
            "Iteration 16, loss = 1.55665547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17, loss = 1.55082849\n",
            "Iteration 18, loss = 1.54589207\n",
            "Iteration 19, loss = 1.54032790\n",
            "Iteration 20, loss = 1.53410576\n",
            "Iteration 21, loss = 1.52705007\n",
            "Iteration 22, loss = 1.51914241\n",
            "Iteration 23, loss = 1.51099440\n",
            "Iteration 24, loss = 1.50198310\n",
            "Iteration 25, loss = 1.49317665\n",
            "Iteration 26, loss = 1.48250669\n",
            "Iteration 27, loss = 1.47285721\n",
            "Iteration 28, loss = 1.46238274\n",
            "Iteration 29, loss = 1.45157982\n",
            "Iteration 30, loss = 1.43986635\n",
            "Iteration 31, loss = 1.42837437\n",
            "Iteration 32, loss = 1.41634715\n",
            "Iteration 33, loss = 1.40399012\n",
            "Iteration 34, loss = 1.39121152\n",
            "Iteration 35, loss = 1.37875056\n",
            "Iteration 36, loss = 1.36597387\n",
            "Iteration 37, loss = 1.35434930\n",
            "Iteration 38, loss = 1.34226016\n",
            "Iteration 39, loss = 1.33011213\n",
            "Iteration 40, loss = 1.31869782\n",
            "Iteration 41, loss = 1.30778911\n",
            "Iteration 42, loss = 1.29808866\n",
            "Iteration 43, loss = 1.28742588\n",
            "Iteration 44, loss = 1.27812874\n",
            "Iteration 45, loss = 1.26899503\n",
            "Iteration 46, loss = 1.26014673\n",
            "Iteration 47, loss = 1.25236260\n",
            "Iteration 48, loss = 1.24412471\n",
            "Iteration 49, loss = 1.23701670\n",
            "Iteration 50, loss = 1.23038725\n",
            "Iteration 51, loss = 1.22416161\n",
            "Iteration 52, loss = 1.21798784\n",
            "Iteration 53, loss = 1.21192496\n",
            "Iteration 54, loss = 1.20666515\n",
            "Iteration 55, loss = 1.20186352\n",
            "Iteration 56, loss = 1.19708598\n",
            "Iteration 57, loss = 1.19268953\n",
            "Iteration 58, loss = 1.18813924\n",
            "Iteration 59, loss = 1.18436231\n",
            "Iteration 60, loss = 1.18055788\n",
            "Iteration 61, loss = 1.17717629\n",
            "Iteration 62, loss = 1.17348344\n",
            "Iteration 63, loss = 1.17077862\n",
            "Iteration 64, loss = 1.16749763\n",
            "Iteration 65, loss = 1.16470256\n",
            "Iteration 66, loss = 1.16148507\n",
            "Iteration 67, loss = 1.15840990\n",
            "Iteration 68, loss = 1.15691058\n",
            "Iteration 69, loss = 1.15467678\n",
            "Iteration 70, loss = 1.15202155\n",
            "Iteration 71, loss = 1.14933759\n",
            "Iteration 72, loss = 1.14721753\n",
            "Iteration 73, loss = 1.14539588\n",
            "Iteration 74, loss = 1.14306694\n",
            "Iteration 75, loss = 1.14153489\n",
            "Iteration 76, loss = 1.13940940\n",
            "Iteration 77, loss = 1.13807668\n",
            "Iteration 78, loss = 1.13611499\n",
            "Iteration 79, loss = 1.13413481\n",
            "Iteration 80, loss = 1.13318631\n",
            "Iteration 81, loss = 1.13147534\n",
            "Iteration 82, loss = 1.12980868\n",
            "Iteration 83, loss = 1.12852515\n",
            "Iteration 84, loss = 1.12766212\n",
            "Iteration 85, loss = 1.12572748\n",
            "Iteration 86, loss = 1.12417374\n",
            "Iteration 87, loss = 1.12271654\n",
            "Iteration 88, loss = 1.12287192\n",
            "Iteration 89, loss = 1.12074940\n",
            "Iteration 90, loss = 1.11889924\n",
            "Iteration 91, loss = 1.11826709\n",
            "Iteration 92, loss = 1.11763982\n",
            "Iteration 93, loss = 1.11633260\n",
            "Iteration 94, loss = 1.11459680\n",
            "Iteration 95, loss = 1.11517399\n",
            "Iteration 96, loss = 1.11301749\n",
            "Iteration 97, loss = 1.11129289\n",
            "Iteration 98, loss = 1.11022777\n",
            "Iteration 99, loss = 1.10966599\n",
            "Iteration 100, loss = 1.10851612\n",
            "Iteration 1, loss = 1.76341305\n",
            "Iteration 2, loss = 1.63477649\n",
            "Iteration 3, loss = 1.58884824\n",
            "Iteration 4, loss = 1.56194190\n",
            "Iteration 5, loss = 1.49145982\n",
            "Iteration 6, loss = 1.41409115\n",
            "Iteration 7, loss = 1.35914969\n",
            "Iteration 8, loss = 1.28523221\n",
            "Iteration 9, loss = 1.23252169\n",
            "Iteration 10, loss = 1.21293907\n",
            "Iteration 11, loss = 1.18825887\n",
            "Iteration 12, loss = 1.16056091\n",
            "Iteration 13, loss = 1.15250767\n",
            "Iteration 14, loss = 1.14649813\n",
            "Iteration 15, loss = 1.13222825\n",
            "Iteration 16, loss = 1.12788871\n",
            "Iteration 17, loss = 1.12296860\n",
            "Iteration 18, loss = 1.11212930\n",
            "Iteration 19, loss = 1.10938726\n",
            "Iteration 20, loss = 1.10472856\n",
            "Iteration 21, loss = 1.10373138\n",
            "Iteration 22, loss = 1.10154567\n",
            "Iteration 23, loss = 1.09614776\n",
            "Iteration 24, loss = 1.09442693\n",
            "Iteration 25, loss = 1.08978635\n",
            "Iteration 26, loss = 1.08789362\n",
            "Iteration 27, loss = 1.08778031\n",
            "Iteration 28, loss = 1.08042505\n",
            "Iteration 29, loss = 1.07831568\n",
            "Iteration 30, loss = 1.07560817\n",
            "Iteration 31, loss = 1.07067143\n",
            "Iteration 32, loss = 1.06865765\n",
            "Iteration 33, loss = 1.07457901\n",
            "Iteration 34, loss = 1.09738748\n",
            "Iteration 35, loss = 1.08053037\n",
            "Iteration 36, loss = 1.06888522\n",
            "Iteration 37, loss = 1.10237081\n",
            "Iteration 38, loss = 1.08021666\n",
            "Iteration 39, loss = 1.06853015\n",
            "Iteration 40, loss = 1.09574292\n",
            "Iteration 41, loss = 1.06155976\n",
            "Iteration 42, loss = 1.08058159\n",
            "Iteration 43, loss = 1.02953896\n",
            "Iteration 44, loss = 1.04009955\n",
            "Iteration 45, loss = 1.02306570\n",
            "Iteration 46, loss = 1.01371480\n",
            "Iteration 47, loss = 1.00011965\n",
            "Iteration 48, loss = 0.99804286\n",
            "Iteration 49, loss = 1.00782452\n",
            "Iteration 50, loss = 1.00248126\n",
            "Iteration 51, loss = 0.98812222\n",
            "Iteration 52, loss = 0.97766182\n",
            "Iteration 53, loss = 0.97714983\n",
            "Iteration 54, loss = 0.96715130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 0.96909979\n",
            "Iteration 56, loss = 0.93274855\n",
            "Iteration 57, loss = 0.93839371\n",
            "Iteration 58, loss = 0.94109178\n",
            "Iteration 59, loss = 0.94718381\n",
            "Iteration 60, loss = 0.93618180\n",
            "Iteration 61, loss = 0.93909475\n",
            "Iteration 62, loss = 0.93392308\n",
            "Iteration 63, loss = 0.89587488\n",
            "Iteration 64, loss = 0.91168353\n",
            "Iteration 65, loss = 0.90716638\n",
            "Iteration 66, loss = 0.90164992\n",
            "Iteration 67, loss = 0.88995982\n",
            "Iteration 68, loss = 0.88976728\n",
            "Iteration 69, loss = 0.90270891\n",
            "Iteration 70, loss = 0.89369411\n",
            "Iteration 71, loss = 0.89014185\n",
            "Iteration 72, loss = 0.88404611\n",
            "Iteration 73, loss = 0.89835961\n",
            "Iteration 74, loss = 0.85785036\n",
            "Iteration 75, loss = 0.86994466\n",
            "Iteration 76, loss = 0.84304949\n",
            "Iteration 77, loss = 0.83986950\n",
            "Iteration 78, loss = 0.85059805\n",
            "Iteration 79, loss = 0.84272026\n",
            "Iteration 80, loss = 0.83918233\n",
            "Iteration 81, loss = 0.86361918\n",
            "Iteration 82, loss = 0.86028032\n",
            "Iteration 83, loss = 0.83097846\n",
            "Iteration 84, loss = 0.82167624\n",
            "Iteration 85, loss = 0.83461803\n",
            "Iteration 86, loss = 0.90240030\n",
            "Iteration 87, loss = 0.91399582\n",
            "Iteration 88, loss = 0.89925695\n",
            "Iteration 89, loss = 0.85355919\n",
            "Iteration 90, loss = 0.83001117\n",
            "Iteration 91, loss = 0.85477182\n",
            "Iteration 92, loss = 0.81587341\n",
            "Iteration 93, loss = 0.81339922\n",
            "Iteration 94, loss = 0.80706552\n",
            "Iteration 95, loss = 0.81502899\n",
            "Iteration 96, loss = 0.82414661\n",
            "Iteration 97, loss = 0.82479676\n",
            "Iteration 98, loss = 0.81946216\n",
            "Iteration 99, loss = 0.82858181\n",
            "Iteration 100, loss = 0.82279545\n",
            "Iteration 1, loss = 1.74170331\n",
            "Iteration 2, loss = 1.73587536\n",
            "Iteration 3, loss = 1.73016550\n",
            "Iteration 4, loss = 1.72471297\n",
            "Iteration 5, loss = 1.71945912\n",
            "Iteration 6, loss = 1.71450376\n",
            "Iteration 7, loss = 1.70945188\n",
            "Iteration 8, loss = 1.70489894\n",
            "Iteration 9, loss = 1.70041819\n",
            "Iteration 10, loss = 1.69596970\n",
            "Iteration 11, loss = 1.69190128\n",
            "Iteration 12, loss = 1.68792697\n",
            "Iteration 13, loss = 1.68385212\n",
            "Iteration 14, loss = 1.68028727\n",
            "Iteration 15, loss = 1.67646201\n",
            "Iteration 16, loss = 1.67269801\n",
            "Iteration 17, loss = 1.66945298\n",
            "Iteration 18, loss = 1.66608300\n",
            "Iteration 19, loss = 1.66307669\n",
            "Iteration 20, loss = 1.65989339\n",
            "Iteration 21, loss = 1.65718691\n",
            "Iteration 22, loss = 1.65430541\n",
            "Iteration 23, loss = 1.65185563\n",
            "Iteration 24, loss = 1.64923570\n",
            "Iteration 25, loss = 1.64695601\n",
            "Iteration 26, loss = 1.64443110\n",
            "Iteration 27, loss = 1.64233062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 1.64014419\n",
            "Iteration 29, loss = 1.63816210\n",
            "Iteration 30, loss = 1.63596132\n",
            "Iteration 31, loss = 1.63412320\n",
            "Iteration 32, loss = 1.63195487\n",
            "Iteration 33, loss = 1.63015711\n",
            "Iteration 34, loss = 1.62849105\n",
            "Iteration 35, loss = 1.62647075\n",
            "Iteration 36, loss = 1.62480036\n",
            "Iteration 37, loss = 1.62312939\n",
            "Iteration 38, loss = 1.62161653\n",
            "Iteration 39, loss = 1.62001049\n",
            "Iteration 40, loss = 1.61855371\n",
            "Iteration 41, loss = 1.61703598\n",
            "Iteration 42, loss = 1.61592938\n",
            "Iteration 43, loss = 1.61433029\n",
            "Iteration 44, loss = 1.61307375\n",
            "Iteration 45, loss = 1.61194258\n",
            "Iteration 46, loss = 1.61051878\n",
            "Iteration 47, loss = 1.60936873\n",
            "Iteration 48, loss = 1.60820606\n",
            "Iteration 49, loss = 1.60691699\n",
            "Iteration 50, loss = 1.60590193\n",
            "Iteration 51, loss = 1.60468289\n",
            "Iteration 52, loss = 1.60367219\n",
            "Iteration 53, loss = 1.60268368\n",
            "Iteration 54, loss = 1.60172377\n",
            "Iteration 55, loss = 1.60074532\n",
            "Iteration 56, loss = 1.59977720\n",
            "Iteration 57, loss = 1.59896189\n",
            "Iteration 58, loss = 1.59794439\n",
            "Iteration 59, loss = 1.59716008\n",
            "Iteration 60, loss = 1.59631354\n",
            "Iteration 61, loss = 1.59560063\n",
            "Iteration 62, loss = 1.59469648\n",
            "Iteration 63, loss = 1.59404397\n",
            "Iteration 64, loss = 1.59316211\n",
            "Iteration 65, loss = 1.59256290\n",
            "Iteration 66, loss = 1.59175658\n",
            "Iteration 67, loss = 1.59108973\n",
            "Iteration 68, loss = 1.59047211\n",
            "Iteration 69, loss = 1.58978519\n",
            "Iteration 70, loss = 1.58927994\n",
            "Iteration 71, loss = 1.58853383\n",
            "Iteration 72, loss = 1.58799453\n",
            "Iteration 73, loss = 1.58743751\n",
            "Iteration 74, loss = 1.58690633\n",
            "Iteration 75, loss = 1.58631537\n",
            "Iteration 76, loss = 1.58572139\n",
            "Iteration 77, loss = 1.58523980\n",
            "Iteration 78, loss = 1.58470051\n",
            "Iteration 79, loss = 1.58422540\n",
            "Iteration 80, loss = 1.58369298\n",
            "Iteration 81, loss = 1.58319697\n",
            "Iteration 82, loss = 1.58276413\n",
            "Iteration 83, loss = 1.58225101\n",
            "Iteration 84, loss = 1.58184788\n",
            "Iteration 85, loss = 1.58135889\n",
            "Iteration 86, loss = 1.58083266\n",
            "Iteration 87, loss = 1.58040152\n",
            "Iteration 88, loss = 1.57993772\n",
            "Iteration 89, loss = 1.57953368\n",
            "Iteration 90, loss = 1.57906214\n",
            "Iteration 91, loss = 1.57866715\n",
            "Iteration 92, loss = 1.57817212\n",
            "Iteration 93, loss = 1.57771421\n",
            "Iteration 94, loss = 1.57727302\n",
            "Iteration 95, loss = 1.57689869\n",
            "Iteration 96, loss = 1.57640659\n",
            "Iteration 97, loss = 1.57597172\n",
            "Iteration 98, loss = 1.57552121\n",
            "Iteration 99, loss = 1.57513294\n",
            "Iteration 100, loss = 1.57464852\n",
            "Iteration 1, loss = 1.73001728\n",
            "Iteration 2, loss = 1.68263976\n",
            "Iteration 3, loss = 1.64906283\n",
            "Iteration 4, loss = 1.62705980\n",
            "Iteration 5, loss = 1.61247802\n",
            "Iteration 6, loss = 1.60320367\n",
            "Iteration 7, loss = 1.59438893\n",
            "Iteration 8, loss = 1.59137504\n",
            "Iteration 9, loss = 1.58850715\n",
            "Iteration 10, loss = 1.58644443\n",
            "Iteration 11, loss = 1.58404518\n",
            "Iteration 12, loss = 1.58304374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 13, loss = 1.57920423\n",
            "Iteration 14, loss = 1.57572483\n",
            "Iteration 15, loss = 1.57186845\n",
            "Iteration 16, loss = 1.56798668\n",
            "Iteration 17, loss = 1.56369903\n",
            "Iteration 18, loss = 1.55900728\n",
            "Iteration 19, loss = 1.55476639\n",
            "Iteration 20, loss = 1.54966548\n",
            "Iteration 21, loss = 1.54480919\n",
            "Iteration 22, loss = 1.53949791\n",
            "Iteration 23, loss = 1.53378851\n",
            "Iteration 24, loss = 1.52749704\n",
            "Iteration 25, loss = 1.52113893\n",
            "Iteration 26, loss = 1.51380393\n",
            "Iteration 27, loss = 1.50636913\n",
            "Iteration 28, loss = 1.49829946\n",
            "Iteration 29, loss = 1.49014150\n",
            "Iteration 30, loss = 1.48109253\n",
            "Iteration 31, loss = 1.47230210\n",
            "Iteration 32, loss = 1.46262710\n",
            "Iteration 33, loss = 1.45302264\n",
            "Iteration 34, loss = 1.44266775\n",
            "Iteration 35, loss = 1.43208648\n",
            "Iteration 36, loss = 1.42101922\n",
            "Iteration 37, loss = 1.41078129\n",
            "Iteration 38, loss = 1.39983719\n",
            "Iteration 39, loss = 1.38831478\n",
            "Iteration 40, loss = 1.37731589\n",
            "Iteration 41, loss = 1.36645669\n",
            "Iteration 42, loss = 1.35602690\n",
            "Iteration 43, loss = 1.34493308\n",
            "Iteration 44, loss = 1.33458920\n",
            "Iteration 45, loss = 1.32450514\n",
            "Iteration 46, loss = 1.31403249\n",
            "Iteration 47, loss = 1.30487361\n",
            "Iteration 48, loss = 1.29500780\n",
            "Iteration 49, loss = 1.28606236\n",
            "Iteration 50, loss = 1.27748645\n",
            "Iteration 51, loss = 1.26965044\n",
            "Iteration 52, loss = 1.26163827\n",
            "Iteration 53, loss = 1.25381821\n",
            "Iteration 54, loss = 1.24693599\n",
            "Iteration 55, loss = 1.24023228\n",
            "Iteration 56, loss = 1.23410791\n",
            "Iteration 57, loss = 1.22803344\n",
            "Iteration 58, loss = 1.22208054\n",
            "Iteration 59, loss = 1.21703018\n",
            "Iteration 60, loss = 1.21202529\n",
            "Iteration 61, loss = 1.20754831\n",
            "Iteration 62, loss = 1.20259924\n",
            "Iteration 63, loss = 1.19878053\n",
            "Iteration 64, loss = 1.19466979\n",
            "Iteration 65, loss = 1.19119113\n",
            "Iteration 66, loss = 1.18719819\n",
            "Iteration 67, loss = 1.18319155\n",
            "Iteration 68, loss = 1.18087493\n",
            "Iteration 69, loss = 1.17823920\n",
            "Iteration 70, loss = 1.17509877\n",
            "Iteration 71, loss = 1.17161282\n",
            "Iteration 72, loss = 1.16907028\n",
            "Iteration 73, loss = 1.16692748\n",
            "Iteration 74, loss = 1.16428309\n",
            "Iteration 75, loss = 1.16202470\n",
            "Iteration 76, loss = 1.15960027\n",
            "Iteration 77, loss = 1.15799463\n",
            "Iteration 78, loss = 1.15583082\n",
            "Iteration 79, loss = 1.15351950\n",
            "Iteration 80, loss = 1.15206387\n",
            "Iteration 81, loss = 1.15015035\n",
            "Iteration 82, loss = 1.14853210\n",
            "Iteration 83, loss = 1.14665973\n",
            "Iteration 84, loss = 1.14561634\n",
            "Iteration 85, loss = 1.14356509\n",
            "Iteration 86, loss = 1.14190299\n",
            "Iteration 87, loss = 1.14013802\n",
            "Iteration 88, loss = 1.14008295\n",
            "Iteration 89, loss = 1.13816826\n",
            "Iteration 90, loss = 1.13616626\n",
            "Iteration 91, loss = 1.13497720\n",
            "Iteration 92, loss = 1.13437892\n",
            "Iteration 93, loss = 1.13342924\n",
            "Iteration 94, loss = 1.13150899\n",
            "Iteration 95, loss = 1.13129463\n",
            "Iteration 96, loss = 1.12926536\n",
            "Iteration 97, loss = 1.12807157\n",
            "Iteration 98, loss = 1.12675769\n",
            "Iteration 99, loss = 1.12613066\n",
            "Iteration 100, loss = 1.12510210\n",
            "Iteration 1, loss = 1.67871372\n",
            "Iteration 2, loss = 1.61761332\n",
            "Iteration 3, loss = 1.59364148\n",
            "Iteration 4, loss = 1.57578893\n",
            "Iteration 5, loss = 1.55144974\n",
            "Iteration 6, loss = 1.51743827\n",
            "Iteration 7, loss = 1.47547716\n",
            "Iteration 8, loss = 1.43324618\n",
            "Iteration 9, loss = 1.38245092\n",
            "Iteration 10, loss = 1.33328648\n",
            "Iteration 11, loss = 1.28496033\n",
            "Iteration 12, loss = 1.24807260\n",
            "Iteration 13, loss = 1.21427029\n",
            "Iteration 14, loss = 1.19019159\n",
            "Iteration 15, loss = 1.16952357\n",
            "Iteration 16, loss = 1.15773853\n",
            "Iteration 17, loss = 1.14716490\n",
            "Iteration 18, loss = 1.13607863\n",
            "Iteration 19, loss = 1.12851061\n",
            "Iteration 20, loss = 1.12348505\n",
            "Iteration 21, loss = 1.11993204\n",
            "Iteration 22, loss = 1.11602098\n",
            "Iteration 23, loss = 1.11364218\n",
            "Iteration 24, loss = 1.11125363\n",
            "Iteration 25, loss = 1.10922229\n",
            "Iteration 26, loss = 1.11294046\n",
            "Iteration 27, loss = 1.10065280\n",
            "Iteration 28, loss = 1.11109579\n",
            "Iteration 29, loss = 1.10984625\n",
            "Iteration 30, loss = 1.10425733\n",
            "Iteration 31, loss = 1.10477536\n",
            "Iteration 32, loss = 1.10587402\n",
            "Iteration 33, loss = 1.10481341\n",
            "Iteration 34, loss = 1.10535267\n",
            "Iteration 35, loss = 1.10718528\n",
            "Iteration 36, loss = 1.10274857\n",
            "Iteration 37, loss = 1.10684556\n",
            "Iteration 38, loss = 1.09848732\n",
            "Iteration 39, loss = 1.10591314\n",
            "Iteration 40, loss = 1.09781209\n",
            "Iteration 41, loss = 1.09828342\n",
            "Iteration 42, loss = 1.09024634\n",
            "Iteration 43, loss = 1.08969289\n",
            "Iteration 44, loss = 1.08991421\n",
            "Iteration 45, loss = 1.08744675\n",
            "Iteration 46, loss = 1.08514770\n",
            "Iteration 47, loss = 1.08792153\n",
            "Iteration 48, loss = 1.08630194\n",
            "Iteration 49, loss = 1.08194064\n",
            "Iteration 50, loss = 1.08560455\n",
            "Iteration 51, loss = 1.08348505\n",
            "Iteration 52, loss = 1.07519448\n",
            "Iteration 53, loss = 1.07048422\n",
            "Iteration 54, loss = 1.06992894\n",
            "Iteration 55, loss = 1.07015936\n",
            "Iteration 56, loss = 1.06623822\n",
            "Iteration 57, loss = 1.06634774\n",
            "Iteration 58, loss = 1.06238512\n",
            "Iteration 59, loss = 1.05804794\n",
            "Iteration 60, loss = 1.05407372\n",
            "Iteration 61, loss = 1.05377660\n",
            "Iteration 62, loss = 1.05370816\n",
            "Iteration 63, loss = 1.05156380\n",
            "Iteration 64, loss = 1.04657985\n",
            "Iteration 65, loss = 1.04089530\n",
            "Iteration 66, loss = 1.04541655\n",
            "Iteration 67, loss = 1.03008316\n",
            "Iteration 68, loss = 1.02797292\n",
            "Iteration 69, loss = 1.02372027\n",
            "Iteration 70, loss = 1.02436583\n",
            "Iteration 71, loss = 1.02864411\n",
            "Iteration 72, loss = 1.01245110\n",
            "Iteration 73, loss = 1.00841645\n",
            "Iteration 74, loss = 1.00559973\n",
            "Iteration 75, loss = 1.00406645\n",
            "Iteration 76, loss = 0.99136396\n",
            "Iteration 77, loss = 0.98816805\n",
            "Iteration 78, loss = 0.98369629\n",
            "Iteration 79, loss = 0.98315087\n",
            "Iteration 80, loss = 0.97552872\n",
            "Iteration 81, loss = 0.96887806\n",
            "Iteration 82, loss = 0.96569948\n",
            "Iteration 83, loss = 0.96152981\n",
            "Iteration 84, loss = 0.97980913"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 85, loss = 0.98317683\n",
            "Iteration 86, loss = 0.98530015\n",
            "Iteration 87, loss = 0.97282085\n",
            "Iteration 88, loss = 0.96558691\n",
            "Iteration 89, loss = 0.97034546\n",
            "Iteration 90, loss = 0.94555508\n",
            "Iteration 91, loss = 0.94440235\n",
            "Iteration 92, loss = 0.94159303\n",
            "Iteration 93, loss = 0.93367043\n",
            "Iteration 94, loss = 0.92207905\n",
            "Iteration 95, loss = 0.90941996\n",
            "Iteration 96, loss = 0.92799308\n",
            "Iteration 97, loss = 0.91938999\n",
            "Iteration 98, loss = 0.91034515\n",
            "Iteration 99, loss = 0.90021899\n",
            "Iteration 100, loss = 0.89473282\n",
            "Iteration 1, loss = 1.74233261\n",
            "Iteration 2, loss = 1.73383541\n",
            "Iteration 3, loss = 1.72605003\n",
            "Iteration 4, loss = 1.71805844\n",
            "Iteration 5, loss = 1.70994028\n",
            "Iteration 6, loss = 1.70315564\n",
            "Iteration 7, loss = 1.69654683\n",
            "Iteration 8, loss = 1.69027984\n",
            "Iteration 9, loss = 1.68368236\n",
            "Iteration 10, loss = 1.67773946\n",
            "Iteration 11, loss = 1.67240854\n",
            "Iteration 12, loss = 1.66675153\n",
            "Iteration 13, loss = 1.66184400\n",
            "Iteration 14, loss = 1.65713783\n",
            "Iteration 15, loss = 1.65270148\n",
            "Iteration 16, loss = 1.64853688\n",
            "Iteration 17, loss = 1.64430044\n",
            "Iteration 18, loss = 1.64040386\n",
            "Iteration 19, loss = 1.63700260\n",
            "Iteration 20, loss = 1.63390237\n",
            "Iteration 21, loss = 1.63053817\n",
            "Iteration 22, loss = 1.62770161\n",
            "Iteration 23, loss = 1.62498829\n",
            "Iteration 24, loss = 1.62258924\n",
            "Iteration 25, loss = 1.62016710\n",
            "Iteration 26, loss = 1.61824211\n",
            "Iteration 27, loss = 1.61585230\n",
            "Iteration 28, loss = 1.61410480\n",
            "Iteration 29, loss = 1.61242433\n",
            "Iteration 30, loss = 1.61031426\n",
            "Iteration 31, loss = 1.60898022\n",
            "Iteration 32, loss = 1.60754702\n",
            "Iteration 33, loss = 1.60628661\n",
            "Iteration 34, loss = 1.60493984\n",
            "Iteration 35, loss = 1.60387408\n",
            "Iteration 36, loss = 1.60266867\n",
            "Iteration 37, loss = 1.60162719\n",
            "Iteration 38, loss = 1.60085076\n",
            "Iteration 39, loss = 1.59993382\n",
            "Iteration 40, loss = 1.59873920\n",
            "Iteration 41, loss = 1.59801114\n",
            "Iteration 42, loss = 1.59726017\n",
            "Iteration 43, loss = 1.59635108\n",
            "Iteration 44, loss = 1.59578091\n",
            "Iteration 45, loss = 1.59508157\n",
            "Iteration 46, loss = 1.59446184\n",
            "Iteration 47, loss = 1.59374205\n",
            "Iteration 48, loss = 1.59311941\n",
            "Iteration 49, loss = 1.59260539\n",
            "Iteration 50, loss = 1.59193307\n",
            "Iteration 51, loss = 1.59139992\n",
            "Iteration 52, loss = 1.59078509\n",
            "Iteration 53, loss = 1.59030257\n",
            "Iteration 54, loss = 1.58970342\n",
            "Iteration 55, loss = 1.58914796\n",
            "Iteration 56, loss = 1.58858184\n",
            "Iteration 57, loss = 1.58810846\n",
            "Iteration 58, loss = 1.58760713\n",
            "Iteration 59, loss = 1.58698721\n",
            "Iteration 60, loss = 1.58649166\n",
            "Iteration 61, loss = 1.58603474\n",
            "Iteration 62, loss = 1.58542876\n",
            "Iteration 63, loss = 1.58489937\n",
            "Iteration 64, loss = 1.58438796\n",
            "Iteration 65, loss = 1.58384652\n",
            "Iteration 66, loss = 1.58337000\n",
            "Iteration 67, loss = 1.58278647\n",
            "Iteration 68, loss = 1.58225834\n",
            "Iteration 69, loss = 1.58173180\n",
            "Iteration 70, loss = 1.58124592\n",
            "Iteration 71, loss = 1.58065222\n",
            "Iteration 72, loss = 1.58012191\n",
            "Iteration 73, loss = 1.57957813\n",
            "Iteration 74, loss = 1.57903848\n",
            "Iteration 75, loss = 1.57849795\n",
            "Iteration 76, loss = 1.57795984\n",
            "Iteration 77, loss = 1.57740399\n",
            "Iteration 78, loss = 1.57683793\n",
            "Iteration 79, loss = 1.57627287\n",
            "Iteration 80, loss = 1.57570311\n",
            "Iteration 81, loss = 1.57514153"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 82, loss = 1.57459812\n",
            "Iteration 83, loss = 1.57399800\n",
            "Iteration 84, loss = 1.57342463\n",
            "Iteration 85, loss = 1.57284175\n",
            "Iteration 86, loss = 1.57225993\n",
            "Iteration 87, loss = 1.57166103\n",
            "Iteration 88, loss = 1.57108159\n",
            "Iteration 89, loss = 1.57046987\n",
            "Iteration 90, loss = 1.56992720\n",
            "Iteration 91, loss = 1.56928918\n",
            "Iteration 92, loss = 1.56866363\n",
            "Iteration 93, loss = 1.56804610\n",
            "Iteration 94, loss = 1.56745152\n",
            "Iteration 95, loss = 1.56679677\n",
            "Iteration 96, loss = 1.56618340\n",
            "Iteration 97, loss = 1.56553378\n",
            "Iteration 98, loss = 1.56492003\n",
            "Iteration 99, loss = 1.56428947\n",
            "Iteration 100, loss = 1.56364216\n",
            "Iteration 1, loss = 1.72591055\n",
            "Iteration 2, loss = 1.65926675\n",
            "Iteration 3, loss = 1.62275271\n",
            "Iteration 4, loss = 1.60262185\n",
            "Iteration 5, loss = 1.59425148\n",
            "Iteration 6, loss = 1.59482770\n",
            "Iteration 7, loss = 1.59386655\n",
            "Iteration 8, loss = 1.59026744\n",
            "Iteration 9, loss = 1.58429703\n",
            "Iteration 10, loss = 1.57602395\n",
            "Iteration 11, loss = 1.56779175\n",
            "Iteration 12, loss = 1.56144912\n",
            "Iteration 13, loss = 1.55498962\n",
            "Iteration 14, loss = 1.54934595\n",
            "Iteration 15, loss = 1.54369799\n",
            "Iteration 16, loss = 1.53765782\n",
            "Iteration 17, loss = 1.52984882\n",
            "Iteration 18, loss = 1.52208484\n",
            "Iteration 19, loss = 1.51334066\n",
            "Iteration 20, loss = 1.50433490\n",
            "Iteration 21, loss = 1.49455659\n",
            "Iteration 22, loss = 1.48519406\n",
            "Iteration 23, loss = 1.47472044\n",
            "Iteration 24, loss = 1.46455167\n",
            "Iteration 25, loss = 1.45299656\n",
            "Iteration 26, loss = 1.44145550\n",
            "Iteration 27, loss = 1.42891694\n",
            "Iteration 28, loss = 1.41683744\n",
            "Iteration 29, loss = 1.40402536\n",
            "Iteration 30, loss = 1.39104213\n",
            "Iteration 31, loss = 1.37800747\n",
            "Iteration 32, loss = 1.36437676\n",
            "Iteration 33, loss = 1.35149283\n",
            "Iteration 34, loss = 1.33853081\n",
            "Iteration 35, loss = 1.32587860\n",
            "Iteration 36, loss = 1.31354738\n",
            "Iteration 37, loss = 1.30135395\n",
            "Iteration 38, loss = 1.29168281\n",
            "Iteration 39, loss = 1.27959637\n",
            "Iteration 40, loss = 1.26891457\n",
            "Iteration 41, loss = 1.25865299\n",
            "Iteration 42, loss = 1.24956317\n",
            "Iteration 43, loss = 1.24062854\n",
            "Iteration 44, loss = 1.23226243\n",
            "Iteration 45, loss = 1.22451656\n",
            "Iteration 46, loss = 1.21743205\n",
            "Iteration 47, loss = 1.21049396\n",
            "Iteration 48, loss = 1.20415233\n",
            "Iteration 49, loss = 1.19881967\n",
            "Iteration 50, loss = 1.19269818\n",
            "Iteration 51, loss = 1.18791813\n",
            "Iteration 52, loss = 1.18294407\n",
            "Iteration 53, loss = 1.17832254\n",
            "Iteration 54, loss = 1.17377299\n",
            "Iteration 55, loss = 1.16985603\n",
            "Iteration 56, loss = 1.16619259\n",
            "Iteration 57, loss = 1.16292763\n",
            "Iteration 58, loss = 1.15951797\n",
            "Iteration 59, loss = 1.15595305\n",
            "Iteration 60, loss = 1.15313606\n",
            "Iteration 61, loss = 1.15136760\n",
            "Iteration 62, loss = 1.14812257\n",
            "Iteration 63, loss = 1.14531468\n",
            "Iteration 64, loss = 1.14344137\n",
            "Iteration 65, loss = 1.14068419\n",
            "Iteration 66, loss = 1.14020377"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 67, loss = 1.13647009\n",
            "Iteration 68, loss = 1.13467332\n",
            "Iteration 69, loss = 1.13306717\n",
            "Iteration 70, loss = 1.13148033\n",
            "Iteration 71, loss = 1.12932332\n",
            "Iteration 72, loss = 1.12781320\n",
            "Iteration 73, loss = 1.12673444\n",
            "Iteration 74, loss = 1.12539577\n",
            "Iteration 75, loss = 1.12434063\n",
            "Iteration 76, loss = 1.12236494\n",
            "Iteration 77, loss = 1.12181711\n",
            "Iteration 78, loss = 1.12034409\n",
            "Iteration 79, loss = 1.11845028\n",
            "Iteration 80, loss = 1.11722386\n",
            "Iteration 81, loss = 1.11647346\n",
            "Iteration 82, loss = 1.11565462\n",
            "Iteration 83, loss = 1.11399007\n",
            "Iteration 84, loss = 1.11304028\n",
            "Iteration 85, loss = 1.11249884\n",
            "Iteration 86, loss = 1.11125705\n",
            "Iteration 87, loss = 1.11176909\n",
            "Iteration 88, loss = 1.10922138\n",
            "Iteration 89, loss = 1.10843730\n",
            "Iteration 90, loss = 1.10944956\n",
            "Iteration 91, loss = 1.10742230\n",
            "Iteration 92, loss = 1.10690240\n",
            "Iteration 93, loss = 1.10536288\n",
            "Iteration 94, loss = 1.10464707\n",
            "Iteration 95, loss = 1.10330151\n",
            "Iteration 96, loss = 1.10303454\n",
            "Iteration 97, loss = 1.10168581\n",
            "Iteration 98, loss = 1.10086475\n",
            "Iteration 99, loss = 1.10101563\n",
            "Iteration 100, loss = 1.09996421\n",
            "Iteration 1, loss = 1.70559540\n",
            "Iteration 2, loss = 1.59571587\n",
            "Iteration 3, loss = 1.57334511\n",
            "Iteration 4, loss = 1.51230911\n",
            "Iteration 5, loss = 1.46542989\n",
            "Iteration 6, loss = 1.40132154\n",
            "Iteration 7, loss = 1.31756737\n",
            "Iteration 8, loss = 1.27074736\n",
            "Iteration 9, loss = 1.22051840\n",
            "Iteration 10, loss = 1.17909715\n",
            "Iteration 11, loss = 1.16333662\n",
            "Iteration 12, loss = 1.15240143\n",
            "Iteration 13, loss = 1.11946515\n",
            "Iteration 14, loss = 1.13348918\n",
            "Iteration 15, loss = 1.11610530\n",
            "Iteration 16, loss = 1.11122486\n",
            "Iteration 17, loss = 1.11318518\n",
            "Iteration 18, loss = 1.11704455\n",
            "Iteration 19, loss = 1.09923980\n",
            "Iteration 20, loss = 1.09696883\n",
            "Iteration 21, loss = 1.09015532\n",
            "Iteration 22, loss = 1.08657473\n",
            "Iteration 23, loss = 1.08444623\n",
            "Iteration 24, loss = 1.08951174\n",
            "Iteration 25, loss = 1.07225436\n",
            "Iteration 26, loss = 1.08508348\n",
            "Iteration 27, loss = 1.07732233\n",
            "Iteration 28, loss = 1.07144838\n",
            "Iteration 29, loss = 1.06081982\n",
            "Iteration 30, loss = 1.08508126\n",
            "Iteration 31, loss = 1.04609674\n",
            "Iteration 32, loss = 1.05802154\n",
            "Iteration 33, loss = 1.03822050\n",
            "Iteration 34, loss = 1.04041441\n",
            "Iteration 35, loss = 1.03390333\n",
            "Iteration 36, loss = 1.02006101\n",
            "Iteration 37, loss = 1.01291522\n",
            "Iteration 38, loss = 1.01578204\n",
            "Iteration 39, loss = 1.03265057\n",
            "Iteration 40, loss = 1.05316104\n",
            "Iteration 41, loss = 1.00223692\n",
            "Iteration 42, loss = 0.98167026\n",
            "Iteration 43, loss = 0.96950731\n",
            "Iteration 44, loss = 0.98647680\n",
            "Iteration 45, loss = 0.98781551\n",
            "Iteration 46, loss = 0.96487691\n",
            "Iteration 47, loss = 0.97398782\n",
            "Iteration 48, loss = 0.96215563\n",
            "Iteration 49, loss = 0.95321412\n",
            "Iteration 50, loss = 0.92397649\n",
            "Iteration 51, loss = 0.93333431\n",
            "Iteration 52, loss = 0.92958718\n",
            "Iteration 53, loss = 0.91215341\n",
            "Iteration 54, loss = 0.91006287\n",
            "Iteration 55, loss = 0.91274019\n",
            "Iteration 56, loss = 0.89403258"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 57, loss = 0.89389251\n",
            "Iteration 58, loss = 0.90498783\n",
            "Iteration 59, loss = 0.88173235\n",
            "Iteration 60, loss = 0.87658494\n",
            "Iteration 61, loss = 0.88251677\n",
            "Iteration 62, loss = 0.90236226\n",
            "Iteration 63, loss = 0.86471725\n",
            "Iteration 64, loss = 0.88267388\n",
            "Iteration 65, loss = 0.86657930\n",
            "Iteration 66, loss = 0.87258702\n",
            "Iteration 67, loss = 0.96059195\n",
            "Iteration 68, loss = 0.94312956\n",
            "Iteration 69, loss = 0.91627338\n",
            "Iteration 70, loss = 0.97212884\n",
            "Iteration 71, loss = 0.91674915\n",
            "Iteration 72, loss = 0.86086615\n",
            "Iteration 73, loss = 0.90640299\n",
            "Iteration 74, loss = 0.89924757\n",
            "Iteration 75, loss = 0.86892408\n",
            "Iteration 76, loss = 0.89351258\n",
            "Iteration 77, loss = 0.84631531\n",
            "Iteration 78, loss = 0.87837552\n",
            "Iteration 79, loss = 0.83851689\n",
            "Iteration 80, loss = 0.83199781\n",
            "Iteration 81, loss = 0.84896857\n",
            "Iteration 82, loss = 0.91184481\n",
            "Iteration 83, loss = 0.98676949\n",
            "Iteration 84, loss = 1.03648155\n",
            "Iteration 85, loss = 0.92148041\n",
            "Iteration 86, loss = 0.89400272\n",
            "Iteration 87, loss = 0.88980135\n",
            "Iteration 88, loss = 0.91156701\n",
            "Iteration 89, loss = 0.85938312\n",
            "Iteration 90, loss = 0.86586646\n",
            "Iteration 91, loss = 0.81421655\n",
            "Iteration 92, loss = 0.85790565\n",
            "Iteration 93, loss = 0.82748015\n",
            "Iteration 94, loss = 0.80991233\n",
            "Iteration 95, loss = 0.80619700\n",
            "Iteration 96, loss = 0.80040543\n",
            "Iteration 97, loss = 0.80664692\n",
            "Iteration 98, loss = 0.80027582\n",
            "Iteration 99, loss = 0.82213115\n",
            "Iteration 100, loss = 0.83611997\n",
            "Iteration 1, loss = 1.64826118\n",
            "Iteration 2, loss = 1.64315224\n",
            "Iteration 3, loss = 1.63833227\n",
            "Iteration 4, loss = 1.63399383\n",
            "Iteration 5, loss = 1.63043267\n",
            "Iteration 6, loss = 1.62610520\n",
            "Iteration 7, loss = 1.62286451\n",
            "Iteration 8, loss = 1.62001779\n",
            "Iteration 9, loss = 1.61770714\n",
            "Iteration 10, loss = 1.61479930\n",
            "Iteration 11, loss = 1.61238365\n",
            "Iteration 12, loss = 1.61073057\n",
            "Iteration 13, loss = 1.60877246\n",
            "Iteration 14, loss = 1.60712411\n",
            "Iteration 15, loss = 1.60569711\n",
            "Iteration 16, loss = 1.60438022\n",
            "Iteration 17, loss = 1.60336467\n",
            "Iteration 18, loss = 1.60289401\n",
            "Iteration 19, loss = 1.60191430\n",
            "Iteration 20, loss = 1.60101939\n",
            "Iteration 21, loss = 1.60055177\n",
            "Iteration 22, loss = 1.60014998\n",
            "Iteration 23, loss = 1.59964393\n",
            "Iteration 24, loss = 1.59940600\n",
            "Iteration 25, loss = 1.59900994\n",
            "Iteration 26, loss = 1.59874122\n",
            "Iteration 27, loss = 1.59853202\n",
            "Iteration 28, loss = 1.59841951\n",
            "Iteration 29, loss = 1.59829212\n",
            "Iteration 30, loss = 1.59819203\n",
            "Iteration 31, loss = 1.59803104\n",
            "Iteration 32, loss = 1.59783788\n",
            "Iteration 33, loss = 1.59778054\n",
            "Iteration 34, loss = 1.59767254\n",
            "Iteration 35, loss = 1.59760134\n",
            "Iteration 36, loss = 1.59757665\n",
            "Iteration 37, loss = 1.59746749\n",
            "Iteration 38, loss = 1.59748245\n",
            "Iteration 39, loss = 1.59741206\n",
            "Iteration 40, loss = 1.59727190\n",
            "Iteration 41, loss = 1.59718097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 42, loss = 1.59715065\n",
            "Iteration 43, loss = 1.59711832\n",
            "Iteration 44, loss = 1.59701514\n",
            "Iteration 45, loss = 1.59694722\n",
            "Iteration 46, loss = 1.59685834\n",
            "Iteration 47, loss = 1.59679208\n",
            "Iteration 48, loss = 1.59672582\n",
            "Iteration 49, loss = 1.59666446\n",
            "Iteration 50, loss = 1.59659890\n",
            "Iteration 51, loss = 1.59656742\n",
            "Iteration 52, loss = 1.59646502\n",
            "Iteration 53, loss = 1.59644880\n",
            "Iteration 54, loss = 1.59633306\n",
            "Iteration 55, loss = 1.59625513\n",
            "Iteration 56, loss = 1.59618380\n",
            "Iteration 57, loss = 1.59618436\n",
            "Iteration 58, loss = 1.59609448\n",
            "Iteration 59, loss = 1.59597931\n",
            "Iteration 60, loss = 1.59592842\n",
            "Iteration 61, loss = 1.59591701\n",
            "Iteration 62, loss = 1.59581739\n",
            "Iteration 63, loss = 1.59571492\n",
            "Iteration 64, loss = 1.59567104\n",
            "Iteration 65, loss = 1.59556957\n",
            "Iteration 66, loss = 1.59556646\n",
            "Iteration 67, loss = 1.59543896\n",
            "Iteration 68, loss = 1.59539738\n",
            "Iteration 69, loss = 1.59529462\n",
            "Iteration 70, loss = 1.59526469\n",
            "Iteration 71, loss = 1.59515371\n",
            "Iteration 72, loss = 1.59507961\n",
            "Iteration 73, loss = 1.59501887\n",
            "Iteration 74, loss = 1.59494512\n",
            "Iteration 75, loss = 1.59490610\n",
            "Iteration 76, loss = 1.59483766\n",
            "Iteration 77, loss = 1.59474638\n",
            "Iteration 78, loss = 1.59466386\n",
            "Iteration 79, loss = 1.59458563\n",
            "Iteration 80, loss = 1.59452150\n",
            "Iteration 81, loss = 1.59444742\n",
            "Iteration 82, loss = 1.59441319\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.63882952\n",
            "Iteration 2, loss = 1.60771544\n",
            "Iteration 3, loss = 1.60034298\n",
            "Iteration 4, loss = 1.60277773\n",
            "Iteration 5, loss = 1.60721882\n",
            "Iteration 6, loss = 1.60589676\n",
            "Iteration 7, loss = 1.60068493\n",
            "Iteration 8, loss = 1.59718573\n",
            "Iteration 9, loss = 1.59415223\n",
            "Iteration 10, loss = 1.59426308\n",
            "Iteration 11, loss = 1.59477180\n",
            "Iteration 12, loss = 1.59478432\n",
            "Iteration 13, loss = 1.59355967\n",
            "Iteration 14, loss = 1.59192010\n",
            "Iteration 15, loss = 1.59063205\n",
            "Iteration 16, loss = 1.58986093\n",
            "Iteration 17, loss = 1.58887803\n",
            "Iteration 18, loss = 1.58864819\n",
            "Iteration 19, loss = 1.58740146\n",
            "Iteration 20, loss = 1.58594305\n",
            "Iteration 21, loss = 1.58450755\n",
            "Iteration 22, loss = 1.58326690\n",
            "Iteration 23, loss = 1.58200763\n",
            "Iteration 24, loss = 1.58068621\n",
            "Iteration 25, loss = 1.57922131\n",
            "Iteration 26, loss = 1.57795337\n",
            "Iteration 27, loss = 1.57616253\n",
            "Iteration 28, loss = 1.57462293\n",
            "Iteration 29, loss = 1.57293824\n",
            "Iteration 30, loss = 1.57136735\n",
            "Iteration 31, loss = 1.56879801\n",
            "Iteration 32, loss = 1.56649095\n",
            "Iteration 33, loss = 1.56396428\n",
            "Iteration 34, loss = 1.56164532\n",
            "Iteration 35, loss = 1.55904947\n",
            "Iteration 36, loss = 1.55647586\n",
            "Iteration 37, loss = 1.55345647\n",
            "Iteration 38, loss = 1.55132761\n",
            "Iteration 39, loss = 1.54768366\n",
            "Iteration 40, loss = 1.54388358\n",
            "Iteration 41, loss = 1.54011295\n",
            "Iteration 42, loss = 1.53668524\n",
            "Iteration 43, loss = 1.53304807\n",
            "Iteration 44, loss = 1.52853743\n",
            "Iteration 45, loss = 1.52415146\n",
            "Iteration 46, loss = 1.51998288\n",
            "Iteration 47, loss = 1.51571139\n",
            "Iteration 48, loss = 1.51079425\n",
            "Iteration 49, loss = 1.50581086\n",
            "Iteration 50, loss = 1.50064246\n",
            "Iteration 51, loss = 1.49601560\n",
            "Iteration 52, loss = 1.48986947\n",
            "Iteration 53, loss = 1.48480481\n",
            "Iteration 54, loss = 1.47827676\n",
            "Iteration 55, loss = 1.47229052\n",
            "Iteration 56, loss = 1.46607292\n",
            "Iteration 57, loss = 1.46040081\n",
            "Iteration 58, loss = 1.45409030\n",
            "Iteration 59, loss = 1.44691083\n",
            "Iteration 60, loss = 1.44032033\n",
            "Iteration 61, loss = 1.43443914\n",
            "Iteration 62, loss = 1.42728733\n",
            "Iteration 63, loss = 1.42045203\n",
            "Iteration 64, loss = 1.41405121\n",
            "Iteration 65, loss = 1.40674291\n",
            "Iteration 66, loss = 1.40070106\n",
            "Iteration 67, loss = 1.39297349\n",
            "Iteration 68, loss = 1.38623739\n",
            "Iteration 69, loss = 1.37945631\n",
            "Iteration 70, loss = 1.37321618\n",
            "Iteration 71, loss = 1.36586109\n",
            "Iteration 72, loss = 1.35913389\n",
            "Iteration 73, loss = 1.35283234\n",
            "Iteration 74, loss = 1.34628515\n",
            "Iteration 75, loss = 1.34026265\n",
            "Iteration 76, loss = 1.33418505\n",
            "Iteration 77, loss = 1.32769078\n",
            "Iteration 78, loss = 1.32140462\n",
            "Iteration 79, loss = 1.31539395\n",
            "Iteration 80, loss = 1.30953624\n",
            "Iteration 81, loss = 1.30397294\n",
            "Iteration 82, loss = 1.29898582\n",
            "Iteration 83, loss = 1.29310096\n",
            "Iteration 84, loss = 1.28791208\n",
            "Iteration 85, loss = 1.28305230\n",
            "Iteration 86, loss = 1.27797931\n",
            "Iteration 87, loss = 1.27322432\n",
            "Iteration 88, loss = 1.26852203\n",
            "Iteration 89, loss = 1.26423997\n",
            "Iteration 90, loss = 1.26051104\n",
            "Iteration 91, loss = 1.25626722\n",
            "Iteration 92, loss = 1.25198429\n",
            "Iteration 93, loss = 1.24795162\n",
            "Iteration 94, loss = 1.24473185\n",
            "Iteration 95, loss = 1.24057690\n",
            "Iteration 96, loss = 1.23713088\n",
            "Iteration 97, loss = 1.23352406\n",
            "Iteration 98, loss = 1.23009879\n",
            "Iteration 99, loss = 1.22791459\n",
            "Iteration 100, loss = 1.22483199\n",
            "Iteration 1, loss = 1.71677826\n",
            "Iteration 2, loss = 1.62617743\n",
            "Iteration 3, loss = 1.65257914\n",
            "Iteration 4, loss = 1.61460454\n",
            "Iteration 5, loss = 1.60566067\n",
            "Iteration 6, loss = 1.60128095\n",
            "Iteration 7, loss = 1.58802818\n",
            "Iteration 8, loss = 1.57623886\n",
            "Iteration 9, loss = 1.56112443\n",
            "Iteration 10, loss = 1.54696774\n",
            "Iteration 11, loss = 1.53430255\n",
            "Iteration 12, loss = 1.50935474\n",
            "Iteration 13, loss = 1.47347737\n",
            "Iteration 14, loss = 1.44446116\n",
            "Iteration 15, loss = 1.40911478\n",
            "Iteration 16, loss = 1.36603629\n",
            "Iteration 17, loss = 1.33012760"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 18, loss = 1.28887079\n",
            "Iteration 19, loss = 1.26213569\n",
            "Iteration 20, loss = 1.23112489\n",
            "Iteration 21, loss = 1.21208357\n",
            "Iteration 22, loss = 1.19093524\n",
            "Iteration 23, loss = 1.17765873\n",
            "Iteration 24, loss = 1.16827576\n",
            "Iteration 25, loss = 1.15553272\n",
            "Iteration 26, loss = 1.15361200\n",
            "Iteration 27, loss = 1.14294642\n",
            "Iteration 28, loss = 1.14148017\n",
            "Iteration 29, loss = 1.13341135\n",
            "Iteration 30, loss = 1.13964999\n",
            "Iteration 31, loss = 1.12760304\n",
            "Iteration 32, loss = 1.12306735\n",
            "Iteration 33, loss = 1.12369701\n",
            "Iteration 34, loss = 1.11833027\n",
            "Iteration 35, loss = 1.12404793\n",
            "Iteration 36, loss = 1.12334271\n",
            "Iteration 37, loss = 1.11875975\n",
            "Iteration 38, loss = 1.12766015\n",
            "Iteration 39, loss = 1.11997096\n",
            "Iteration 40, loss = 1.12401495\n",
            "Iteration 41, loss = 1.11202456\n",
            "Iteration 42, loss = 1.12684222\n",
            "Iteration 43, loss = 1.11717244\n",
            "Iteration 44, loss = 1.10869735\n",
            "Iteration 45, loss = 1.10991442\n",
            "Iteration 46, loss = 1.10702941\n",
            "Iteration 47, loss = 1.11086786\n",
            "Iteration 48, loss = 1.10595810\n",
            "Iteration 49, loss = 1.11285442\n",
            "Iteration 50, loss = 1.10906836\n",
            "Iteration 51, loss = 1.11509508\n",
            "Iteration 52, loss = 1.10634892\n",
            "Iteration 53, loss = 1.11264639\n",
            "Iteration 54, loss = 1.10666376\n",
            "Iteration 55, loss = 1.10885646\n",
            "Iteration 56, loss = 1.10466252\n",
            "Iteration 57, loss = 1.10804814\n",
            "Iteration 58, loss = 1.10691757\n",
            "Iteration 59, loss = 1.10354707\n",
            "Iteration 60, loss = 1.10904748\n",
            "Iteration 61, loss = 1.10932798\n",
            "Iteration 62, loss = 1.10887996\n",
            "Iteration 63, loss = 1.10384255\n",
            "Iteration 64, loss = 1.10746503\n",
            "Iteration 65, loss = 1.10532500\n",
            "Iteration 66, loss = 1.10793384\n",
            "Iteration 67, loss = 1.09753457\n",
            "Iteration 68, loss = 1.09664024\n",
            "Iteration 69, loss = 1.09621686\n",
            "Iteration 70, loss = 1.09680104\n",
            "Iteration 71, loss = 1.09128614\n",
            "Iteration 72, loss = 1.09483499\n",
            "Iteration 73, loss = 1.08931736\n",
            "Iteration 74, loss = 1.09167931\n",
            "Iteration 75, loss = 1.09439155\n",
            "Iteration 76, loss = 1.09225859\n",
            "Iteration 77, loss = 1.08833381\n",
            "Iteration 78, loss = 1.08705372\n",
            "Iteration 79, loss = 1.08305179\n",
            "Iteration 80, loss = 1.08565800\n",
            "Iteration 81, loss = 1.08863903\n",
            "Iteration 82, loss = 1.09149739\n",
            "Iteration 83, loss = 1.08370055\n",
            "Iteration 84, loss = 1.07918527\n",
            "Iteration 85, loss = 1.07511654\n",
            "Iteration 86, loss = 1.07162018\n",
            "Iteration 87, loss = 1.07543915\n",
            "Iteration 88, loss = 1.07118669\n",
            "Iteration 89, loss = 1.06719359\n",
            "Iteration 90, loss = 1.07382420\n",
            "Iteration 91, loss = 1.06393993\n",
            "Iteration 92, loss = 1.07194507\n",
            "Iteration 93, loss = 1.06423125\n",
            "Iteration 94, loss = 1.06839300\n",
            "Iteration 95, loss = 1.05202749\n",
            "Iteration 96, loss = 1.05860476\n",
            "Iteration 97, loss = 1.04580101\n",
            "Iteration 98, loss = 1.04738820\n",
            "Iteration 99, loss = 1.06089332\n",
            "Iteration 100, loss = 1.04662869\n",
            "Iteration 1, loss = 1.72532763\n",
            "Iteration 2, loss = 1.71785084\n",
            "Iteration 3, loss = 1.71101554\n",
            "Iteration 4, loss = 1.70397732\n",
            "Iteration 5, loss = 1.69680386\n",
            "Iteration 6, loss = 1.69085617\n",
            "Iteration 7, loss = 1.68505421\n",
            "Iteration 8, loss = 1.67955758\n",
            "Iteration 9, loss = 1.67372421\n",
            "Iteration 10, loss = 1.66848933\n",
            "Iteration 11, loss = 1.66380140\n",
            "Iteration 12, loss = 1.65879550\n",
            "Iteration 13, loss = 1.65447020\n",
            "Iteration 14, loss = 1.65031391\n",
            "Iteration 15, loss = 1.64639304\n",
            "Iteration 16, loss = 1.64270927\n",
            "Iteration 17, loss = 1.63894534\n",
            "Iteration 18, loss = 1.63548422\n",
            "Iteration 19, loss = 1.63246857\n",
            "Iteration 20, loss = 1.62971526\n",
            "Iteration 21, loss = 1.62670760\n",
            "Iteration 22, loss = 1.62417960\n",
            "Iteration 23, loss = 1.62175220\n",
            "Iteration 24, loss = 1.61961114\n",
            "Iteration 25, loss = 1.61743054\n",
            "Iteration 26, loss = 1.61570908\n",
            "Iteration 27, loss = 1.61354238\n",
            "Iteration 28, loss = 1.61196808\n",
            "Iteration 29, loss = 1.61044515\n",
            "Iteration 30, loss = 1.60851443\n",
            "Iteration 31, loss = 1.60730016\n",
            "Iteration 32, loss = 1.60599008\n",
            "Iteration 33, loss = 1.60483417\n",
            "Iteration 34, loss = 1.60359289\n",
            "Iteration 35, loss = 1.60260434\n",
            "Iteration 36, loss = 1.60148411\n",
            "Iteration 37, loss = 1.60051755\n",
            "Iteration 38, loss = 1.59978590\n",
            "Iteration 39, loss = 1.59893056\n",
            "Iteration 40, loss = 1.59780466\n",
            "Iteration 41, loss = 1.59711510\n",
            "Iteration 42, loss = 1.59640467\n",
            "Iteration 43, loss = 1.59554279\n",
            "Iteration 44, loss = 1.59500228\n",
            "Iteration 45, loss = 1.59433315\n",
            "Iteration 46, loss = 1.59373851\n",
            "Iteration 47, loss = 1.59304595\n",
            "Iteration 48, loss = 1.59244971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 1.59195465\n",
            "Iteration 50, loss = 1.59130730\n",
            "Iteration 51, loss = 1.59079789\n",
            "Iteration 52, loss = 1.59020483\n",
            "Iteration 53, loss = 1.58973601\n",
            "Iteration 54, loss = 1.58916274\n",
            "Iteration 55, loss = 1.58862389\n",
            "Iteration 56, loss = 1.58807856\n",
            "Iteration 57, loss = 1.58762682\n",
            "Iteration 58, loss = 1.58713948\n",
            "Iteration 59, loss = 1.58654150\n",
            "Iteration 60, loss = 1.58606483\n",
            "Iteration 61, loss = 1.58562369\n",
            "Iteration 62, loss = 1.58504084\n",
            "Iteration 63, loss = 1.58452702\n",
            "Iteration 64, loss = 1.58403356\n",
            "Iteration 65, loss = 1.58351385\n",
            "Iteration 66, loss = 1.58305170\n",
            "Iteration 67, loss = 1.58249088\n",
            "Iteration 68, loss = 1.58198211\n",
            "Iteration 69, loss = 1.58147339\n",
            "Iteration 70, loss = 1.58100481\n",
            "Iteration 71, loss = 1.58043064\n",
            "Iteration 72, loss = 1.57991817\n",
            "Iteration 73, loss = 1.57939156\n",
            "Iteration 74, loss = 1.57887046\n",
            "Iteration 75, loss = 1.57834647\n",
            "Iteration 76, loss = 1.57782822\n",
            "Iteration 77, loss = 1.57728816\n",
            "Iteration 78, loss = 1.57674032\n",
            "Iteration 79, loss = 1.57619267\n",
            "Iteration 80, loss = 1.57563991\n",
            "Iteration 81, loss = 1.57509553\n",
            "Iteration 82, loss = 1.57456693\n",
            "Iteration 83, loss = 1.57398539\n",
            "Iteration 84, loss = 1.57342779\n",
            "Iteration 85, loss = 1.57286173\n",
            "Iteration 86, loss = 1.57229563\n",
            "Iteration 87, loss = 1.57171281\n",
            "Iteration 88, loss = 1.57114886\n",
            "Iteration 89, loss = 1.57055376\n",
            "Iteration 90, loss = 1.57002199\n",
            "Iteration 91, loss = 1.56940211\n",
            "Iteration 92, loss = 1.56879153\n",
            "Iteration 93, loss = 1.56818799\n",
            "Iteration 94, loss = 1.56760722\n",
            "Iteration 95, loss = 1.56696816\n",
            "Iteration 96, loss = 1.56636825\n",
            "Iteration 97, loss = 1.56573391\n",
            "Iteration 98, loss = 1.56513295\n",
            "Iteration 99, loss = 1.56451496\n",
            "Iteration 100, loss = 1.56388389\n",
            "Iteration 1, loss = 1.71092658\n",
            "Iteration 2, loss = 1.65214711\n",
            "Iteration 3, loss = 1.61990394\n",
            "Iteration 4, loss = 1.60161067\n",
            "Iteration 5, loss = 1.59342602\n",
            "Iteration 6, loss = 1.59340827\n",
            "Iteration 7, loss = 1.59215666\n",
            "Iteration 8, loss = 1.58882575\n",
            "Iteration 9, loss = 1.58341670\n",
            "Iteration 10, loss = 1.57571468\n",
            "Iteration 11, loss = 1.56793487\n",
            "Iteration 12, loss = 1.56168580\n",
            "Iteration 13, loss = 1.55523170\n",
            "Iteration 14, loss = 1.54954648\n",
            "Iteration 15, loss = 1.54384798\n",
            "Iteration 16, loss = 1.53779680\n",
            "Iteration 17, loss = 1.52998067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 1.52229236\n",
            "Iteration 19, loss = 1.51355215\n",
            "Iteration 20, loss = 1.50448336\n",
            "Iteration 21, loss = 1.49467122\n",
            "Iteration 22, loss = 1.48518652\n",
            "Iteration 23, loss = 1.47456494\n",
            "Iteration 24, loss = 1.46421801\n",
            "Iteration 25, loss = 1.45252557\n",
            "Iteration 26, loss = 1.44084054\n",
            "Iteration 27, loss = 1.42813832\n",
            "Iteration 28, loss = 1.41589376\n",
            "Iteration 29, loss = 1.40291935\n",
            "Iteration 30, loss = 1.38973141\n",
            "Iteration 31, loss = 1.37652811\n",
            "Iteration 32, loss = 1.36270507\n",
            "Iteration 33, loss = 1.34967325\n",
            "Iteration 34, loss = 1.33656021\n",
            "Iteration 35, loss = 1.32379379\n",
            "Iteration 36, loss = 1.31132544\n",
            "Iteration 37, loss = 1.29904701\n",
            "Iteration 38, loss = 1.28930513\n",
            "Iteration 39, loss = 1.27711194\n",
            "Iteration 40, loss = 1.26644059\n",
            "Iteration 41, loss = 1.25615999\n",
            "Iteration 42, loss = 1.24704316\n",
            "Iteration 43, loss = 1.23808852\n",
            "Iteration 44, loss = 1.22977697\n",
            "Iteration 45, loss = 1.22206345\n",
            "Iteration 46, loss = 1.21500965\n",
            "Iteration 47, loss = 1.20812097\n",
            "Iteration 48, loss = 1.20181974\n",
            "Iteration 49, loss = 1.19658036\n",
            "Iteration 50, loss = 1.19048173\n",
            "Iteration 51, loss = 1.18573233\n",
            "Iteration 52, loss = 1.18088939\n",
            "Iteration 53, loss = 1.17630782\n",
            "Iteration 54, loss = 1.17182209\n",
            "Iteration 55, loss = 1.16799521\n",
            "Iteration 56, loss = 1.16438763\n",
            "Iteration 57, loss = 1.16112657\n",
            "Iteration 58, loss = 1.15779244\n",
            "Iteration 59, loss = 1.15431905\n",
            "Iteration 60, loss = 1.15154180\n",
            "Iteration 61, loss = 1.14985318\n",
            "Iteration 62, loss = 1.14662754\n",
            "Iteration 63, loss = 1.14384669\n",
            "Iteration 64, loss = 1.14196844\n",
            "Iteration 65, loss = 1.13929149\n",
            "Iteration 66, loss = 1.13879551\n",
            "Iteration 67, loss = 1.13516016\n",
            "Iteration 68, loss = 1.13336222\n",
            "Iteration 69, loss = 1.13177705\n",
            "Iteration 70, loss = 1.13017244\n",
            "Iteration 71, loss = 1.12807433\n",
            "Iteration 72, loss = 1.12659020\n",
            "Iteration 73, loss = 1.12546845\n",
            "Iteration 74, loss = 1.12413491\n",
            "Iteration 75, loss = 1.12304711\n",
            "Iteration 76, loss = 1.12107400\n",
            "Iteration 77, loss = 1.12053594\n",
            "Iteration 78, loss = 1.11905695\n",
            "Iteration 79, loss = 1.11718766\n",
            "Iteration 80, loss = 1.11595993\n",
            "Iteration 81, loss = 1.11523409\n",
            "Iteration 82, loss = 1.11438679\n",
            "Iteration 83, loss = 1.11270438\n",
            "Iteration 84, loss = 1.11169243\n",
            "Iteration 85, loss = 1.11110826\n",
            "Iteration 86, loss = 1.10988967\n",
            "Iteration 87, loss = 1.11050147\n",
            "Iteration 88, loss = 1.10788895\n",
            "Iteration 89, loss = 1.10697508\n",
            "Iteration 90, loss = 1.10775082\n",
            "Iteration 91, loss = 1.10592166\n",
            "Iteration 92, loss = 1.10534991\n",
            "Iteration 93, loss = 1.10378441\n",
            "Iteration 94, loss = 1.10301311\n",
            "Iteration 95, loss = 1.10168209\n",
            "Iteration 96, loss = 1.10136369\n",
            "Iteration 97, loss = 1.10000856\n",
            "Iteration 98, loss = 1.09918791\n",
            "Iteration 99, loss = 1.09927479\n",
            "Iteration 100, loss = 1.09810874\n",
            "Iteration 1, loss = 1.69255213\n",
            "Iteration 2, loss = 1.59534678\n",
            "Iteration 3, loss = 1.57217310\n",
            "Iteration 4, loss = 1.51475279\n",
            "Iteration 5, loss = 1.46724851\n",
            "Iteration 6, loss = 1.39809677\n",
            "Iteration 7, loss = 1.31567187\n",
            "Iteration 8, loss = 1.26316678\n",
            "Iteration 9, loss = 1.21452078\n",
            "Iteration 10, loss = 1.17277297\n",
            "Iteration 11, loss = 1.15589010\n",
            "Iteration 12, loss = 1.14859605\n",
            "Iteration 13, loss = 1.11546246\n",
            "Iteration 14, loss = 1.13026004\n",
            "Iteration 15, loss = 1.11683811\n",
            "Iteration 16, loss = 1.10601175\n",
            "Iteration 17, loss = 1.11351286\n",
            "Iteration 18, loss = 1.11878482\n",
            "Iteration 19, loss = 1.09848823\n",
            "Iteration 20, loss = 1.09668425\n",
            "Iteration 21, loss = 1.09199642\n",
            "Iteration 22, loss = 1.08269274\n",
            "Iteration 23, loss = 1.08339769\n",
            "Iteration 24, loss = 1.08504956\n",
            "Iteration 25, loss = 1.06785716\n",
            "Iteration 26, loss = 1.08506672\n",
            "Iteration 27, loss = 1.07470760\n",
            "Iteration 28, loss = 1.07197889\n",
            "Iteration 29, loss = 1.05762990\n",
            "Iteration 30, loss = 1.07811907\n",
            "Iteration 31, loss = 1.04565142\n",
            "Iteration 32, loss = 1.04808859\n",
            "Iteration 33, loss = 1.03350057\n",
            "Iteration 34, loss = 1.03096653\n",
            "Iteration 35, loss = 1.02226318\n",
            "Iteration 36, loss = 1.00713198\n",
            "Iteration 37, loss = 0.99636432\n",
            "Iteration 38, loss = 1.00259410\n",
            "Iteration 39, loss = 1.02965020\n",
            "Iteration 40, loss = 1.04786849\n",
            "Iteration 41, loss = 0.99825207\n",
            "Iteration 42, loss = 0.97263539\n",
            "Iteration 43, loss = 0.95393483\n",
            "Iteration 44, loss = 0.97642457\n",
            "Iteration 45, loss = 0.98265786\n",
            "Iteration 46, loss = 0.95656263\n",
            "Iteration 47, loss = 0.96676467\n",
            "Iteration 48, loss = 0.95468670\n",
            "Iteration 49, loss = 0.94263831\n",
            "Iteration 50, loss = 0.91261218\n",
            "Iteration 51, loss = 0.92260180\n",
            "Iteration 52, loss = 0.91806534\n",
            "Iteration 53, loss = 0.90180311\n",
            "Iteration 54, loss = 0.89983615\n",
            "Iteration 55, loss = 0.90202896\n",
            "Iteration 56, loss = 0.88108572\n",
            "Iteration 57, loss = 0.88018766\n",
            "Iteration 58, loss = 0.89290087\n",
            "Iteration 59, loss = 0.86957241\n",
            "Iteration 60, loss = 0.86561254\n",
            "Iteration 61, loss = 0.87024581\n",
            "Iteration 62, loss = 0.89599796\n",
            "Iteration 63, loss = 0.85112984\n",
            "Iteration 64, loss = 0.86811240\n",
            "Iteration 65, loss = 0.85295182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 66, loss = 0.85468463\n",
            "Iteration 67, loss = 0.94611804\n",
            "Iteration 68, loss = 0.92946880\n",
            "Iteration 69, loss = 0.89889324\n",
            "Iteration 70, loss = 0.97201842\n",
            "Iteration 71, loss = 0.92729626\n",
            "Iteration 72, loss = 0.86677405\n",
            "Iteration 73, loss = 0.90787761\n",
            "Iteration 74, loss = 0.91148354\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.63337655\n",
            "Iteration 2, loss = 1.63113020\n",
            "Iteration 3, loss = 1.62919000\n",
            "Iteration 4, loss = 1.62707072\n",
            "Iteration 5, loss = 1.62498841\n",
            "Iteration 6, loss = 1.62308107\n",
            "Iteration 7, loss = 1.62149208\n",
            "Iteration 8, loss = 1.62000863\n",
            "Iteration 9, loss = 1.61842032\n",
            "Iteration 10, loss = 1.61669187\n",
            "Iteration 11, loss = 1.61529446\n",
            "Iteration 12, loss = 1.61387459\n",
            "Iteration 13, loss = 1.61252598\n",
            "Iteration 14, loss = 1.61119268\n",
            "Iteration 15, loss = 1.60994117\n",
            "Iteration 16, loss = 1.60870304\n",
            "Iteration 17, loss = 1.60753270\n",
            "Iteration 18, loss = 1.60652248\n",
            "Iteration 19, loss = 1.60544847\n",
            "Iteration 20, loss = 1.60438522\n",
            "Iteration 21, loss = 1.60332819\n",
            "Iteration 22, loss = 1.60243740\n",
            "Iteration 23, loss = 1.60146720\n",
            "Iteration 24, loss = 1.60071094\n",
            "Iteration 25, loss = 1.59976112\n",
            "Iteration 26, loss = 1.59905010\n",
            "Iteration 27, loss = 1.59811751\n",
            "Iteration 28, loss = 1.59744014\n",
            "Iteration 29, loss = 1.59672099\n",
            "Iteration 30, loss = 1.59585528\n",
            "Iteration 31, loss = 1.59520639\n",
            "Iteration 32, loss = 1.59447432\n",
            "Iteration 33, loss = 1.59389088\n",
            "Iteration 34, loss = 1.59318043\n",
            "Iteration 35, loss = 1.59258289\n",
            "Iteration 36, loss = 1.59193621\n",
            "Iteration 37, loss = 1.59135694\n",
            "Iteration 38, loss = 1.59078332\n",
            "Iteration 39, loss = 1.59028218\n",
            "Iteration 40, loss = 1.58953614\n",
            "Iteration 41, loss = 1.58899472\n",
            "Iteration 42, loss = 1.58850229\n",
            "Iteration 43, loss = 1.58784742\n",
            "Iteration 44, loss = 1.58739020\n",
            "Iteration 45, loss = 1.58682889\n",
            "Iteration 46, loss = 1.58631124\n",
            "Iteration 47, loss = 1.58576767\n",
            "Iteration 48, loss = 1.58524814\n",
            "Iteration 49, loss = 1.58476895\n",
            "Iteration 50, loss = 1.58419129\n",
            "Iteration 51, loss = 1.58375487\n",
            "Iteration 52, loss = 1.58317641\n",
            "Iteration 53, loss = 1.58271879\n",
            "Iteration 54, loss = 1.58216008\n",
            "Iteration 55, loss = 1.58164621\n",
            "Iteration 56, loss = 1.58112776\n",
            "Iteration 57, loss = 1.58065738\n",
            "Iteration 58, loss = 1.58016932\n",
            "Iteration 59, loss = 1.57959512\n",
            "Iteration 60, loss = 1.57908901\n",
            "Iteration 61, loss = 1.57864338\n",
            "Iteration 62, loss = 1.57805880\n",
            "Iteration 63, loss = 1.57754084\n",
            "Iteration 64, loss = 1.57701643\n",
            "Iteration 65, loss = 1.57649256\n",
            "Iteration 66, loss = 1.57600155\n",
            "Iteration 67, loss = 1.57543941\n",
            "Iteration 68, loss = 1.57491174\n",
            "Iteration 69, loss = 1.57437767\n",
            "Iteration 70, loss = 1.57387599\n",
            "Iteration 71, loss = 1.57328553\n",
            "Iteration 72, loss = 1.57274435\n",
            "Iteration 73, loss = 1.57218525\n",
            "Iteration 74, loss = 1.57164026\n",
            "Iteration 75, loss = 1.57108233\n",
            "Iteration 76, loss = 1.57052969\n",
            "Iteration 77, loss = 1.56996436\n",
            "Iteration 78, loss = 1.56938101\n",
            "Iteration 79, loss = 1.56879517\n",
            "Iteration 80, loss = 1.56819749\n",
            "Iteration 81, loss = 1.56761568\n",
            "Iteration 82, loss = 1.56703701\n",
            "Iteration 83, loss = 1.56642300\n",
            "Iteration 84, loss = 1.56581356\n",
            "Iteration 85, loss = 1.56520276\n",
            "Iteration 86, loss = 1.56459978\n",
            "Iteration 87, loss = 1.56396392\n",
            "Iteration 88, loss = 1.56335202\n",
            "Iteration 89, loss = 1.56270663\n",
            "Iteration 90, loss = 1.56210310\n",
            "Iteration 91, loss = 1.56145535\n",
            "Iteration 92, loss = 1.56078833\n",
            "Iteration 93, loss = 1.56011453\n",
            "Iteration 94, loss = 1.55949427\n",
            "Iteration 95, loss = 1.55880055\n",
            "Iteration 96, loss = 1.55813787\n",
            "Iteration 97, loss = 1.55745196\n",
            "Iteration 98, loss = 1.55679688\n",
            "Iteration 99, loss = 1.55611168\n",
            "Iteration 100, loss = 1.55542031\n",
            "Iteration 1, loss = 1.62964304\n",
            "Iteration 2, loss = 1.61161281\n",
            "Iteration 3, loss = 1.60135669\n",
            "Iteration 4, loss = 1.59287470\n",
            "Iteration 5, loss = 1.58802880\n",
            "Iteration 6, loss = 1.58391371\n",
            "Iteration 7, loss = 1.58060731\n",
            "Iteration 8, loss = 1.57727943\n",
            "Iteration 9, loss = 1.57282289\n",
            "Iteration 10, loss = 1.56742853\n",
            "Iteration 11, loss = 1.56185214\n",
            "Iteration 12, loss = 1.55606358\n",
            "Iteration 13, loss = 1.54989011\n",
            "Iteration 14, loss = 1.54352446\n",
            "Iteration 15, loss = 1.53708160\n",
            "Iteration 16, loss = 1.53026218\n",
            "Iteration 17, loss = 1.52224242\n",
            "Iteration 18, loss = 1.51436831\n",
            "Iteration 19, loss = 1.50586557\n",
            "Iteration 20, loss = 1.49703644\n",
            "Iteration 21, loss = 1.48768805\n",
            "Iteration 22, loss = 1.47837194\n",
            "Iteration 23, loss = 1.46812514\n",
            "Iteration 24, loss = 1.45834034\n",
            "Iteration 25, loss = 1.44755566\n",
            "Iteration 26, loss = 1.43707360\n",
            "Iteration 27, loss = 1.42569475\n",
            "Iteration 28, loss = 1.41500719\n",
            "Iteration 29, loss = 1.40383824\n",
            "Iteration 30, loss = 1.39252328\n",
            "Iteration 31, loss = 1.38131726\n",
            "Iteration 32, loss = 1.36974601\n",
            "Iteration 33, loss = 1.35881549\n",
            "Iteration 34, loss = 1.34802654\n",
            "Iteration 35, loss = 1.33743136\n",
            "Iteration 36, loss = 1.32699087\n",
            "Iteration 37, loss = 1.31690065\n",
            "Iteration 38, loss = 1.30853749\n",
            "Iteration 39, loss = 1.29835212\n",
            "Iteration 40, loss = 1.28939190\n",
            "Iteration 41, loss = 1.28061772\n",
            "Iteration 42, loss = 1.27270044\n",
            "Iteration 43, loss = 1.26484980"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 44, loss = 1.25741730\n",
            "Iteration 45, loss = 1.25043286\n",
            "Iteration 46, loss = 1.24401908\n",
            "Iteration 47, loss = 1.23767852\n",
            "Iteration 48, loss = 1.23162967\n",
            "Iteration 49, loss = 1.22651720\n",
            "Iteration 50, loss = 1.22068507\n",
            "Iteration 51, loss = 1.21606068\n",
            "Iteration 52, loss = 1.21113908\n",
            "Iteration 53, loss = 1.20663395\n",
            "Iteration 54, loss = 1.20200542\n",
            "Iteration 55, loss = 1.19803717\n",
            "Iteration 56, loss = 1.19428055\n",
            "Iteration 57, loss = 1.19093094\n",
            "Iteration 58, loss = 1.18731210\n",
            "Iteration 59, loss = 1.18361188\n",
            "Iteration 60, loss = 1.18057574\n",
            "Iteration 61, loss = 1.17843591\n",
            "Iteration 62, loss = 1.17499199\n",
            "Iteration 63, loss = 1.17212574\n",
            "Iteration 64, loss = 1.16988664\n",
            "Iteration 65, loss = 1.16707006\n",
            "Iteration 66, loss = 1.16568415\n",
            "Iteration 67, loss = 1.16239409\n",
            "Iteration 68, loss = 1.16028566\n",
            "Iteration 69, loss = 1.15837902\n",
            "Iteration 70, loss = 1.15663158\n",
            "Iteration 71, loss = 1.15428941\n",
            "Iteration 72, loss = 1.15247414\n",
            "Iteration 73, loss = 1.15103530\n",
            "Iteration 74, loss = 1.14937377\n",
            "Iteration 75, loss = 1.14813283\n",
            "Iteration 76, loss = 1.14631292\n",
            "Iteration 77, loss = 1.14514885\n",
            "Iteration 78, loss = 1.14335767\n",
            "Iteration 79, loss = 1.14175499\n",
            "Iteration 80, loss = 1.14029510\n",
            "Iteration 81, loss = 1.13914667\n",
            "Iteration 82, loss = 1.13829196\n",
            "Iteration 83, loss = 1.13668580\n",
            "Iteration 84, loss = 1.13531571\n",
            "Iteration 85, loss = 1.13452022\n",
            "Iteration 86, loss = 1.13336856\n",
            "Iteration 87, loss = 1.13329547\n",
            "Iteration 88, loss = 1.13111148\n",
            "Iteration 89, loss = 1.13018216\n",
            "Iteration 90, loss = 1.13004718\n",
            "Iteration 91, loss = 1.12899886\n",
            "Iteration 92, loss = 1.12779571\n",
            "Iteration 93, loss = 1.12655471\n",
            "Iteration 94, loss = 1.12595588\n",
            "Iteration 95, loss = 1.12458472\n",
            "Iteration 96, loss = 1.12401569\n",
            "Iteration 97, loss = 1.12288788\n",
            "Iteration 98, loss = 1.12213144\n",
            "Iteration 99, loss = 1.12195414\n",
            "Iteration 100, loss = 1.12087267\n",
            "Iteration 1, loss = 1.62554202\n",
            "Iteration 2, loss = 1.59517062\n",
            "Iteration 3, loss = 1.55653982\n",
            "Iteration 4, loss = 1.52209426\n",
            "Iteration 5, loss = 1.47523141\n",
            "Iteration 6, loss = 1.40591641\n",
            "Iteration 7, loss = 1.34346049\n",
            "Iteration 8, loss = 1.28441513\n",
            "Iteration 9, loss = 1.23233611\n",
            "Iteration 10, loss = 1.19411278\n",
            "Iteration 11, loss = 1.16789480\n",
            "Iteration 12, loss = 1.14859184\n",
            "Iteration 13, loss = 1.13531146\n",
            "Iteration 14, loss = 1.12612462\n",
            "Iteration 15, loss = 1.11958503\n",
            "Iteration 16, loss = 1.11290159\n",
            "Iteration 17, loss = 1.11310062\n",
            "Iteration 18, loss = 1.11784505\n",
            "Iteration 19, loss = 1.11794797\n",
            "Iteration 20, loss = 1.10238546\n",
            "Iteration 21, loss = 1.11465264"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 22, loss = 1.10938239\n",
            "Iteration 23, loss = 1.10802283\n",
            "Iteration 24, loss = 1.10258328\n",
            "Iteration 25, loss = 1.10538641\n",
            "Iteration 26, loss = 1.09417857\n",
            "Iteration 27, loss = 1.10400986\n",
            "Iteration 28, loss = 1.09169089\n",
            "Iteration 29, loss = 1.09457578\n",
            "Iteration 30, loss = 1.10248467\n",
            "Iteration 31, loss = 1.08746748\n",
            "Iteration 32, loss = 1.09288995\n",
            "Iteration 33, loss = 1.09040386\n",
            "Iteration 34, loss = 1.08593625\n",
            "Iteration 35, loss = 1.09314611\n",
            "Iteration 36, loss = 1.09456219\n",
            "Iteration 37, loss = 1.08225159\n",
            "Iteration 38, loss = 1.09880916\n",
            "Iteration 39, loss = 1.08164320\n",
            "Iteration 40, loss = 1.09121061\n",
            "Iteration 41, loss = 1.07357384\n",
            "Iteration 42, loss = 1.08843819\n",
            "Iteration 43, loss = 1.06829995\n",
            "Iteration 44, loss = 1.05883924\n",
            "Iteration 45, loss = 1.05949455\n",
            "Iteration 46, loss = 1.05346096\n",
            "Iteration 47, loss = 1.05189175\n",
            "Iteration 48, loss = 1.04882279\n",
            "Iteration 49, loss = 1.04408307\n",
            "Iteration 50, loss = 1.04849045\n",
            "Iteration 51, loss = 1.03936707\n",
            "Iteration 52, loss = 1.03946798\n",
            "Iteration 53, loss = 1.03525664\n",
            "Iteration 54, loss = 1.02936341\n",
            "Iteration 55, loss = 1.02742998\n",
            "Iteration 56, loss = 1.01754403\n",
            "Iteration 57, loss = 1.02175300\n",
            "Iteration 58, loss = 1.00817855\n",
            "Iteration 59, loss = 1.00601224\n",
            "Iteration 60, loss = 1.00065108\n",
            "Iteration 61, loss = 0.99655224\n",
            "Iteration 62, loss = 0.99692891\n",
            "Iteration 63, loss = 0.98339743\n",
            "Iteration 64, loss = 0.98658995\n",
            "Iteration 65, loss = 0.97861127\n",
            "Iteration 66, loss = 1.00667201\n",
            "Iteration 67, loss = 1.00085994\n",
            "Iteration 68, loss = 0.98893258\n",
            "Iteration 69, loss = 0.98231850\n",
            "Iteration 70, loss = 0.96421118\n",
            "Iteration 71, loss = 0.98560131\n",
            "Iteration 72, loss = 0.96710701\n",
            "Iteration 73, loss = 0.96528236\n",
            "Iteration 74, loss = 0.95309128\n",
            "Iteration 75, loss = 0.95246928\n",
            "Iteration 76, loss = 0.94026015\n",
            "Iteration 77, loss = 0.93065084\n",
            "Iteration 78, loss = 0.92674167\n",
            "Iteration 79, loss = 0.92154407\n",
            "Iteration 80, loss = 0.92619657\n",
            "Iteration 81, loss = 0.92786767\n",
            "Iteration 82, loss = 0.95917157\n",
            "Iteration 83, loss = 0.94886779\n",
            "Iteration 84, loss = 0.94041150\n",
            "Iteration 85, loss = 0.92382657\n",
            "Iteration 86, loss = 0.91888012\n",
            "Iteration 87, loss = 0.93726911\n",
            "Iteration 88, loss = 0.92421099\n",
            "Iteration 89, loss = 0.90301021\n",
            "Iteration 90, loss = 0.91498385\n",
            "Iteration 91, loss = 0.90434728\n",
            "Iteration 92, loss = 0.89908419\n",
            "Iteration 93, loss = 0.87764468\n",
            "Iteration 94, loss = 0.88708903\n",
            "Iteration 95, loss = 0.89319970\n",
            "Iteration 96, loss = 0.88235916\n",
            "Iteration 97, loss = 0.88221503\n",
            "Iteration 98, loss = 0.87472724\n",
            "Iteration 99, loss = 0.89334314\n",
            "Iteration 100, loss = 0.87916650\n",
            "Iteration 1, loss = 1.63327884\n",
            "Iteration 2, loss = 1.63002559\n",
            "Iteration 3, loss = 1.62687768\n",
            "Iteration 4, loss = 1.62409185\n",
            "Iteration 5, loss = 1.62151224\n",
            "Iteration 6, loss = 1.61893790\n",
            "Iteration 7, loss = 1.61656113\n",
            "Iteration 8, loss = 1.61429101\n",
            "Iteration 9, loss = 1.61200003\n",
            "Iteration 10, loss = 1.61045500\n",
            "Iteration 11, loss = 1.60886386\n",
            "Iteration 12, loss = 1.60680628\n",
            "Iteration 13, loss = 1.60534571\n",
            "Iteration 14, loss = 1.60403512\n",
            "Iteration 15, loss = 1.60274792\n",
            "Iteration 16, loss = 1.60134753\n",
            "Iteration 17, loss = 1.60034543\n",
            "Iteration 18, loss = 1.59942712\n",
            "Iteration 19, loss = 1.59822793\n",
            "Iteration 20, loss = 1.59730764\n",
            "Iteration 21, loss = 1.59642243\n",
            "Iteration 22, loss = 1.59558718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 23, loss = 1.59490255\n",
            "Iteration 24, loss = 1.59413316\n",
            "Iteration 25, loss = 1.59341991\n",
            "Iteration 26, loss = 1.59279910\n",
            "Iteration 27, loss = 1.59211899\n",
            "Iteration 28, loss = 1.59150120\n",
            "Iteration 29, loss = 1.59084615\n",
            "Iteration 30, loss = 1.59022694\n",
            "Iteration 31, loss = 1.58964645\n",
            "Iteration 32, loss = 1.58903340\n",
            "Iteration 33, loss = 1.58847602\n",
            "Iteration 34, loss = 1.58784760\n",
            "Iteration 35, loss = 1.58727460\n",
            "Iteration 36, loss = 1.58668011\n",
            "Iteration 37, loss = 1.58611084\n",
            "Iteration 38, loss = 1.58551711\n",
            "Iteration 39, loss = 1.58494293\n",
            "Iteration 40, loss = 1.58429887\n",
            "Iteration 41, loss = 1.58376390\n",
            "Iteration 42, loss = 1.58312774\n",
            "Iteration 43, loss = 1.58255951\n",
            "Iteration 44, loss = 1.58192224\n",
            "Iteration 45, loss = 1.58127721\n",
            "Iteration 46, loss = 1.58069365\n",
            "Iteration 47, loss = 1.58003008\n",
            "Iteration 48, loss = 1.57941724\n",
            "Iteration 49, loss = 1.57878846\n",
            "Iteration 50, loss = 1.57816879\n",
            "Iteration 51, loss = 1.57755741\n",
            "Iteration 52, loss = 1.57688777\n",
            "Iteration 53, loss = 1.57631574\n",
            "Iteration 54, loss = 1.57562378\n",
            "Iteration 55, loss = 1.57491378\n",
            "Iteration 56, loss = 1.57426368\n",
            "Iteration 57, loss = 1.57360040\n",
            "Iteration 58, loss = 1.57294677\n",
            "Iteration 59, loss = 1.57224188\n",
            "Iteration 60, loss = 1.57156909\n",
            "Iteration 61, loss = 1.57085328\n",
            "Iteration 62, loss = 1.57014861\n",
            "Iteration 63, loss = 1.56943173\n",
            "Iteration 64, loss = 1.56875112\n",
            "Iteration 65, loss = 1.56804104\n",
            "Iteration 66, loss = 1.56730406\n",
            "Iteration 67, loss = 1.56659610\n",
            "Iteration 68, loss = 1.56587075\n",
            "Iteration 69, loss = 1.56513680\n",
            "Iteration 70, loss = 1.56430019\n",
            "Iteration 71, loss = 1.56357242\n",
            "Iteration 72, loss = 1.56276545\n",
            "Iteration 73, loss = 1.56212235\n",
            "Iteration 74, loss = 1.56126361\n",
            "Iteration 75, loss = 1.56047510\n",
            "Iteration 76, loss = 1.55968603\n",
            "Iteration 77, loss = 1.55885716\n",
            "Iteration 78, loss = 1.55802674\n",
            "Iteration 79, loss = 1.55728173\n",
            "Iteration 80, loss = 1.55643890\n",
            "Iteration 81, loss = 1.55561499\n",
            "Iteration 82, loss = 1.55474152\n",
            "Iteration 83, loss = 1.55389396\n",
            "Iteration 84, loss = 1.55301213\n",
            "Iteration 85, loss = 1.55222301\n",
            "Iteration 86, loss = 1.55127238\n",
            "Iteration 87, loss = 1.55041999\n",
            "Iteration 88, loss = 1.54950501\n",
            "Iteration 89, loss = 1.54860525\n",
            "Iteration 90, loss = 1.54765440\n",
            "Iteration 91, loss = 1.54673770\n",
            "Iteration 92, loss = 1.54582462\n",
            "Iteration 93, loss = 1.54488834\n",
            "Iteration 94, loss = 1.54396378\n",
            "Iteration 95, loss = 1.54301765\n",
            "Iteration 96, loss = 1.54199639\n",
            "Iteration 97, loss = 1.54115291\n",
            "Iteration 98, loss = 1.54006547\n",
            "Iteration 99, loss = 1.53905427\n",
            "Iteration 100, loss = 1.53803358\n",
            "Iteration 1, loss = 1.63067604\n",
            "Iteration 2, loss = 1.60818080\n",
            "Iteration 3, loss = 1.59656944\n",
            "Iteration 4, loss = 1.59234373\n",
            "Iteration 5, loss = 1.58920781\n",
            "Iteration 6, loss = 1.58434242\n",
            "Iteration 7, loss = 1.57878939\n",
            "Iteration 8, loss = 1.57203574\n",
            "Iteration 9, loss = 1.56664395\n",
            "Iteration 10, loss = 1.55871657\n",
            "Iteration 11, loss = 1.55252853\n",
            "Iteration 12, loss = 1.54595801\n",
            "Iteration 13, loss = 1.53896170\n",
            "Iteration 14, loss = 1.53036193\n",
            "Iteration 15, loss = 1.52146123\n",
            "Iteration 16, loss = 1.51186438\n",
            "Iteration 17, loss = 1.50203422\n",
            "Iteration 18, loss = 1.49100213\n",
            "Iteration 19, loss = 1.47934249\n",
            "Iteration 20, loss = 1.46800269\n",
            "Iteration 21, loss = 1.45511108\n",
            "Iteration 22, loss = 1.44253614\n",
            "Iteration 23, loss = 1.42801287\n",
            "Iteration 24, loss = 1.41453355\n",
            "Iteration 25, loss = 1.40032449\n",
            "Iteration 26, loss = 1.38586359\n",
            "Iteration 27, loss = 1.37162220\n",
            "Iteration 28, loss = 1.35747521\n",
            "Iteration 29, loss = 1.34332354\n",
            "Iteration 30, loss = 1.33056167\n",
            "Iteration 31, loss = 1.31632218\n",
            "Iteration 32, loss = 1.30323186\n",
            "Iteration 33, loss = 1.29108772\n",
            "Iteration 34, loss = 1.27929389\n",
            "Iteration 35, loss = 1.26796061\n",
            "Iteration 36, loss = 1.25799428\n",
            "Iteration 37, loss = 1.24781698\n",
            "Iteration 38, loss = 1.23851730\n",
            "Iteration 39, loss = 1.22991300\n",
            "Iteration 40, loss = 1.22182903\n",
            "Iteration 41, loss = 1.21504652\n",
            "Iteration 42, loss = 1.20784808\n",
            "Iteration 43, loss = 1.20162268\n",
            "Iteration 44, loss = 1.19572503\n",
            "Iteration 45, loss = 1.18979530\n",
            "Iteration 46, loss = 1.18471304\n",
            "Iteration 47, loss = 1.17989329\n",
            "Iteration 48, loss = 1.17545120\n",
            "Iteration 49, loss = 1.17111964\n",
            "Iteration 50, loss = 1.16738287\n",
            "Iteration 51, loss = 1.16411534\n",
            "Iteration 52, loss = 1.16025185\n",
            "Iteration 53, loss = 1.15744696\n",
            "Iteration 54, loss = 1.15459700\n",
            "Iteration 55, loss = 1.15124325\n",
            "Iteration 56, loss = 1.14838999\n",
            "Iteration 57, loss = 1.14629469\n",
            "Iteration 58, loss = 1.14390315\n",
            "Iteration 59, loss = 1.14197951\n",
            "Iteration 60, loss = 1.13948004\n",
            "Iteration 61, loss = 1.13712796\n",
            "Iteration 62, loss = 1.13537660\n",
            "Iteration 63, loss = 1.13335128\n",
            "Iteration 64, loss = 1.13153348\n",
            "Iteration 65, loss = 1.13028695\n",
            "Iteration 66, loss = 1.12904513\n",
            "Iteration 67, loss = 1.12696350\n",
            "Iteration 68, loss = 1.12599556\n",
            "Iteration 69, loss = 1.12444855\n",
            "Iteration 70, loss = 1.12230697\n",
            "Iteration 71, loss = 1.12171060\n",
            "Iteration 72, loss = 1.11964861\n",
            "Iteration 73, loss = 1.11985601\n",
            "Iteration 74, loss = 1.11793372\n",
            "Iteration 75, loss = 1.11665341\n",
            "Iteration 76, loss = 1.11519809\n",
            "Iteration 77, loss = 1.11386550\n",
            "Iteration 78, loss = 1.11304129\n",
            "Iteration 79, loss = 1.11252828\n",
            "Iteration 80, loss = 1.11163713\n",
            "Iteration 81, loss = 1.11053671\n",
            "Iteration 82, loss = 1.10890912\n",
            "Iteration 83, loss = 1.10895106\n",
            "Iteration 84, loss = 1.10720080\n",
            "Iteration 85, loss = 1.10857663\n",
            "Iteration 86, loss = 1.10564405\n",
            "Iteration 87, loss = 1.10491417\n",
            "Iteration 88, loss = 1.10405349\n",
            "Iteration 89, loss = 1.10292041\n",
            "Iteration 90, loss = 1.10204602\n",
            "Iteration 91, loss = 1.10089574\n",
            "Iteration 92, loss = 1.10043176\n",
            "Iteration 93, loss = 1.10061374\n",
            "Iteration 94, loss = 1.09913063\n",
            "Iteration 95, loss = 1.09878923\n",
            "Iteration 96, loss = 1.09702803\n",
            "Iteration 97, loss = 1.09637827\n",
            "Iteration 98, loss = 1.09525850\n",
            "Iteration 99, loss = 1.09480186\n",
            "Iteration 100, loss = 1.09373257\n",
            "Iteration 1, loss = 1.69161108\n",
            "Iteration 2, loss = 1.58941805\n",
            "Iteration 3, loss = 1.56088360\n",
            "Iteration 4, loss = 1.50908376\n",
            "Iteration 5, loss = 1.44327072\n",
            "Iteration 6, loss = 1.37231413\n",
            "Iteration 7, loss = 1.29228460\n",
            "Iteration 8, loss = 1.23340835\n",
            "Iteration 9, loss = 1.19021397\n",
            "Iteration 10, loss = 1.15188529\n",
            "Iteration 11, loss = 1.13491735\n",
            "Iteration 12, loss = 1.12153232\n",
            "Iteration 13, loss = 1.11850565\n",
            "Iteration 14, loss = 1.12452776\n",
            "Iteration 15, loss = 1.11647195\n",
            "Iteration 16, loss = 1.10060036\n",
            "Iteration 17, loss = 1.10246130\n",
            "Iteration 18, loss = 1.08818613\n",
            "Iteration 19, loss = 1.08660765\n",
            "Iteration 20, loss = 1.07397222\n",
            "Iteration 21, loss = 1.08012840\n",
            "Iteration 22, loss = 1.07078352\n",
            "Iteration 23, loss = 1.05957310\n",
            "Iteration 24, loss = 1.05951129\n",
            "Iteration 25, loss = 1.04657411\n",
            "Iteration 26, loss = 1.03447226\n",
            "Iteration 27, loss = 1.03274444\n",
            "Iteration 28, loss = 1.02150654\n",
            "Iteration 29, loss = 1.01394412\n",
            "Iteration 30, loss = 1.03514108\n",
            "Iteration 31, loss = 1.06087294\n",
            "Iteration 32, loss = 1.05110199\n",
            "Iteration 33, loss = 1.06269520\n",
            "Iteration 34, loss = 1.03527802\n",
            "Iteration 35, loss = 0.99133027\n",
            "Iteration 36, loss = 1.03242066\n",
            "Iteration 37, loss = 1.01817864\n",
            "Iteration 38, loss = 1.01111590\n",
            "Iteration 39, loss = 0.98410079\n",
            "Iteration 40, loss = 0.97562129\n",
            "Iteration 41, loss = 0.99227315\n",
            "Iteration 42, loss = 0.97023532\n",
            "Iteration 43, loss = 0.96547160\n",
            "Iteration 44, loss = 0.99573955\n",
            "Iteration 45, loss = 0.94211016\n",
            "Iteration 46, loss = 0.94177133\n",
            "Iteration 47, loss = 0.95510820\n",
            "Iteration 48, loss = 0.93620408\n",
            "Iteration 49, loss = 0.92509232\n",
            "Iteration 50, loss = 0.91103259\n",
            "Iteration 51, loss = 0.91205264\n",
            "Iteration 52, loss = 0.90805880\n",
            "Iteration 53, loss = 0.90394589\n",
            "Iteration 54, loss = 0.90835477\n",
            "Iteration 55, loss = 0.91572182\n",
            "Iteration 56, loss = 0.88954969\n",
            "Iteration 57, loss = 0.89280860"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 58, loss = 0.89416614\n",
            "Iteration 59, loss = 0.89360440\n",
            "Iteration 60, loss = 0.92168129\n",
            "Iteration 61, loss = 0.87282988\n",
            "Iteration 62, loss = 0.87580416\n",
            "Iteration 63, loss = 0.86298000\n",
            "Iteration 64, loss = 0.85396633\n",
            "Iteration 65, loss = 0.85098317\n",
            "Iteration 66, loss = 0.84800450\n",
            "Iteration 67, loss = 0.85127879\n",
            "Iteration 68, loss = 0.85446958\n",
            "Iteration 69, loss = 0.86688113\n",
            "Iteration 70, loss = 0.89633772\n",
            "Iteration 71, loss = 0.86235317\n",
            "Iteration 72, loss = 0.84037450\n",
            "Iteration 73, loss = 0.83584043\n",
            "Iteration 74, loss = 0.83335955\n",
            "Iteration 75, loss = 0.86225135\n",
            "Iteration 76, loss = 0.82937847\n",
            "Iteration 77, loss = 0.81255135\n",
            "Iteration 78, loss = 0.81569888\n",
            "Iteration 79, loss = 0.81981653\n",
            "Iteration 80, loss = 0.81052367\n",
            "Iteration 81, loss = 0.81731324\n",
            "Iteration 82, loss = 0.81777986\n",
            "Iteration 83, loss = 0.87118652\n",
            "Iteration 84, loss = 0.92252706\n",
            "Iteration 85, loss = 0.91573352\n",
            "Iteration 86, loss = 1.02094889\n",
            "Iteration 87, loss = 1.09542289\n",
            "Iteration 88, loss = 1.17621056\n",
            "Iteration 89, loss = 1.06671758\n",
            "Iteration 90, loss = 0.94845915\n",
            "Iteration 91, loss = 0.93344969\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.72085842\n",
            "Iteration 2, loss = 1.71354998\n",
            "Iteration 3, loss = 1.70619767\n",
            "Iteration 4, loss = 1.69976450\n",
            "Iteration 5, loss = 1.69278143\n",
            "Iteration 6, loss = 1.68694886\n",
            "Iteration 7, loss = 1.68132758\n",
            "Iteration 8, loss = 1.67541997\n",
            "Iteration 9, loss = 1.66993200\n",
            "Iteration 10, loss = 1.66518176\n",
            "Iteration 11, loss = 1.66060154\n",
            "Iteration 12, loss = 1.65581198\n",
            "Iteration 13, loss = 1.65150490\n",
            "Iteration 14, loss = 1.64789591\n",
            "Iteration 15, loss = 1.64412441\n",
            "Iteration 16, loss = 1.64051842\n",
            "Iteration 17, loss = 1.63702388\n",
            "Iteration 18, loss = 1.63412245\n",
            "Iteration 19, loss = 1.63132227\n",
            "Iteration 20, loss = 1.62888116\n",
            "Iteration 21, loss = 1.62629251\n",
            "Iteration 22, loss = 1.62405421\n",
            "Iteration 23, loss = 1.62182075\n",
            "Iteration 24, loss = 1.61992657\n",
            "Iteration 25, loss = 1.61807401\n",
            "Iteration 26, loss = 1.61634692\n",
            "Iteration 27, loss = 1.61464012\n",
            "Iteration 28, loss = 1.61309717\n",
            "Iteration 29, loss = 1.61192459\n",
            "Iteration 30, loss = 1.61069641\n",
            "Iteration 31, loss = 1.60952377\n",
            "Iteration 32, loss = 1.60843960\n",
            "Iteration 33, loss = 1.60767183\n",
            "Iteration 34, loss = 1.60682253\n",
            "Iteration 35, loss = 1.60595043\n",
            "Iteration 36, loss = 1.60530305\n",
            "Iteration 37, loss = 1.60448027\n",
            "Iteration 38, loss = 1.60399976\n",
            "Iteration 39, loss = 1.60346155\n",
            "Iteration 40, loss = 1.60284930\n",
            "Iteration 41, loss = 1.60258017\n",
            "Iteration 42, loss = 1.60202604\n",
            "Iteration 43, loss = 1.60174209\n",
            "Iteration 44, loss = 1.60130474\n",
            "Iteration 45, loss = 1.60101767\n",
            "Iteration 46, loss = 1.60068623\n",
            "Iteration 47, loss = 1.60044047\n",
            "Iteration 48, loss = 1.60016277\n",
            "Iteration 49, loss = 1.59997453\n",
            "Iteration 50, loss = 1.59978502\n",
            "Iteration 51, loss = 1.59959977\n",
            "Iteration 52, loss = 1.59941985\n",
            "Iteration 53, loss = 1.59935057\n",
            "Iteration 54, loss = 1.59906611\n",
            "Iteration 55, loss = 1.59897260\n",
            "Iteration 56, loss = 1.59884315\n",
            "Iteration 57, loss = 1.59875187\n",
            "Iteration 58, loss = 1.59867839\n",
            "Iteration 59, loss = 1.59850638\n",
            "Iteration 60, loss = 1.59845686\n",
            "Iteration 61, loss = 1.59834429\n",
            "Iteration 62, loss = 1.59826634\n",
            "Iteration 63, loss = 1.59818538\n",
            "Iteration 64, loss = 1.59812205\n",
            "Iteration 65, loss = 1.59808031\n",
            "Iteration 66, loss = 1.59799027\n",
            "Iteration 67, loss = 1.59797668\n",
            "Iteration 68, loss = 1.59790195\n",
            "Iteration 69, loss = 1.59786919\n",
            "Iteration 70, loss = 1.59771937\n",
            "Iteration 71, loss = 1.59767128\n",
            "Iteration 72, loss = 1.59760951\n",
            "Iteration 73, loss = 1.59760255\n",
            "Iteration 74, loss = 1.59752694\n",
            "Iteration 75, loss = 1.59744895\n",
            "Iteration 76, loss = 1.59742250\n",
            "Iteration 77, loss = 1.59735376\n",
            "Iteration 78, loss = 1.59729071\n",
            "Iteration 79, loss = 1.59730086\n",
            "Iteration 80, loss = 1.59722449\n",
            "Iteration 81, loss = 1.59720719\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.70521919\n",
            "Iteration 2, loss = 1.65030480\n",
            "Iteration 3, loss = 1.61706951\n",
            "Iteration 4, loss = 1.60400086\n",
            "Iteration 5, loss = 1.60006769\n",
            "Iteration 6, loss = 1.60335147\n",
            "Iteration 7, loss = 1.60757310\n",
            "Iteration 8, loss = 1.60819421\n",
            "Iteration 9, loss = 1.60635707\n",
            "Iteration 10, loss = 1.60435907\n",
            "Iteration 11, loss = 1.60297292\n",
            "Iteration 12, loss = 1.60068688\n",
            "Iteration 13, loss = 1.59909301\n",
            "Iteration 14, loss = 1.59736881\n",
            "Iteration 15, loss = 1.59509516\n",
            "Iteration 16, loss = 1.59523808\n",
            "Iteration 17, loss = 1.59487172\n",
            "Iteration 18, loss = 1.59434897\n",
            "Iteration 19, loss = 1.59401236\n",
            "Iteration 20, loss = 1.59372348\n",
            "Iteration 21, loss = 1.59285072\n",
            "Iteration 22, loss = 1.59213322\n",
            "Iteration 23, loss = 1.59083956\n",
            "Iteration 24, loss = 1.58999396\n",
            "Iteration 25, loss = 1.58923689\n",
            "Iteration 26, loss = 1.58857109\n",
            "Iteration 27, loss = 1.58799019\n",
            "Iteration 28, loss = 1.58740224\n",
            "Iteration 29, loss = 1.58608860\n",
            "Iteration 30, loss = 1.58553479\n",
            "Iteration 31, loss = 1.58384656\n",
            "Iteration 32, loss = 1.58283755\n",
            "Iteration 33, loss = 1.58178637\n",
            "Iteration 34, loss = 1.58079791\n",
            "Iteration 35, loss = 1.57940481\n",
            "Iteration 36, loss = 1.57804610\n",
            "Iteration 37, loss = 1.57670182\n",
            "Iteration 38, loss = 1.57503438\n",
            "Iteration 39, loss = 1.57363057\n",
            "Iteration 40, loss = 1.57157449\n",
            "Iteration 41, loss = 1.57028693\n",
            "Iteration 42, loss = 1.56793030\n",
            "Iteration 43, loss = 1.56643124\n",
            "Iteration 44, loss = 1.56425602\n",
            "Iteration 45, loss = 1.56129636\n",
            "Iteration 46, loss = 1.55917900\n",
            "Iteration 47, loss = 1.55625059\n",
            "Iteration 48, loss = 1.55381444\n",
            "Iteration 49, loss = 1.55082109\n",
            "Iteration 50, loss = 1.54804865\n",
            "Iteration 51, loss = 1.54538213\n",
            "Iteration 52, loss = 1.54164650\n",
            "Iteration 53, loss = 1.53891128\n",
            "Iteration 54, loss = 1.53484670\n",
            "Iteration 55, loss = 1.53092085\n",
            "Iteration 56, loss = 1.52706878\n",
            "Iteration 57, loss = 1.52326991\n",
            "Iteration 58, loss = 1.51920978\n",
            "Iteration 59, loss = 1.51465759\n",
            "Iteration 60, loss = 1.51017398\n",
            "Iteration 61, loss = 1.50531769\n",
            "Iteration 62, loss = 1.50040375\n",
            "Iteration 63, loss = 1.49531427\n",
            "Iteration 64, loss = 1.49043550\n",
            "Iteration 65, loss = 1.48531600\n",
            "Iteration 66, loss = 1.47972097\n",
            "Iteration 67, loss = 1.47427509\n",
            "Iteration 68, loss = 1.46890107\n",
            "Iteration 69, loss = 1.46286033\n",
            "Iteration 70, loss = 1.45621069\n",
            "Iteration 71, loss = 1.45055212\n",
            "Iteration 72, loss = 1.44406636\n",
            "Iteration 73, loss = 1.43871217\n",
            "Iteration 74, loss = 1.43221348\n",
            "Iteration 75, loss = 1.42556132\n",
            "Iteration 76, loss = 1.41910807\n",
            "Iteration 77, loss = 1.41246389\n",
            "Iteration 78, loss = 1.40583652\n",
            "Iteration 79, loss = 1.40016933\n",
            "Iteration 80, loss = 1.39335382\n",
            "Iteration 81, loss = 1.38690480\n",
            "Iteration 82, loss = 1.37995309\n",
            "Iteration 83, loss = 1.37412752\n",
            "Iteration 84, loss = 1.36715007\n",
            "Iteration 85, loss = 1.36142669\n",
            "Iteration 86, loss = 1.35480864\n",
            "Iteration 87, loss = 1.34869950\n",
            "Iteration 88, loss = 1.34250081\n",
            "Iteration 89, loss = 1.33639966\n",
            "Iteration 90, loss = 1.33018415\n",
            "Iteration 91, loss = 1.32450084\n",
            "Iteration 92, loss = 1.31900490\n",
            "Iteration 93, loss = 1.31361519\n",
            "Iteration 94, loss = 1.30822497\n",
            "Iteration 95, loss = 1.30287339\n",
            "Iteration 96, loss = 1.29727251\n",
            "Iteration 97, loss = 1.29327089\n",
            "Iteration 98, loss = 1.28769164\n",
            "Iteration 99, loss = 1.28288487\n",
            "Iteration 100, loss = 1.27814737\n",
            "Iteration 1, loss = 1.69838450\n",
            "Iteration 2, loss = 1.71240798\n",
            "Iteration 3, loss = 1.64492603\n",
            "Iteration 4, loss = 1.63329804\n",
            "Iteration 5, loss = 1.61600369\n",
            "Iteration 6, loss = 1.60642879\n",
            "Iteration 7, loss = 1.60479199\n",
            "Iteration 8, loss = 1.60266913\n",
            "Iteration 9, loss = 1.59374949\n",
            "Iteration 10, loss = 1.58316861\n",
            "Iteration 11, loss = 1.57790021\n",
            "Iteration 12, loss = 1.57339669\n",
            "Iteration 13, loss = 1.56634224\n",
            "Iteration 14, loss = 1.55322554\n",
            "Iteration 15, loss = 1.53556331\n",
            "Iteration 16, loss = 1.51967449\n",
            "Iteration 17, loss = 1.49784377\n",
            "Iteration 18, loss = 1.46887295\n",
            "Iteration 19, loss = 1.43279915\n",
            "Iteration 20, loss = 1.39423894\n",
            "Iteration 21, loss = 1.35485841\n",
            "Iteration 22, loss = 1.31860406\n",
            "Iteration 23, loss = 1.28086942\n",
            "Iteration 24, loss = 1.25598534\n",
            "Iteration 25, loss = 1.23133528\n",
            "Iteration 26, loss = 1.20658705\n",
            "Iteration 27, loss = 1.19443185\n",
            "Iteration 28, loss = 1.17550929\n",
            "Iteration 29, loss = 1.16599057\n",
            "Iteration 30, loss = 1.16078810\n",
            "Iteration 31, loss = 1.15640780\n",
            "Iteration 32, loss = 1.14538263\n",
            "Iteration 33, loss = 1.14222872\n",
            "Iteration 34, loss = 1.13355478\n",
            "Iteration 35, loss = 1.13526328\n",
            "Iteration 36, loss = 1.13518493\n",
            "Iteration 37, loss = 1.12916785\n",
            "Iteration 38, loss = 1.12870941\n",
            "Iteration 39, loss = 1.11867354\n",
            "Iteration 40, loss = 1.12608519\n",
            "Iteration 41, loss = 1.11834916\n",
            "Iteration 42, loss = 1.12137396\n",
            "Iteration 43, loss = 1.12439491\n",
            "Iteration 44, loss = 1.12224961\n",
            "Iteration 45, loss = 1.11083249\n",
            "Iteration 46, loss = 1.11713973\n",
            "Iteration 47, loss = 1.11577244\n",
            "Iteration 48, loss = 1.11496533\n",
            "Iteration 49, loss = 1.11406725\n",
            "Iteration 50, loss = 1.11032939\n",
            "Iteration 51, loss = 1.11663512\n",
            "Iteration 52, loss = 1.10851546\n",
            "Iteration 53, loss = 1.11685817\n",
            "Iteration 54, loss = 1.11063095\n",
            "Iteration 55, loss = 1.10913131\n",
            "Iteration 56, loss = 1.10946856\n",
            "Iteration 57, loss = 1.10964113\n",
            "Iteration 58, loss = 1.10911325\n",
            "Iteration 59, loss = 1.10932482\n",
            "Iteration 60, loss = 1.10750167\n",
            "Iteration 61, loss = 1.10214084\n",
            "Iteration 62, loss = 1.10323306\n",
            "Iteration 63, loss = 1.10211493\n",
            "Iteration 64, loss = 1.10114482\n",
            "Iteration 65, loss = 1.10434753\n",
            "Iteration 66, loss = 1.10227582\n",
            "Iteration 67, loss = 1.10653104\n",
            "Iteration 68, loss = 1.10791103\n",
            "Iteration 69, loss = 1.10835399\n",
            "Iteration 70, loss = 1.09816195\n",
            "Iteration 71, loss = 1.10169362\n",
            "Iteration 72, loss = 1.09593432\n",
            "Iteration 73, loss = 1.10394931\n",
            "Iteration 74, loss = 1.09837278\n",
            "Iteration 75, loss = 1.09818617\n",
            "Iteration 76, loss = 1.09564639\n",
            "Iteration 77, loss = 1.09304211\n",
            "Iteration 78, loss = 1.08929274\n",
            "Iteration 79, loss = 1.09674308\n",
            "Iteration 80, loss = 1.09764307\n",
            "Iteration 81, loss = 1.09409862\n",
            "Iteration 82, loss = 1.08767253\n",
            "Iteration 83, loss = 1.09152215\n",
            "Iteration 84, loss = 1.08541186\n",
            "Iteration 85, loss = 1.08686098\n",
            "Iteration 86, loss = 1.08780669\n",
            "Iteration 87, loss = 1.08342455\n",
            "Iteration 88, loss = 1.07929927\n",
            "Iteration 89, loss = 1.08058177\n",
            "Iteration 90, loss = 1.07443507\n",
            "Iteration 91, loss = 1.07599756\n",
            "Iteration 92, loss = 1.07912597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 93, loss = 1.07186208\n",
            "Iteration 94, loss = 1.07496054\n",
            "Iteration 95, loss = 1.07041636\n",
            "Iteration 96, loss = 1.06375437\n",
            "Iteration 97, loss = 1.06774690\n",
            "Iteration 98, loss = 1.05520818\n",
            "Iteration 99, loss = 1.06262079\n",
            "Iteration 100, loss = 1.05062943\n",
            "Iteration 1, loss = 1.62687904\n",
            "Iteration 2, loss = 1.62417997\n",
            "Iteration 3, loss = 1.62159229\n",
            "Iteration 4, loss = 1.61932143\n",
            "Iteration 5, loss = 1.61720852\n",
            "Iteration 6, loss = 1.61509820\n",
            "Iteration 7, loss = 1.61315970\n",
            "Iteration 8, loss = 1.61128549\n",
            "Iteration 9, loss = 1.60939771\n",
            "Iteration 10, loss = 1.60815624\n",
            "Iteration 11, loss = 1.60687508\n",
            "Iteration 12, loss = 1.60514788\n",
            "Iteration 13, loss = 1.60394723\n",
            "Iteration 14, loss = 1.60287979\n",
            "Iteration 15, loss = 1.60180601\n",
            "Iteration 16, loss = 1.60063257\n",
            "Iteration 17, loss = 1.59980077\n",
            "Iteration 18, loss = 1.59902975\n",
            "Iteration 19, loss = 1.59800616\n",
            "Iteration 20, loss = 1.59722373\n",
            "Iteration 21, loss = 1.59646194\n",
            "Iteration 22, loss = 1.59574362\n",
            "Iteration 23, loss = 1.59513606\n",
            "Iteration 24, loss = 1.59446386\n",
            "Iteration 25, loss = 1.59383041\n",
            "Iteration 26, loss = 1.59327045\n",
            "Iteration 27, loss = 1.59266042\n",
            "Iteration 28, loss = 1.59210004\n",
            "Iteration 29, loss = 1.59150334\n",
            "Iteration 30, loss = 1.59094232\n",
            "Iteration 31, loss = 1.59040199\n",
            "Iteration 32, loss = 1.58983702\n",
            "Iteration 33, loss = 1.58932002\n",
            "Iteration 34, loss = 1.58874056\n",
            "Iteration 35, loss = 1.58820287\n",
            "Iteration 36, loss = 1.58765116\n",
            "Iteration 37, loss = 1.58711688\n",
            "Iteration 38, loss = 1.58655873\n",
            "Iteration 39, loss = 1.58602085\n",
            "Iteration 40, loss = 1.58541923\n",
            "Iteration 41, loss = 1.58491747\n",
            "Iteration 42, loss = 1.58431795\n",
            "Iteration 43, loss = 1.58378390\n",
            "Iteration 44, loss = 1.58318532\n",
            "Iteration 45, loss = 1.58257574\n",
            "Iteration 46, loss = 1.58202566\n",
            "Iteration 47, loss = 1.58139923\n",
            "Iteration 48, loss = 1.58082157\n",
            "Iteration 49, loss = 1.58022614\n",
            "Iteration 50, loss = 1.57964065\n",
            "Iteration 51, loss = 1.57906249\n",
            "Iteration 52, loss = 1.57842798\n",
            "Iteration 53, loss = 1.57788459\n",
            "Iteration 54, loss = 1.57722971\n",
            "Iteration 55, loss = 1.57655547\n",
            "Iteration 56, loss = 1.57593712\n",
            "Iteration 57, loss = 1.57530645\n",
            "Iteration 58, loss = 1.57468433\n",
            "Iteration 59, loss = 1.57401268\n",
            "Iteration 60, loss = 1.57337157\n",
            "Iteration 61, loss = 1.57268880\n",
            "Iteration 62, loss = 1.57201596\n",
            "Iteration 63, loss = 1.57133120\n",
            "Iteration 64, loss = 1.57068006\n",
            "Iteration 65, loss = 1.57000081\n",
            "Iteration 66, loss = 1.56929509\n",
            "Iteration 67, loss = 1.56861587\n",
            "Iteration 68, loss = 1.56792151\n",
            "Iteration 69, loss = 1.56721445\n",
            "Iteration 70, loss = 1.56641082\n",
            "Iteration 71, loss = 1.56571048\n",
            "Iteration 72, loss = 1.56493368\n",
            "Iteration 73, loss = 1.56431172\n",
            "Iteration 74, loss = 1.56348589\n",
            "Iteration 75, loss = 1.56272535\n",
            "Iteration 76, loss = 1.56196328\n",
            "Iteration 77, loss = 1.56116230\n",
            "Iteration 78, loss = 1.56035950\n",
            "Iteration 79, loss = 1.55963649\n",
            "Iteration 80, loss = 1.55882166\n",
            "Iteration 81, loss = 1.55802247\n",
            "Iteration 82, loss = 1.55717571\n",
            "Iteration 83, loss = 1.55635288\n",
            "Iteration 84, loss = 1.55549782\n",
            "Iteration 85, loss = 1.55472845\n",
            "Iteration 86, loss = 1.55380606\n",
            "Iteration 87, loss = 1.55297308\n",
            "Iteration 88, loss = 1.55208372\n",
            "Iteration 89, loss = 1.55120721\n",
            "Iteration 90, loss = 1.55027939\n",
            "Iteration 91, loss = 1.54938476\n",
            "Iteration 92, loss = 1.54849216\n",
            "Iteration 93, loss = 1.54757473\n",
            "Iteration 94, loss = 1.54667031\n",
            "Iteration 95, loss = 1.54574561\n",
            "Iteration 96, loss = 1.54474501\n",
            "Iteration 97, loss = 1.54391370\n",
            "Iteration 98, loss = 1.54285040\n",
            "Iteration 99, loss = 1.54185792\n",
            "Iteration 100, loss = 1.54085483\n",
            "Iteration 1, loss = 1.62521311\n",
            "Iteration 2, loss = 1.60657489\n",
            "Iteration 3, loss = 1.59691726\n",
            "Iteration 4, loss = 1.59295930\n",
            "Iteration 5, loss = 1.58966415\n",
            "Iteration 6, loss = 1.58482042\n",
            "Iteration 7, loss = 1.57959702\n",
            "Iteration 8, loss = 1.57340196\n",
            "Iteration 9, loss = 1.56841505\n",
            "Iteration 10, loss = 1.56093215\n",
            "Iteration 11, loss = 1.55496555\n",
            "Iteration 12, loss = 1.54836623\n",
            "Iteration 13, loss = 1.54143376\n",
            "Iteration 14, loss = 1.53296552\n",
            "Iteration 15, loss = 1.52419068\n",
            "Iteration 16, loss = 1.51472037\n",
            "Iteration 17, loss = 1.50487091\n",
            "Iteration 18, loss = 1.49373560\n",
            "Iteration 19, loss = 1.48200906\n",
            "Iteration 20, loss = 1.47050499\n",
            "Iteration 21, loss = 1.45739323\n",
            "Iteration 22, loss = 1.44456977\n",
            "Iteration 23, loss = 1.42980016\n",
            "Iteration 24, loss = 1.41595820\n",
            "Iteration 25, loss = 1.40137500\n",
            "Iteration 26, loss = 1.38649858\n",
            "Iteration 27, loss = 1.37183876\n",
            "Iteration 28, loss = 1.35723556\n",
            "Iteration 29, loss = 1.34274106\n",
            "Iteration 30, loss = 1.32952384\n",
            "Iteration 31, loss = 1.31488873\n",
            "Iteration 32, loss = 1.30146295\n",
            "Iteration 33, loss = 1.28896306\n",
            "Iteration 34, loss = 1.27691567\n",
            "Iteration 35, loss = 1.26534843\n",
            "Iteration 36, loss = 1.25518838\n",
            "Iteration 37, loss = 1.24479906\n",
            "Iteration 38, loss = 1.23543757\n",
            "Iteration 39, loss = 1.22673638\n",
            "Iteration 40, loss = 1.21861359\n",
            "Iteration 41, loss = 1.21168563\n",
            "Iteration 42, loss = 1.20462799\n",
            "Iteration 43, loss = 1.19822535\n",
            "Iteration 44, loss = 1.19236793\n",
            "Iteration 45, loss = 1.18667658\n",
            "Iteration 46, loss = 1.18164265\n",
            "Iteration 47, loss = 1.17691471\n",
            "Iteration 48, loss = 1.17251276\n",
            "Iteration 49, loss = 1.16822531\n",
            "Iteration 50, loss = 1.16456383\n",
            "Iteration 51, loss = 1.16120390\n",
            "Iteration 52, loss = 1.15756818\n",
            "Iteration 53, loss = 1.15478370\n",
            "Iteration 54, loss = 1.15197734\n",
            "Iteration 55, loss = 1.14876436\n",
            "Iteration 56, loss = 1.14600391\n",
            "Iteration 57, loss = 1.14391378\n",
            "Iteration 58, loss = 1.14154600\n",
            "Iteration 59, loss = 1.13972150\n",
            "Iteration 60, loss = 1.13728949\n",
            "Iteration 61, loss = 1.13502231\n",
            "Iteration 62, loss = 1.13330994\n",
            "Iteration 63, loss = 1.13131949\n",
            "Iteration 64, loss = 1.12952645\n",
            "Iteration 65, loss = 1.12820578\n",
            "Iteration 66, loss = 1.12714272\n",
            "Iteration 67, loss = 1.12502681\n",
            "Iteration 68, loss = 1.12401276\n",
            "Iteration 69, loss = 1.12252384\n",
            "Iteration 70, loss = 1.12053336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 71, loss = 1.11989253\n",
            "Iteration 72, loss = 1.11788413\n",
            "Iteration 73, loss = 1.11798325\n",
            "Iteration 74, loss = 1.11612877\n",
            "Iteration 75, loss = 1.11489862\n",
            "Iteration 76, loss = 1.11343352\n",
            "Iteration 77, loss = 1.11212214\n",
            "Iteration 78, loss = 1.11130893\n",
            "Iteration 79, loss = 1.11063290\n",
            "Iteration 80, loss = 1.10981591\n",
            "Iteration 81, loss = 1.10868842\n",
            "Iteration 82, loss = 1.10715888\n",
            "Iteration 83, loss = 1.10704100\n",
            "Iteration 84, loss = 1.10534299\n",
            "Iteration 85, loss = 1.10654686\n",
            "Iteration 86, loss = 1.10367197\n",
            "Iteration 87, loss = 1.10291144\n",
            "Iteration 88, loss = 1.10208210\n",
            "Iteration 89, loss = 1.10093298\n",
            "Iteration 90, loss = 1.10001590\n",
            "Iteration 91, loss = 1.09882960\n",
            "Iteration 92, loss = 1.09834648\n",
            "Iteration 93, loss = 1.09842012\n",
            "Iteration 94, loss = 1.09688830\n",
            "Iteration 95, loss = 1.09649958\n",
            "Iteration 96, loss = 1.09471268\n",
            "Iteration 97, loss = 1.09403890\n",
            "Iteration 98, loss = 1.09284072\n",
            "Iteration 99, loss = 1.09234084\n",
            "Iteration 100, loss = 1.09119613\n",
            "Iteration 1, loss = 1.68460163\n",
            "Iteration 2, loss = 1.58738871\n",
            "Iteration 3, loss = 1.56395555\n",
            "Iteration 4, loss = 1.51040745\n",
            "Iteration 5, loss = 1.44236120\n",
            "Iteration 6, loss = 1.37108524\n",
            "Iteration 7, loss = 1.29035980\n",
            "Iteration 8, loss = 1.22950513\n",
            "Iteration 9, loss = 1.18985712\n",
            "Iteration 10, loss = 1.15053766\n",
            "Iteration 11, loss = 1.13361143\n",
            "Iteration 12, loss = 1.12197041\n",
            "Iteration 13, loss = 1.11634244\n",
            "Iteration 14, loss = 1.12414793\n",
            "Iteration 15, loss = 1.11062300\n",
            "Iteration 16, loss = 1.09627613\n",
            "Iteration 17, loss = 1.09893105\n",
            "Iteration 18, loss = 1.08550957\n",
            "Iteration 19, loss = 1.08158780\n",
            "Iteration 20, loss = 1.06796940\n",
            "Iteration 21, loss = 1.07219383\n",
            "Iteration 22, loss = 1.06604390\n",
            "Iteration 23, loss = 1.04939790\n",
            "Iteration 24, loss = 1.05184626\n",
            "Iteration 25, loss = 1.04264948\n",
            "Iteration 26, loss = 1.03211683\n",
            "Iteration 27, loss = 1.01755481\n",
            "Iteration 28, loss = 1.01830303\n",
            "Iteration 29, loss = 0.99750166\n",
            "Iteration 30, loss = 1.00381945\n",
            "Iteration 31, loss = 1.03398022\n",
            "Iteration 32, loss = 1.03437927\n",
            "Iteration 33, loss = 1.05432243\n",
            "Iteration 34, loss = 1.06007749\n",
            "Iteration 35, loss = 1.00872791\n",
            "Iteration 36, loss = 1.01742871\n",
            "Iteration 37, loss = 0.96170121\n",
            "Iteration 38, loss = 0.99632448\n",
            "Iteration 39, loss = 0.98455746\n",
            "Iteration 40, loss = 0.95935420\n",
            "Iteration 41, loss = 0.95461735\n",
            "Iteration 42, loss = 0.93818541\n",
            "Iteration 43, loss = 0.93669617\n",
            "Iteration 44, loss = 0.93803033\n",
            "Iteration 45, loss = 0.94309362\n",
            "Iteration 46, loss = 0.93142315\n",
            "Iteration 47, loss = 0.92565550\n",
            "Iteration 48, loss = 0.93023010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 0.92589082\n",
            "Iteration 50, loss = 0.92714407\n",
            "Iteration 51, loss = 0.92316398\n",
            "Iteration 52, loss = 0.93061390\n",
            "Iteration 53, loss = 0.93641389\n",
            "Iteration 54, loss = 0.96795055\n",
            "Iteration 55, loss = 0.93630331\n",
            "Iteration 56, loss = 0.92663403\n",
            "Iteration 57, loss = 0.91963583\n",
            "Iteration 58, loss = 0.91101719\n",
            "Iteration 59, loss = 0.90353692\n",
            "Iteration 60, loss = 0.87162003\n",
            "Iteration 61, loss = 0.88039702\n",
            "Iteration 62, loss = 0.86941042\n",
            "Iteration 63, loss = 0.87866443\n",
            "Iteration 64, loss = 0.85888598\n",
            "Iteration 65, loss = 0.84867688\n",
            "Iteration 66, loss = 0.84334247\n",
            "Iteration 67, loss = 0.84449077\n",
            "Iteration 68, loss = 0.84583425\n",
            "Iteration 69, loss = 0.86025907\n",
            "Iteration 70, loss = 0.85443759\n",
            "Iteration 71, loss = 0.85228781\n",
            "Iteration 72, loss = 0.85531579\n",
            "Iteration 73, loss = 0.85236440\n",
            "Iteration 74, loss = 0.87338850\n",
            "Iteration 75, loss = 0.82547816\n",
            "Iteration 76, loss = 0.83371282\n",
            "Iteration 77, loss = 0.82961904\n",
            "Iteration 78, loss = 0.81118635\n",
            "Iteration 79, loss = 0.81659396\n",
            "Iteration 80, loss = 0.80964137\n",
            "Iteration 81, loss = 0.81461217\n",
            "Iteration 82, loss = 0.79670206\n",
            "Iteration 83, loss = 0.84274837\n",
            "Iteration 84, loss = 0.83235708\n",
            "Iteration 85, loss = 0.84716699\n",
            "Iteration 86, loss = 0.91114502\n",
            "Iteration 87, loss = 0.93535426\n",
            "Iteration 88, loss = 0.94600772\n",
            "Iteration 89, loss = 0.92454770\n",
            "Iteration 90, loss = 0.94543916\n",
            "Iteration 91, loss = 0.91881147\n",
            "Iteration 92, loss = 0.84918029\n",
            "Iteration 93, loss = 0.83438373\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.63539309\n",
            "Iteration 2, loss = 1.63382428\n",
            "Iteration 3, loss = 1.63222227\n",
            "Iteration 4, loss = 1.63085279\n",
            "Iteration 5, loss = 1.62922712\n",
            "Iteration 6, loss = 1.62796291\n",
            "Iteration 7, loss = 1.62675801\n",
            "Iteration 8, loss = 1.62535565\n",
            "Iteration 9, loss = 1.62407836\n",
            "Iteration 10, loss = 1.62299533\n",
            "Iteration 11, loss = 1.62191705\n",
            "Iteration 12, loss = 1.62075030\n",
            "Iteration 13, loss = 1.61968723\n",
            "Iteration 14, loss = 1.61878895\n",
            "Iteration 15, loss = 1.61783309\n",
            "Iteration 16, loss = 1.61687580\n",
            "Iteration 17, loss = 1.61590615\n",
            "Iteration 18, loss = 1.61512765\n",
            "Iteration 19, loss = 1.61434873\n",
            "Iteration 20, loss = 1.61368645\n",
            "Iteration 21, loss = 1.61289533\n",
            "Iteration 22, loss = 1.61222169\n",
            "Iteration 23, loss = 1.61150820\n",
            "Iteration 24, loss = 1.61090740\n",
            "Iteration 25, loss = 1.61027532\n",
            "Iteration 26, loss = 1.60965444\n",
            "Iteration 27, loss = 1.60901860\n",
            "Iteration 28, loss = 1.60840610\n",
            "Iteration 29, loss = 1.60795861\n",
            "Iteration 30, loss = 1.60745405\n",
            "Iteration 31, loss = 1.60692100\n",
            "Iteration 32, loss = 1.60642257\n",
            "Iteration 33, loss = 1.60602701\n",
            "Iteration 34, loss = 1.60559940\n",
            "Iteration 35, loss = 1.60512314\n",
            "Iteration 36, loss = 1.60472886\n",
            "Iteration 37, loss = 1.60424292\n",
            "Iteration 38, loss = 1.60388712\n",
            "Iteration 39, loss = 1.60348907\n",
            "Iteration 40, loss = 1.60304869\n",
            "Iteration 41, loss = 1.60275807\n",
            "Iteration 42, loss = 1.60231254\n",
            "Iteration 43, loss = 1.60197384\n",
            "Iteration 44, loss = 1.60158206\n",
            "Iteration 45, loss = 1.60122747\n",
            "Iteration 46, loss = 1.60083093\n",
            "Iteration 47, loss = 1.60050118\n",
            "Iteration 48, loss = 1.60012319\n",
            "Iteration 49, loss = 1.59980391\n",
            "Iteration 50, loss = 1.59947936\n",
            "Iteration 51, loss = 1.59912767\n",
            "Iteration 52, loss = 1.59880712\n",
            "Iteration 53, loss = 1.59854540\n",
            "Iteration 54, loss = 1.59812467\n",
            "Iteration 55, loss = 1.59781394\n",
            "Iteration 56, loss = 1.59750119\n",
            "Iteration 57, loss = 1.59718805\n",
            "Iteration 58, loss = 1.59689292\n",
            "Iteration 59, loss = 1.59651343\n",
            "Iteration 60, loss = 1.59623245\n",
            "Iteration 61, loss = 1.59590526\n",
            "Iteration 62, loss = 1.59559897\n",
            "Iteration 63, loss = 1.59527624\n",
            "Iteration 64, loss = 1.59498622\n",
            "Iteration 65, loss = 1.59468045\n",
            "Iteration 66, loss = 1.59436102\n",
            "Iteration 67, loss = 1.59407166\n",
            "Iteration 68, loss = 1.59375983\n",
            "Iteration 69, loss = 1.59346372\n",
            "Iteration 70, loss = 1.59310590\n",
            "Iteration 71, loss = 1.59280420\n",
            "Iteration 72, loss = 1.59247770\n",
            "Iteration 73, loss = 1.59218779\n",
            "Iteration 74, loss = 1.59187002\n",
            "Iteration 75, loss = 1.59154944\n",
            "Iteration 76, loss = 1.59125325\n",
            "Iteration 77, loss = 1.59091802\n",
            "Iteration 78, loss = 1.59058267\n",
            "Iteration 79, loss = 1.59031216\n",
            "Iteration 80, loss = 1.58996795\n",
            "Iteration 81, loss = 1.58965974\n",
            "Iteration 82, loss = 1.58932172\n",
            "Iteration 83, loss = 1.58900366\n",
            "Iteration 84, loss = 1.58864357\n",
            "Iteration 85, loss = 1.58836445\n",
            "Iteration 86, loss = 1.58798713\n",
            "Iteration 87, loss = 1.58768351\n",
            "Iteration 88, loss = 1.58731880\n",
            "Iteration 89, loss = 1.58699004\n",
            "Iteration 90, loss = 1.58663549\n",
            "Iteration 91, loss = 1.58628534\n",
            "Iteration 92, loss = 1.58594955\n",
            "Iteration 93, loss = 1.58558306\n",
            "Iteration 94, loss = 1.58525614\n",
            "Iteration 95, loss = 1.58484297\n",
            "Iteration 96, loss = 1.58428121\n",
            "Iteration 97, loss = 1.58393557\n",
            "Iteration 98, loss = 1.58345443\n",
            "Iteration 99, loss = 1.58305550\n",
            "Iteration 100, loss = 1.58264429\n",
            "Iteration 1, loss = 1.63234930\n",
            "Iteration 2, loss = 1.62013419\n",
            "Iteration 3, loss = 1.61124383\n",
            "Iteration 4, loss = 1.60601977\n",
            "Iteration 5, loss = 1.60152880\n",
            "Iteration 6, loss = 1.59884783\n",
            "Iteration 7, loss = 1.59716187\n",
            "Iteration 8, loss = 1.59490128\n",
            "Iteration 9, loss = 1.59261203\n",
            "Iteration 10, loss = 1.59058486\n",
            "Iteration 11, loss = 1.58854089\n",
            "Iteration 12, loss = 1.58584187\n",
            "Iteration 13, loss = 1.58323827\n",
            "Iteration 14, loss = 1.58008635\n",
            "Iteration 15, loss = 1.57675596\n",
            "Iteration 16, loss = 1.57356519\n",
            "Iteration 17, loss = 1.56996021\n",
            "Iteration 18, loss = 1.56567545\n",
            "Iteration 19, loss = 1.56125365\n",
            "Iteration 20, loss = 1.55661469\n",
            "Iteration 21, loss = 1.55134615\n",
            "Iteration 22, loss = 1.54609364\n",
            "Iteration 23, loss = 1.53954473\n",
            "Iteration 24, loss = 1.53336457\n",
            "Iteration 25, loss = 1.52620937\n",
            "Iteration 26, loss = 1.51848572\n",
            "Iteration 27, loss = 1.51056815\n",
            "Iteration 28, loss = 1.50225928\n",
            "Iteration 29, loss = 1.49348488\n",
            "Iteration 30, loss = 1.48462977\n",
            "Iteration 31, loss = 1.47399439\n",
            "Iteration 32, loss = 1.46366911\n",
            "Iteration 33, loss = 1.45320005\n",
            "Iteration 34, loss = 1.44242279\n",
            "Iteration 35, loss = 1.43092959\n",
            "Iteration 36, loss = 1.41948186\n",
            "Iteration 37, loss = 1.40762822\n",
            "Iteration 38, loss = 1.39581980\n",
            "Iteration 39, loss = 1.38388132\n",
            "Iteration 40, loss = 1.37179920\n",
            "Iteration 41, loss = 1.36063224\n",
            "Iteration 42, loss = 1.34876120\n",
            "Iteration 43, loss = 1.33733468\n",
            "Iteration 44, loss = 1.32637096\n",
            "Iteration 45, loss = 1.31533582\n",
            "Iteration 46, loss = 1.30501638\n",
            "Iteration 47, loss = 1.29478022\n",
            "Iteration 48, loss = 1.28514340\n",
            "Iteration 49, loss = 1.27585012\n",
            "Iteration 50, loss = 1.26747217\n",
            "Iteration 51, loss = 1.25912700\n",
            "Iteration 52, loss = 1.25094028\n",
            "Iteration 53, loss = 1.24386654\n",
            "Iteration 54, loss = 1.23672054\n",
            "Iteration 55, loss = 1.22984738\n",
            "Iteration 56, loss = 1.22338753\n",
            "Iteration 57, loss = 1.21765034\n",
            "Iteration 58, loss = 1.21215330\n",
            "Iteration 59, loss = 1.20707753\n",
            "Iteration 60, loss = 1.20191043\n",
            "Iteration 61, loss = 1.19710460\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 62, loss = 1.19275695\n",
            "Iteration 63, loss = 1.18844142\n",
            "Iteration 64, loss = 1.18459628\n",
            "Iteration 65, loss = 1.18109404\n",
            "Iteration 66, loss = 1.17782737\n",
            "Iteration 67, loss = 1.17427752\n",
            "Iteration 68, loss = 1.17152560\n",
            "Iteration 69, loss = 1.16836163\n",
            "Iteration 70, loss = 1.16517528\n",
            "Iteration 71, loss = 1.16299921\n",
            "Iteration 72, loss = 1.15995818\n",
            "Iteration 73, loss = 1.15830023\n",
            "Iteration 74, loss = 1.15574746\n",
            "Iteration 75, loss = 1.15340784\n",
            "Iteration 76, loss = 1.15118023\n",
            "Iteration 77, loss = 1.14905360\n",
            "Iteration 78, loss = 1.14721361\n",
            "Iteration 79, loss = 1.14572048\n",
            "Iteration 80, loss = 1.14392817\n",
            "Iteration 81, loss = 1.14228872\n",
            "Iteration 82, loss = 1.14027721\n",
            "Iteration 83, loss = 1.13926765\n",
            "Iteration 84, loss = 1.13711319\n",
            "Iteration 85, loss = 1.13687508\n",
            "Iteration 86, loss = 1.13463447\n",
            "Iteration 87, loss = 1.13316643\n",
            "Iteration 88, loss = 1.13185446\n",
            "Iteration 89, loss = 1.13040981\n",
            "Iteration 90, loss = 1.12935673\n",
            "Iteration 91, loss = 1.12792718\n",
            "Iteration 92, loss = 1.12699799\n",
            "Iteration 93, loss = 1.12649784\n",
            "Iteration 94, loss = 1.12502675\n",
            "Iteration 95, loss = 1.12413359\n",
            "Iteration 96, loss = 1.12226206\n",
            "Iteration 97, loss = 1.12167657\n",
            "Iteration 98, loss = 1.12037282\n",
            "Iteration 99, loss = 1.11948203\n",
            "Iteration 100, loss = 1.11851591\n",
            "Iteration 1, loss = 1.62154928\n",
            "Iteration 2, loss = 1.60047912\n",
            "Iteration 3, loss = 1.59494693\n",
            "Iteration 4, loss = 1.57371795\n",
            "Iteration 5, loss = 1.55019585\n",
            "Iteration 6, loss = 1.50578365\n",
            "Iteration 7, loss = 1.45759142\n",
            "Iteration 8, loss = 1.39469561\n",
            "Iteration 9, loss = 1.33401489\n",
            "Iteration 10, loss = 1.26831760\n",
            "Iteration 11, loss = 1.22867402\n",
            "Iteration 12, loss = 1.19128435\n",
            "Iteration 13, loss = 1.16987928\n",
            "Iteration 14, loss = 1.15260247\n",
            "Iteration 15, loss = 1.13730740\n",
            "Iteration 16, loss = 1.12571185\n",
            "Iteration 17, loss = 1.12287860\n",
            "Iteration 18, loss = 1.11265459\n",
            "Iteration 19, loss = 1.11050003\n",
            "Iteration 20, loss = 1.10539141\n",
            "Iteration 21, loss = 1.10492580\n",
            "Iteration 22, loss = 1.09787735\n",
            "Iteration 23, loss = 1.09666006\n",
            "Iteration 24, loss = 1.09484494\n",
            "Iteration 25, loss = 1.08797209\n",
            "Iteration 26, loss = 1.08626624\n",
            "Iteration 27, loss = 1.08282044\n",
            "Iteration 28, loss = 1.08846221\n",
            "Iteration 29, loss = 1.07723052\n",
            "Iteration 30, loss = 1.07832231\n",
            "Iteration 31, loss = 1.09454150\n",
            "Iteration 32, loss = 1.07686173\n",
            "Iteration 33, loss = 1.08303597\n",
            "Iteration 34, loss = 1.07448836\n",
            "Iteration 35, loss = 1.07058811\n",
            "Iteration 36, loss = 1.07347900\n",
            "Iteration 37, loss = 1.07102059\n",
            "Iteration 38, loss = 1.06153351\n",
            "Iteration 39, loss = 1.05758363\n",
            "Iteration 40, loss = 1.05389887\n",
            "Iteration 41, loss = 1.05186185\n",
            "Iteration 42, loss = 1.04389544\n",
            "Iteration 43, loss = 1.04423649\n",
            "Iteration 44, loss = 1.04379693\n",
            "Iteration 45, loss = 1.03664838\n",
            "Iteration 46, loss = 1.03159542\n",
            "Iteration 47, loss = 1.03318813\n",
            "Iteration 48, loss = 1.02470160\n",
            "Iteration 49, loss = 1.02249018\n",
            "Iteration 50, loss = 1.02478848\n",
            "Iteration 51, loss = 1.00899362\n",
            "Iteration 52, loss = 1.02103585\n",
            "Iteration 53, loss = 1.00754817\n",
            "Iteration 54, loss = 1.00984200\n",
            "Iteration 55, loss = 0.99707455\n",
            "Iteration 56, loss = 0.99662174\n",
            "Iteration 57, loss = 0.99375267\n",
            "Iteration 58, loss = 0.98321951\n",
            "Iteration 59, loss = 0.98566976\n",
            "Iteration 60, loss = 0.97000636\n",
            "Iteration 61, loss = 0.97294046\n",
            "Iteration 62, loss = 0.96089116\n",
            "Iteration 63, loss = 0.95787218\n",
            "Iteration 64, loss = 0.94991207\n",
            "Iteration 65, loss = 0.94785683\n",
            "Iteration 66, loss = 0.94221293\n",
            "Iteration 67, loss = 0.93793017\n",
            "Iteration 68, loss = 0.93243034\n",
            "Iteration 69, loss = 0.93388058\n",
            "Iteration 70, loss = 0.93446165\n",
            "Iteration 71, loss = 0.94213995\n",
            "Iteration 72, loss = 0.93469684\n",
            "Iteration 73, loss = 0.94793831\n",
            "Iteration 74, loss = 0.91696120\n",
            "Iteration 75, loss = 0.92876255\n",
            "Iteration 76, loss = 0.91173328\n",
            "Iteration 77, loss = 0.90157115\n",
            "Iteration 78, loss = 0.90261598\n",
            "Iteration 79, loss = 0.89797886\n",
            "Iteration 80, loss = 0.88687869\n",
            "Iteration 81, loss = 0.88108087\n",
            "Iteration 82, loss = 0.88072497\n",
            "Iteration 83, loss = 0.88556041\n",
            "Iteration 84, loss = 0.87105748\n",
            "Iteration 85, loss = 0.89102144\n",
            "Iteration 86, loss = 0.89024302\n",
            "Iteration 87, loss = 0.89258616\n",
            "Iteration 88, loss = 0.87752291\n",
            "Iteration 89, loss = 0.87749811\n",
            "Iteration 90, loss = 0.86832709\n",
            "Iteration 91, loss = 0.86565680\n",
            "Iteration 92, loss = 0.87548976\n",
            "Iteration 93, loss = 0.86745708\n",
            "Iteration 94, loss = 0.86830181\n",
            "Iteration 95, loss = 0.86500497\n",
            "Iteration 96, loss = 0.84814321\n",
            "Iteration 97, loss = 0.84763877\n",
            "Iteration 98, loss = 0.84809612\n",
            "Iteration 99, loss = 0.84748665\n",
            "Iteration 100, loss = 0.83524141\n",
            "Iteration 1, loss = 1.65308931\n",
            "Iteration 2, loss = 1.64945469\n",
            "Iteration 3, loss = 1.64600089\n",
            "Iteration 4, loss = 1.64286777\n",
            "Iteration 5, loss = 1.63991642\n",
            "Iteration 6, loss = 1.63694222\n",
            "Iteration 7, loss = 1.63420792\n",
            "Iteration 8, loss = 1.63136762\n",
            "Iteration 9, loss = 1.62911366\n",
            "Iteration 10, loss = 1.62692525\n",
            "Iteration 11, loss = 1.62479796\n",
            "Iteration 12, loss = 1.62281381\n",
            "Iteration 13, loss = 1.62070033\n",
            "Iteration 14, loss = 1.61922333\n",
            "Iteration 15, loss = 1.61747732\n",
            "Iteration 16, loss = 1.61612631\n",
            "Iteration 17, loss = 1.61479142\n",
            "Iteration 18, loss = 1.61327558\n",
            "Iteration 19, loss = 1.61228220\n",
            "Iteration 20, loss = 1.61096835\n",
            "Iteration 21, loss = 1.61018253\n",
            "Iteration 22, loss = 1.60912127\n",
            "Iteration 23, loss = 1.60828618\n",
            "Iteration 24, loss = 1.60743827\n",
            "Iteration 25, loss = 1.60666892\n",
            "Iteration 26, loss = 1.60597180\n",
            "Iteration 27, loss = 1.60529650\n",
            "Iteration 28, loss = 1.60475100\n",
            "Iteration 29, loss = 1.60416609\n",
            "Iteration 30, loss = 1.60358891\n",
            "Iteration 31, loss = 1.60298956\n",
            "Iteration 32, loss = 1.60250934\n",
            "Iteration 33, loss = 1.60194481\n",
            "Iteration 34, loss = 1.60148657\n",
            "Iteration 35, loss = 1.60102286\n",
            "Iteration 36, loss = 1.60049057\n",
            "Iteration 37, loss = 1.60003422\n",
            "Iteration 38, loss = 1.59969221\n",
            "Iteration 39, loss = 1.59921539\n",
            "Iteration 40, loss = 1.59870000\n",
            "Iteration 41, loss = 1.59827462\n",
            "Iteration 42, loss = 1.59785910\n",
            "Iteration 43, loss = 1.59747835\n",
            "Iteration 44, loss = 1.59702274\n",
            "Iteration 45, loss = 1.59663163\n",
            "Iteration 46, loss = 1.59615420\n",
            "Iteration 47, loss = 1.59573590\n",
            "Iteration 48, loss = 1.59530060\n",
            "Iteration 49, loss = 1.59488683\n",
            "Iteration 50, loss = 1.59447585\n",
            "Iteration 51, loss = 1.59405783\n",
            "Iteration 52, loss = 1.59368267\n",
            "Iteration 53, loss = 1.59318952\n",
            "Iteration 54, loss = 1.59278639\n",
            "Iteration 55, loss = 1.59234124\n",
            "Iteration 56, loss = 1.59191129\n",
            "Iteration 57, loss = 1.59151982\n",
            "Iteration 58, loss = 1.59105676\n",
            "Iteration 59, loss = 1.59061463\n",
            "Iteration 60, loss = 1.59019252\n",
            "Iteration 61, loss = 1.58977919\n",
            "Iteration 62, loss = 1.58933530\n",
            "Iteration 63, loss = 1.58886421\n",
            "Iteration 64, loss = 1.58845206\n",
            "Iteration 65, loss = 1.58796057\n",
            "Iteration 66, loss = 1.58754501\n",
            "Iteration 67, loss = 1.58704948\n",
            "Iteration 68, loss = 1.58659141\n",
            "Iteration 69, loss = 1.58613248\n",
            "Iteration 70, loss = 1.58573167\n",
            "Iteration 71, loss = 1.58517488\n",
            "Iteration 72, loss = 1.58470364\n",
            "Iteration 73, loss = 1.58421051\n",
            "Iteration 74, loss = 1.58372689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 75, loss = 1.58326573\n",
            "Iteration 76, loss = 1.58274327\n",
            "Iteration 77, loss = 1.58222459\n",
            "Iteration 78, loss = 1.58174118\n",
            "Iteration 79, loss = 1.58122985\n",
            "Iteration 80, loss = 1.58066637\n",
            "Iteration 81, loss = 1.58019950\n",
            "Iteration 82, loss = 1.57965383\n",
            "Iteration 83, loss = 1.57913246\n",
            "Iteration 84, loss = 1.57855508\n",
            "Iteration 85, loss = 1.57798308\n",
            "Iteration 86, loss = 1.57742424\n",
            "Iteration 87, loss = 1.57687609\n",
            "Iteration 88, loss = 1.57629519\n",
            "Iteration 89, loss = 1.57571942\n",
            "Iteration 90, loss = 1.57512211\n",
            "Iteration 91, loss = 1.57452093\n",
            "Iteration 92, loss = 1.57394868\n",
            "Iteration 93, loss = 1.57330888\n",
            "Iteration 94, loss = 1.57268276\n",
            "Iteration 95, loss = 1.57208955\n",
            "Iteration 96, loss = 1.57144641\n",
            "Iteration 97, loss = 1.57083945\n",
            "Iteration 98, loss = 1.57013468\n",
            "Iteration 99, loss = 1.56952821\n",
            "Iteration 100, loss = 1.56881180\n",
            "Iteration 1, loss = 1.64626883\n",
            "Iteration 2, loss = 1.61941665\n",
            "Iteration 3, loss = 1.60681963\n",
            "Iteration 4, loss = 1.60517268\n",
            "Iteration 5, loss = 1.60562874\n",
            "Iteration 6, loss = 1.60096014\n",
            "Iteration 7, loss = 1.59588127\n",
            "Iteration 8, loss = 1.59002459\n",
            "Iteration 9, loss = 1.58560716\n",
            "Iteration 10, loss = 1.58253797\n",
            "Iteration 11, loss = 1.57902011\n",
            "Iteration 12, loss = 1.57546873\n",
            "Iteration 13, loss = 1.57031660\n",
            "Iteration 14, loss = 1.56504577\n",
            "Iteration 15, loss = 1.55930768\n",
            "Iteration 16, loss = 1.55355474\n",
            "Iteration 17, loss = 1.54755952\n",
            "Iteration 18, loss = 1.53960502\n",
            "Iteration 19, loss = 1.53127064\n",
            "Iteration 20, loss = 1.52280709\n",
            "Iteration 21, loss = 1.51299470\n",
            "Iteration 22, loss = 1.50295906\n",
            "Iteration 23, loss = 1.49154657\n",
            "Iteration 24, loss = 1.47979171\n",
            "Iteration 25, loss = 1.46769628\n",
            "Iteration 26, loss = 1.45504796\n",
            "Iteration 27, loss = 1.44119644\n",
            "Iteration 28, loss = 1.42727352\n",
            "Iteration 29, loss = 1.41247255\n",
            "Iteration 30, loss = 1.39806115\n",
            "Iteration 31, loss = 1.38308416\n",
            "Iteration 32, loss = 1.36766852\n",
            "Iteration 33, loss = 1.35340219\n",
            "Iteration 34, loss = 1.33909596\n",
            "Iteration 35, loss = 1.32510114\n",
            "Iteration 36, loss = 1.31107556\n",
            "Iteration 37, loss = 1.29780379\n",
            "Iteration 38, loss = 1.28647622\n",
            "Iteration 39, loss = 1.27388658\n",
            "Iteration 40, loss = 1.26314304\n",
            "Iteration 41, loss = 1.25322864\n",
            "Iteration 42, loss = 1.24291088\n",
            "Iteration 43, loss = 1.23396254\n",
            "Iteration 44, loss = 1.22596271\n",
            "Iteration 45, loss = 1.21822439\n",
            "Iteration 46, loss = 1.21056711\n",
            "Iteration 47, loss = 1.20398586\n",
            "Iteration 48, loss = 1.19808899\n",
            "Iteration 49, loss = 1.19283873\n",
            "Iteration 50, loss = 1.18631548\n",
            "Iteration 51, loss = 1.18198915\n",
            "Iteration 52, loss = 1.17825892\n",
            "Iteration 53, loss = 1.17301662\n",
            "Iteration 54, loss = 1.16937886\n",
            "Iteration 55, loss = 1.16596925\n",
            "Iteration 56, loss = 1.16169678\n",
            "Iteration 57, loss = 1.15861938\n",
            "Iteration 58, loss = 1.15559450\n",
            "Iteration 59, loss = 1.15268563\n",
            "Iteration 60, loss = 1.15075580\n",
            "Iteration 61, loss = 1.14731959\n",
            "Iteration 62, loss = 1.14497559\n",
            "Iteration 63, loss = 1.14238436\n",
            "Iteration 64, loss = 1.14056390\n",
            "Iteration 65, loss = 1.13824222\n",
            "Iteration 66, loss = 1.13660219\n",
            "Iteration 67, loss = 1.13454456\n",
            "Iteration 68, loss = 1.13264759\n",
            "Iteration 69, loss = 1.13147291\n",
            "Iteration 70, loss = 1.13018317\n",
            "Iteration 71, loss = 1.12805643\n",
            "Iteration 72, loss = 1.12680709\n",
            "Iteration 73, loss = 1.12536867\n",
            "Iteration 74, loss = 1.12350352\n",
            "Iteration 75, loss = 1.12245749\n",
            "Iteration 76, loss = 1.12133575\n",
            "Iteration 77, loss = 1.12029503\n",
            "Iteration 78, loss = 1.11912745\n",
            "Iteration 79, loss = 1.11751399\n",
            "Iteration 80, loss = 1.11635598\n",
            "Iteration 81, loss = 1.11608733\n",
            "Iteration 82, loss = 1.11592997\n",
            "Iteration 83, loss = 1.11409616\n",
            "Iteration 84, loss = 1.11241539\n",
            "Iteration 85, loss = 1.11139059\n",
            "Iteration 86, loss = 1.11052658\n",
            "Iteration 87, loss = 1.10993845\n",
            "Iteration 88, loss = 1.10892212\n",
            "Iteration 89, loss = 1.10755730\n",
            "Iteration 90, loss = 1.10676401\n",
            "Iteration 91, loss = 1.10600115\n",
            "Iteration 92, loss = 1.10549511\n",
            "Iteration 93, loss = 1.10423832\n",
            "Iteration 94, loss = 1.10333257\n",
            "Iteration 95, loss = 1.10278463\n",
            "Iteration 96, loss = 1.10208726\n",
            "Iteration 97, loss = 1.10172640\n",
            "Iteration 98, loss = 1.10008764\n",
            "Iteration 99, loss = 1.09966635\n",
            "Iteration 100, loss = 1.09969395\n",
            "Iteration 1, loss = 1.66792933\n",
            "Iteration 2, loss = 1.59972462\n",
            "Iteration 3, loss = 1.56736357\n",
            "Iteration 4, loss = 1.49519685\n",
            "Iteration 5, loss = 1.42952600\n",
            "Iteration 6, loss = 1.33027288\n",
            "Iteration 7, loss = 1.27208057\n",
            "Iteration 8, loss = 1.22030457\n",
            "Iteration 9, loss = 1.16640301\n",
            "Iteration 10, loss = 1.15742979\n",
            "Iteration 11, loss = 1.13583990\n",
            "Iteration 12, loss = 1.12410095\n",
            "Iteration 13, loss = 1.12209108\n",
            "Iteration 14, loss = 1.12126661\n",
            "Iteration 15, loss = 1.12822476\n",
            "Iteration 16, loss = 1.12435942\n",
            "Iteration 17, loss = 1.11499891\n",
            "Iteration 18, loss = 1.12148882\n",
            "Iteration 19, loss = 1.13701974\n",
            "Iteration 20, loss = 1.11268052\n",
            "Iteration 21, loss = 1.12351294\n",
            "Iteration 22, loss = 1.11398903\n",
            "Iteration 23, loss = 1.11077285\n",
            "Iteration 24, loss = 1.09868702\n",
            "Iteration 25, loss = 1.10442822\n",
            "Iteration 26, loss = 1.09778599\n",
            "Iteration 27, loss = 1.09220562\n",
            "Iteration 28, loss = 1.09250978\n",
            "Iteration 29, loss = 1.08935033\n",
            "Iteration 30, loss = 1.09475379\n",
            "Iteration 31, loss = 1.08646158\n",
            "Iteration 32, loss = 1.07612451\n",
            "Iteration 33, loss = 1.08295541\n",
            "Iteration 34, loss = 1.07095307\n",
            "Iteration 35, loss = 1.06686872\n",
            "Iteration 36, loss = 1.05543948\n",
            "Iteration 37, loss = 1.05236698\n",
            "Iteration 38, loss = 1.05275855\n",
            "Iteration 39, loss = 1.04154580\n",
            "Iteration 40, loss = 1.02766531\n",
            "Iteration 41, loss = 1.01912321\n",
            "Iteration 42, loss = 1.01150182\n",
            "Iteration 43, loss = 1.05537935\n",
            "Iteration 44, loss = 0.99218901\n",
            "Iteration 45, loss = 0.98203092\n",
            "Iteration 46, loss = 1.00227157\n",
            "Iteration 47, loss = 0.97444692\n",
            "Iteration 48, loss = 0.96262124\n",
            "Iteration 49, loss = 0.96998546\n",
            "Iteration 50, loss = 0.97063235\n",
            "Iteration 51, loss = 1.00147564\n",
            "Iteration 52, loss = 0.99307726\n",
            "Iteration 53, loss = 0.93315896\n",
            "Iteration 54, loss = 0.97664001\n",
            "Iteration 55, loss = 0.96772582\n",
            "Iteration 56, loss = 0.91439956\n",
            "Iteration 57, loss = 0.91892777\n",
            "Iteration 58, loss = 0.92858208\n",
            "Iteration 59, loss = 0.95178813\n",
            "Iteration 60, loss = 0.90889997\n",
            "Iteration 61, loss = 0.92029864\n",
            "Iteration 62, loss = 0.89358548\n",
            "Iteration 63, loss = 0.87968082\n",
            "Iteration 64, loss = 0.86461866\n",
            "Iteration 65, loss = 0.86158953\n",
            "Iteration 66, loss = 0.87736959\n",
            "Iteration 67, loss = 0.87117811\n",
            "Iteration 68, loss = 0.85146930\n",
            "Iteration 69, loss = 0.87471445\n",
            "Iteration 70, loss = 0.88016241\n",
            "Iteration 71, loss = 0.92465671\n",
            "Iteration 72, loss = 0.87892769\n",
            "Iteration 73, loss = 0.88200146\n",
            "Iteration 74, loss = 0.88848153\n",
            "Iteration 75, loss = 0.92090195\n",
            "Iteration 76, loss = 0.87751801\n",
            "Iteration 77, loss = 0.83929584\n",
            "Iteration 78, loss = 0.84105606\n",
            "Iteration 79, loss = 0.81876410\n",
            "Iteration 80, loss = 0.82042554\n",
            "Iteration 81, loss = 0.86246163\n",
            "Iteration 82, loss = 0.86443483\n",
            "Iteration 83, loss = 0.82832559\n",
            "Iteration 84, loss = 0.85279765\n",
            "Iteration 85, loss = 0.86390106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 86, loss = 0.88952109\n",
            "Iteration 87, loss = 0.82165846\n",
            "Iteration 88, loss = 0.84752681\n",
            "Iteration 89, loss = 0.80888168\n",
            "Iteration 90, loss = 0.80092237\n",
            "Iteration 91, loss = 0.80305102\n",
            "Iteration 92, loss = 0.79725310\n",
            "Iteration 93, loss = 0.80399337\n",
            "Iteration 94, loss = 0.78835915\n",
            "Iteration 95, loss = 0.79758935\n",
            "Iteration 96, loss = 0.79031446\n",
            "Iteration 97, loss = 0.79560315\n",
            "Iteration 98, loss = 0.79140168\n",
            "Iteration 99, loss = 0.77701384\n",
            "Iteration 100, loss = 0.76620280\n",
            "Iteration 1, loss = 1.63610989\n",
            "Iteration 2, loss = 1.63204518\n",
            "Iteration 3, loss = 1.62833570\n",
            "Iteration 4, loss = 1.62484658\n",
            "Iteration 5, loss = 1.62157114\n",
            "Iteration 6, loss = 1.61897080\n",
            "Iteration 7, loss = 1.61645903\n",
            "Iteration 8, loss = 1.61450763\n",
            "Iteration 9, loss = 1.61222044\n",
            "Iteration 10, loss = 1.61031258\n",
            "Iteration 11, loss = 1.60866438\n",
            "Iteration 12, loss = 1.60750042\n",
            "Iteration 13, loss = 1.60618456\n",
            "Iteration 14, loss = 1.60483517\n",
            "Iteration 15, loss = 1.60406646\n",
            "Iteration 16, loss = 1.60329997\n",
            "Iteration 17, loss = 1.60235646\n",
            "Iteration 18, loss = 1.60195311\n",
            "Iteration 19, loss = 1.60135594\n",
            "Iteration 20, loss = 1.60095544\n",
            "Iteration 21, loss = 1.60053922\n",
            "Iteration 22, loss = 1.60016955\n",
            "Iteration 23, loss = 1.59984810\n",
            "Iteration 24, loss = 1.59969499\n",
            "Iteration 25, loss = 1.59944373\n",
            "Iteration 26, loss = 1.59942259\n",
            "Iteration 27, loss = 1.59919988\n",
            "Iteration 28, loss = 1.59905196\n",
            "Iteration 29, loss = 1.59896011\n",
            "Iteration 30, loss = 1.59895364\n",
            "Iteration 31, loss = 1.59877240\n",
            "Iteration 32, loss = 1.59873761\n",
            "Iteration 33, loss = 1.59868544\n",
            "Iteration 34, loss = 1.59863211\n",
            "Iteration 35, loss = 1.59854934\n",
            "Iteration 36, loss = 1.59850064\n",
            "Iteration 37, loss = 1.59843045\n",
            "Iteration 38, loss = 1.59847056\n",
            "Iteration 39, loss = 1.59839160\n",
            "Iteration 40, loss = 1.59828371\n",
            "Iteration 41, loss = 1.59823438\n",
            "Iteration 42, loss = 1.59818272\n",
            "Iteration 43, loss = 1.59817790\n",
            "Iteration 44, loss = 1.59809730\n",
            "Iteration 45, loss = 1.59809923\n",
            "Iteration 46, loss = 1.59799638\n",
            "Iteration 47, loss = 1.59794760\n",
            "Iteration 48, loss = 1.59788996\n",
            "Iteration 49, loss = 1.59785973\n",
            "Iteration 50, loss = 1.59782607\n",
            "Iteration 51, loss = 1.59777197\n",
            "Iteration 52, loss = 1.59776247\n",
            "Iteration 53, loss = 1.59765334\n",
            "Iteration 54, loss = 1.59763320\n",
            "Iteration 55, loss = 1.59755107\n",
            "Iteration 56, loss = 1.59749645\n",
            "Iteration 57, loss = 1.59748822\n",
            "Iteration 58, loss = 1.59740633\n",
            "Iteration 59, loss = 1.59734432\n",
            "Iteration 60, loss = 1.59731395\n",
            "Iteration 61, loss = 1.59729511\n",
            "Iteration 62, loss = 1.59722005\n",
            "Iteration 63, loss = 1.59714967\n",
            "Iteration 64, loss = 1.59715178\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.62989625\n",
            "Iteration 2, loss = 1.60606675\n",
            "Iteration 3, loss = 1.60124800\n",
            "Iteration 4, loss = 1.60298785\n",
            "Iteration 5, loss = 1.60498696\n",
            "Iteration 6, loss = 1.60331584\n",
            "Iteration 7, loss = 1.60227875\n",
            "Iteration 8, loss = 1.59778710\n",
            "Iteration 9, loss = 1.59672526\n",
            "Iteration 10, loss = 1.59648975\n",
            "Iteration 11, loss = 1.59665404\n",
            "Iteration 12, loss = 1.59646328\n",
            "Iteration 13, loss = 1.59589037\n",
            "Iteration 14, loss = 1.59448604\n",
            "Iteration 15, loss = 1.59366178\n",
            "Iteration 16, loss = 1.59329690\n",
            "Iteration 17, loss = 1.59303779\n",
            "Iteration 18, loss = 1.59194398\n",
            "Iteration 19, loss = 1.59118214\n",
            "Iteration 20, loss = 1.59044578\n",
            "Iteration 21, loss = 1.58956004\n",
            "Iteration 22, loss = 1.58910993\n",
            "Iteration 23, loss = 1.58793508\n",
            "Iteration 24, loss = 1.58683079\n",
            "Iteration 25, loss = 1.58578774\n",
            "Iteration 26, loss = 1.58487404\n",
            "Iteration 27, loss = 1.58333679\n",
            "Iteration 28, loss = 1.58232423\n",
            "Iteration 29, loss = 1.58095154\n",
            "Iteration 30, loss = 1.57982511\n",
            "Iteration 31, loss = 1.57827018\n",
            "Iteration 32, loss = 1.57622733\n",
            "Iteration 33, loss = 1.57453846\n",
            "Iteration 34, loss = 1.57276355\n",
            "Iteration 35, loss = 1.57053128\n",
            "Iteration 36, loss = 1.56846663\n",
            "Iteration 37, loss = 1.56606257\n",
            "Iteration 38, loss = 1.56445617\n",
            "Iteration 39, loss = 1.56161563\n",
            "Iteration 40, loss = 1.55839410\n",
            "Iteration 41, loss = 1.55549905\n",
            "Iteration 42, loss = 1.55245473\n",
            "Iteration 43, loss = 1.54971215\n",
            "Iteration 44, loss = 1.54600148\n",
            "Iteration 45, loss = 1.54288069\n",
            "Iteration 46, loss = 1.53871008\n",
            "Iteration 47, loss = 1.53473092\n",
            "Iteration 48, loss = 1.53055033\n",
            "Iteration 49, loss = 1.52647755\n",
            "Iteration 50, loss = 1.52209482\n",
            "Iteration 51, loss = 1.51725503\n",
            "Iteration 52, loss = 1.51308070\n",
            "Iteration 53, loss = 1.50716045\n",
            "Iteration 54, loss = 1.50246690\n",
            "Iteration 55, loss = 1.49693259\n",
            "Iteration 56, loss = 1.49134567\n",
            "Iteration 57, loss = 1.48600107\n",
            "Iteration 58, loss = 1.47980197\n",
            "Iteration 59, loss = 1.47353025\n",
            "Iteration 60, loss = 1.46741780\n",
            "Iteration 61, loss = 1.46131892\n",
            "Iteration 62, loss = 1.45501796\n",
            "Iteration 63, loss = 1.44810576\n",
            "Iteration 64, loss = 1.44159131\n",
            "Iteration 65, loss = 1.43461526\n",
            "Iteration 66, loss = 1.42848603\n",
            "Iteration 67, loss = 1.42137459\n",
            "Iteration 68, loss = 1.41432751\n",
            "Iteration 69, loss = 1.40772498\n",
            "Iteration 70, loss = 1.40127250\n",
            "Iteration 71, loss = 1.39346834\n",
            "Iteration 72, loss = 1.38655015\n",
            "Iteration 73, loss = 1.37945565\n",
            "Iteration 74, loss = 1.37281752\n",
            "Iteration 75, loss = 1.36643333\n",
            "Iteration 76, loss = 1.35951066\n",
            "Iteration 77, loss = 1.35258209\n",
            "Iteration 78, loss = 1.34632471\n",
            "Iteration 79, loss = 1.33968609\n",
            "Iteration 80, loss = 1.33312715\n",
            "Iteration 81, loss = 1.32785363\n",
            "Iteration 82, loss = 1.32155267\n",
            "Iteration 83, loss = 1.31564586\n",
            "Iteration 84, loss = 1.30965621\n",
            "Iteration 85, loss = 1.30396800\n",
            "Iteration 86, loss = 1.29848584\n",
            "Iteration 87, loss = 1.29333278\n",
            "Iteration 88, loss = 1.28813440\n",
            "Iteration 89, loss = 1.28275917\n",
            "Iteration 90, loss = 1.27767450\n",
            "Iteration 91, loss = 1.27313074\n",
            "Iteration 92, loss = 1.26873532\n",
            "Iteration 93, loss = 1.26407226\n",
            "Iteration 94, loss = 1.25966283\n",
            "Iteration 95, loss = 1.25579018\n",
            "Iteration 96, loss = 1.25163226\n",
            "Iteration 97, loss = 1.24827643\n",
            "Iteration 98, loss = 1.24386603\n",
            "Iteration 99, loss = 1.24042573\n",
            "Iteration 100, loss = 1.23704618\n",
            "Iteration 1, loss = 1.70134591\n",
            "Iteration 2, loss = 1.63403476\n",
            "Iteration 3, loss = 1.63593936\n",
            "Iteration 4, loss = 1.62109623\n",
            "Iteration 5, loss = 1.59462367\n",
            "Iteration 6, loss = 1.60207080\n",
            "Iteration 7, loss = 1.60384167\n",
            "Iteration 8, loss = 1.58083428\n",
            "Iteration 9, loss = 1.56858315\n",
            "Iteration 10, loss = 1.56130350\n",
            "Iteration 11, loss = 1.54842711\n",
            "Iteration 12, loss = 1.52639267\n",
            "Iteration 13, loss = 1.49898349\n",
            "Iteration 14, loss = 1.46403825\n",
            "Iteration 15, loss = 1.42560657\n",
            "Iteration 16, loss = 1.38090441\n",
            "Iteration 17, loss = 1.34158283\n",
            "Iteration 18, loss = 1.29902932\n",
            "Iteration 19, loss = 1.26151975\n",
            "Iteration 20, loss = 1.22953744\n",
            "Iteration 21, loss = 1.20713313\n",
            "Iteration 22, loss = 1.18608980\n",
            "Iteration 23, loss = 1.17060813\n",
            "Iteration 24, loss = 1.15689852\n",
            "Iteration 25, loss = 1.14794741\n",
            "Iteration 26, loss = 1.14184939\n",
            "Iteration 27, loss = 1.13379737\n",
            "Iteration 28, loss = 1.13164354\n",
            "Iteration 29, loss = 1.12518132\n",
            "Iteration 30, loss = 1.12743660\n",
            "Iteration 31, loss = 1.12460943\n",
            "Iteration 32, loss = 1.11790231\n",
            "Iteration 33, loss = 1.11708003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 34, loss = 1.11290246\n",
            "Iteration 35, loss = 1.11381769\n",
            "Iteration 36, loss = 1.10990803\n",
            "Iteration 37, loss = 1.10766756\n",
            "Iteration 38, loss = 1.11183253\n",
            "Iteration 39, loss = 1.10950125\n",
            "Iteration 40, loss = 1.10594245\n",
            "Iteration 41, loss = 1.10802130\n",
            "Iteration 42, loss = 1.10368329\n",
            "Iteration 43, loss = 1.10896988\n",
            "Iteration 44, loss = 1.10354797\n",
            "Iteration 45, loss = 1.10644220\n",
            "Iteration 46, loss = 1.10158892\n",
            "Iteration 47, loss = 1.10692750\n",
            "Iteration 48, loss = 1.09907570\n",
            "Iteration 49, loss = 1.10893322\n",
            "Iteration 50, loss = 1.09969178\n",
            "Iteration 51, loss = 1.10463271\n",
            "Iteration 52, loss = 1.10291991\n",
            "Iteration 53, loss = 1.09996583\n",
            "Iteration 54, loss = 1.09889780\n",
            "Iteration 55, loss = 1.09775688\n",
            "Iteration 56, loss = 1.09633131\n",
            "Iteration 57, loss = 1.09595024\n",
            "Iteration 58, loss = 1.09471243\n",
            "Iteration 59, loss = 1.09235345\n",
            "Iteration 60, loss = 1.10010656\n",
            "Iteration 61, loss = 1.09729516\n",
            "Iteration 62, loss = 1.08992183\n",
            "Iteration 63, loss = 1.08959218\n",
            "Iteration 64, loss = 1.08808072\n",
            "Iteration 65, loss = 1.08815404\n",
            "Iteration 66, loss = 1.08972501\n",
            "Iteration 67, loss = 1.08255287\n",
            "Iteration 68, loss = 1.08600416\n",
            "Iteration 69, loss = 1.07876450\n",
            "Iteration 70, loss = 1.08880875\n",
            "Iteration 71, loss = 1.07464103\n",
            "Iteration 72, loss = 1.08023355\n",
            "Iteration 73, loss = 1.07592423\n",
            "Iteration 74, loss = 1.07008268\n",
            "Iteration 75, loss = 1.07012980\n",
            "Iteration 76, loss = 1.07075845\n",
            "Iteration 77, loss = 1.06690200\n",
            "Iteration 78, loss = 1.06500698\n",
            "Iteration 79, loss = 1.05991850\n",
            "Iteration 80, loss = 1.05408958\n",
            "Iteration 81, loss = 1.05669920\n",
            "Iteration 82, loss = 1.04782070\n",
            "Iteration 83, loss = 1.05210362\n",
            "Iteration 84, loss = 1.05087854\n",
            "Iteration 85, loss = 1.03860434\n",
            "Iteration 86, loss = 1.03664486\n",
            "Iteration 87, loss = 1.03510924\n",
            "Iteration 88, loss = 1.02830379\n",
            "Iteration 89, loss = 1.03059377\n",
            "Iteration 90, loss = 1.02077290\n",
            "Iteration 91, loss = 1.01879602\n",
            "Iteration 92, loss = 1.01784310\n",
            "Iteration 93, loss = 1.00969558\n",
            "Iteration 94, loss = 1.00837581\n",
            "Iteration 95, loss = 1.00769624\n",
            "Iteration 96, loss = 0.99776363\n",
            "Iteration 97, loss = 1.00081732\n",
            "Iteration 98, loss = 0.98837197\n",
            "Iteration 99, loss = 0.99510710\n",
            "Iteration 100, loss = 0.98161890\n",
            "Iteration 1, loss = 1.65480829\n",
            "Iteration 2, loss = 1.65125579\n",
            "Iteration 3, loss = 1.64786746\n",
            "Iteration 4, loss = 1.64477680\n",
            "Iteration 5, loss = 1.64184768\n",
            "Iteration 6, loss = 1.63889210\n",
            "Iteration 7, loss = 1.63616853\n",
            "Iteration 8, loss = 1.63332280\n",
            "Iteration 9, loss = 1.63104076\n",
            "Iteration 10, loss = 1.62881767\n",
            "Iteration 11, loss = 1.62664430\n",
            "Iteration 12, loss = 1.62460664\n",
            "Iteration 13, loss = 1.62243579\n",
            "Iteration 14, loss = 1.62088270\n",
            "Iteration 15, loss = 1.61906479\n",
            "Iteration 16, loss = 1.61762820\n",
            "Iteration 17, loss = 1.61620826\n",
            "Iteration 18, loss = 1.61460768\n",
            "Iteration 19, loss = 1.61352776\n",
            "Iteration 20, loss = 1.61212621\n",
            "Iteration 21, loss = 1.61125362\n",
            "Iteration 22, loss = 1.61010072\n",
            "Iteration 23, loss = 1.60919386\n",
            "Iteration 24, loss = 1.60827581\n",
            "Iteration 25, loss = 1.60743385\n",
            "Iteration 26, loss = 1.60668174\n",
            "Iteration 27, loss = 1.60594608\n",
            "Iteration 28, loss = 1.60535327\n",
            "Iteration 29, loss = 1.60472549\n",
            "Iteration 30, loss = 1.60410875\n",
            "Iteration 31, loss = 1.60346290\n",
            "Iteration 32, loss = 1.60297438\n",
            "Iteration 33, loss = 1.60238711\n",
            "Iteration 34, loss = 1.60192112\n",
            "Iteration 35, loss = 1.60144409\n",
            "Iteration 36, loss = 1.60089588\n",
            "Iteration 37, loss = 1.60044376\n",
            "Iteration 38, loss = 1.60010394\n",
            "Iteration 39, loss = 1.59964737\n",
            "Iteration 40, loss = 1.59912910\n",
            "Iteration 41, loss = 1.59871236\n",
            "Iteration 42, loss = 1.59831806\n",
            "Iteration 43, loss = 1.59795593\n",
            "Iteration 44, loss = 1.59750930\n",
            "Iteration 45, loss = 1.59714142\n",
            "Iteration 46, loss = 1.59667829\n",
            "Iteration 47, loss = 1.59628268\n",
            "Iteration 48, loss = 1.59586443\n",
            "Iteration 49, loss = 1.59547132\n",
            "Iteration 50, loss = 1.59507746\n",
            "Iteration 51, loss = 1.59468606\n",
            "Iteration 52, loss = 1.59432623\n",
            "Iteration 53, loss = 1.59386447\n",
            "Iteration 54, loss = 1.59348390\n",
            "Iteration 55, loss = 1.59306215\n",
            "Iteration 56, loss = 1.59265734\n",
            "Iteration 57, loss = 1.59228892\n",
            "Iteration 58, loss = 1.59185116\n",
            "Iteration 59, loss = 1.59143663\n",
            "Iteration 60, loss = 1.59103889\n",
            "Iteration 61, loss = 1.59065332\n",
            "Iteration 62, loss = 1.59023274\n",
            "Iteration 63, loss = 1.58979198\n",
            "Iteration 64, loss = 1.58940757\n",
            "Iteration 65, loss = 1.58894428\n",
            "Iteration 66, loss = 1.58855508\n",
            "Iteration 67, loss = 1.58809119\n",
            "Iteration 68, loss = 1.58766153\n",
            "Iteration 69, loss = 1.58723103\n",
            "Iteration 70, loss = 1.58685509\n",
            "Iteration 71, loss = 1.58633511\n",
            "Iteration 72, loss = 1.58589360\n",
            "Iteration 73, loss = 1.58543182\n",
            "Iteration 74, loss = 1.58497943\n",
            "Iteration 75, loss = 1.58454832\n",
            "Iteration 76, loss = 1.58405842\n",
            "Iteration 77, loss = 1.58357275\n",
            "Iteration 78, loss = 1.58311880\n",
            "Iteration 79, loss = 1.58264320\n",
            "Iteration 80, loss = 1.58211425\n",
            "Iteration 81, loss = 1.58167771\n",
            "Iteration 82, loss = 1.58116602\n",
            "Iteration 83, loss = 1.58067886\n",
            "Iteration 84, loss = 1.58013812\n",
            "Iteration 85, loss = 1.57960263\n",
            "Iteration 86, loss = 1.57907914\n",
            "Iteration 87, loss = 1.57856485\n",
            "Iteration 88, loss = 1.57802111\n",
            "Iteration 89, loss = 1.57748328\n",
            "Iteration 90, loss = 1.57692365\n",
            "Iteration 91, loss = 1.57636068\n",
            "Iteration 92, loss = 1.57582408\n",
            "Iteration 93, loss = 1.57522433\n",
            "Iteration 94, loss = 1.57463754\n",
            "Iteration 95, loss = 1.57408118\n",
            "Iteration 96, loss = 1.57347701\n",
            "Iteration 97, loss = 1.57290663\n",
            "Iteration 98, loss = 1.57224759\n",
            "Iteration 99, loss = 1.57167856\n",
            "Iteration 100, loss = 1.57100496\n",
            "Iteration 1, loss = 1.64813631\n",
            "Iteration 2, loss = 1.62123039\n",
            "Iteration 3, loss = 1.60740825\n",
            "Iteration 4, loss = 1.60456979\n",
            "Iteration 5, loss = 1.60530176\n",
            "Iteration 6, loss = 1.60200540\n",
            "Iteration 7, loss = 1.59767055\n",
            "Iteration 8, loss = 1.59153434\n",
            "Iteration 9, loss = 1.58656966\n",
            "Iteration 10, loss = 1.58315880\n",
            "Iteration 11, loss = 1.58004536\n",
            "Iteration 12, loss = 1.57692355\n",
            "Iteration 13, loss = 1.57242578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14, loss = 1.56723242\n",
            "Iteration 15, loss = 1.56166237\n",
            "Iteration 16, loss = 1.55602673\n",
            "Iteration 17, loss = 1.55022930\n",
            "Iteration 18, loss = 1.54260066\n",
            "Iteration 19, loss = 1.53483586\n",
            "Iteration 20, loss = 1.52657303\n",
            "Iteration 21, loss = 1.51718152\n",
            "Iteration 22, loss = 1.50750050\n",
            "Iteration 23, loss = 1.49645033\n",
            "Iteration 24, loss = 1.48506103\n",
            "Iteration 25, loss = 1.47336156\n",
            "Iteration 26, loss = 1.46096630\n",
            "Iteration 27, loss = 1.44740335\n",
            "Iteration 28, loss = 1.43366651\n",
            "Iteration 29, loss = 1.41912501\n",
            "Iteration 30, loss = 1.40502000\n",
            "Iteration 31, loss = 1.39008305\n",
            "Iteration 32, loss = 1.37480444\n",
            "Iteration 33, loss = 1.36045556\n",
            "Iteration 34, loss = 1.34607976\n",
            "Iteration 35, loss = 1.33197607\n",
            "Iteration 36, loss = 1.31776696\n",
            "Iteration 37, loss = 1.30426834\n",
            "Iteration 38, loss = 1.29268603\n",
            "Iteration 39, loss = 1.27972407\n",
            "Iteration 40, loss = 1.26861989\n",
            "Iteration 41, loss = 1.25832463\n",
            "Iteration 42, loss = 1.24777720\n",
            "Iteration 43, loss = 1.23839774\n",
            "Iteration 44, loss = 1.23004351\n",
            "Iteration 45, loss = 1.22206304\n",
            "Iteration 46, loss = 1.21407860\n",
            "Iteration 47, loss = 1.20713389\n",
            "Iteration 48, loss = 1.20105353\n",
            "Iteration 49, loss = 1.19561878\n",
            "Iteration 50, loss = 1.18885647\n",
            "Iteration 51, loss = 1.18422579\n",
            "Iteration 52, loss = 1.18029933\n",
            "Iteration 53, loss = 1.17500891\n",
            "Iteration 54, loss = 1.17115876\n",
            "Iteration 55, loss = 1.16767076\n",
            "Iteration 56, loss = 1.16333414\n",
            "Iteration 57, loss = 1.15999362\n",
            "Iteration 58, loss = 1.15691521\n",
            "Iteration 59, loss = 1.15398920\n",
            "Iteration 60, loss = 1.15195139\n",
            "Iteration 61, loss = 1.14845017\n",
            "Iteration 62, loss = 1.14604580\n",
            "Iteration 63, loss = 1.14344417\n",
            "Iteration 64, loss = 1.14150132\n",
            "Iteration 65, loss = 1.13921224\n",
            "Iteration 66, loss = 1.13748143\n",
            "Iteration 67, loss = 1.13543609\n",
            "Iteration 68, loss = 1.13350333\n",
            "Iteration 69, loss = 1.13217100\n",
            "Iteration 70, loss = 1.13085578\n",
            "Iteration 71, loss = 1.12875919\n",
            "Iteration 72, loss = 1.12763673\n",
            "Iteration 73, loss = 1.12612758\n",
            "Iteration 74, loss = 1.12414398\n",
            "Iteration 75, loss = 1.12297695\n",
            "Iteration 76, loss = 1.12190195\n",
            "Iteration 77, loss = 1.12096077\n",
            "Iteration 78, loss = 1.11960384\n",
            "Iteration 79, loss = 1.11801757\n",
            "Iteration 80, loss = 1.11699817\n",
            "Iteration 81, loss = 1.11655353\n",
            "Iteration 82, loss = 1.11633200\n",
            "Iteration 83, loss = 1.11439746\n",
            "Iteration 84, loss = 1.11280746\n",
            "Iteration 85, loss = 1.11181641\n",
            "Iteration 86, loss = 1.11093103\n",
            "Iteration 87, loss = 1.11026756\n",
            "Iteration 88, loss = 1.10923296\n",
            "Iteration 89, loss = 1.10786200\n",
            "Iteration 90, loss = 1.10700229\n",
            "Iteration 91, loss = 1.10625507\n",
            "Iteration 92, loss = 1.10575273\n",
            "Iteration 93, loss = 1.10443421\n",
            "Iteration 94, loss = 1.10347651\n",
            "Iteration 95, loss = 1.10282807\n",
            "Iteration 96, loss = 1.10208765\n",
            "Iteration 97, loss = 1.10163501\n",
            "Iteration 98, loss = 1.10004178\n",
            "Iteration 99, loss = 1.09956493\n",
            "Iteration 100, loss = 1.09946632\n",
            "Iteration 1, loss = 1.65973350\n",
            "Iteration 2, loss = 1.60043155\n",
            "Iteration 3, loss = 1.56555966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 1.49892787\n",
            "Iteration 5, loss = 1.43200505\n",
            "Iteration 6, loss = 1.33410553\n",
            "Iteration 7, loss = 1.27344480\n",
            "Iteration 8, loss = 1.21777897\n",
            "Iteration 9, loss = 1.16524510\n",
            "Iteration 10, loss = 1.15602963\n",
            "Iteration 11, loss = 1.13505060\n",
            "Iteration 12, loss = 1.12401619\n",
            "Iteration 13, loss = 1.12090473\n",
            "Iteration 14, loss = 1.12022556\n",
            "Iteration 15, loss = 1.12760659\n",
            "Iteration 16, loss = 1.12453841\n",
            "Iteration 17, loss = 1.11521086\n",
            "Iteration 18, loss = 1.12025523\n",
            "Iteration 19, loss = 1.13419083\n",
            "Iteration 20, loss = 1.11355942\n",
            "Iteration 21, loss = 1.12307884\n",
            "Iteration 22, loss = 1.11202577\n",
            "Iteration 23, loss = 1.11159196\n",
            "Iteration 24, loss = 1.10047473\n",
            "Iteration 25, loss = 1.10493119\n",
            "Iteration 26, loss = 1.10015282\n",
            "Iteration 27, loss = 1.09453132\n",
            "Iteration 28, loss = 1.09570692\n",
            "Iteration 29, loss = 1.08912806\n",
            "Iteration 30, loss = 1.09952837\n",
            "Iteration 31, loss = 1.09379339\n",
            "Iteration 32, loss = 1.08115117\n",
            "Iteration 33, loss = 1.08800274\n",
            "Iteration 34, loss = 1.07868751\n",
            "Iteration 35, loss = 1.07598298\n",
            "Iteration 36, loss = 1.06746585\n",
            "Iteration 37, loss = 1.06258214\n",
            "Iteration 38, loss = 1.06702168\n",
            "Iteration 39, loss = 1.04397132\n",
            "Iteration 40, loss = 1.04604451\n",
            "Iteration 41, loss = 1.03078556\n",
            "Iteration 42, loss = 1.02560833\n",
            "Iteration 43, loss = 1.05218180\n",
            "Iteration 44, loss = 1.00340398\n",
            "Iteration 45, loss = 0.99755794\n",
            "Iteration 46, loss = 1.01904983\n",
            "Iteration 47, loss = 0.99151353\n",
            "Iteration 48, loss = 0.98226852\n",
            "Iteration 49, loss = 0.97686819\n",
            "Iteration 50, loss = 0.97685644\n",
            "Iteration 51, loss = 0.95541038\n",
            "Iteration 52, loss = 0.94149431\n",
            "Iteration 53, loss = 0.94435427\n",
            "Iteration 54, loss = 0.97431319\n",
            "Iteration 55, loss = 0.96592051\n",
            "Iteration 56, loss = 0.92173525\n",
            "Iteration 57, loss = 0.99027871\n",
            "Iteration 58, loss = 0.92281598\n",
            "Iteration 59, loss = 0.91857440\n",
            "Iteration 60, loss = 0.90580621\n",
            "Iteration 61, loss = 0.90641648\n",
            "Iteration 62, loss = 0.88613688\n",
            "Iteration 63, loss = 0.87095676\n",
            "Iteration 64, loss = 0.86374697\n",
            "Iteration 65, loss = 0.86353885\n",
            "Iteration 66, loss = 0.88372181\n",
            "Iteration 67, loss = 0.88236951\n",
            "Iteration 68, loss = 0.84905782\n",
            "Iteration 69, loss = 0.87926761\n",
            "Iteration 70, loss = 0.87563961\n",
            "Iteration 71, loss = 0.94728146\n",
            "Iteration 72, loss = 0.88863606\n",
            "Iteration 73, loss = 0.90076161\n",
            "Iteration 74, loss = 0.90304750\n",
            "Iteration 75, loss = 0.88629629\n",
            "Iteration 76, loss = 0.85335934\n",
            "Iteration 77, loss = 0.83959595\n",
            "Iteration 78, loss = 0.84149400\n",
            "Iteration 79, loss = 0.83453732\n",
            "Iteration 80, loss = 0.84761004\n",
            "Iteration 81, loss = 0.89804814\n",
            "Iteration 82, loss = 0.81366207\n",
            "Iteration 83, loss = 0.86679695\n",
            "Iteration 84, loss = 0.83530666\n",
            "Iteration 85, loss = 0.83834391\n",
            "Iteration 86, loss = 0.82453409\n",
            "Iteration 87, loss = 0.80556019\n",
            "Iteration 88, loss = 0.80925064\n",
            "Iteration 89, loss = 0.78886433\n",
            "Iteration 90, loss = 0.79096484\n",
            "Iteration 91, loss = 0.78315182\n",
            "Iteration 92, loss = 0.78760969\n",
            "Iteration 93, loss = 0.78495395\n",
            "Iteration 94, loss = 0.77974743\n",
            "Iteration 95, loss = 0.79142601\n",
            "Iteration 96, loss = 0.78581234\n",
            "Iteration 97, loss = 0.79668579\n",
            "Iteration 98, loss = 0.78608371\n",
            "Iteration 99, loss = 0.78184426\n",
            "Iteration 100, loss = 0.76574637\n",
            "Iteration 1, loss = 1.63513006\n",
            "Iteration 2, loss = 1.63334379\n",
            "Iteration 3, loss = 1.63166034\n",
            "Iteration 4, loss = 1.63012298\n",
            "Iteration 5, loss = 1.62875013\n",
            "Iteration 6, loss = 1.62731769\n",
            "Iteration 7, loss = 1.62591536\n",
            "Iteration 8, loss = 1.62457479\n",
            "Iteration 9, loss = 1.62342156\n",
            "Iteration 10, loss = 1.62222741\n",
            "Iteration 11, loss = 1.62106994\n",
            "Iteration 12, loss = 1.61990569\n",
            "Iteration 13, loss = 1.61867575\n",
            "Iteration 14, loss = 1.61770983\n",
            "Iteration 15, loss = 1.61658529\n",
            "Iteration 16, loss = 1.61565530\n",
            "Iteration 17, loss = 1.61465027\n",
            "Iteration 18, loss = 1.61359709\n",
            "Iteration 19, loss = 1.61277183\n",
            "Iteration 20, loss = 1.61174212\n",
            "Iteration 21, loss = 1.61104014\n",
            "Iteration 22, loss = 1.61012371\n",
            "Iteration 23, loss = 1.60933091\n",
            "Iteration 24, loss = 1.60856388\n",
            "Iteration 25, loss = 1.60778058\n",
            "Iteration 26, loss = 1.60714272\n",
            "Iteration 27, loss = 1.60642327\n",
            "Iteration 28, loss = 1.60578740\n",
            "Iteration 29, loss = 1.60519634\n",
            "Iteration 30, loss = 1.60463543\n",
            "Iteration 31, loss = 1.60392854\n",
            "Iteration 32, loss = 1.60346588\n",
            "Iteration 33, loss = 1.60289827\n",
            "Iteration 34, loss = 1.60240217\n",
            "Iteration 35, loss = 1.60190088\n",
            "Iteration 36, loss = 1.60138652\n",
            "Iteration 37, loss = 1.60091591\n",
            "Iteration 38, loss = 1.60057806\n",
            "Iteration 39, loss = 1.60014624\n",
            "Iteration 40, loss = 1.59966126\n",
            "Iteration 41, loss = 1.59926090\n",
            "Iteration 42, loss = 1.59889581\n",
            "Iteration 43, loss = 1.59855628\n",
            "Iteration 44, loss = 1.59814659\n",
            "Iteration 45, loss = 1.59788998\n",
            "Iteration 46, loss = 1.59751743\n",
            "Iteration 47, loss = 1.59718366\n",
            "Iteration 48, loss = 1.59683177\n",
            "Iteration 49, loss = 1.59648747\n",
            "Iteration 50, loss = 1.59618930\n",
            "Iteration 51, loss = 1.59590529\n",
            "Iteration 52, loss = 1.59569611\n",
            "Iteration 53, loss = 1.59530308\n",
            "Iteration 54, loss = 1.59505227\n",
            "Iteration 55, loss = 1.59476680\n",
            "Iteration 56, loss = 1.59447297\n",
            "Iteration 57, loss = 1.59422244\n",
            "Iteration 58, loss = 1.59392687\n",
            "Iteration 59, loss = 1.59366017\n",
            "Iteration 60, loss = 1.59340436\n",
            "Iteration 61, loss = 1.59318392\n",
            "Iteration 62, loss = 1.59291700\n",
            "Iteration 63, loss = 1.59262500\n",
            "Iteration 64, loss = 1.59238809\n",
            "Iteration 65, loss = 1.59210286\n",
            "Iteration 66, loss = 1.59184783\n",
            "Iteration 67, loss = 1.59157158\n",
            "Iteration 68, loss = 1.59131979\n",
            "Iteration 69, loss = 1.59107197\n",
            "Iteration 70, loss = 1.59081520\n",
            "Iteration 71, loss = 1.59053999\n",
            "Iteration 72, loss = 1.59028215\n",
            "Iteration 73, loss = 1.59000970\n",
            "Iteration 74, loss = 1.58973090\n",
            "Iteration 75, loss = 1.58948388\n",
            "Iteration 76, loss = 1.58921417\n",
            "Iteration 77, loss = 1.58893584\n",
            "Iteration 78, loss = 1.58867783\n",
            "Iteration 79, loss = 1.58839501\n",
            "Iteration 80, loss = 1.58809890\n",
            "Iteration 81, loss = 1.58784032\n",
            "Iteration 82, loss = 1.58754638\n",
            "Iteration 83, loss = 1.58729356\n",
            "Iteration 84, loss = 1.58698809\n",
            "Iteration 85, loss = 1.58668420\n",
            "Iteration 86, loss = 1.58638607\n",
            "Iteration 87, loss = 1.58610490\n",
            "Iteration 88, loss = 1.58579213\n",
            "Iteration 89, loss = 1.58549786\n",
            "Iteration 90, loss = 1.58518257\n",
            "Iteration 91, loss = 1.58486951\n",
            "Iteration 92, loss = 1.58458299\n",
            "Iteration 93, loss = 1.58424161\n",
            "Iteration 94, loss = 1.58391283\n",
            "Iteration 95, loss = 1.58360928\n",
            "Iteration 96, loss = 1.58327710\n",
            "Iteration 97, loss = 1.58296395\n",
            "Iteration 98, loss = 1.58259940\n",
            "Iteration 99, loss = 1.58228949\n",
            "Iteration 100, loss = 1.58191310\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.63220923\n",
            "Iteration 2, loss = 1.61830793\n",
            "Iteration 3, loss = 1.60833655\n",
            "Iteration 4, loss = 1.60221398\n",
            "Iteration 5, loss = 1.59982000\n",
            "Iteration 6, loss = 1.59722263\n",
            "Iteration 7, loss = 1.59560976\n",
            "Iteration 8, loss = 1.59335757\n",
            "Iteration 9, loss = 1.59088080\n",
            "Iteration 10, loss = 1.58865287\n",
            "Iteration 11, loss = 1.58618375\n",
            "Iteration 12, loss = 1.58377526\n",
            "Iteration 13, loss = 1.58121576\n",
            "Iteration 14, loss = 1.57808123\n",
            "Iteration 15, loss = 1.57474618\n",
            "Iteration 16, loss = 1.57118240\n",
            "Iteration 17, loss = 1.56780117\n",
            "Iteration 18, loss = 1.56330885\n",
            "Iteration 19, loss = 1.55863955\n",
            "Iteration 20, loss = 1.55358729\n",
            "Iteration 21, loss = 1.54806389\n",
            "Iteration 22, loss = 1.54216105\n",
            "Iteration 23, loss = 1.53534584\n",
            "Iteration 24, loss = 1.52839429\n",
            "Iteration 25, loss = 1.52101046\n",
            "Iteration 26, loss = 1.51311565\n",
            "Iteration 27, loss = 1.50433958\n",
            "Iteration 28, loss = 1.49527618\n",
            "Iteration 29, loss = 1.48527423\n",
            "Iteration 30, loss = 1.47526466\n",
            "Iteration 31, loss = 1.46460632\n",
            "Iteration 32, loss = 1.45345837\n",
            "Iteration 33, loss = 1.44224117\n",
            "Iteration 34, loss = 1.43054696\n",
            "Iteration 35, loss = 1.41861326\n",
            "Iteration 36, loss = 1.40643864\n",
            "Iteration 37, loss = 1.39410077\n",
            "Iteration 38, loss = 1.38272187\n",
            "Iteration 39, loss = 1.37005637\n",
            "Iteration 40, loss = 1.35833320\n",
            "Iteration 41, loss = 1.34675508\n",
            "Iteration 42, loss = 1.33549094\n",
            "Iteration 43, loss = 1.32460585\n",
            "Iteration 44, loss = 1.31394149\n",
            "Iteration 45, loss = 1.30408498\n",
            "Iteration 46, loss = 1.29401812\n",
            "Iteration 47, loss = 1.28466812\n",
            "Iteration 48, loss = 1.27601471\n",
            "Iteration 49, loss = 1.26817578\n",
            "Iteration 50, loss = 1.25940341\n",
            "Iteration 51, loss = 1.25187052\n",
            "Iteration 52, loss = 1.24541620\n",
            "Iteration 53, loss = 1.23828533\n",
            "Iteration 54, loss = 1.23232137\n",
            "Iteration 55, loss = 1.22625179\n",
            "Iteration 56, loss = 1.22048825\n",
            "Iteration 57, loss = 1.21530280\n",
            "Iteration 58, loss = 1.21003415\n",
            "Iteration 59, loss = 1.20543720\n",
            "Iteration 60, loss = 1.20140755\n",
            "Iteration 61, loss = 1.19687055\n",
            "Iteration 62, loss = 1.19295704\n",
            "Iteration 63, loss = 1.18901014\n",
            "Iteration 64, loss = 1.18572093\n",
            "Iteration 65, loss = 1.18221225\n",
            "Iteration 66, loss = 1.17916884\n",
            "Iteration 67, loss = 1.17621993\n",
            "Iteration 68, loss = 1.17320210\n",
            "Iteration 69, loss = 1.17082195\n",
            "Iteration 70, loss = 1.16826043\n",
            "Iteration 71, loss = 1.16548813\n",
            "Iteration 72, loss = 1.16337058\n",
            "Iteration 73, loss = 1.16095180\n",
            "Iteration 74, loss = 1.15842632\n",
            "Iteration 75, loss = 1.15647613\n",
            "Iteration 76, loss = 1.15449966\n",
            "Iteration 77, loss = 1.15320121\n",
            "Iteration 78, loss = 1.15117207\n",
            "Iteration 79, loss = 1.14899198\n",
            "Iteration 80, loss = 1.14737093\n",
            "Iteration 81, loss = 1.14637833\n",
            "Iteration 82, loss = 1.14500896\n",
            "Iteration 83, loss = 1.14300317\n",
            "Iteration 84, loss = 1.14153970\n",
            "Iteration 85, loss = 1.13998642\n",
            "Iteration 86, loss = 1.13857265\n",
            "Iteration 87, loss = 1.13767887\n",
            "Iteration 88, loss = 1.13631691\n",
            "Iteration 89, loss = 1.13484543\n",
            "Iteration 90, loss = 1.13358609\n",
            "Iteration 91, loss = 1.13270804\n",
            "Iteration 92, loss = 1.13172497\n",
            "Iteration 93, loss = 1.13045317\n",
            "Iteration 94, loss = 1.12932728\n",
            "Iteration 95, loss = 1.12841666\n",
            "Iteration 96, loss = 1.12748706\n",
            "Iteration 97, loss = 1.12709671\n",
            "Iteration 98, loss = 1.12544717\n",
            "Iteration 99, loss = 1.12464314\n",
            "Iteration 100, loss = 1.12442991\n",
            "Iteration 1, loss = 1.63437238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 1.60017052\n",
            "Iteration 3, loss = 1.58613178\n",
            "Iteration 4, loss = 1.56825007\n",
            "Iteration 5, loss = 1.53683905\n",
            "Iteration 6, loss = 1.48888492\n",
            "Iteration 7, loss = 1.44826040\n",
            "Iteration 8, loss = 1.38536463\n",
            "Iteration 9, loss = 1.33320504\n",
            "Iteration 10, loss = 1.27571118\n",
            "Iteration 11, loss = 1.23982885\n",
            "Iteration 12, loss = 1.20269382\n",
            "Iteration 13, loss = 1.17931730\n",
            "Iteration 14, loss = 1.15838875\n",
            "Iteration 15, loss = 1.14717424\n",
            "Iteration 16, loss = 1.13560197\n",
            "Iteration 17, loss = 1.13233350\n",
            "Iteration 18, loss = 1.12518273\n",
            "Iteration 19, loss = 1.12650716\n",
            "Iteration 20, loss = 1.11116267\n",
            "Iteration 21, loss = 1.11180072\n",
            "Iteration 22, loss = 1.10568975\n",
            "Iteration 23, loss = 1.10319260\n",
            "Iteration 24, loss = 1.10032070\n",
            "Iteration 25, loss = 1.09914959\n",
            "Iteration 26, loss = 1.09962497\n",
            "Iteration 27, loss = 1.09598762\n",
            "Iteration 28, loss = 1.10006753\n",
            "Iteration 29, loss = 1.09573007\n",
            "Iteration 30, loss = 1.10290581\n",
            "Iteration 31, loss = 1.10249408\n",
            "Iteration 32, loss = 1.09271658\n",
            "Iteration 33, loss = 1.09855693\n",
            "Iteration 34, loss = 1.09304451\n",
            "Iteration 35, loss = 1.09339311\n",
            "Iteration 36, loss = 1.09173774\n",
            "Iteration 37, loss = 1.08319349\n",
            "Iteration 38, loss = 1.08647576\n",
            "Iteration 39, loss = 1.07916726\n",
            "Iteration 40, loss = 1.07954828\n",
            "Iteration 41, loss = 1.06929596\n",
            "Iteration 42, loss = 1.07454393\n",
            "Iteration 43, loss = 1.06601602\n",
            "Iteration 44, loss = 1.06613354\n",
            "Iteration 45, loss = 1.06056363\n",
            "Iteration 46, loss = 1.05414386\n",
            "Iteration 47, loss = 1.05199163\n",
            "Iteration 48, loss = 1.04385644\n",
            "Iteration 49, loss = 1.04980218\n",
            "Iteration 50, loss = 1.03513885\n",
            "Iteration 51, loss = 1.03619410\n",
            "Iteration 52, loss = 1.02943667\n",
            "Iteration 53, loss = 1.01851446\n",
            "Iteration 54, loss = 1.02306235\n",
            "Iteration 55, loss = 1.00977100\n",
            "Iteration 56, loss = 1.00119612\n",
            "Iteration 57, loss = 1.02275682\n",
            "Iteration 58, loss = 0.99873825\n",
            "Iteration 59, loss = 0.98431600\n",
            "Iteration 60, loss = 0.99093572\n",
            "Iteration 61, loss = 0.97557288\n",
            "Iteration 62, loss = 0.97500016\n",
            "Iteration 63, loss = 0.96591113\n",
            "Iteration 64, loss = 0.96598348\n",
            "Iteration 65, loss = 0.95560104\n",
            "Iteration 66, loss = 0.94998206\n",
            "Iteration 67, loss = 0.94555090\n",
            "Iteration 68, loss = 0.94061660\n",
            "Iteration 69, loss = 0.94056086\n",
            "Iteration 70, loss = 0.94110721\n",
            "Iteration 71, loss = 0.91947669\n",
            "Iteration 72, loss = 0.92524973\n",
            "Iteration 73, loss = 0.91358368\n",
            "Iteration 74, loss = 0.90854126\n",
            "Iteration 75, loss = 0.91600293\n",
            "Iteration 76, loss = 0.90386428\n",
            "Iteration 77, loss = 0.90551900\n",
            "Iteration 78, loss = 0.89668506\n",
            "Iteration 79, loss = 0.90511769\n",
            "Iteration 80, loss = 0.89448054\n",
            "Iteration 81, loss = 0.89674410\n",
            "Iteration 82, loss = 0.92932480\n",
            "Iteration 83, loss = 0.88284159\n",
            "Iteration 84, loss = 0.88862203\n",
            "Iteration 85, loss = 0.88933228\n",
            "Iteration 86, loss = 0.88900280\n",
            "Iteration 87, loss = 0.86109278\n",
            "Iteration 88, loss = 0.86744625\n",
            "Iteration 89, loss = 0.85591044\n",
            "Iteration 90, loss = 0.85793131\n",
            "Iteration 91, loss = 0.84935465\n",
            "Iteration 92, loss = 0.85017498\n",
            "Iteration 93, loss = 0.84127595\n",
            "Iteration 94, loss = 0.84136052\n",
            "Iteration 95, loss = 0.84046040\n",
            "Iteration 96, loss = 0.83725848\n",
            "Iteration 97, loss = 0.83535521\n",
            "Iteration 98, loss = 0.83114511\n",
            "Iteration 99, loss = 0.82965427\n",
            "Iteration 100, loss = 0.82887350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.68792727\n",
            "Iteration 2, loss = 1.68262991\n",
            "Iteration 3, loss = 1.67747935\n",
            "Iteration 4, loss = 1.67286137\n",
            "Iteration 5, loss = 1.66868104\n",
            "Iteration 6, loss = 1.66431802\n",
            "Iteration 7, loss = 1.66008929\n",
            "Iteration 8, loss = 1.65620237\n",
            "Iteration 9, loss = 1.65200764\n",
            "Iteration 10, loss = 1.64816078\n",
            "Iteration 11, loss = 1.64498913\n",
            "Iteration 12, loss = 1.64139474\n",
            "Iteration 13, loss = 1.63817933\n",
            "Iteration 14, loss = 1.63508116\n",
            "Iteration 15, loss = 1.63173870\n",
            "Iteration 16, loss = 1.62915029\n",
            "Iteration 17, loss = 1.62635126\n",
            "Iteration 18, loss = 1.62346527\n",
            "Iteration 19, loss = 1.62130017\n",
            "Iteration 20, loss = 1.61874956\n",
            "Iteration 21, loss = 1.61640928\n",
            "Iteration 22, loss = 1.61401384\n",
            "Iteration 23, loss = 1.61206982\n",
            "Iteration 24, loss = 1.60995369\n",
            "Iteration 25, loss = 1.60801187\n",
            "Iteration 26, loss = 1.60624571\n",
            "Iteration 27, loss = 1.60466386\n",
            "Iteration 28, loss = 1.60284498\n",
            "Iteration 29, loss = 1.60121249\n",
            "Iteration 30, loss = 1.59971102\n",
            "Iteration 31, loss = 1.59832476\n",
            "Iteration 32, loss = 1.59696048\n",
            "Iteration 33, loss = 1.59562765\n",
            "Iteration 34, loss = 1.59443534\n",
            "Iteration 35, loss = 1.59344211\n",
            "Iteration 36, loss = 1.59222124\n",
            "Iteration 37, loss = 1.59119903\n",
            "Iteration 38, loss = 1.59022614\n",
            "Iteration 39, loss = 1.58915499\n",
            "Iteration 40, loss = 1.58837370\n",
            "Iteration 41, loss = 1.58745478\n",
            "Iteration 42, loss = 1.58669476\n",
            "Iteration 43, loss = 1.58577654\n",
            "Iteration 44, loss = 1.58503723\n",
            "Iteration 45, loss = 1.58432452\n",
            "Iteration 46, loss = 1.58354409\n",
            "Iteration 47, loss = 1.58304796\n",
            "Iteration 48, loss = 1.58229455\n",
            "Iteration 49, loss = 1.58163328\n",
            "Iteration 50, loss = 1.58088673\n",
            "Iteration 51, loss = 1.58035283\n",
            "Iteration 52, loss = 1.57969840\n",
            "Iteration 53, loss = 1.57907216\n",
            "Iteration 54, loss = 1.57848392\n",
            "Iteration 55, loss = 1.57786580\n",
            "Iteration 56, loss = 1.57732220\n",
            "Iteration 57, loss = 1.57671993\n",
            "Iteration 58, loss = 1.57615840\n",
            "Iteration 59, loss = 1.57559004\n",
            "Iteration 60, loss = 1.57501850\n",
            "Iteration 61, loss = 1.57440001\n",
            "Iteration 62, loss = 1.57384420\n",
            "Iteration 63, loss = 1.57332055\n",
            "Iteration 64, loss = 1.57274594\n",
            "Iteration 65, loss = 1.57213858\n",
            "Iteration 66, loss = 1.57155661\n",
            "Iteration 67, loss = 1.57099881\n",
            "Iteration 68, loss = 1.57042840\n",
            "Iteration 69, loss = 1.56988982\n",
            "Iteration 70, loss = 1.56926685\n",
            "Iteration 71, loss = 1.56869430\n",
            "Iteration 72, loss = 1.56811556\n",
            "Iteration 73, loss = 1.56753954\n",
            "Iteration 74, loss = 1.56695035\n",
            "Iteration 75, loss = 1.56635383\n",
            "Iteration 76, loss = 1.56579886\n",
            "Iteration 77, loss = 1.56523835\n",
            "Iteration 78, loss = 1.56461006\n",
            "Iteration 79, loss = 1.56398538\n",
            "Iteration 80, loss = 1.56337148\n",
            "Iteration 81, loss = 1.56279387\n",
            "Iteration 82, loss = 1.56219104\n",
            "Iteration 83, loss = 1.56156287\n",
            "Iteration 84, loss = 1.56096482\n",
            "Iteration 85, loss = 1.56034561\n",
            "Iteration 86, loss = 1.55974193\n",
            "Iteration 87, loss = 1.55909331\n",
            "Iteration 88, loss = 1.55848142\n",
            "Iteration 89, loss = 1.55790323\n",
            "Iteration 90, loss = 1.55719870\n",
            "Iteration 91, loss = 1.55657443\n",
            "Iteration 92, loss = 1.55592331\n",
            "Iteration 93, loss = 1.55526876\n",
            "Iteration 94, loss = 1.55463394\n",
            "Iteration 95, loss = 1.55400595\n",
            "Iteration 96, loss = 1.55329766\n",
            "Iteration 97, loss = 1.55267086\n",
            "Iteration 98, loss = 1.55200342\n",
            "Iteration 99, loss = 1.55131303\n",
            "Iteration 100, loss = 1.55064711\n",
            "Iteration 1, loss = 1.68006805\n",
            "Iteration 2, loss = 1.63834257\n",
            "Iteration 3, loss = 1.60960622\n",
            "Iteration 4, loss = 1.59396466\n",
            "Iteration 5, loss = 1.58769008\n",
            "Iteration 6, loss = 1.58526690\n",
            "Iteration 7, loss = 1.58521257\n",
            "Iteration 8, loss = 1.58274583\n",
            "Iteration 9, loss = 1.57823013\n",
            "Iteration 10, loss = 1.57191823\n",
            "Iteration 11, loss = 1.56411898\n",
            "Iteration 12, loss = 1.55836234\n",
            "Iteration 13, loss = 1.55295384\n",
            "Iteration 14, loss = 1.54784589\n",
            "Iteration 15, loss = 1.54375008\n",
            "Iteration 16, loss = 1.53815727\n",
            "Iteration 17, loss = 1.53208705\n",
            "Iteration 18, loss = 1.52556594\n",
            "Iteration 19, loss = 1.51868605\n",
            "Iteration 20, loss = 1.51091397\n",
            "Iteration 21, loss = 1.50315344\n",
            "Iteration 22, loss = 1.49459425\n",
            "Iteration 23, loss = 1.48573258\n",
            "Iteration 24, loss = 1.47665392\n",
            "Iteration 25, loss = 1.46746457\n",
            "Iteration 26, loss = 1.45760655\n",
            "Iteration 27, loss = 1.44686652\n",
            "Iteration 28, loss = 1.43662088\n",
            "Iteration 29, loss = 1.42575985\n",
            "Iteration 30, loss = 1.41448697\n",
            "Iteration 31, loss = 1.40286824\n",
            "Iteration 32, loss = 1.39163596\n",
            "Iteration 33, loss = 1.38010090\n",
            "Iteration 34, loss = 1.36818358\n",
            "Iteration 35, loss = 1.35684555\n",
            "Iteration 36, loss = 1.34568549\n",
            "Iteration 37, loss = 1.33362571\n",
            "Iteration 38, loss = 1.32287214\n",
            "Iteration 39, loss = 1.31188990\n",
            "Iteration 40, loss = 1.30188537\n",
            "Iteration 41, loss = 1.29119380\n",
            "Iteration 42, loss = 1.28252551\n",
            "Iteration 43, loss = 1.27234505\n",
            "Iteration 44, loss = 1.26364372\n",
            "Iteration 45, loss = 1.25524291\n",
            "Iteration 46, loss = 1.24706397\n",
            "Iteration 47, loss = 1.24039270\n",
            "Iteration 48, loss = 1.23254528\n",
            "Iteration 49, loss = 1.22571398\n",
            "Iteration 50, loss = 1.21919938\n",
            "Iteration 51, loss = 1.21308929\n",
            "Iteration 52, loss = 1.20704591\n",
            "Iteration 53, loss = 1.20169118\n",
            "Iteration 54, loss = 1.19630163\n",
            "Iteration 55, loss = 1.19128806\n",
            "Iteration 56, loss = 1.18687627\n",
            "Iteration 57, loss = 1.18236384\n",
            "Iteration 58, loss = 1.17847590\n",
            "Iteration 59, loss = 1.17443903\n",
            "Iteration 60, loss = 1.17061803\n",
            "Iteration 61, loss = 1.16666800\n",
            "Iteration 62, loss = 1.16358759\n",
            "Iteration 63, loss = 1.16070012\n",
            "Iteration 64, loss = 1.15742166\n",
            "Iteration 65, loss = 1.15434307\n",
            "Iteration 66, loss = 1.15155117\n",
            "Iteration 67, loss = 1.14902160\n",
            "Iteration 68, loss = 1.14604827\n",
            "Iteration 69, loss = 1.14375391\n",
            "Iteration 70, loss = 1.14115818\n",
            "Iteration 71, loss = 1.13911518\n",
            "Iteration 72, loss = 1.13655485\n",
            "Iteration 73, loss = 1.13493775\n",
            "Iteration 74, loss = 1.13247668\n",
            "Iteration 75, loss = 1.13033647\n",
            "Iteration 76, loss = 1.12908648\n",
            "Iteration 77, loss = 1.12709558\n",
            "Iteration 78, loss = 1.12498577\n",
            "Iteration 79, loss = 1.12403764\n",
            "Iteration 80, loss = 1.12191515\n",
            "Iteration 81, loss = 1.11990984\n",
            "Iteration 82, loss = 1.11796885\n",
            "Iteration 83, loss = 1.11743421\n",
            "Iteration 84, loss = 1.11544099\n",
            "Iteration 85, loss = 1.11363522\n",
            "Iteration 86, loss = 1.11225959\n",
            "Iteration 87, loss = 1.11072190\n",
            "Iteration 88, loss = 1.10962832\n",
            "Iteration 89, loss = 1.10948553\n",
            "Iteration 90, loss = 1.10644897\n",
            "Iteration 91, loss = 1.10542110\n",
            "Iteration 92, loss = 1.10345613\n",
            "Iteration 93, loss = 1.10227139\n",
            "Iteration 94, loss = 1.10116047\n",
            "Iteration 95, loss = 1.10140582\n",
            "Iteration 96, loss = 1.09878388\n",
            "Iteration 97, loss = 1.09891576\n",
            "Iteration 98, loss = 1.09668111\n",
            "Iteration 99, loss = 1.09453351\n",
            "Iteration 100, loss = 1.09337122\n",
            "Iteration 1, loss = 1.69078955\n",
            "Iteration 2, loss = 1.62000727\n",
            "Iteration 3, loss = 1.59321574\n",
            "Iteration 4, loss = 1.55870989\n",
            "Iteration 5, loss = 1.51254766\n",
            "Iteration 6, loss = 1.46393684\n",
            "Iteration 7, loss = 1.39936118\n",
            "Iteration 8, loss = 1.33091179\n",
            "Iteration 9, loss = 1.28260909\n",
            "Iteration 10, loss = 1.22566747\n",
            "Iteration 11, loss = 1.19792457\n",
            "Iteration 12, loss = 1.17006346\n",
            "Iteration 13, loss = 1.14279606\n",
            "Iteration 14, loss = 1.13575849\n",
            "Iteration 15, loss = 1.12999550\n",
            "Iteration 16, loss = 1.11777018\n",
            "Iteration 17, loss = 1.11884448\n",
            "Iteration 18, loss = 1.11389075\n",
            "Iteration 19, loss = 1.10486300\n",
            "Iteration 20, loss = 1.11154698\n",
            "Iteration 21, loss = 1.11597868\n",
            "Iteration 22, loss = 1.10799701\n",
            "Iteration 23, loss = 1.11877481\n",
            "Iteration 24, loss = 1.09966105\n",
            "Iteration 25, loss = 1.10709215\n",
            "Iteration 26, loss = 1.10545825\n",
            "Iteration 27, loss = 1.11999296\n",
            "Iteration 28, loss = 1.09109194\n",
            "Iteration 29, loss = 1.10141952\n",
            "Iteration 30, loss = 1.08683884\n",
            "Iteration 31, loss = 1.08425755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 32, loss = 1.07398908\n",
            "Iteration 33, loss = 1.07452520\n",
            "Iteration 34, loss = 1.06004683\n",
            "Iteration 35, loss = 1.06560964\n",
            "Iteration 36, loss = 1.05831903\n",
            "Iteration 37, loss = 1.04577075\n",
            "Iteration 38, loss = 1.05232650\n",
            "Iteration 39, loss = 1.02865265\n",
            "Iteration 40, loss = 1.04799665\n",
            "Iteration 41, loss = 1.01807284\n",
            "Iteration 42, loss = 1.01603385\n",
            "Iteration 43, loss = 1.01302939\n",
            "Iteration 44, loss = 1.00143314\n",
            "Iteration 45, loss = 0.99208022\n",
            "Iteration 46, loss = 0.97870178\n",
            "Iteration 47, loss = 0.98891639\n",
            "Iteration 48, loss = 1.00827348\n",
            "Iteration 49, loss = 1.03301435\n",
            "Iteration 50, loss = 1.04117413\n",
            "Iteration 51, loss = 1.02988145\n",
            "Iteration 52, loss = 0.99260004\n",
            "Iteration 53, loss = 0.95608720\n",
            "Iteration 54, loss = 0.94966579\n",
            "Iteration 55, loss = 0.94680569\n",
            "Iteration 56, loss = 0.95035338\n",
            "Iteration 57, loss = 0.91913018\n",
            "Iteration 58, loss = 0.93312891\n",
            "Iteration 59, loss = 0.91932803\n",
            "Iteration 60, loss = 0.91586605\n",
            "Iteration 61, loss = 0.90256368\n",
            "Iteration 62, loss = 0.91435282\n",
            "Iteration 63, loss = 0.90932815\n",
            "Iteration 64, loss = 0.88927203\n",
            "Iteration 65, loss = 0.90648623\n",
            "Iteration 66, loss = 0.93977283\n",
            "Iteration 67, loss = 0.91089270\n",
            "Iteration 68, loss = 0.90267349\n",
            "Iteration 69, loss = 0.90621732\n",
            "Iteration 70, loss = 0.88316453\n",
            "Iteration 71, loss = 0.86836714\n",
            "Iteration 72, loss = 0.90311763\n",
            "Iteration 73, loss = 0.87272167\n",
            "Iteration 74, loss = 0.86357446\n",
            "Iteration 75, loss = 0.88201194\n",
            "Iteration 76, loss = 0.85915848\n",
            "Iteration 77, loss = 0.85274417\n",
            "Iteration 78, loss = 0.85773567\n",
            "Iteration 79, loss = 0.84260970\n",
            "Iteration 80, loss = 0.83055410\n",
            "Iteration 81, loss = 0.83648733\n",
            "Iteration 82, loss = 0.81965422\n",
            "Iteration 83, loss = 0.83436401\n",
            "Iteration 84, loss = 0.81286020\n",
            "Iteration 85, loss = 0.84449814\n",
            "Iteration 86, loss = 0.84561469\n",
            "Iteration 87, loss = 0.83130277\n",
            "Iteration 88, loss = 0.80319534\n",
            "Iteration 89, loss = 0.82528211\n",
            "Iteration 90, loss = 0.98673559\n",
            "Iteration 91, loss = 0.82294450\n",
            "Iteration 92, loss = 0.89472457\n",
            "Iteration 93, loss = 0.83151215\n",
            "Iteration 94, loss = 0.80864503\n",
            "Iteration 95, loss = 0.84179355\n",
            "Iteration 96, loss = 0.81437711\n",
            "Iteration 97, loss = 0.81340081\n",
            "Iteration 98, loss = 0.81277554\n",
            "Iteration 99, loss = 0.81149209\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.63943299\n",
            "Iteration 2, loss = 1.63602905\n",
            "Iteration 3, loss = 1.63245758\n",
            "Iteration 4, loss = 1.62983611\n",
            "Iteration 5, loss = 1.62737338\n",
            "Iteration 6, loss = 1.62481010\n",
            "Iteration 7, loss = 1.62234527\n",
            "Iteration 8, loss = 1.62005152\n",
            "Iteration 9, loss = 1.61781273\n",
            "Iteration 10, loss = 1.61568497\n",
            "Iteration 11, loss = 1.61412979\n",
            "Iteration 12, loss = 1.61241575\n",
            "Iteration 13, loss = 1.61086363\n",
            "Iteration 14, loss = 1.60953875\n",
            "Iteration 15, loss = 1.60792222\n",
            "Iteration 16, loss = 1.60701543\n",
            "Iteration 17, loss = 1.60585199\n",
            "Iteration 18, loss = 1.60471784\n",
            "Iteration 19, loss = 1.60406197\n",
            "Iteration 20, loss = 1.60312817\n",
            "Iteration 21, loss = 1.60240069\n",
            "Iteration 22, loss = 1.60158933\n",
            "Iteration 23, loss = 1.60113445\n",
            "Iteration 24, loss = 1.60050488\n",
            "Iteration 25, loss = 1.60000148\n",
            "Iteration 26, loss = 1.59963680\n",
            "Iteration 27, loss = 1.59940157\n",
            "Iteration 28, loss = 1.59893301\n",
            "Iteration 29, loss = 1.59863688\n",
            "Iteration 30, loss = 1.59836999\n",
            "Iteration 31, loss = 1.59815300\n",
            "Iteration 32, loss = 1.59795690\n",
            "Iteration 33, loss = 1.59776598\n",
            "Iteration 34, loss = 1.59763243\n",
            "Iteration 35, loss = 1.59761306\n",
            "Iteration 36, loss = 1.59743261\n",
            "Iteration 37, loss = 1.59731541\n",
            "Iteration 38, loss = 1.59725009\n",
            "Iteration 39, loss = 1.59710622\n",
            "Iteration 40, loss = 1.59707181\n",
            "Iteration 41, loss = 1.59697252\n",
            "Iteration 42, loss = 1.59697408\n",
            "Iteration 43, loss = 1.59683293\n",
            "Iteration 44, loss = 1.59677468\n",
            "Iteration 45, loss = 1.59672688\n",
            "Iteration 46, loss = 1.59665715\n",
            "Iteration 47, loss = 1.59671015\n",
            "Iteration 48, loss = 1.59659020\n",
            "Iteration 49, loss = 1.59651537\n",
            "Iteration 50, loss = 1.59643386\n",
            "Iteration 51, loss = 1.59644122\n",
            "Iteration 52, loss = 1.59637319\n",
            "Iteration 53, loss = 1.59629095\n",
            "Iteration 54, loss = 1.59622287\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.63767640\n",
            "Iteration 2, loss = 1.61258815\n",
            "Iteration 3, loss = 1.59868571\n",
            "Iteration 4, loss = 1.59741905\n",
            "Iteration 5, loss = 1.60137820\n",
            "Iteration 6, loss = 1.60313439\n",
            "Iteration 7, loss = 1.60225317\n",
            "Iteration 8, loss = 1.59970434\n",
            "Iteration 9, loss = 1.59782907\n",
            "Iteration 10, loss = 1.59690482\n",
            "Iteration 11, loss = 1.59498515\n",
            "Iteration 12, loss = 1.59530170\n",
            "Iteration 13, loss = 1.59484764\n",
            "Iteration 14, loss = 1.59432284\n",
            "Iteration 15, loss = 1.59322669\n",
            "Iteration 16, loss = 1.59259521\n",
            "Iteration 17, loss = 1.59147437\n",
            "Iteration 18, loss = 1.59059299\n",
            "Iteration 19, loss = 1.59060373\n",
            "Iteration 20, loss = 1.58962987\n",
            "Iteration 21, loss = 1.58928660\n",
            "Iteration 22, loss = 1.58812457\n",
            "Iteration 23, loss = 1.58683959\n",
            "Iteration 24, loss = 1.58573450\n",
            "Iteration 25, loss = 1.58488680\n",
            "Iteration 26, loss = 1.58408458\n",
            "Iteration 27, loss = 1.58317558\n",
            "Iteration 28, loss = 1.58178473\n",
            "Iteration 29, loss = 1.58065639\n",
            "Iteration 30, loss = 1.57920209\n",
            "Iteration 31, loss = 1.57762053\n",
            "Iteration 32, loss = 1.57627166\n",
            "Iteration 33, loss = 1.57479087\n",
            "Iteration 34, loss = 1.57296950\n",
            "Iteration 35, loss = 1.57169746\n",
            "Iteration 36, loss = 1.57008571\n",
            "Iteration 37, loss = 1.56731127\n",
            "Iteration 38, loss = 1.56552038\n",
            "Iteration 39, loss = 1.56310374\n",
            "Iteration 40, loss = 1.56076879\n",
            "Iteration 41, loss = 1.55798647\n",
            "Iteration 42, loss = 1.55595486\n",
            "Iteration 43, loss = 1.55272003\n",
            "Iteration 44, loss = 1.54965697\n",
            "Iteration 45, loss = 1.54671087\n",
            "Iteration 46, loss = 1.54340343\n",
            "Iteration 47, loss = 1.54115366\n",
            "Iteration 48, loss = 1.53674603\n",
            "Iteration 49, loss = 1.53275173\n",
            "Iteration 50, loss = 1.52871134\n",
            "Iteration 51, loss = 1.52521175\n",
            "Iteration 52, loss = 1.52070498\n",
            "Iteration 53, loss = 1.51637699\n",
            "Iteration 54, loss = 1.51168329\n",
            "Iteration 55, loss = 1.50677066\n",
            "Iteration 56, loss = 1.50200539\n",
            "Iteration 57, loss = 1.49677158\n",
            "Iteration 58, loss = 1.49171435\n",
            "Iteration 59, loss = 1.48630413\n",
            "Iteration 60, loss = 1.48081115\n",
            "Iteration 61, loss = 1.47466594\n",
            "Iteration 62, loss = 1.46949198\n",
            "Iteration 63, loss = 1.46348050\n",
            "Iteration 64, loss = 1.45733634\n",
            "Iteration 65, loss = 1.45105235\n",
            "Iteration 66, loss = 1.44461898\n",
            "Iteration 67, loss = 1.43834901\n",
            "Iteration 68, loss = 1.43185759\n",
            "Iteration 69, loss = 1.42588255\n",
            "Iteration 70, loss = 1.41858615\n",
            "Iteration 71, loss = 1.41211656\n",
            "Iteration 72, loss = 1.40551572\n",
            "Iteration 73, loss = 1.39895146\n",
            "Iteration 74, loss = 1.39215571\n",
            "Iteration 75, loss = 1.38538585\n",
            "Iteration 76, loss = 1.37921180\n",
            "Iteration 77, loss = 1.37319767\n",
            "Iteration 78, loss = 1.36609445\n",
            "Iteration 79, loss = 1.35958573\n",
            "Iteration 80, loss = 1.35293707\n",
            "Iteration 81, loss = 1.34712985\n",
            "Iteration 82, loss = 1.34096296\n",
            "Iteration 83, loss = 1.33472997\n",
            "Iteration 84, loss = 1.32897359\n",
            "Iteration 85, loss = 1.32323950\n",
            "Iteration 86, loss = 1.31769225\n",
            "Iteration 87, loss = 1.31188910\n",
            "Iteration 88, loss = 1.30654910\n",
            "Iteration 89, loss = 1.30204500\n",
            "Iteration 90, loss = 1.29596659\n",
            "Iteration 91, loss = 1.29109505\n",
            "Iteration 92, loss = 1.28618118\n",
            "Iteration 93, loss = 1.28125937\n",
            "Iteration 94, loss = 1.27674504\n",
            "Iteration 95, loss = 1.27271251\n",
            "Iteration 96, loss = 1.26789072\n",
            "Iteration 97, loss = 1.26413334\n",
            "Iteration 98, loss = 1.26012360\n",
            "Iteration 99, loss = 1.25572104\n",
            "Iteration 100, loss = 1.25224031\n",
            "Iteration 1, loss = 1.74933235\n",
            "Iteration 2, loss = 1.63732659\n",
            "Iteration 3, loss = 1.67111095\n",
            "Iteration 4, loss = 1.64614056\n",
            "Iteration 5, loss = 1.60451281\n",
            "Iteration 6, loss = 1.60179434\n",
            "Iteration 7, loss = 1.60588283\n",
            "Iteration 8, loss = 1.59961819\n",
            "Iteration 9, loss = 1.59098461\n",
            "Iteration 10, loss = 1.58040112\n",
            "Iteration 11, loss = 1.57323382\n",
            "Iteration 12, loss = 1.56700127\n",
            "Iteration 13, loss = 1.55419261\n",
            "Iteration 14, loss = 1.53775507\n",
            "Iteration 15, loss = 1.51783179\n",
            "Iteration 16, loss = 1.49203603\n",
            "Iteration 17, loss = 1.45971043\n",
            "Iteration 18, loss = 1.42917597\n",
            "Iteration 19, loss = 1.39260227\n",
            "Iteration 20, loss = 1.35406693\n",
            "Iteration 21, loss = 1.31885244\n",
            "Iteration 22, loss = 1.28701241\n",
            "Iteration 23, loss = 1.25934241\n",
            "Iteration 24, loss = 1.23416648\n",
            "Iteration 25, loss = 1.21260324\n",
            "Iteration 26, loss = 1.19711617\n",
            "Iteration 27, loss = 1.18128812\n",
            "Iteration 28, loss = 1.16894473\n",
            "Iteration 29, loss = 1.15946877\n",
            "Iteration 30, loss = 1.14920705\n",
            "Iteration 31, loss = 1.14249587\n",
            "Iteration 32, loss = 1.13797443\n",
            "Iteration 33, loss = 1.13400578\n",
            "Iteration 34, loss = 1.13034222\n",
            "Iteration 35, loss = 1.12736064\n",
            "Iteration 36, loss = 1.12780400\n",
            "Iteration 37, loss = 1.12204934\n",
            "Iteration 38, loss = 1.12139575\n",
            "Iteration 39, loss = 1.11770800\n",
            "Iteration 40, loss = 1.12146711\n",
            "Iteration 41, loss = 1.11387507\n",
            "Iteration 42, loss = 1.11808606\n",
            "Iteration 43, loss = 1.11294744\n",
            "Iteration 44, loss = 1.11230822\n",
            "Iteration 45, loss = 1.11397236\n",
            "Iteration 46, loss = 1.11339680\n",
            "Iteration 47, loss = 1.11589547\n",
            "Iteration 48, loss = 1.11156532\n",
            "Iteration 49, loss = 1.12081335\n",
            "Iteration 50, loss = 1.11244976\n",
            "Iteration 51, loss = 1.11363780\n",
            "Iteration 52, loss = 1.11478350\n",
            "Iteration 53, loss = 1.10729209\n",
            "Iteration 54, loss = 1.11393943\n",
            "Iteration 55, loss = 1.11200060\n",
            "Iteration 56, loss = 1.11009444\n",
            "Iteration 57, loss = 1.10262078\n",
            "Iteration 58, loss = 1.10721576\n",
            "Iteration 59, loss = 1.10661999\n",
            "Iteration 60, loss = 1.10874249\n",
            "Iteration 61, loss = 1.10056434\n",
            "Iteration 62, loss = 1.10622509\n",
            "Iteration 63, loss = 1.11133371\n",
            "Iteration 64, loss = 1.10224775\n",
            "Iteration 65, loss = 1.10331973\n",
            "Iteration 66, loss = 1.10318186\n",
            "Iteration 67, loss = 1.10846485\n",
            "Iteration 68, loss = 1.10326056\n",
            "Iteration 69, loss = 1.10051201\n",
            "Iteration 70, loss = 1.10270800\n",
            "Iteration 71, loss = 1.09853775\n",
            "Iteration 72, loss = 1.09414741\n",
            "Iteration 73, loss = 1.09938308\n",
            "Iteration 74, loss = 1.09472733\n",
            "Iteration 75, loss = 1.09468598\n",
            "Iteration 76, loss = 1.09445018\n",
            "Iteration 77, loss = 1.09183007\n",
            "Iteration 78, loss = 1.09685842\n",
            "Iteration 79, loss = 1.09168383\n",
            "Iteration 80, loss = 1.08812170\n",
            "Iteration 81, loss = 1.08993703\n",
            "Iteration 82, loss = 1.08577396\n",
            "Iteration 83, loss = 1.08824955\n",
            "Iteration 84, loss = 1.08249305\n",
            "Iteration 85, loss = 1.08676297\n",
            "Iteration 86, loss = 1.08525620\n",
            "Iteration 87, loss = 1.07784887\n",
            "Iteration 88, loss = 1.07998442\n",
            "Iteration 89, loss = 1.08479112\n",
            "Iteration 90, loss = 1.07245341\n",
            "Iteration 91, loss = 1.08239918\n",
            "Iteration 92, loss = 1.07385178\n",
            "Iteration 93, loss = 1.07754806\n",
            "Iteration 94, loss = 1.06626761\n",
            "Iteration 95, loss = 1.07690769\n",
            "Iteration 96, loss = 1.06760758\n",
            "Iteration 97, loss = 1.07612561\n",
            "Iteration 98, loss = 1.06410837\n",
            "Iteration 99, loss = 1.06799081\n",
            "Iteration 100, loss = 1.06825843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.68708989\n",
            "Iteration 2, loss = 1.68202309\n",
            "Iteration 3, loss = 1.67711666\n",
            "Iteration 4, loss = 1.67266972\n",
            "Iteration 5, loss = 1.66865987\n",
            "Iteration 6, loss = 1.66446349\n",
            "Iteration 7, loss = 1.66040265\n",
            "Iteration 8, loss = 1.65666074\n",
            "Iteration 9, loss = 1.65261203\n",
            "Iteration 10, loss = 1.64888727\n",
            "Iteration 11, loss = 1.64583458\n",
            "Iteration 12, loss = 1.64235099\n",
            "Iteration 13, loss = 1.63924682\n",
            "Iteration 14, loss = 1.63623593\n",
            "Iteration 15, loss = 1.63300079\n",
            "Iteration 16, loss = 1.63047280\n",
            "Iteration 17, loss = 1.62775558\n",
            "Iteration 18, loss = 1.62493865\n",
            "Iteration 19, loss = 1.62282259\n",
            "Iteration 20, loss = 1.62032507\n",
            "Iteration 21, loss = 1.61802396\n",
            "Iteration 22, loss = 1.61567351\n",
            "Iteration 23, loss = 1.61374953\n",
            "Iteration 24, loss = 1.61165883\n",
            "Iteration 25, loss = 1.60972705\n",
            "Iteration 26, loss = 1.60796446\n",
            "Iteration 27, loss = 1.60638404\n",
            "Iteration 28, loss = 1.60455917\n",
            "Iteration 29, loss = 1.60292253\n",
            "Iteration 30, loss = 1.60140608\n",
            "Iteration 31, loss = 1.60000284\n",
            "Iteration 32, loss = 1.59861512\n",
            "Iteration 33, loss = 1.59726154\n",
            "Iteration 34, loss = 1.59603917\n",
            "Iteration 35, loss = 1.59501237\n",
            "Iteration 36, loss = 1.59376343\n",
            "Iteration 37, loss = 1.59271275\n",
            "Iteration 38, loss = 1.59170394\n",
            "Iteration 39, loss = 1.59060300\n",
            "Iteration 40, loss = 1.58979100\n",
            "Iteration 41, loss = 1.58884127\n",
            "Iteration 42, loss = 1.58805029\n",
            "Iteration 43, loss = 1.58710358\n",
            "Iteration 44, loss = 1.58634302\n",
            "Iteration 45, loss = 1.58560595\n",
            "Iteration 46, loss = 1.58479951\n",
            "Iteration 47, loss = 1.58429387\n",
            "Iteration 48, loss = 1.58352335\n",
            "Iteration 49, loss = 1.58285232\n",
            "Iteration 50, loss = 1.58208776\n",
            "Iteration 51, loss = 1.58154946\n",
            "Iteration 52, loss = 1.58088718\n",
            "Iteration 53, loss = 1.58026272\n",
            "Iteration 54, loss = 1.57967971\n",
            "Iteration 55, loss = 1.57905839\n",
            "Iteration 56, loss = 1.57853210\n",
            "Iteration 57, loss = 1.57793288\n",
            "Iteration 58, loss = 1.57738387\n",
            "Iteration 59, loss = 1.57682906\n",
            "Iteration 60, loss = 1.57627100\n",
            "Iteration 61, loss = 1.57566612\n",
            "Iteration 62, loss = 1.57512187\n",
            "Iteration 63, loss = 1.57462704\n",
            "Iteration 64, loss = 1.57407309\n",
            "Iteration 65, loss = 1.57347634\n",
            "Iteration 66, loss = 1.57291525\n",
            "Iteration 67, loss = 1.57238028\n",
            "Iteration 68, loss = 1.57183674\n",
            "Iteration 69, loss = 1.57132029\n",
            "Iteration 70, loss = 1.57072278\n",
            "Iteration 71, loss = 1.57017494\n",
            "Iteration 72, loss = 1.56961960\n",
            "Iteration 73, loss = 1.56907042\n",
            "Iteration 74, loss = 1.56850647\n",
            "Iteration 75, loss = 1.56793678\n",
            "Iteration 76, loss = 1.56740723\n",
            "Iteration 77, loss = 1.56687215\n",
            "Iteration 78, loss = 1.56626993\n",
            "Iteration 79, loss = 1.56567216\n",
            "Iteration 80, loss = 1.56508826\n",
            "Iteration 81, loss = 1.56453318\n",
            "Iteration 82, loss = 1.56395871\n",
            "Iteration 83, loss = 1.56335909\n",
            "Iteration 84, loss = 1.56278987\n",
            "Iteration 85, loss = 1.56219514\n",
            "Iteration 86, loss = 1.56161946\n",
            "Iteration 87, loss = 1.56099901\n",
            "Iteration 88, loss = 1.56041372\n",
            "Iteration 89, loss = 1.55986002\n",
            "Iteration 90, loss = 1.55918809\n",
            "Iteration 91, loss = 1.55858956\n",
            "Iteration 92, loss = 1.55796717\n",
            "Iteration 93, loss = 1.55734088\n",
            "Iteration 94, loss = 1.55673281\n",
            "Iteration 95, loss = 1.55613011\n",
            "Iteration 96, loss = 1.55545340\n",
            "Iteration 97, loss = 1.55485134\n",
            "Iteration 98, loss = 1.55421148\n",
            "Iteration 99, loss = 1.55355136\n",
            "Iteration 100, loss = 1.55291183\n",
            "Iteration 1, loss = 1.67948854\n",
            "Iteration 2, loss = 1.63923578\n",
            "Iteration 3, loss = 1.61158368\n",
            "Iteration 4, loss = 1.59580573\n",
            "Iteration 5, loss = 1.58869596\n",
            "Iteration 6, loss = 1.58516918\n",
            "Iteration 7, loss = 1.58457903\n",
            "Iteration 8, loss = 1.58281779\n",
            "Iteration 9, loss = 1.57920596\n",
            "Iteration 10, loss = 1.57411088\n",
            "Iteration 11, loss = 1.56687779\n",
            "Iteration 12, loss = 1.56102473\n",
            "Iteration 13, loss = 1.55530057\n",
            "Iteration 14, loss = 1.54990580\n",
            "Iteration 15, loss = 1.54582579\n",
            "Iteration 16, loss = 1.54028284\n",
            "Iteration 17, loss = 1.53461365\n",
            "Iteration 18, loss = 1.52862121\n",
            "Iteration 19, loss = 1.52192740\n",
            "Iteration 20, loss = 1.51441237\n",
            "Iteration 21, loss = 1.50675396\n",
            "Iteration 22, loss = 1.49828540\n",
            "Iteration 23, loss = 1.48941624\n",
            "Iteration 24, loss = 1.48046057\n",
            "Iteration 25, loss = 1.47137036\n",
            "Iteration 26, loss = 1.46164417\n",
            "Iteration 27, loss = 1.45102557\n",
            "Iteration 28, loss = 1.44080975\n",
            "Iteration 29, loss = 1.42991804\n",
            "Iteration 30, loss = 1.41863894\n",
            "Iteration 31, loss = 1.40699111\n",
            "Iteration 32, loss = 1.39572786\n",
            "Iteration 33, loss = 1.38414293\n",
            "Iteration 34, loss = 1.37214936\n",
            "Iteration 35, loss = 1.36061763\n",
            "Iteration 36, loss = 1.34932310\n",
            "Iteration 37, loss = 1.33713567\n",
            "Iteration 38, loss = 1.32617485\n",
            "Iteration 39, loss = 1.31505233\n",
            "Iteration 40, loss = 1.30488661\n",
            "Iteration 41, loss = 1.29396556\n",
            "Iteration 42, loss = 1.28506715\n",
            "Iteration 43, loss = 1.27469354\n",
            "Iteration 44, loss = 1.26582897\n",
            "Iteration 45, loss = 1.25728655\n",
            "Iteration 46, loss = 1.24893363\n",
            "Iteration 47, loss = 1.24189177\n",
            "Iteration 48, loss = 1.23402445\n",
            "Iteration 49, loss = 1.22711487\n",
            "Iteration 50, loss = 1.22047186\n",
            "Iteration 51, loss = 1.21422158\n",
            "Iteration 52, loss = 1.20807734\n",
            "Iteration 53, loss = 1.20267508\n",
            "Iteration 54, loss = 1.19718511\n",
            "Iteration 55, loss = 1.19213749\n",
            "Iteration 56, loss = 1.18770690\n",
            "Iteration 57, loss = 1.18313379\n",
            "Iteration 58, loss = 1.17922172\n",
            "Iteration 59, loss = 1.17514130\n",
            "Iteration 60, loss = 1.17128598\n",
            "Iteration 61, loss = 1.16736638\n",
            "Iteration 62, loss = 1.16427182\n",
            "Iteration 63, loss = 1.16136965\n",
            "Iteration 64, loss = 1.15814175\n",
            "Iteration 65, loss = 1.15506972\n",
            "Iteration 66, loss = 1.15230767\n",
            "Iteration 67, loss = 1.14976503\n",
            "Iteration 68, loss = 1.14680049\n",
            "Iteration 69, loss = 1.14454803\n",
            "Iteration 70, loss = 1.14203834\n",
            "Iteration 71, loss = 1.14001009\n",
            "Iteration 72, loss = 1.13752756\n",
            "Iteration 73, loss = 1.13593756\n",
            "Iteration 74, loss = 1.13351345\n",
            "Iteration 75, loss = 1.13144654\n",
            "Iteration 76, loss = 1.13020191\n",
            "Iteration 77, loss = 1.12824680\n",
            "Iteration 78, loss = 1.12618624\n",
            "Iteration 79, loss = 1.12533840\n",
            "Iteration 80, loss = 1.12328768\n",
            "Iteration 81, loss = 1.12133383\n",
            "Iteration 82, loss = 1.11943737\n",
            "Iteration 83, loss = 1.11888753\n",
            "Iteration 84, loss = 1.11701066\n",
            "Iteration 85, loss = 1.11518693\n",
            "Iteration 86, loss = 1.11392686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 87, loss = 1.11246784\n",
            "Iteration 88, loss = 1.11141273\n",
            "Iteration 89, loss = 1.11111009\n",
            "Iteration 90, loss = 1.10830992\n",
            "Iteration 91, loss = 1.10738761\n",
            "Iteration 92, loss = 1.10558564\n",
            "Iteration 93, loss = 1.10437513\n",
            "Iteration 94, loss = 1.10322586\n",
            "Iteration 95, loss = 1.10353535\n",
            "Iteration 96, loss = 1.10104282\n",
            "Iteration 97, loss = 1.10098062\n",
            "Iteration 98, loss = 1.09885707\n",
            "Iteration 99, loss = 1.09705297\n",
            "Iteration 100, loss = 1.09606155\n",
            "Iteration 1, loss = 1.68491375\n",
            "Iteration 2, loss = 1.61700425\n",
            "Iteration 3, loss = 1.59347512\n",
            "Iteration 4, loss = 1.55542775\n",
            "Iteration 5, loss = 1.51252996\n",
            "Iteration 6, loss = 1.45941776\n",
            "Iteration 7, loss = 1.39426687\n",
            "Iteration 8, loss = 1.32236497\n",
            "Iteration 9, loss = 1.27428399\n",
            "Iteration 10, loss = 1.21771802\n",
            "Iteration 11, loss = 1.19208405\n",
            "Iteration 12, loss = 1.16762323\n",
            "Iteration 13, loss = 1.14038702\n",
            "Iteration 14, loss = 1.13684959\n",
            "Iteration 15, loss = 1.12865737\n",
            "Iteration 16, loss = 1.11423821\n",
            "Iteration 17, loss = 1.11860811\n",
            "Iteration 18, loss = 1.11252236\n",
            "Iteration 19, loss = 1.10378130\n",
            "Iteration 20, loss = 1.11050350\n",
            "Iteration 21, loss = 1.11137266\n",
            "Iteration 22, loss = 1.10602852\n",
            "Iteration 23, loss = 1.11544916\n",
            "Iteration 24, loss = 1.09699529\n",
            "Iteration 25, loss = 1.10378744\n",
            "Iteration 26, loss = 1.10295320\n",
            "Iteration 27, loss = 1.12056814\n",
            "Iteration 28, loss = 1.09017085\n",
            "Iteration 29, loss = 1.10108850\n",
            "Iteration 30, loss = 1.08905289\n",
            "Iteration 31, loss = 1.08392922\n",
            "Iteration 32, loss = 1.07337119\n",
            "Iteration 33, loss = 1.07830361\n",
            "Iteration 34, loss = 1.06049829\n",
            "Iteration 35, loss = 1.06519610\n",
            "Iteration 36, loss = 1.05148280\n",
            "Iteration 37, loss = 1.04300162\n",
            "Iteration 38, loss = 1.04811300\n",
            "Iteration 39, loss = 1.02506410\n",
            "Iteration 40, loss = 1.04317210\n",
            "Iteration 41, loss = 1.01664712\n",
            "Iteration 42, loss = 1.01291465\n",
            "Iteration 43, loss = 1.00763865\n",
            "Iteration 44, loss = 0.99799453\n",
            "Iteration 45, loss = 0.98903005\n",
            "Iteration 46, loss = 0.98075693\n",
            "Iteration 47, loss = 0.99476373\n",
            "Iteration 48, loss = 0.99805494\n",
            "Iteration 49, loss = 1.01280086\n",
            "Iteration 50, loss = 1.01169849\n",
            "Iteration 51, loss = 0.99892939\n",
            "Iteration 52, loss = 0.97588391\n",
            "Iteration 53, loss = 0.94560892\n",
            "Iteration 54, loss = 0.93815699\n",
            "Iteration 55, loss = 0.93577875\n",
            "Iteration 56, loss = 0.93899312\n",
            "Iteration 57, loss = 0.91184774\n",
            "Iteration 58, loss = 0.91807355\n",
            "Iteration 59, loss = 0.90658548\n",
            "Iteration 60, loss = 0.89905847\n",
            "Iteration 61, loss = 0.88676223\n",
            "Iteration 62, loss = 0.88555950\n",
            "Iteration 63, loss = 0.88553751\n",
            "Iteration 64, loss = 0.88177829\n",
            "Iteration 65, loss = 0.88598106\n",
            "Iteration 66, loss = 0.91837267\n",
            "Iteration 67, loss = 0.88910503\n",
            "Iteration 68, loss = 0.87324655\n",
            "Iteration 69, loss = 0.87785454\n",
            "Iteration 70, loss = 0.85934056\n",
            "Iteration 71, loss = 0.85489378\n",
            "Iteration 72, loss = 0.84198571\n",
            "Iteration 73, loss = 0.83246984\n",
            "Iteration 74, loss = 0.83181088\n",
            "Iteration 75, loss = 0.82940558\n",
            "Iteration 76, loss = 0.83202795\n",
            "Iteration 77, loss = 0.83109015\n",
            "Iteration 78, loss = 0.81856441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 79, loss = 0.82460843\n",
            "Iteration 80, loss = 0.82726474\n",
            "Iteration 81, loss = 0.81614225\n",
            "Iteration 82, loss = 0.81301871\n",
            "Iteration 83, loss = 0.84990301\n",
            "Iteration 84, loss = 0.80982915\n",
            "Iteration 85, loss = 0.82078880\n",
            "Iteration 86, loss = 0.91162900\n",
            "Iteration 87, loss = 0.84775578\n",
            "Iteration 88, loss = 0.79415877\n",
            "Iteration 89, loss = 0.80353898\n",
            "Iteration 90, loss = 0.91129357\n",
            "Iteration 91, loss = 0.79249596\n",
            "Iteration 92, loss = 0.83967768\n",
            "Iteration 93, loss = 0.85698500\n",
            "Iteration 94, loss = 0.82918625\n",
            "Iteration 95, loss = 0.80824812\n",
            "Iteration 96, loss = 0.83948642\n",
            "Iteration 97, loss = 0.85379014\n",
            "Iteration 98, loss = 0.76913780\n",
            "Iteration 99, loss = 0.80263282\n",
            "Iteration 100, loss = 0.77569790\n",
            "Iteration 1, loss = 1.65503802\n",
            "Iteration 2, loss = 1.65240465\n",
            "Iteration 3, loss = 1.64977659\n",
            "Iteration 4, loss = 1.64734919\n",
            "Iteration 5, loss = 1.64518070\n",
            "Iteration 6, loss = 1.64286994\n",
            "Iteration 7, loss = 1.64063045\n",
            "Iteration 8, loss = 1.63848627\n",
            "Iteration 9, loss = 1.63621434\n",
            "Iteration 10, loss = 1.63396117\n",
            "Iteration 11, loss = 1.63220702\n",
            "Iteration 12, loss = 1.63015958\n",
            "Iteration 13, loss = 1.62830687\n",
            "Iteration 14, loss = 1.62647255\n",
            "Iteration 15, loss = 1.62452125\n",
            "Iteration 16, loss = 1.62291733\n",
            "Iteration 17, loss = 1.62128952\n",
            "Iteration 18, loss = 1.61954365\n",
            "Iteration 19, loss = 1.61821755\n",
            "Iteration 20, loss = 1.61661765\n",
            "Iteration 21, loss = 1.61507977\n",
            "Iteration 22, loss = 1.61357568\n",
            "Iteration 23, loss = 1.61235237\n",
            "Iteration 24, loss = 1.61091530\n",
            "Iteration 25, loss = 1.60956165\n",
            "Iteration 26, loss = 1.60830437\n",
            "Iteration 27, loss = 1.60721603\n",
            "Iteration 28, loss = 1.60588519\n",
            "Iteration 29, loss = 1.60470370\n",
            "Iteration 30, loss = 1.60355164\n",
            "Iteration 31, loss = 1.60244471\n",
            "Iteration 32, loss = 1.60133634\n",
            "Iteration 33, loss = 1.60026121\n",
            "Iteration 34, loss = 1.59927233\n",
            "Iteration 35, loss = 1.59841044\n",
            "Iteration 36, loss = 1.59729320\n",
            "Iteration 37, loss = 1.59644301\n",
            "Iteration 38, loss = 1.59557398\n",
            "Iteration 39, loss = 1.59468190\n",
            "Iteration 40, loss = 1.59398030\n",
            "Iteration 41, loss = 1.59320802\n",
            "Iteration 42, loss = 1.59253292\n",
            "Iteration 43, loss = 1.59170514\n",
            "Iteration 44, loss = 1.59102191\n",
            "Iteration 45, loss = 1.59032313\n",
            "Iteration 46, loss = 1.58956763\n",
            "Iteration 47, loss = 1.58902723\n",
            "Iteration 48, loss = 1.58835256\n",
            "Iteration 49, loss = 1.58769514\n",
            "Iteration 50, loss = 1.58698217\n",
            "Iteration 51, loss = 1.58646986\n",
            "Iteration 52, loss = 1.58580837\n",
            "Iteration 53, loss = 1.58520164\n",
            "Iteration 54, loss = 1.58464397\n",
            "Iteration 55, loss = 1.58404596\n",
            "Iteration 56, loss = 1.58356260\n",
            "Iteration 57, loss = 1.58297773\n",
            "Iteration 58, loss = 1.58244931\n",
            "Iteration 59, loss = 1.58193700\n",
            "Iteration 60, loss = 1.58140712\n",
            "Iteration 61, loss = 1.58083249\n",
            "Iteration 62, loss = 1.58029925\n",
            "Iteration 63, loss = 1.57989772\n",
            "Iteration 64, loss = 1.57939750\n",
            "Iteration 65, loss = 1.57881741\n",
            "Iteration 66, loss = 1.57831922\n",
            "Iteration 67, loss = 1.57785800\n",
            "Iteration 68, loss = 1.57738552\n",
            "Iteration 69, loss = 1.57693684\n",
            "Iteration 70, loss = 1.57643879\n",
            "Iteration 71, loss = 1.57597414\n",
            "Iteration 72, loss = 1.57549241\n",
            "Iteration 73, loss = 1.57504442\n",
            "Iteration 74, loss = 1.57457816\n",
            "Iteration 75, loss = 1.57411083\n",
            "Iteration 76, loss = 1.57368517\n",
            "Iteration 77, loss = 1.57326693\n",
            "Iteration 78, loss = 1.57276692\n",
            "Iteration 79, loss = 1.57226664\n",
            "Iteration 80, loss = 1.57182855"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 81, loss = 1.57137110\n",
            "Iteration 82, loss = 1.57091927\n",
            "Iteration 83, loss = 1.57046676\n",
            "Iteration 84, loss = 1.57004090\n",
            "Iteration 85, loss = 1.56956072\n",
            "Iteration 86, loss = 1.56911693\n",
            "Iteration 87, loss = 1.56863574\n",
            "Iteration 88, loss = 1.56822375\n",
            "Iteration 89, loss = 1.56776642\n",
            "Iteration 90, loss = 1.56728179\n",
            "Iteration 91, loss = 1.56681097\n",
            "Iteration 92, loss = 1.56634977\n",
            "Iteration 93, loss = 1.56588298\n",
            "Iteration 94, loss = 1.56541260\n",
            "Iteration 95, loss = 1.56498832\n",
            "Iteration 96, loss = 1.56447356\n",
            "Iteration 97, loss = 1.56401463\n",
            "Iteration 98, loss = 1.56354387\n",
            "Iteration 99, loss = 1.56305570\n",
            "Iteration 100, loss = 1.56258354\n",
            "Iteration 1, loss = 1.65067235\n",
            "Iteration 2, loss = 1.62819424\n",
            "Iteration 3, loss = 1.61086284\n",
            "Iteration 4, loss = 1.59910994\n",
            "Iteration 5, loss = 1.59391616\n",
            "Iteration 6, loss = 1.58888180\n",
            "Iteration 7, loss = 1.58500994\n",
            "Iteration 8, loss = 1.58143088\n",
            "Iteration 9, loss = 1.57749572\n",
            "Iteration 10, loss = 1.57572300\n",
            "Iteration 11, loss = 1.57299422\n",
            "Iteration 12, loss = 1.57002691\n",
            "Iteration 13, loss = 1.56625586\n",
            "Iteration 14, loss = 1.56241227\n",
            "Iteration 15, loss = 1.55876352\n",
            "Iteration 16, loss = 1.55421842\n",
            "Iteration 17, loss = 1.54969695\n",
            "Iteration 18, loss = 1.54542587\n",
            "Iteration 19, loss = 1.54034915\n",
            "Iteration 20, loss = 1.53517036\n",
            "Iteration 21, loss = 1.52951478\n",
            "Iteration 22, loss = 1.52374061\n",
            "Iteration 23, loss = 1.51739985\n",
            "Iteration 24, loss = 1.51079648\n",
            "Iteration 25, loss = 1.50400165\n",
            "Iteration 26, loss = 1.49674008\n",
            "Iteration 27, loss = 1.48899394\n",
            "Iteration 28, loss = 1.48128512\n",
            "Iteration 29, loss = 1.47313636\n",
            "Iteration 30, loss = 1.46466107\n",
            "Iteration 31, loss = 1.45585066\n",
            "Iteration 32, loss = 1.44709403\n",
            "Iteration 33, loss = 1.43793850\n",
            "Iteration 34, loss = 1.42804143\n",
            "Iteration 35, loss = 1.41824861\n",
            "Iteration 36, loss = 1.40838061\n",
            "Iteration 37, loss = 1.39753362\n",
            "Iteration 38, loss = 1.38725478\n",
            "Iteration 39, loss = 1.37666587\n",
            "Iteration 40, loss = 1.36651017\n",
            "Iteration 41, loss = 1.35545347\n",
            "Iteration 42, loss = 1.34597308\n",
            "Iteration 43, loss = 1.33505151\n",
            "Iteration 44, loss = 1.32512806\n",
            "Iteration 45, loss = 1.31564768\n",
            "Iteration 46, loss = 1.30608632\n",
            "Iteration 47, loss = 1.29728115\n",
            "Iteration 48, loss = 1.28816167\n",
            "Iteration 49, loss = 1.27967143\n",
            "Iteration 50, loss = 1.27147422\n",
            "Iteration 51, loss = 1.26382471\n",
            "Iteration 52, loss = 1.25614985\n",
            "Iteration 53, loss = 1.24927884\n",
            "Iteration 54, loss = 1.24231666\n",
            "Iteration 55, loss = 1.23579097\n",
            "Iteration 56, loss = 1.22991113\n",
            "Iteration 57, loss = 1.22402094\n",
            "Iteration 58, loss = 1.21876447\n",
            "Iteration 59, loss = 1.21350121\n",
            "Iteration 60, loss = 1.20857652\n",
            "Iteration 61, loss = 1.20356945\n",
            "Iteration 62, loss = 1.19943689\n",
            "Iteration 63, loss = 1.19552385\n",
            "Iteration 64, loss = 1.19134410\n",
            "Iteration 65, loss = 1.18740077\n",
            "Iteration 66, loss = 1.18394545\n",
            "Iteration 67, loss = 1.18058869\n",
            "Iteration 68, loss = 1.17699870\n",
            "Iteration 69, loss = 1.17416911\n",
            "Iteration 70, loss = 1.17100788\n",
            "Iteration 71, loss = 1.16837665\n",
            "Iteration 72, loss = 1.16542722\n",
            "Iteration 73, loss = 1.16332398\n",
            "Iteration 74, loss = 1.16051661\n",
            "Iteration 75, loss = 1.15797096\n",
            "Iteration 76, loss = 1.15640797\n",
            "Iteration 77, loss = 1.15424401\n",
            "Iteration 78, loss = 1.15179296\n",
            "Iteration 79, loss = 1.15034871\n",
            "Iteration 80, loss = 1.14821099\n",
            "Iteration 81, loss = 1.14638884\n",
            "Iteration 82, loss = 1.14423598\n",
            "Iteration 83, loss = 1.14309610\n",
            "Iteration 84, loss = 1.14121812\n",
            "Iteration 85, loss = 1.13945807\n",
            "Iteration 86, loss = 1.13811206\n",
            "Iteration 87, loss = 1.13669510\n",
            "Iteration 88, loss = 1.13570261\n",
            "Iteration 89, loss = 1.13452600\n",
            "Iteration 90, loss = 1.13252700\n",
            "Iteration 91, loss = 1.13156602\n",
            "Iteration 92, loss = 1.13023336\n",
            "Iteration 93, loss = 1.12899318\n",
            "Iteration 94, loss = 1.12792751\n",
            "Iteration 95, loss = 1.12776062\n",
            "Iteration 96, loss = 1.12584532\n",
            "Iteration 97, loss = 1.12541186\n",
            "Iteration 98, loss = 1.12420567\n",
            "Iteration 99, loss = 1.12275144\n",
            "Iteration 100, loss = 1.12208574\n",
            "Iteration 1, loss = 1.63811199\n",
            "Iteration 2, loss = 1.60297647\n",
            "Iteration 3, loss = 1.58261775\n",
            "Iteration 4, loss = 1.55911965\n",
            "Iteration 5, loss = 1.53099794\n",
            "Iteration 6, loss = 1.49376436\n",
            "Iteration 7, loss = 1.44789532\n",
            "Iteration 8, loss = 1.39095096\n",
            "Iteration 9, loss = 1.34225903\n",
            "Iteration 10, loss = 1.29010290\n",
            "Iteration 11, loss = 1.24946213\n",
            "Iteration 12, loss = 1.21005841\n",
            "Iteration 13, loss = 1.18358615\n",
            "Iteration 14, loss = 1.16317805\n",
            "Iteration 15, loss = 1.14781807\n",
            "Iteration 16, loss = 1.13704988\n",
            "Iteration 17, loss = 1.12971943\n",
            "Iteration 18, loss = 1.11912361\n",
            "Iteration 19, loss = 1.11781802\n",
            "Iteration 20, loss = 1.11635017\n",
            "Iteration 21, loss = 1.10902535\n",
            "Iteration 22, loss = 1.10776328\n",
            "Iteration 23, loss = 1.10266029\n",
            "Iteration 24, loss = 1.09989443\n",
            "Iteration 25, loss = 1.09630129\n",
            "Iteration 26, loss = 1.09449829\n",
            "Iteration 27, loss = 1.09135063\n",
            "Iteration 28, loss = 1.09220510\n",
            "Iteration 29, loss = 1.08855071\n",
            "Iteration 30, loss = 1.08398657\n",
            "Iteration 31, loss = 1.08658594\n",
            "Iteration 32, loss = 1.07892620\n",
            "Iteration 33, loss = 1.07738595\n",
            "Iteration 34, loss = 1.07544393\n",
            "Iteration 35, loss = 1.07393309\n",
            "Iteration 36, loss = 1.07115887\n",
            "Iteration 37, loss = 1.06161790\n",
            "Iteration 38, loss = 1.05823146\n",
            "Iteration 39, loss = 1.05131715\n",
            "Iteration 40, loss = 1.05039256\n",
            "Iteration 41, loss = 1.04344956\n",
            "Iteration 42, loss = 1.04192124\n",
            "Iteration 43, loss = 1.03578382\n",
            "Iteration 44, loss = 1.02995047\n",
            "Iteration 45, loss = 1.02443739\n",
            "Iteration 46, loss = 1.02089024\n",
            "Iteration 47, loss = 1.04726191\n",
            "Iteration 48, loss = 1.03202788\n",
            "Iteration 49, loss = 1.01456740\n",
            "Iteration 50, loss = 1.02082637\n",
            "Iteration 51, loss = 1.01578118\n",
            "Iteration 52, loss = 0.99145970\n",
            "Iteration 53, loss = 1.00023894\n",
            "Iteration 54, loss = 0.98816111\n",
            "Iteration 55, loss = 0.97810926\n",
            "Iteration 56, loss = 0.97359771\n",
            "Iteration 57, loss = 0.96502350\n",
            "Iteration 58, loss = 0.96346710\n",
            "Iteration 59, loss = 0.96699249\n",
            "Iteration 60, loss = 0.95317699\n",
            "Iteration 61, loss = 0.96132517\n",
            "Iteration 62, loss = 0.94455570\n",
            "Iteration 63, loss = 0.94855022\n",
            "Iteration 64, loss = 0.94313103\n",
            "Iteration 65, loss = 0.93578158\n",
            "Iteration 66, loss = 0.92453494\n",
            "Iteration 67, loss = 0.92472072\n",
            "Iteration 68, loss = 0.91897629\n",
            "Iteration 69, loss = 0.91513462\n",
            "Iteration 70, loss = 0.91545278\n",
            "Iteration 71, loss = 0.91021727\n",
            "Iteration 72, loss = 0.90323537\n",
            "Iteration 73, loss = 0.89946825\n",
            "Iteration 74, loss = 0.89165997\n",
            "Iteration 75, loss = 0.89458590\n",
            "Iteration 76, loss = 0.90063853\n",
            "Iteration 77, loss = 0.89019804\n",
            "Iteration 78, loss = 0.88568610\n",
            "Iteration 79, loss = 0.88510354\n",
            "Iteration 80, loss = 0.87679866\n",
            "Iteration 81, loss = 0.87758672\n",
            "Iteration 82, loss = 0.86582289\n",
            "Iteration 83, loss = 0.87308549\n",
            "Iteration 84, loss = 0.86276349\n",
            "Iteration 85, loss = 0.88252775\n",
            "Iteration 86, loss = 0.87735463\n",
            "Iteration 87, loss = 0.86403719\n",
            "Iteration 88, loss = 0.87449070\n",
            "Iteration 89, loss = 0.91391130\n",
            "Iteration 90, loss = 0.87471500\n",
            "Iteration 91, loss = 0.87953011\n",
            "Iteration 92, loss = 0.83675004\n",
            "Iteration 93, loss = 0.86860769\n",
            "Iteration 94, loss = 0.85254342\n",
            "Iteration 95, loss = 0.86544638\n",
            "Iteration 96, loss = 0.85611654\n",
            "Iteration 97, loss = 0.84146691\n",
            "Iteration 98, loss = 0.86080212\n",
            "Iteration 99, loss = 0.83950900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100, loss = 0.86558261\n",
            "Iteration 1, loss = 1.73394460\n",
            "Iteration 2, loss = 1.72784221\n",
            "Iteration 3, loss = 1.72166571\n",
            "Iteration 4, loss = 1.71573270\n",
            "Iteration 5, loss = 1.70987326\n",
            "Iteration 6, loss = 1.70455639\n",
            "Iteration 7, loss = 1.69933517\n",
            "Iteration 8, loss = 1.69416221\n",
            "Iteration 9, loss = 1.68962509\n",
            "Iteration 10, loss = 1.68495843\n",
            "Iteration 11, loss = 1.68043476\n",
            "Iteration 12, loss = 1.67599950\n",
            "Iteration 13, loss = 1.67187518\n",
            "Iteration 14, loss = 1.66829651\n",
            "Iteration 15, loss = 1.66445100\n",
            "Iteration 16, loss = 1.66074614\n",
            "Iteration 17, loss = 1.65752011\n",
            "Iteration 18, loss = 1.65425676\n",
            "Iteration 19, loss = 1.65103910\n",
            "Iteration 20, loss = 1.64835116\n",
            "Iteration 21, loss = 1.64551953\n",
            "Iteration 22, loss = 1.64303776\n",
            "Iteration 23, loss = 1.64032950\n",
            "Iteration 24, loss = 1.63840245\n",
            "Iteration 25, loss = 1.63584816\n",
            "Iteration 26, loss = 1.63368286\n",
            "Iteration 27, loss = 1.63168203\n",
            "Iteration 28, loss = 1.62992124\n",
            "Iteration 29, loss = 1.62822506\n",
            "Iteration 30, loss = 1.62643979\n",
            "Iteration 31, loss = 1.62473048\n",
            "Iteration 32, loss = 1.62348011\n",
            "Iteration 33, loss = 1.62186347\n",
            "Iteration 34, loss = 1.62062188\n",
            "Iteration 35, loss = 1.61938434\n",
            "Iteration 36, loss = 1.61821281\n",
            "Iteration 37, loss = 1.61723143\n",
            "Iteration 38, loss = 1.61605911\n",
            "Iteration 39, loss = 1.61506906\n",
            "Iteration 40, loss = 1.61416864\n",
            "Iteration 41, loss = 1.61326428\n",
            "Iteration 42, loss = 1.61253539\n",
            "Iteration 43, loss = 1.61168688\n",
            "Iteration 44, loss = 1.61103573\n",
            "Iteration 45, loss = 1.61019341\n",
            "Iteration 46, loss = 1.60955021\n",
            "Iteration 47, loss = 1.60887312\n",
            "Iteration 48, loss = 1.60836275\n",
            "Iteration 49, loss = 1.60777292\n",
            "Iteration 50, loss = 1.60731046\n",
            "Iteration 51, loss = 1.60668636\n",
            "Iteration 52, loss = 1.60625319\n",
            "Iteration 53, loss = 1.60567723\n",
            "Iteration 54, loss = 1.60525258\n",
            "Iteration 55, loss = 1.60484458\n",
            "Iteration 56, loss = 1.60437555\n",
            "Iteration 57, loss = 1.60394639\n",
            "Iteration 58, loss = 1.60354308\n",
            "Iteration 59, loss = 1.60311045\n",
            "Iteration 60, loss = 1.60275756\n",
            "Iteration 61, loss = 1.60238697\n",
            "Iteration 62, loss = 1.60196114\n",
            "Iteration 63, loss = 1.60158756\n",
            "Iteration 64, loss = 1.60122482\n",
            "Iteration 65, loss = 1.60081202\n",
            "Iteration 66, loss = 1.60047456\n",
            "Iteration 67, loss = 1.60014357\n",
            "Iteration 68, loss = 1.59976454\n",
            "Iteration 69, loss = 1.59946287\n",
            "Iteration 70, loss = 1.59903628\n",
            "Iteration 71, loss = 1.59871952\n",
            "Iteration 72, loss = 1.59835032\n",
            "Iteration 73, loss = 1.59809529\n",
            "Iteration 74, loss = 1.59766976\n",
            "Iteration 75, loss = 1.59726149\n",
            "Iteration 76, loss = 1.59695183\n",
            "Iteration 77, loss = 1.59658658\n",
            "Iteration 78, loss = 1.59627590\n",
            "Iteration 79, loss = 1.59586518\n",
            "Iteration 80, loss = 1.59552688\n",
            "Iteration 81, loss = 1.59515543\n",
            "Iteration 82, loss = 1.59482673\n",
            "Iteration 83, loss = 1.59445621\n",
            "Iteration 84, loss = 1.59408955\n",
            "Iteration 85, loss = 1.59371746\n",
            "Iteration 86, loss = 1.59334381\n",
            "Iteration 87, loss = 1.59299679\n",
            "Iteration 88, loss = 1.59260297\n",
            "Iteration 89, loss = 1.59224132\n",
            "Iteration 90, loss = 1.59185492\n",
            "Iteration 91, loss = 1.59149193\n",
            "Iteration 92, loss = 1.59113372\n",
            "Iteration 93, loss = 1.59071452\n",
            "Iteration 94, loss = 1.59032332\n",
            "Iteration 95, loss = 1.58995310\n",
            "Iteration 96, loss = 1.58952820\n",
            "Iteration 97, loss = 1.58916089\n",
            "Iteration 98, loss = 1.58873764\n",
            "Iteration 99, loss = 1.58831695\n",
            "Iteration 100, loss = 1.58795316\n",
            "Iteration 1, loss = 1.71952437\n",
            "Iteration 2, loss = 1.67143577\n",
            "Iteration 3, loss = 1.63775575\n",
            "Iteration 4, loss = 1.61777604\n",
            "Iteration 5, loss = 1.60753488\n",
            "Iteration 6, loss = 1.60688622\n",
            "Iteration 7, loss = 1.60669645\n",
            "Iteration 8, loss = 1.60629947\n",
            "Iteration 9, loss = 1.60497208\n",
            "Iteration 10, loss = 1.60091777\n",
            "Iteration 11, loss = 1.59643009\n",
            "Iteration 12, loss = 1.59194912\n",
            "Iteration 13, loss = 1.58760943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14, loss = 1.58290068\n",
            "Iteration 15, loss = 1.57913121\n",
            "Iteration 16, loss = 1.57569995\n",
            "Iteration 17, loss = 1.57153086\n",
            "Iteration 18, loss = 1.56706194\n",
            "Iteration 19, loss = 1.56261507\n",
            "Iteration 20, loss = 1.55726295\n",
            "Iteration 21, loss = 1.55197395\n",
            "Iteration 22, loss = 1.54587175\n",
            "Iteration 23, loss = 1.53857856\n",
            "Iteration 24, loss = 1.53199244\n",
            "Iteration 25, loss = 1.52355729\n",
            "Iteration 26, loss = 1.51542798\n",
            "Iteration 27, loss = 1.50685516\n",
            "Iteration 28, loss = 1.49747029\n",
            "Iteration 29, loss = 1.48793077\n",
            "Iteration 30, loss = 1.47774684\n",
            "Iteration 31, loss = 1.46630744\n",
            "Iteration 32, loss = 1.45523107\n",
            "Iteration 33, loss = 1.44387711\n",
            "Iteration 34, loss = 1.43157462\n",
            "Iteration 35, loss = 1.41916683\n",
            "Iteration 36, loss = 1.40685169\n",
            "Iteration 37, loss = 1.39445935\n",
            "Iteration 38, loss = 1.38154297\n",
            "Iteration 39, loss = 1.36884741\n",
            "Iteration 40, loss = 1.35604291\n",
            "Iteration 41, loss = 1.34411504\n",
            "Iteration 42, loss = 1.33169414\n",
            "Iteration 43, loss = 1.31996115\n",
            "Iteration 44, loss = 1.30868242\n",
            "Iteration 45, loss = 1.29721839\n",
            "Iteration 46, loss = 1.28658660\n",
            "Iteration 47, loss = 1.27648641\n",
            "Iteration 48, loss = 1.26684701\n",
            "Iteration 49, loss = 1.25776092\n",
            "Iteration 50, loss = 1.24930284\n",
            "Iteration 51, loss = 1.24091070\n",
            "Iteration 52, loss = 1.23321530\n",
            "Iteration 53, loss = 1.22595699\n",
            "Iteration 54, loss = 1.21935856\n",
            "Iteration 55, loss = 1.21318021\n",
            "Iteration 56, loss = 1.20793837\n",
            "Iteration 57, loss = 1.20186855\n",
            "Iteration 58, loss = 1.19659438\n",
            "Iteration 59, loss = 1.19140586\n",
            "Iteration 60, loss = 1.18713965\n",
            "Iteration 61, loss = 1.18345323\n",
            "Iteration 62, loss = 1.17920089\n",
            "Iteration 63, loss = 1.17493059\n",
            "Iteration 64, loss = 1.17160918\n",
            "Iteration 65, loss = 1.16849399\n",
            "Iteration 66, loss = 1.16540301\n",
            "Iteration 67, loss = 1.16271653\n",
            "Iteration 68, loss = 1.15923856\n",
            "Iteration 69, loss = 1.15679766\n",
            "Iteration 70, loss = 1.15414288\n",
            "Iteration 71, loss = 1.15191024\n",
            "Iteration 72, loss = 1.14910846\n",
            "Iteration 73, loss = 1.14879094\n",
            "Iteration 74, loss = 1.14520825\n",
            "Iteration 75, loss = 1.14287188\n",
            "Iteration 76, loss = 1.14202851\n",
            "Iteration 77, loss = 1.13920648\n",
            "Iteration 78, loss = 1.13788091\n",
            "Iteration 79, loss = 1.13669237\n",
            "Iteration 80, loss = 1.13406264\n",
            "Iteration 81, loss = 1.13255804\n",
            "Iteration 82, loss = 1.13154417\n",
            "Iteration 83, loss = 1.13002601\n",
            "Iteration 84, loss = 1.12845350\n",
            "Iteration 85, loss = 1.12705716\n",
            "Iteration 86, loss = 1.12567742\n",
            "Iteration 87, loss = 1.12477390\n",
            "Iteration 88, loss = 1.12336183\n",
            "Iteration 89, loss = 1.12227301\n",
            "Iteration 90, loss = 1.12082592\n",
            "Iteration 91, loss = 1.11995005\n",
            "Iteration 92, loss = 1.11937563\n",
            "Iteration 93, loss = 1.11746579\n",
            "Iteration 94, loss = 1.11645018\n",
            "Iteration 95, loss = 1.11689656\n",
            "Iteration 96, loss = 1.11503744\n",
            "Iteration 97, loss = 1.11465687\n",
            "Iteration 98, loss = 1.11339236\n",
            "Iteration 99, loss = 1.11167200\n",
            "Iteration 100, loss = 1.11151380\n",
            "Iteration 1, loss = 1.66482505\n",
            "Iteration 2, loss = 1.63336950\n",
            "Iteration 3, loss = 1.58925326\n",
            "Iteration 4, loss = 1.55736630\n",
            "Iteration 5, loss = 1.51576328\n",
            "Iteration 6, loss = 1.44979653\n",
            "Iteration 7, loss = 1.38053825\n",
            "Iteration 8, loss = 1.31349360\n",
            "Iteration 9, loss = 1.25271784\n",
            "Iteration 10, loss = 1.19329074\n",
            "Iteration 11, loss = 1.15593898\n",
            "Iteration 12, loss = 1.13957871\n",
            "Iteration 13, loss = 1.11978212\n",
            "Iteration 14, loss = 1.10603966\n",
            "Iteration 15, loss = 1.10817396\n",
            "Iteration 16, loss = 1.10462864\n",
            "Iteration 17, loss = 1.09277119\n",
            "Iteration 18, loss = 1.10049862\n",
            "Iteration 19, loss = 1.09132492\n",
            "Iteration 20, loss = 1.08386613\n",
            "Iteration 21, loss = 1.07579769\n",
            "Iteration 22, loss = 1.06939025\n",
            "Iteration 23, loss = 1.06398841\n",
            "Iteration 24, loss = 1.07523266\n",
            "Iteration 25, loss = 1.06167885\n",
            "Iteration 26, loss = 1.06163088\n",
            "Iteration 27, loss = 1.07038589\n",
            "Iteration 28, loss = 1.04197887\n",
            "Iteration 29, loss = 1.02266443\n",
            "Iteration 30, loss = 1.03034260\n",
            "Iteration 31, loss = 1.03842427\n",
            "Iteration 32, loss = 1.00798108\n",
            "Iteration 33, loss = 1.01396411\n",
            "Iteration 34, loss = 1.01822134\n",
            "Iteration 35, loss = 0.99262819\n",
            "Iteration 36, loss = 0.98853514\n",
            "Iteration 37, loss = 0.95481808\n",
            "Iteration 38, loss = 0.97193183\n",
            "Iteration 39, loss = 0.95900513\n",
            "Iteration 40, loss = 0.93792135\n",
            "Iteration 41, loss = 0.93676806\n",
            "Iteration 42, loss = 0.96755914\n",
            "Iteration 43, loss = 0.96510008\n",
            "Iteration 44, loss = 0.94134997\n",
            "Iteration 45, loss = 0.93553278\n",
            "Iteration 46, loss = 0.92228220\n",
            "Iteration 47, loss = 0.89447308\n",
            "Iteration 48, loss = 0.90532168\n",
            "Iteration 49, loss = 0.88353799\n",
            "Iteration 50, loss = 0.87618180\n",
            "Iteration 51, loss = 0.91034990\n",
            "Iteration 52, loss = 0.87003330\n",
            "Iteration 53, loss = 0.86968191\n",
            "Iteration 54, loss = 0.87384934\n",
            "Iteration 55, loss = 0.87461797\n",
            "Iteration 56, loss = 0.86645648\n",
            "Iteration 57, loss = 0.86921179\n",
            "Iteration 58, loss = 0.87919475\n",
            "Iteration 59, loss = 0.93983046\n",
            "Iteration 60, loss = 0.87479412\n",
            "Iteration 61, loss = 0.90531708\n",
            "Iteration 62, loss = 0.91372032\n",
            "Iteration 63, loss = 0.88869045\n",
            "Iteration 64, loss = 0.90131541\n",
            "Iteration 65, loss = 0.86627328\n",
            "Iteration 66, loss = 0.90091291\n",
            "Iteration 67, loss = 0.86465457\n",
            "Iteration 68, loss = 0.83232291\n",
            "Iteration 69, loss = 0.83023783\n",
            "Iteration 70, loss = 0.81994006\n",
            "Iteration 71, loss = 0.81961504\n",
            "Iteration 72, loss = 0.81762745\n",
            "Iteration 73, loss = 0.85537371\n",
            "Iteration 74, loss = 0.80085563\n",
            "Iteration 75, loss = 0.81626808\n",
            "Iteration 76, loss = 0.80793643\n",
            "Iteration 77, loss = 0.81989363\n",
            "Iteration 78, loss = 0.80756118\n",
            "Iteration 79, loss = 0.80637389\n",
            "Iteration 80, loss = 0.81534190\n",
            "Iteration 81, loss = 0.81580522\n",
            "Iteration 82, loss = 0.82336787\n",
            "Iteration 83, loss = 0.77641856\n",
            "Iteration 84, loss = 0.81020009\n",
            "Iteration 85, loss = 0.78643626\n",
            "Iteration 86, loss = 0.80940427\n",
            "Iteration 87, loss = 0.78685073\n",
            "Iteration 88, loss = 0.78806623\n",
            "Iteration 89, loss = 0.78151032\n",
            "Iteration 90, loss = 0.79955069\n",
            "Iteration 91, loss = 0.83205934\n",
            "Iteration 92, loss = 0.86068128\n",
            "Iteration 93, loss = 0.83419230\n",
            "Iteration 94, loss = 0.83752577\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.64950596\n",
            "Iteration 2, loss = 1.64600410\n",
            "Iteration 3, loss = 1.64310423\n",
            "Iteration 4, loss = 1.63994889\n",
            "Iteration 5, loss = 1.63705886\n",
            "Iteration 6, loss = 1.63462731\n",
            "Iteration 7, loss = 1.63186940\n",
            "Iteration 8, loss = 1.62965856\n",
            "Iteration 9, loss = 1.62732179\n",
            "Iteration 10, loss = 1.62528798\n",
            "Iteration 11, loss = 1.62347915\n",
            "Iteration 12, loss = 1.62184274\n",
            "Iteration 13, loss = 1.62009351\n",
            "Iteration 14, loss = 1.61866758\n",
            "Iteration 15, loss = 1.61697635\n",
            "Iteration 16, loss = 1.61583506\n",
            "Iteration 17, loss = 1.61457428\n",
            "Iteration 18, loss = 1.61325904\n",
            "Iteration 19, loss = 1.61222898\n",
            "Iteration 20, loss = 1.61112302\n",
            "Iteration 21, loss = 1.61030984\n",
            "Iteration 22, loss = 1.60917560\n",
            "Iteration 23, loss = 1.60821126\n",
            "Iteration 24, loss = 1.60746169\n",
            "Iteration 25, loss = 1.60671415\n",
            "Iteration 26, loss = 1.60607146\n",
            "Iteration 27, loss = 1.60530631\n",
            "Iteration 28, loss = 1.60464972\n",
            "Iteration 29, loss = 1.60403786\n",
            "Iteration 30, loss = 1.60375870\n",
            "Iteration 31, loss = 1.60312623\n",
            "Iteration 32, loss = 1.60256866\n",
            "Iteration 33, loss = 1.60237166\n",
            "Iteration 34, loss = 1.60188734\n",
            "Iteration 35, loss = 1.60151260\n",
            "Iteration 36, loss = 1.60116104\n",
            "Iteration 37, loss = 1.60086646\n",
            "Iteration 38, loss = 1.60063409\n",
            "Iteration 39, loss = 1.60043746\n",
            "Iteration 40, loss = 1.60012337\n",
            "Iteration 41, loss = 1.59992995\n",
            "Iteration 42, loss = 1.59971286\n",
            "Iteration 43, loss = 1.59952245\n",
            "Iteration 44, loss = 1.59941880\n",
            "Iteration 45, loss = 1.59924641\n",
            "Iteration 46, loss = 1.59907918\n",
            "Iteration 47, loss = 1.59898542\n",
            "Iteration 48, loss = 1.59882600\n",
            "Iteration 49, loss = 1.59876355\n",
            "Iteration 50, loss = 1.59865623\n",
            "Iteration 51, loss = 1.59857338\n",
            "Iteration 52, loss = 1.59846721\n",
            "Iteration 53, loss = 1.59843612\n",
            "Iteration 54, loss = 1.59836201\n",
            "Iteration 55, loss = 1.59829341\n",
            "Iteration 56, loss = 1.59826111\n",
            "Iteration 57, loss = 1.59815830\n",
            "Iteration 58, loss = 1.59809306\n",
            "Iteration 59, loss = 1.59800245\n",
            "Iteration 60, loss = 1.59796051\n",
            "Iteration 61, loss = 1.59792273\n",
            "Iteration 62, loss = 1.59787717\n",
            "Iteration 63, loss = 1.59778513\n",
            "Iteration 64, loss = 1.59773771\n",
            "Iteration 65, loss = 1.59773123\n",
            "Iteration 66, loss = 1.59763982\n",
            "Iteration 67, loss = 1.59765935\n",
            "Iteration 68, loss = 1.59755279\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.64609961\n",
            "Iteration 2, loss = 1.62000713\n",
            "Iteration 3, loss = 1.60838828\n",
            "Iteration 4, loss = 1.60221351\n",
            "Iteration 5, loss = 1.60117924\n",
            "Iteration 6, loss = 1.60398006\n",
            "Iteration 7, loss = 1.60306122\n",
            "Iteration 8, loss = 1.60325347\n",
            "Iteration 9, loss = 1.60254134\n",
            "Iteration 10, loss = 1.60042671\n",
            "Iteration 11, loss = 1.59806271\n",
            "Iteration 12, loss = 1.59590306\n",
            "Iteration 13, loss = 1.59609624\n",
            "Iteration 14, loss = 1.59576019\n",
            "Iteration 15, loss = 1.59587995\n",
            "Iteration 16, loss = 1.59575404\n",
            "Iteration 17, loss = 1.59494003\n",
            "Iteration 18, loss = 1.59391727\n",
            "Iteration 19, loss = 1.59343204\n",
            "Iteration 20, loss = 1.59240140\n",
            "Iteration 21, loss = 1.59240448\n",
            "Iteration 22, loss = 1.59160266\n",
            "Iteration 23, loss = 1.59041601\n",
            "Iteration 24, loss = 1.58999056\n",
            "Iteration 25, loss = 1.58872770\n",
            "Iteration 26, loss = 1.58812165\n",
            "Iteration 27, loss = 1.58733799\n",
            "Iteration 28, loss = 1.58623073\n",
            "Iteration 29, loss = 1.58539064\n",
            "Iteration 30, loss = 1.58431707\n",
            "Iteration 31, loss = 1.58299420\n",
            "Iteration 32, loss = 1.58209033\n",
            "Iteration 33, loss = 1.58104732\n",
            "Iteration 34, loss = 1.57956027\n",
            "Iteration 35, loss = 1.57780427\n",
            "Iteration 36, loss = 1.57633849\n",
            "Iteration 37, loss = 1.57479380\n",
            "Iteration 38, loss = 1.57297016\n",
            "Iteration 39, loss = 1.57130419\n",
            "Iteration 40, loss = 1.56912525\n",
            "Iteration 41, loss = 1.56721104\n",
            "Iteration 42, loss = 1.56522952\n",
            "Iteration 43, loss = 1.56255109\n",
            "Iteration 44, loss = 1.56060185\n",
            "Iteration 45, loss = 1.55758616\n",
            "Iteration 46, loss = 1.55483044\n",
            "Iteration 47, loss = 1.55200406\n",
            "Iteration 48, loss = 1.54908642\n",
            "Iteration 49, loss = 1.54578440\n",
            "Iteration 50, loss = 1.54270950\n",
            "Iteration 51, loss = 1.53897748\n",
            "Iteration 52, loss = 1.53537684\n",
            "Iteration 53, loss = 1.53157390\n",
            "Iteration 54, loss = 1.52767449\n",
            "Iteration 55, loss = 1.52352734\n",
            "Iteration 56, loss = 1.51969985\n",
            "Iteration 57, loss = 1.51491972\n",
            "Iteration 58, loss = 1.51055773\n",
            "Iteration 59, loss = 1.50553013\n",
            "Iteration 60, loss = 1.50080612\n",
            "Iteration 61, loss = 1.49616875\n",
            "Iteration 62, loss = 1.49087755\n",
            "Iteration 63, loss = 1.48538403\n",
            "Iteration 64, loss = 1.48008397\n",
            "Iteration 65, loss = 1.47482546\n",
            "Iteration 66, loss = 1.46888929\n",
            "Iteration 67, loss = 1.46373441\n",
            "Iteration 68, loss = 1.45742803\n",
            "Iteration 69, loss = 1.45206884\n",
            "Iteration 70, loss = 1.44588038\n",
            "Iteration 71, loss = 1.43985737\n",
            "Iteration 72, loss = 1.43347816\n",
            "Iteration 73, loss = 1.42839179\n",
            "Iteration 74, loss = 1.42140650\n",
            "Iteration 75, loss = 1.41512218\n",
            "Iteration 76, loss = 1.40931994\n",
            "Iteration 77, loss = 1.40282569\n",
            "Iteration 78, loss = 1.39726583\n",
            "Iteration 79, loss = 1.39071228\n",
            "Iteration 80, loss = 1.38458730\n",
            "Iteration 81, loss = 1.37850660\n",
            "Iteration 82, loss = 1.37286155\n",
            "Iteration 83, loss = 1.36692085\n",
            "Iteration 84, loss = 1.36079544\n",
            "Iteration 85, loss = 1.35485893\n",
            "Iteration 86, loss = 1.34908374\n",
            "Iteration 87, loss = 1.34374326\n",
            "Iteration 88, loss = 1.33800456\n",
            "Iteration 89, loss = 1.33285528\n",
            "Iteration 90, loss = 1.32717043\n",
            "Iteration 91, loss = 1.32209715\n",
            "Iteration 92, loss = 1.31731300\n",
            "Iteration 93, loss = 1.31176958\n",
            "Iteration 94, loss = 1.30677776\n",
            "Iteration 95, loss = 1.30238115\n",
            "Iteration 96, loss = 1.29715687\n",
            "Iteration 97, loss = 1.29314005\n",
            "Iteration 98, loss = 1.28827098\n",
            "Iteration 99, loss = 1.28379839\n",
            "Iteration 100, loss = 1.28016111\n",
            "Iteration 1, loss = 1.71325633\n",
            "Iteration 2, loss = 1.64146053\n",
            "Iteration 3, loss = 1.64111423\n",
            "Iteration 4, loss = 1.62949003\n",
            "Iteration 5, loss = 1.60734625\n",
            "Iteration 6, loss = 1.60309605\n",
            "Iteration 7, loss = 1.60233066\n",
            "Iteration 8, loss = 1.59626717\n",
            "Iteration 9, loss = 1.59081225\n",
            "Iteration 10, loss = 1.58163264\n",
            "Iteration 11, loss = 1.57514634\n",
            "Iteration 12, loss = 1.56892444\n",
            "Iteration 13, loss = 1.55787662\n",
            "Iteration 14, loss = 1.54481028\n",
            "Iteration 15, loss = 1.52472024\n",
            "Iteration 16, loss = 1.50373003\n",
            "Iteration 17, loss = 1.47761853\n",
            "Iteration 18, loss = 1.44580172\n",
            "Iteration 19, loss = 1.41153577\n",
            "Iteration 20, loss = 1.37235348\n",
            "Iteration 21, loss = 1.33652286\n",
            "Iteration 22, loss = 1.30170235\n",
            "Iteration 23, loss = 1.26975174\n",
            "Iteration 24, loss = 1.24671731\n",
            "Iteration 25, loss = 1.21912573\n",
            "Iteration 26, loss = 1.20191597\n",
            "Iteration 27, loss = 1.18927733\n",
            "Iteration 28, loss = 1.17603784\n",
            "Iteration 29, loss = 1.16793489\n",
            "Iteration 30, loss = 1.15970748\n",
            "Iteration 31, loss = 1.15384679\n",
            "Iteration 32, loss = 1.14716430\n",
            "Iteration 33, loss = 1.14238784\n",
            "Iteration 34, loss = 1.13729358\n",
            "Iteration 35, loss = 1.13108030\n",
            "Iteration 36, loss = 1.12936998\n",
            "Iteration 37, loss = 1.12884482\n",
            "Iteration 38, loss = 1.12389194\n",
            "Iteration 39, loss = 1.12445873\n",
            "Iteration 40, loss = 1.11957969\n",
            "Iteration 41, loss = 1.11777307\n",
            "Iteration 42, loss = 1.12114885\n",
            "Iteration 43, loss = 1.11678659\n",
            "Iteration 44, loss = 1.11557706\n",
            "Iteration 45, loss = 1.11457618\n",
            "Iteration 46, loss = 1.11207089\n",
            "Iteration 47, loss = 1.11223656\n",
            "Iteration 48, loss = 1.11203356\n",
            "Iteration 49, loss = 1.11037243\n",
            "Iteration 50, loss = 1.11178615\n",
            "Iteration 51, loss = 1.10807535\n",
            "Iteration 52, loss = 1.11054754\n",
            "Iteration 53, loss = 1.10822133\n",
            "Iteration 54, loss = 1.10832368\n",
            "Iteration 55, loss = 1.10859659\n",
            "Iteration 56, loss = 1.11269523\n",
            "Iteration 57, loss = 1.10898636\n",
            "Iteration 58, loss = 1.11193092\n",
            "Iteration 59, loss = 1.10811809\n",
            "Iteration 60, loss = 1.10466905\n",
            "Iteration 61, loss = 1.10852557\n",
            "Iteration 62, loss = 1.10759502\n",
            "Iteration 63, loss = 1.10580023\n",
            "Iteration 64, loss = 1.10362381\n",
            "Iteration 65, loss = 1.10768649\n",
            "Iteration 66, loss = 1.10278621\n",
            "Iteration 67, loss = 1.10382784\n",
            "Iteration 68, loss = 1.10221420\n",
            "Iteration 69, loss = 1.10348866\n",
            "Iteration 70, loss = 1.10436501\n",
            "Iteration 71, loss = 1.10518370\n",
            "Iteration 72, loss = 1.10244831\n",
            "Iteration 73, loss = 1.11096792\n",
            "Iteration 74, loss = 1.09920801\n",
            "Iteration 75, loss = 1.09743742\n",
            "Iteration 76, loss = 1.11575467\n",
            "Iteration 77, loss = 1.09627965\n",
            "Iteration 78, loss = 1.10632705\n",
            "Iteration 79, loss = 1.10093578\n",
            "Iteration 80, loss = 1.09351480\n",
            "Iteration 81, loss = 1.09586162\n",
            "Iteration 82, loss = 1.09175088\n",
            "Iteration 83, loss = 1.09228543\n",
            "Iteration 84, loss = 1.09059954\n",
            "Iteration 85, loss = 1.08578611\n",
            "Iteration 86, loss = 1.08311349\n",
            "Iteration 87, loss = 1.08426770\n",
            "Iteration 88, loss = 1.07994200\n",
            "Iteration 89, loss = 1.08301558\n",
            "Iteration 90, loss = 1.07953879\n",
            "Iteration 91, loss = 1.07762843\n",
            "Iteration 92, loss = 1.07655976\n",
            "Iteration 93, loss = 1.07273929\n",
            "Iteration 94, loss = 1.07050200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 95, loss = 1.07111924\n",
            "Iteration 96, loss = 1.07184982\n",
            "Iteration 97, loss = 1.06897678\n",
            "Iteration 98, loss = 1.06417330\n",
            "Iteration 99, loss = 1.06175065\n",
            "Iteration 100, loss = 1.06232746\n",
            "Iteration 1, loss = 1.72217712\n",
            "Iteration 2, loss = 1.71679515\n",
            "Iteration 3, loss = 1.71132850\n",
            "Iteration 4, loss = 1.70605131\n",
            "Iteration 5, loss = 1.70081806\n",
            "Iteration 6, loss = 1.69608657\n",
            "Iteration 7, loss = 1.69137286\n",
            "Iteration 8, loss = 1.68671547\n",
            "Iteration 9, loss = 1.68262397\n",
            "Iteration 10, loss = 1.67839324\n",
            "Iteration 11, loss = 1.67428162\n",
            "Iteration 12, loss = 1.67023961\n",
            "Iteration 13, loss = 1.66646184\n",
            "Iteration 14, loss = 1.66320578\n",
            "Iteration 15, loss = 1.65966466\n",
            "Iteration 16, loss = 1.65625452\n",
            "Iteration 17, loss = 1.65329253\n",
            "Iteration 18, loss = 1.65027679\n",
            "Iteration 19, loss = 1.64729913\n",
            "Iteration 20, loss = 1.64481822\n",
            "Iteration 21, loss = 1.64219283\n",
            "Iteration 22, loss = 1.63989774\n",
            "Iteration 23, loss = 1.63737178\n",
            "Iteration 24, loss = 1.63560210\n",
            "Iteration 25, loss = 1.63321630\n",
            "Iteration 26, loss = 1.63120126\n",
            "Iteration 27, loss = 1.62934119\n",
            "Iteration 28, loss = 1.62770986\n",
            "Iteration 29, loss = 1.62613570\n",
            "Iteration 30, loss = 1.62447353\n",
            "Iteration 31, loss = 1.62287878\n",
            "Iteration 32, loss = 1.62172532\n",
            "Iteration 33, loss = 1.62021667\n",
            "Iteration 34, loss = 1.61906567\n",
            "Iteration 35, loss = 1.61791668\n",
            "Iteration 36, loss = 1.61683138\n",
            "Iteration 37, loss = 1.61592421\n",
            "Iteration 38, loss = 1.61483232\n",
            "Iteration 39, loss = 1.61391413\n",
            "Iteration 40, loss = 1.61308034\n",
            "Iteration 41, loss = 1.61224260\n",
            "Iteration 42, loss = 1.61156869\n",
            "Iteration 43, loss = 1.61078048\n",
            "Iteration 44, loss = 1.61018072\n",
            "Iteration 45, loss = 1.60939507\n",
            "Iteration 46, loss = 1.60879854\n",
            "Iteration 47, loss = 1.60817129\n",
            "Iteration 48, loss = 1.60770002\n",
            "Iteration 49, loss = 1.60715379\n",
            "Iteration 50, loss = 1.60672653\n",
            "Iteration 51, loss = 1.60614403\n",
            "Iteration 52, loss = 1.60574334\n",
            "Iteration 53, loss = 1.60520629\n",
            "Iteration 54, loss = 1.60481300\n",
            "Iteration 55, loss = 1.60443318\n",
            "Iteration 56, loss = 1.60399416\n",
            "Iteration 57, loss = 1.60359444\n",
            "Iteration 58, loss = 1.60321588\n",
            "Iteration 59, loss = 1.60280946\n",
            "Iteration 60, loss = 1.60248119\n",
            "Iteration 61, loss = 1.60213318\n",
            "Iteration 62, loss = 1.60173476\n",
            "Iteration 63, loss = 1.60138121\n",
            "Iteration 64, loss = 1.60103954\n",
            "Iteration 65, loss = 1.60065387\n",
            "Iteration 66, loss = 1.60033352\n",
            "Iteration 67, loss = 1.60002264\n",
            "Iteration 68, loss = 1.59966298\n",
            "Iteration 69, loss = 1.59937771\n",
            "Iteration 70, loss = 1.59897662\n",
            "Iteration 71, loss = 1.59867620\n",
            "Iteration 72, loss = 1.59832448\n",
            "Iteration 73, loss = 1.59808367\n",
            "Iteration 74, loss = 1.59767962\n",
            "Iteration 75, loss = 1.59729283\n",
            "Iteration 76, loss = 1.59699790\n",
            "Iteration 77, loss = 1.59665045\n",
            "Iteration 78, loss = 1.59635545\n",
            "Iteration 79, loss = 1.59596632\n",
            "Iteration 80, loss = 1.59564355\n",
            "Iteration 81, loss = 1.59529060\n",
            "Iteration 82, loss = 1.59497869\n",
            "Iteration 83, loss = 1.59462570\n",
            "Iteration 84, loss = 1.59427724\n",
            "Iteration 85, loss = 1.59392077\n",
            "Iteration 86, loss = 1.59356570\n",
            "Iteration 87, loss = 1.59323583\n",
            "Iteration 88, loss = 1.59286026\n",
            "Iteration 89, loss = 1.59251735\n",
            "Iteration 90, loss = 1.59214713\n",
            "Iteration 91, loss = 1.59180138\n",
            "Iteration 92, loss = 1.59146074\n",
            "Iteration 93, loss = 1.59106093\n",
            "Iteration 94, loss = 1.59068730\n",
            "Iteration 95, loss = 1.59033370\n",
            "Iteration 96, loss = 1.58992950\n",
            "Iteration 97, loss = 1.58958129\n",
            "Iteration 98, loss = 1.58917565\n",
            "Iteration 99, loss = 1.58877488\n",
            "Iteration 100, loss = 1.58842686\n",
            "Iteration 1, loss = 1.70930346\n",
            "Iteration 2, loss = 1.66601865\n",
            "Iteration 3, loss = 1.63493006\n",
            "Iteration 4, loss = 1.61641074\n",
            "Iteration 5, loss = 1.60699527\n",
            "Iteration 6, loss = 1.60658734\n",
            "Iteration 7, loss = 1.60643318\n",
            "Iteration 8, loss = 1.60594259\n",
            "Iteration 9, loss = 1.60449177\n",
            "Iteration 10, loss = 1.60048911\n",
            "Iteration 11, loss = 1.59607144\n",
            "Iteration 12, loss = 1.59173900\n",
            "Iteration 13, loss = 1.58751216\n",
            "Iteration 14, loss = 1.58293418\n",
            "Iteration 15, loss = 1.57933797\n",
            "Iteration 16, loss = 1.57603300\n",
            "Iteration 17, loss = 1.57200058\n",
            "Iteration 18, loss = 1.56766116\n",
            "Iteration 19, loss = 1.56325548\n",
            "Iteration 20, loss = 1.55795237\n",
            "Iteration 21, loss = 1.55264797\n",
            "Iteration 22, loss = 1.54662013\n",
            "Iteration 23, loss = 1.53934726\n",
            "Iteration 24, loss = 1.53286742\n",
            "Iteration 25, loss = 1.52450755\n",
            "Iteration 26, loss = 1.51647378\n",
            "Iteration 27, loss = 1.50798815\n",
            "Iteration 28, loss = 1.49867786\n",
            "Iteration 29, loss = 1.48921568\n",
            "Iteration 30, loss = 1.47912299\n",
            "Iteration 31, loss = 1.46777697\n",
            "Iteration 32, loss = 1.45676357\n",
            "Iteration 33, loss = 1.44550687\n",
            "Iteration 34, loss = 1.43327661\n",
            "Iteration 35, loss = 1.42095860\n",
            "Iteration 36, loss = 1.40873635\n",
            "Iteration 37, loss = 1.39639497\n",
            "Iteration 38, loss = 1.38353222\n",
            "Iteration 39, loss = 1.37087261\n",
            "Iteration 40, loss = 1.35812827\n",
            "Iteration 41, loss = 1.34622032\n",
            "Iteration 42, loss = 1.33380442\n",
            "Iteration 43, loss = 1.32206748\n",
            "Iteration 44, loss = 1.31075002\n",
            "Iteration 45, loss = 1.29926612\n",
            "Iteration 46, loss = 1.28859901\n",
            "Iteration 47, loss = 1.27844052\n",
            "Iteration 48, loss = 1.26874166\n",
            "Iteration 49, loss = 1.25959858\n",
            "Iteration 50, loss = 1.25107545\n",
            "Iteration 51, loss = 1.24262061\n",
            "Iteration 52, loss = 1.23485667\n",
            "Iteration 53, loss = 1.22753984\n",
            "Iteration 54, loss = 1.22086069\n",
            "Iteration 55, loss = 1.21463194\n",
            "Iteration 56, loss = 1.20929431\n",
            "Iteration 57, loss = 1.20320262\n",
            "Iteration 58, loss = 1.19785009\n",
            "Iteration 59, loss = 1.19265616\n",
            "Iteration 60, loss = 1.18833951\n",
            "Iteration 61, loss = 1.18458694\n",
            "Iteration 62, loss = 1.18034021\n",
            "Iteration 63, loss = 1.17603788\n",
            "Iteration 64, loss = 1.17267817\n",
            "Iteration 65, loss = 1.16952024\n",
            "Iteration 66, loss = 1.16643696\n",
            "Iteration 67, loss = 1.16365061\n",
            "Iteration 68, loss = 1.16022214\n",
            "Iteration 69, loss = 1.15775579\n",
            "Iteration 70, loss = 1.15506984\n",
            "Iteration 71, loss = 1.15279697\n",
            "Iteration 72, loss = 1.15004838\n",
            "Iteration 73, loss = 1.14957557\n",
            "Iteration 74, loss = 1.14610548\n",
            "Iteration 75, loss = 1.14377758\n",
            "Iteration 76, loss = 1.14287957\n",
            "Iteration 77, loss = 1.14005681\n",
            "Iteration 78, loss = 1.13869647\n",
            "Iteration 79, loss = 1.13746518\n",
            "Iteration 80, loss = 1.13489308\n",
            "Iteration 81, loss = 1.13339620\n",
            "Iteration 82, loss = 1.13229924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 83, loss = 1.13072277\n",
            "Iteration 84, loss = 1.12920987\n",
            "Iteration 85, loss = 1.12782482\n",
            "Iteration 86, loss = 1.12643457\n",
            "Iteration 87, loss = 1.12546752\n",
            "Iteration 88, loss = 1.12408561\n",
            "Iteration 89, loss = 1.12290937\n",
            "Iteration 90, loss = 1.12149736\n",
            "Iteration 91, loss = 1.12064956\n",
            "Iteration 92, loss = 1.11994465\n",
            "Iteration 93, loss = 1.11812170\n",
            "Iteration 94, loss = 1.11706938\n",
            "Iteration 95, loss = 1.11746243\n",
            "Iteration 96, loss = 1.11559843\n",
            "Iteration 97, loss = 1.11499583\n",
            "Iteration 98, loss = 1.11385449\n",
            "Iteration 99, loss = 1.11220748\n",
            "Iteration 100, loss = 1.11197072\n",
            "Iteration 1, loss = 1.65862693\n",
            "Iteration 2, loss = 1.63697512\n",
            "Iteration 3, loss = 1.59659699\n",
            "Iteration 4, loss = 1.56825675\n",
            "Iteration 5, loss = 1.53233924\n",
            "Iteration 6, loss = 1.47716955\n",
            "Iteration 7, loss = 1.41921368\n",
            "Iteration 8, loss = 1.35703519\n",
            "Iteration 9, loss = 1.28577805\n",
            "Iteration 10, loss = 1.22652595\n",
            "Iteration 11, loss = 1.17918061\n",
            "Iteration 12, loss = 1.15395089\n",
            "Iteration 13, loss = 1.12972240\n",
            "Iteration 14, loss = 1.11335750\n",
            "Iteration 15, loss = 1.11100589\n",
            "Iteration 16, loss = 1.10690664\n",
            "Iteration 17, loss = 1.09363409\n",
            "Iteration 18, loss = 1.09762412\n",
            "Iteration 19, loss = 1.08968686\n",
            "Iteration 20, loss = 1.08383557\n",
            "Iteration 21, loss = 1.07451188\n",
            "Iteration 22, loss = 1.07101956\n",
            "Iteration 23, loss = 1.06327171\n",
            "Iteration 24, loss = 1.07481827\n",
            "Iteration 25, loss = 1.06266192\n",
            "Iteration 26, loss = 1.06215478\n",
            "Iteration 27, loss = 1.06851405\n",
            "Iteration 28, loss = 1.03808464\n",
            "Iteration 29, loss = 1.02738446\n",
            "Iteration 30, loss = 1.02165226\n",
            "Iteration 31, loss = 1.03385375\n",
            "Iteration 32, loss = 1.00823548\n",
            "Iteration 33, loss = 1.01376273\n",
            "Iteration 34, loss = 1.02985892\n",
            "Iteration 35, loss = 0.99800453\n",
            "Iteration 36, loss = 0.99706724\n",
            "Iteration 37, loss = 0.95439133\n",
            "Iteration 38, loss = 0.97565488\n",
            "Iteration 39, loss = 0.96385997\n",
            "Iteration 40, loss = 0.94034003\n",
            "Iteration 41, loss = 0.94693978\n",
            "Iteration 42, loss = 0.97855294\n",
            "Iteration 43, loss = 0.94490520\n",
            "Iteration 44, loss = 0.92890536\n",
            "Iteration 45, loss = 0.91694389\n",
            "Iteration 46, loss = 0.90492371\n",
            "Iteration 47, loss = 0.89274599\n",
            "Iteration 48, loss = 0.89782932\n",
            "Iteration 49, loss = 0.88239312\n",
            "Iteration 50, loss = 0.87870304\n",
            "Iteration 51, loss = 0.92787760\n",
            "Iteration 52, loss = 0.87369849\n",
            "Iteration 53, loss = 0.86584826\n",
            "Iteration 54, loss = 0.88099550\n",
            "Iteration 55, loss = 0.87927239\n",
            "Iteration 56, loss = 0.86108859\n",
            "Iteration 57, loss = 0.85953495\n",
            "Iteration 58, loss = 0.87642621\n",
            "Iteration 59, loss = 0.95811097\n",
            "Iteration 60, loss = 0.86956718\n",
            "Iteration 61, loss = 0.88547525\n",
            "Iteration 62, loss = 0.89623676\n",
            "Iteration 63, loss = 0.86893051\n",
            "Iteration 64, loss = 0.88928211\n",
            "Iteration 65, loss = 0.83844147\n",
            "Iteration 66, loss = 0.89030196\n",
            "Iteration 67, loss = 0.88173730\n",
            "Iteration 68, loss = 0.85332364\n",
            "Iteration 69, loss = 0.85868417\n",
            "Iteration 70, loss = 0.85590662\n",
            "Iteration 71, loss = 0.81147163\n",
            "Iteration 72, loss = 0.81684972\n",
            "Iteration 73, loss = 0.88319793\n",
            "Iteration 74, loss = 0.80365525\n",
            "Iteration 75, loss = 0.81269908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 76, loss = 0.80205657\n",
            "Iteration 77, loss = 0.83884711\n",
            "Iteration 78, loss = 0.81343644\n",
            "Iteration 79, loss = 0.81330830\n",
            "Iteration 80, loss = 0.82562726\n",
            "Iteration 81, loss = 0.82409647\n",
            "Iteration 82, loss = 0.81813572\n",
            "Iteration 83, loss = 0.78871100\n",
            "Iteration 84, loss = 0.80176973\n",
            "Iteration 85, loss = 0.79234364\n",
            "Iteration 86, loss = 0.80419908\n",
            "Iteration 87, loss = 0.78210092\n",
            "Iteration 88, loss = 0.78198110\n",
            "Iteration 89, loss = 0.77490324\n",
            "Iteration 90, loss = 0.79411950\n",
            "Iteration 91, loss = 0.81111886\n",
            "Iteration 92, loss = 0.82868130\n",
            "Iteration 93, loss = 0.81223284\n",
            "Iteration 94, loss = 0.81460386\n",
            "Iteration 95, loss = 0.80665112\n",
            "Iteration 96, loss = 0.78648355\n",
            "Iteration 97, loss = 0.80057305\n",
            "Iteration 98, loss = 0.81579876\n",
            "Iteration 99, loss = 0.79507553\n",
            "Iteration 100, loss = 0.76658667\n",
            "Iteration 1, loss = 1.64590091\n",
            "Iteration 2, loss = 1.64446044\n",
            "Iteration 3, loss = 1.64320178\n",
            "Iteration 4, loss = 1.64178134\n",
            "Iteration 5, loss = 1.64043152\n",
            "Iteration 6, loss = 1.63925728\n",
            "Iteration 7, loss = 1.63791717\n",
            "Iteration 8, loss = 1.63673752\n",
            "Iteration 9, loss = 1.63551529\n",
            "Iteration 10, loss = 1.63436057\n",
            "Iteration 11, loss = 1.63327749\n",
            "Iteration 12, loss = 1.63220850\n",
            "Iteration 13, loss = 1.63110357\n",
            "Iteration 14, loss = 1.63016377\n",
            "Iteration 15, loss = 1.62900934\n",
            "Iteration 16, loss = 1.62810675\n",
            "Iteration 17, loss = 1.62717066\n",
            "Iteration 18, loss = 1.62615085\n",
            "Iteration 19, loss = 1.62523357\n",
            "Iteration 20, loss = 1.62436204\n",
            "Iteration 21, loss = 1.62354398\n",
            "Iteration 22, loss = 1.62263848\n",
            "Iteration 23, loss = 1.62163077\n",
            "Iteration 24, loss = 1.62088244\n",
            "Iteration 25, loss = 1.61991035\n",
            "Iteration 26, loss = 1.61896343\n",
            "Iteration 27, loss = 1.61791161\n",
            "Iteration 28, loss = 1.61686685\n",
            "Iteration 29, loss = 1.61570200\n",
            "Iteration 30, loss = 1.61458498\n",
            "Iteration 31, loss = 1.61318694\n",
            "Iteration 32, loss = 1.61190768\n",
            "Iteration 33, loss = 1.61071383\n",
            "Iteration 34, loss = 1.60953294\n",
            "Iteration 35, loss = 1.60848805\n",
            "Iteration 36, loss = 1.60753489\n",
            "Iteration 37, loss = 1.60674244\n",
            "Iteration 38, loss = 1.60597010\n",
            "Iteration 39, loss = 1.60527678\n",
            "Iteration 40, loss = 1.60456221\n",
            "Iteration 41, loss = 1.60392511\n",
            "Iteration 42, loss = 1.60333944\n",
            "Iteration 43, loss = 1.60279502\n",
            "Iteration 44, loss = 1.60232692\n",
            "Iteration 45, loss = 1.60179742\n",
            "Iteration 46, loss = 1.60128884\n",
            "Iteration 47, loss = 1.60082878\n",
            "Iteration 48, loss = 1.60034676\n",
            "Iteration 49, loss = 1.59995507\n",
            "Iteration 50, loss = 1.59952125\n",
            "Iteration 51, loss = 1.59906031\n",
            "Iteration 52, loss = 1.59862047\n",
            "Iteration 53, loss = 1.59820470\n",
            "Iteration 54, loss = 1.59781090\n",
            "Iteration 55, loss = 1.59740691\n",
            "Iteration 56, loss = 1.59698727\n",
            "Iteration 57, loss = 1.59657082\n",
            "Iteration 58, loss = 1.59613187\n",
            "Iteration 59, loss = 1.59571978\n",
            "Iteration 60, loss = 1.59537705\n",
            "Iteration 61, loss = 1.59498384\n",
            "Iteration 62, loss = 1.59462262\n",
            "Iteration 63, loss = 1.59418782\n",
            "Iteration 64, loss = 1.59380562\n",
            "Iteration 65, loss = 1.59346243\n",
            "Iteration 66, loss = 1.59307284\n",
            "Iteration 67, loss = 1.59273931\n",
            "Iteration 68, loss = 1.59233021\n",
            "Iteration 69, loss = 1.59197591\n",
            "Iteration 70, loss = 1.59159565\n",
            "Iteration 71, loss = 1.59126244\n",
            "Iteration 72, loss = 1.59086583\n",
            "Iteration 73, loss = 1.59058928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 1.59015061\n",
            "Iteration 75, loss = 1.58975926\n",
            "Iteration 76, loss = 1.58944365\n",
            "Iteration 77, loss = 1.58904113\n",
            "Iteration 78, loss = 1.58868086\n",
            "Iteration 79, loss = 1.58831240\n",
            "Iteration 80, loss = 1.58796740\n",
            "Iteration 81, loss = 1.58759671\n",
            "Iteration 82, loss = 1.58726684\n",
            "Iteration 83, loss = 1.58687517\n",
            "Iteration 84, loss = 1.58651756\n",
            "Iteration 85, loss = 1.58611667\n",
            "Iteration 86, loss = 1.58574631\n",
            "Iteration 87, loss = 1.58539615\n",
            "Iteration 88, loss = 1.58500880\n",
            "Iteration 89, loss = 1.58463328\n",
            "Iteration 90, loss = 1.58424437\n",
            "Iteration 91, loss = 1.58384824\n",
            "Iteration 92, loss = 1.58349779\n",
            "Iteration 93, loss = 1.58308203\n",
            "Iteration 94, loss = 1.58270154\n",
            "Iteration 95, loss = 1.58233155\n",
            "Iteration 96, loss = 1.58188976\n",
            "Iteration 97, loss = 1.58151564\n",
            "Iteration 98, loss = 1.58109123\n",
            "Iteration 99, loss = 1.58069274\n",
            "Iteration 100, loss = 1.58032055\n",
            "Iteration 1, loss = 1.64334687\n",
            "Iteration 2, loss = 1.63101416\n",
            "Iteration 3, loss = 1.62281601\n",
            "Iteration 4, loss = 1.61364175\n",
            "Iteration 5, loss = 1.60570036\n",
            "Iteration 6, loss = 1.60290421\n",
            "Iteration 7, loss = 1.59920254\n",
            "Iteration 8, loss = 1.59797049\n",
            "Iteration 9, loss = 1.59711478\n",
            "Iteration 10, loss = 1.59610275\n",
            "Iteration 11, loss = 1.59495215\n",
            "Iteration 12, loss = 1.59379699\n",
            "Iteration 13, loss = 1.59222339\n",
            "Iteration 14, loss = 1.59037219\n",
            "Iteration 15, loss = 1.58837358\n",
            "Iteration 16, loss = 1.58630458\n",
            "Iteration 17, loss = 1.58372066\n",
            "Iteration 18, loss = 1.58125105\n",
            "Iteration 19, loss = 1.57878961\n",
            "Iteration 20, loss = 1.57599719\n",
            "Iteration 21, loss = 1.57310006\n",
            "Iteration 22, loss = 1.57019969\n",
            "Iteration 23, loss = 1.56622267\n",
            "Iteration 24, loss = 1.56263330\n",
            "Iteration 25, loss = 1.55808038\n",
            "Iteration 26, loss = 1.55363881\n",
            "Iteration 27, loss = 1.54897094\n",
            "Iteration 28, loss = 1.54380932\n",
            "Iteration 29, loss = 1.53852453\n",
            "Iteration 30, loss = 1.53292403\n",
            "Iteration 31, loss = 1.52641225\n",
            "Iteration 32, loss = 1.52014421\n",
            "Iteration 33, loss = 1.51345161\n",
            "Iteration 34, loss = 1.50611812\n",
            "Iteration 35, loss = 1.49850468\n",
            "Iteration 36, loss = 1.49091360\n",
            "Iteration 37, loss = 1.48294296\n",
            "Iteration 38, loss = 1.47426227\n",
            "Iteration 39, loss = 1.46539652\n",
            "Iteration 40, loss = 1.45637912\n",
            "Iteration 41, loss = 1.44739799\n",
            "Iteration 42, loss = 1.43775047\n",
            "Iteration 43, loss = 1.42884946\n",
            "Iteration 44, loss = 1.41959546\n",
            "Iteration 45, loss = 1.40970927\n",
            "Iteration 46, loss = 1.40016337\n",
            "Iteration 47, loss = 1.39096659\n",
            "Iteration 48, loss = 1.38175696\n",
            "Iteration 49, loss = 1.37250839\n",
            "Iteration 50, loss = 1.36348688\n",
            "Iteration 51, loss = 1.35426836\n",
            "Iteration 52, loss = 1.34547325\n",
            "Iteration 53, loss = 1.33663149\n",
            "Iteration 54, loss = 1.32868266\n",
            "Iteration 55, loss = 1.32127049\n",
            "Iteration 56, loss = 1.31384324\n",
            "Iteration 57, loss = 1.30635241\n",
            "Iteration 58, loss = 1.29944696\n",
            "Iteration 59, loss = 1.29276687\n",
            "Iteration 60, loss = 1.28672212\n",
            "Iteration 61, loss = 1.28101909\n",
            "Iteration 62, loss = 1.27510891\n",
            "Iteration 63, loss = 1.26975773\n",
            "Iteration 64, loss = 1.26471960\n",
            "Iteration 65, loss = 1.25972532\n",
            "Iteration 66, loss = 1.25535211\n",
            "Iteration 67, loss = 1.25100190\n",
            "Iteration 68, loss = 1.24645049\n",
            "Iteration 69, loss = 1.24255484\n",
            "Iteration 70, loss = 1.23864874\n",
            "Iteration 71, loss = 1.23490959\n",
            "Iteration 72, loss = 1.23118395\n",
            "Iteration 73, loss = 1.22828622\n",
            "Iteration 74, loss = 1.22465269\n",
            "Iteration 75, loss = 1.22189743\n",
            "Iteration 76, loss = 1.21944690\n",
            "Iteration 77, loss = 1.21570027\n",
            "Iteration 78, loss = 1.21323975\n",
            "Iteration 79, loss = 1.21057340\n",
            "Iteration 80, loss = 1.20772472\n",
            "Iteration 81, loss = 1.20543669\n",
            "Iteration 82, loss = 1.20322633\n",
            "Iteration 83, loss = 1.20087628\n",
            "Iteration 84, loss = 1.19866587\n",
            "Iteration 85, loss = 1.19658765\n",
            "Iteration 86, loss = 1.19456332\n",
            "Iteration 87, loss = 1.19250888\n",
            "Iteration 88, loss = 1.19049888\n",
            "Iteration 89, loss = 1.18893993\n",
            "Iteration 90, loss = 1.18687864\n",
            "Iteration 91, loss = 1.18518724\n",
            "Iteration 92, loss = 1.18366050\n",
            "Iteration 93, loss = 1.18179739\n",
            "Iteration 94, loss = 1.18011781\n",
            "Iteration 95, loss = 1.17915775\n",
            "Iteration 96, loss = 1.17733559\n",
            "Iteration 97, loss = 1.17599763\n",
            "Iteration 98, loss = 1.17429237\n",
            "Iteration 99, loss = 1.17272156\n",
            "Iteration 100, loss = 1.17188498\n",
            "Iteration 1, loss = 1.63108722\n",
            "Iteration 2, loss = 1.60615197\n",
            "Iteration 3, loss = 1.60092322\n",
            "Iteration 4, loss = 1.59451112\n",
            "Iteration 5, loss = 1.58858900\n",
            "Iteration 6, loss = 1.57796673\n",
            "Iteration 7, loss = 1.55688875\n",
            "Iteration 8, loss = 1.52904439\n",
            "Iteration 9, loss = 1.49319668\n",
            "Iteration 10, loss = 1.44751646\n",
            "Iteration 11, loss = 1.40080208\n",
            "Iteration 12, loss = 1.35380855\n",
            "Iteration 13, loss = 1.30860213\n",
            "Iteration 14, loss = 1.27303041\n",
            "Iteration 15, loss = 1.24172264\n",
            "Iteration 16, loss = 1.21632616\n",
            "Iteration 17, loss = 1.19687279\n",
            "Iteration 18, loss = 1.18204237\n",
            "Iteration 19, loss = 1.16994063\n",
            "Iteration 20, loss = 1.16180389\n",
            "Iteration 21, loss = 1.15140425\n",
            "Iteration 22, loss = 1.14609114\n",
            "Iteration 23, loss = 1.14163281\n",
            "Iteration 24, loss = 1.13455396\n",
            "Iteration 25, loss = 1.12978097\n",
            "Iteration 26, loss = 1.12798495\n",
            "Iteration 27, loss = 1.12138242\n",
            "Iteration 28, loss = 1.11721167\n",
            "Iteration 29, loss = 1.11481458\n",
            "Iteration 30, loss = 1.11541317\n",
            "Iteration 31, loss = 1.11432551\n",
            "Iteration 32, loss = 1.10898582\n",
            "Iteration 33, loss = 1.11112669\n",
            "Iteration 34, loss = 1.10902477\n",
            "Iteration 35, loss = 1.10428300\n",
            "Iteration 36, loss = 1.10527735\n",
            "Iteration 37, loss = 1.10584027\n",
            "Iteration 38, loss = 1.10286921\n",
            "Iteration 39, loss = 1.10563340\n",
            "Iteration 40, loss = 1.10029581\n",
            "Iteration 41, loss = 1.10180745\n",
            "Iteration 42, loss = 1.10183734\n",
            "Iteration 43, loss = 1.10133239\n",
            "Iteration 44, loss = 1.09980735\n",
            "Iteration 45, loss = 1.09840955\n",
            "Iteration 46, loss = 1.09879898\n",
            "Iteration 47, loss = 1.09600486\n",
            "Iteration 48, loss = 1.09841939\n",
            "Iteration 49, loss = 1.09569536\n",
            "Iteration 50, loss = 1.09455010\n",
            "Iteration 51, loss = 1.09339770\n",
            "Iteration 52, loss = 1.09231349\n",
            "Iteration 53, loss = 1.09120872\n",
            "Iteration 54, loss = 1.09167914\n",
            "Iteration 55, loss = 1.09331131\n",
            "Iteration 56, loss = 1.09702724\n",
            "Iteration 57, loss = 1.09033591\n",
            "Iteration 58, loss = 1.09092851\n",
            "Iteration 59, loss = 1.08734528\n",
            "Iteration 60, loss = 1.08656930\n",
            "Iteration 61, loss = 1.08403757\n",
            "Iteration 62, loss = 1.08810100\n",
            "Iteration 63, loss = 1.07926585\n",
            "Iteration 64, loss = 1.07837690\n",
            "Iteration 65, loss = 1.07764155\n",
            "Iteration 66, loss = 1.07523221\n",
            "Iteration 67, loss = 1.07478128\n",
            "Iteration 68, loss = 1.07505324\n",
            "Iteration 69, loss = 1.08065860\n",
            "Iteration 70, loss = 1.07375004\n",
            "Iteration 71, loss = 1.07893716\n",
            "Iteration 72, loss = 1.07631089\n",
            "Iteration 73, loss = 1.05973908\n",
            "Iteration 74, loss = 1.06758797\n",
            "Iteration 75, loss = 1.05815868\n",
            "Iteration 76, loss = 1.05910728\n",
            "Iteration 77, loss = 1.05630338\n",
            "Iteration 78, loss = 1.05362642\n",
            "Iteration 79, loss = 1.05689915\n",
            "Iteration 80, loss = 1.04775293\n",
            "Iteration 81, loss = 1.04388787\n",
            "Iteration 82, loss = 1.04330919\n",
            "Iteration 83, loss = 1.03997951\n",
            "Iteration 84, loss = 1.02520704\n",
            "Iteration 85, loss = 1.01910977\n",
            "Iteration 86, loss = 1.01642787\n",
            "Iteration 87, loss = 1.00921729\n",
            "Iteration 88, loss = 1.00607181\n",
            "Iteration 89, loss = 1.00557086\n",
            "Iteration 90, loss = 0.99829396\n",
            "Iteration 91, loss = 0.99756878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 92, loss = 1.00428709\n",
            "Iteration 93, loss = 0.99825600\n",
            "Iteration 94, loss = 0.99045526\n",
            "Iteration 95, loss = 0.97400644\n",
            "Iteration 96, loss = 0.97847471\n",
            "Iteration 97, loss = 0.98535188\n",
            "Iteration 98, loss = 0.95653764\n",
            "Iteration 99, loss = 0.96838976\n",
            "Iteration 100, loss = 0.96417024\n",
            "Iteration 1, loss = 1.64163416\n",
            "Iteration 2, loss = 1.63758619\n",
            "Iteration 3, loss = 1.63410031\n",
            "Iteration 4, loss = 1.63034001\n",
            "Iteration 5, loss = 1.62699637\n",
            "Iteration 6, loss = 1.62394593\n",
            "Iteration 7, loss = 1.62042888\n",
            "Iteration 8, loss = 1.61763481\n",
            "Iteration 9, loss = 1.61518176\n",
            "Iteration 10, loss = 1.61242487\n",
            "Iteration 11, loss = 1.61045167\n",
            "Iteration 12, loss = 1.60805623\n",
            "Iteration 13, loss = 1.60613148\n",
            "Iteration 14, loss = 1.60401250\n",
            "Iteration 15, loss = 1.60232454\n",
            "Iteration 16, loss = 1.60065481\n",
            "Iteration 17, loss = 1.59898221\n",
            "Iteration 18, loss = 1.59748182\n",
            "Iteration 19, loss = 1.59606319\n",
            "Iteration 20, loss = 1.59484601\n",
            "Iteration 21, loss = 1.59376382\n",
            "Iteration 22, loss = 1.59258418\n",
            "Iteration 23, loss = 1.59141115\n",
            "Iteration 24, loss = 1.59044293\n",
            "Iteration 25, loss = 1.58946255\n",
            "Iteration 26, loss = 1.58868286\n",
            "Iteration 27, loss = 1.58778737\n",
            "Iteration 28, loss = 1.58691907\n",
            "Iteration 29, loss = 1.58628520\n",
            "Iteration 30, loss = 1.58543999\n",
            "Iteration 31, loss = 1.58475550\n",
            "Iteration 32, loss = 1.58402724\n",
            "Iteration 33, loss = 1.58346160\n",
            "Iteration 34, loss = 1.58275413\n",
            "Iteration 35, loss = 1.58212810\n",
            "Iteration 36, loss = 1.58146363\n",
            "Iteration 37, loss = 1.58093537\n",
            "Iteration 38, loss = 1.58036011\n",
            "Iteration 39, loss = 1.57974879\n",
            "Iteration 40, loss = 1.57918010\n",
            "Iteration 41, loss = 1.57863754\n",
            "Iteration 42, loss = 1.57805206\n",
            "Iteration 43, loss = 1.57746754\n",
            "Iteration 44, loss = 1.57693561\n",
            "Iteration 45, loss = 1.57641122\n",
            "Iteration 46, loss = 1.57586012\n",
            "Iteration 47, loss = 1.57528815\n",
            "Iteration 48, loss = 1.57477461\n",
            "Iteration 49, loss = 1.57419641\n",
            "Iteration 50, loss = 1.57364283\n",
            "Iteration 51, loss = 1.57315060\n",
            "Iteration 52, loss = 1.57259234\n",
            "Iteration 53, loss = 1.57201171\n",
            "Iteration 54, loss = 1.57147686\n",
            "Iteration 55, loss = 1.57095237\n",
            "Iteration 56, loss = 1.57039402\n",
            "Iteration 57, loss = 1.56986294\n",
            "Iteration 58, loss = 1.56928305\n",
            "Iteration 59, loss = 1.56874568\n",
            "Iteration 60, loss = 1.56818506\n",
            "Iteration 61, loss = 1.56762586\n",
            "Iteration 62, loss = 1.56705883\n",
            "Iteration 63, loss = 1.56650824\n",
            "Iteration 64, loss = 1.56595671\n",
            "Iteration 65, loss = 1.56541247\n",
            "Iteration 66, loss = 1.56480244\n",
            "Iteration 67, loss = 1.56424055\n",
            "Iteration 68, loss = 1.56367765\n",
            "Iteration 69, loss = 1.56311237\n",
            "Iteration 70, loss = 1.56252640\n",
            "Iteration 71, loss = 1.56197732\n",
            "Iteration 72, loss = 1.56135473\n",
            "Iteration 73, loss = 1.56075660\n",
            "Iteration 74, loss = 1.56016199\n",
            "Iteration 75, loss = 1.55960550\n",
            "Iteration 76, loss = 1.55898141\n",
            "Iteration 77, loss = 1.55839606\n",
            "Iteration 78, loss = 1.55776373\n",
            "Iteration 79, loss = 1.55717272\n",
            "Iteration 80, loss = 1.55656284\n",
            "Iteration 81, loss = 1.55601769\n",
            "Iteration 82, loss = 1.55535019\n",
            "Iteration 83, loss = 1.55472178\n",
            "Iteration 84, loss = 1.55409097\n",
            "Iteration 85, loss = 1.55345552\n",
            "Iteration 86, loss = 1.55284661\n",
            "Iteration 87, loss = 1.55221362\n",
            "Iteration 88, loss = 1.55154764\n",
            "Iteration 89, loss = 1.55092437\n",
            "Iteration 90, loss = 1.55027751\n",
            "Iteration 91, loss = 1.54961861\n",
            "Iteration 92, loss = 1.54898989\n",
            "Iteration 93, loss = 1.54829653\n",
            "Iteration 94, loss = 1.54766047\n",
            "Iteration 95, loss = 1.54698518\n",
            "Iteration 96, loss = 1.54632422\n",
            "Iteration 97, loss = 1.54562554\n",
            "Iteration 98, loss = 1.54498415\n",
            "Iteration 99, loss = 1.54427274\n",
            "Iteration 100, loss = 1.54356933\n",
            "Iteration 1, loss = 1.63422935\n",
            "Iteration 2, loss = 1.60522831\n",
            "Iteration 3, loss = 1.59349164\n",
            "Iteration 4, loss = 1.58673286\n",
            "Iteration 5, loss = 1.58367346\n",
            "Iteration 6, loss = 1.58124635\n",
            "Iteration 7, loss = 1.57645352\n",
            "Iteration 8, loss = 1.57038407\n",
            "Iteration 9, loss = 1.56402631\n",
            "Iteration 10, loss = 1.55955697\n",
            "Iteration 11, loss = 1.55337987\n",
            "Iteration 12, loss = 1.54796204\n",
            "Iteration 13, loss = 1.54243929\n",
            "Iteration 14, loss = 1.53720929\n",
            "Iteration 15, loss = 1.53060128\n",
            "Iteration 16, loss = 1.52448644\n",
            "Iteration 17, loss = 1.51808797\n",
            "Iteration 18, loss = 1.51105941\n",
            "Iteration 19, loss = 1.50351925\n",
            "Iteration 20, loss = 1.49547440\n",
            "Iteration 21, loss = 1.48733345\n",
            "Iteration 22, loss = 1.47879242\n",
            "Iteration 23, loss = 1.46978323\n",
            "Iteration 24, loss = 1.46072575\n",
            "Iteration 25, loss = 1.45098262\n",
            "Iteration 26, loss = 1.44125767\n",
            "Iteration 27, loss = 1.43069678\n",
            "Iteration 28, loss = 1.42045469\n",
            "Iteration 29, loss = 1.40994113\n",
            "Iteration 30, loss = 1.39902320\n",
            "Iteration 31, loss = 1.38939572\n",
            "Iteration 32, loss = 1.37730713\n",
            "Iteration 33, loss = 1.36657233\n",
            "Iteration 34, loss = 1.35595796\n",
            "Iteration 35, loss = 1.34513350\n",
            "Iteration 36, loss = 1.33461841\n",
            "Iteration 37, loss = 1.32464801\n",
            "Iteration 38, loss = 1.31436240\n",
            "Iteration 39, loss = 1.30450199\n",
            "Iteration 40, loss = 1.29509549\n",
            "Iteration 41, loss = 1.28581850\n",
            "Iteration 42, loss = 1.27699712\n",
            "Iteration 43, loss = 1.26882510\n",
            "Iteration 44, loss = 1.26067504\n",
            "Iteration 45, loss = 1.25353023\n",
            "Iteration 46, loss = 1.24599038\n",
            "Iteration 47, loss = 1.23881809\n",
            "Iteration 48, loss = 1.23243219\n",
            "Iteration 49, loss = 1.22594958\n",
            "Iteration 50, loss = 1.22005713\n",
            "Iteration 51, loss = 1.21520620\n",
            "Iteration 52, loss = 1.20979521\n",
            "Iteration 53, loss = 1.20435467\n",
            "Iteration 54, loss = 1.19958010\n",
            "Iteration 55, loss = 1.19538092\n",
            "Iteration 56, loss = 1.19108307\n",
            "Iteration 57, loss = 1.18758464\n",
            "Iteration 58, loss = 1.18323734\n",
            "Iteration 59, loss = 1.17973221\n",
            "Iteration 60, loss = 1.17621034\n",
            "Iteration 61, loss = 1.17305841\n",
            "Iteration 62, loss = 1.16986132\n",
            "Iteration 63, loss = 1.16686598\n",
            "Iteration 64, loss = 1.16421366\n",
            "Iteration 65, loss = 1.16184646\n",
            "Iteration 66, loss = 1.15882288\n",
            "Iteration 67, loss = 1.15656775\n",
            "Iteration 68, loss = 1.15429903\n",
            "Iteration 69, loss = 1.15248921\n",
            "Iteration 70, loss = 1.14989808\n",
            "Iteration 71, loss = 1.14838566\n",
            "Iteration 72, loss = 1.14590563\n",
            "Iteration 73, loss = 1.14389299\n",
            "Iteration 74, loss = 1.14249564\n",
            "Iteration 75, loss = 1.14078871\n",
            "Iteration 76, loss = 1.13903017\n",
            "Iteration 77, loss = 1.13745742\n",
            "Iteration 78, loss = 1.13598691\n",
            "Iteration 79, loss = 1.13427645\n",
            "Iteration 80, loss = 1.13376870\n",
            "Iteration 81, loss = 1.13259541\n",
            "Iteration 82, loss = 1.13053432\n",
            "Iteration 83, loss = 1.12939109\n",
            "Iteration 84, loss = 1.12768997\n",
            "Iteration 85, loss = 1.12653022\n",
            "Iteration 86, loss = 1.12542757\n",
            "Iteration 87, loss = 1.12549362\n",
            "Iteration 88, loss = 1.12322530\n",
            "Iteration 89, loss = 1.12230640\n",
            "Iteration 90, loss = 1.12136007\n",
            "Iteration 91, loss = 1.12008683\n",
            "Iteration 92, loss = 1.11916201\n",
            "Iteration 93, loss = 1.11820343\n",
            "Iteration 94, loss = 1.11747013\n",
            "Iteration 95, loss = 1.11638736\n",
            "Iteration 96, loss = 1.11518926\n",
            "Iteration 97, loss = 1.11433842\n",
            "Iteration 98, loss = 1.11367185\n",
            "Iteration 99, loss = 1.11290558\n",
            "Iteration 100, loss = 1.11184923\n",
            "Iteration 1, loss = 1.65126108\n",
            "Iteration 2, loss = 1.59895646\n",
            "Iteration 3, loss = 1.55159269\n",
            "Iteration 4, loss = 1.51717119\n",
            "Iteration 5, loss = 1.45226839\n",
            "Iteration 6, loss = 1.38406971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss = 1.31480627\n",
            "Iteration 8, loss = 1.24474899\n",
            "Iteration 9, loss = 1.19598103\n",
            "Iteration 10, loss = 1.17463104\n",
            "Iteration 11, loss = 1.14595959\n",
            "Iteration 12, loss = 1.13462302\n",
            "Iteration 13, loss = 1.12012247\n",
            "Iteration 14, loss = 1.12626459\n",
            "Iteration 15, loss = 1.11217455\n",
            "Iteration 16, loss = 1.11382684\n",
            "Iteration 17, loss = 1.10678405\n",
            "Iteration 18, loss = 1.12936015\n",
            "Iteration 19, loss = 1.11527622\n",
            "Iteration 20, loss = 1.11210731\n",
            "Iteration 21, loss = 1.11645137\n",
            "Iteration 22, loss = 1.10299625\n",
            "Iteration 23, loss = 1.10014226\n",
            "Iteration 24, loss = 1.09955143\n",
            "Iteration 25, loss = 1.09503147\n",
            "Iteration 26, loss = 1.10527874\n",
            "Iteration 27, loss = 1.08596185\n",
            "Iteration 28, loss = 1.08998215\n",
            "Iteration 29, loss = 1.09132737\n",
            "Iteration 30, loss = 1.07704951\n",
            "Iteration 31, loss = 1.08417243\n",
            "Iteration 32, loss = 1.07084347\n",
            "Iteration 33, loss = 1.06736877\n",
            "Iteration 34, loss = 1.08356440\n",
            "Iteration 35, loss = 1.07045734\n",
            "Iteration 36, loss = 1.06865272\n",
            "Iteration 37, loss = 1.05848094\n",
            "Iteration 38, loss = 1.05219562\n",
            "Iteration 39, loss = 1.03490507\n",
            "Iteration 40, loss = 1.03714567\n",
            "Iteration 41, loss = 1.04191143\n",
            "Iteration 42, loss = 1.03291462\n",
            "Iteration 43, loss = 1.01198320\n",
            "Iteration 44, loss = 1.02629897\n",
            "Iteration 45, loss = 1.01548563\n",
            "Iteration 46, loss = 1.00012316\n",
            "Iteration 47, loss = 0.98892058\n",
            "Iteration 48, loss = 0.99688981\n",
            "Iteration 49, loss = 0.97733030\n",
            "Iteration 50, loss = 0.97232965\n",
            "Iteration 51, loss = 0.96953836\n",
            "Iteration 52, loss = 0.97287254\n",
            "Iteration 53, loss = 0.99470694\n",
            "Iteration 54, loss = 0.97532872\n",
            "Iteration 55, loss = 0.94277989\n",
            "Iteration 56, loss = 0.95454827\n",
            "Iteration 57, loss = 0.95700494\n",
            "Iteration 58, loss = 0.93400383\n",
            "Iteration 59, loss = 0.91476161\n",
            "Iteration 60, loss = 0.90942590\n",
            "Iteration 61, loss = 0.90041095\n",
            "Iteration 62, loss = 0.91757668\n",
            "Iteration 63, loss = 0.91224196\n",
            "Iteration 64, loss = 0.89342452\n",
            "Iteration 65, loss = 0.88764970\n",
            "Iteration 66, loss = 0.88292644\n",
            "Iteration 67, loss = 0.88377510\n",
            "Iteration 68, loss = 0.93628788\n",
            "Iteration 69, loss = 0.91378665\n",
            "Iteration 70, loss = 0.89335977\n",
            "Iteration 71, loss = 0.88993064\n",
            "Iteration 72, loss = 0.86465045\n",
            "Iteration 73, loss = 0.85952009\n",
            "Iteration 74, loss = 0.86161521\n",
            "Iteration 75, loss = 0.85840279\n",
            "Iteration 76, loss = 0.84984558\n",
            "Iteration 77, loss = 0.83705605\n",
            "Iteration 78, loss = 0.84118402\n",
            "Iteration 79, loss = 0.85062705\n",
            "Iteration 80, loss = 0.84473044\n",
            "Iteration 81, loss = 0.83465395\n",
            "Iteration 82, loss = 0.88025284\n",
            "Iteration 83, loss = 0.97314087\n",
            "Iteration 84, loss = 0.84593669\n",
            "Iteration 85, loss = 0.86210710\n",
            "Iteration 86, loss = 0.86282103\n",
            "Iteration 87, loss = 0.84639364\n",
            "Iteration 88, loss = 0.83687091\n",
            "Iteration 89, loss = 0.83521374\n",
            "Iteration 90, loss = 0.82398321\n",
            "Iteration 91, loss = 0.82067798\n",
            "Iteration 92, loss = 0.80719189\n",
            "Iteration 93, loss = 0.83054782\n",
            "Iteration 94, loss = 0.82446372\n",
            "Iteration 95, loss = 0.82090768\n",
            "Iteration 96, loss = 0.81356327\n",
            "Iteration 97, loss = 0.81914533\n",
            "Iteration 98, loss = 0.83574749\n",
            "Iteration 99, loss = 0.82826358\n",
            "Iteration 100, loss = 0.88125158\n",
            "Iteration 1, loss = 1.63415229\n",
            "Iteration 2, loss = 1.63174407\n",
            "Iteration 3, loss = 1.62978681\n",
            "Iteration 4, loss = 1.62757695\n",
            "Iteration 5, loss = 1.62579101\n",
            "Iteration 6, loss = 1.62396972\n",
            "Iteration 7, loss = 1.62217367\n",
            "Iteration 8, loss = 1.62078186\n",
            "Iteration 9, loss = 1.61927741\n",
            "Iteration 10, loss = 1.61787606\n",
            "Iteration 11, loss = 1.61647579\n",
            "Iteration 12, loss = 1.61519707\n",
            "Iteration 13, loss = 1.61398117\n",
            "Iteration 14, loss = 1.61294879\n",
            "Iteration 15, loss = 1.61161397\n",
            "Iteration 16, loss = 1.61058333\n",
            "Iteration 17, loss = 1.60946899\n",
            "Iteration 18, loss = 1.60878349\n",
            "Iteration 19, loss = 1.60792964\n",
            "Iteration 20, loss = 1.60703561\n",
            "Iteration 21, loss = 1.60609107\n",
            "Iteration 22, loss = 1.60557834\n",
            "Iteration 23, loss = 1.60459162\n",
            "Iteration 24, loss = 1.60409764\n",
            "Iteration 25, loss = 1.60343218\n",
            "Iteration 26, loss = 1.60296047\n",
            "Iteration 27, loss = 1.60231479\n",
            "Iteration 28, loss = 1.60172209\n",
            "Iteration 29, loss = 1.60138743\n",
            "Iteration 30, loss = 1.60089304\n",
            "Iteration 31, loss = 1.60044176\n",
            "Iteration 32, loss = 1.60007247\n",
            "Iteration 33, loss = 1.59981547\n",
            "Iteration 34, loss = 1.59930070\n",
            "Iteration 35, loss = 1.59903678\n",
            "Iteration 36, loss = 1.59880244\n",
            "Iteration 37, loss = 1.59850494\n",
            "Iteration 38, loss = 1.59826713\n",
            "Iteration 39, loss = 1.59804634\n",
            "Iteration 40, loss = 1.59775774\n",
            "Iteration 41, loss = 1.59761863\n",
            "Iteration 42, loss = 1.59740767\n",
            "Iteration 43, loss = 1.59720065\n",
            "Iteration 44, loss = 1.59711616\n",
            "Iteration 45, loss = 1.59700695\n",
            "Iteration 46, loss = 1.59686889\n",
            "Iteration 47, loss = 1.59672211\n",
            "Iteration 48, loss = 1.59656024\n",
            "Iteration 49, loss = 1.59646568\n",
            "Iteration 50, loss = 1.59634026\n",
            "Iteration 51, loss = 1.59631793\n",
            "Iteration 52, loss = 1.59612185\n",
            "Iteration 53, loss = 1.59607864\n",
            "Iteration 54, loss = 1.59600287\n",
            "Iteration 55, loss = 1.59590842\n",
            "Iteration 56, loss = 1.59585415\n",
            "Iteration 57, loss = 1.59579905\n",
            "Iteration 58, loss = 1.59565193\n",
            "Iteration 59, loss = 1.59564919\n",
            "Iteration 60, loss = 1.59553590\n",
            "Iteration 61, loss = 1.59548173\n",
            "Iteration 62, loss = 1.59539666\n",
            "Iteration 63, loss = 1.59533309\n",
            "Iteration 64, loss = 1.59528694\n",
            "Iteration 65, loss = 1.59521561\n",
            "Iteration 66, loss = 1.59512884\n",
            "Iteration 67, loss = 1.59507886\n",
            "Iteration 68, loss = 1.59501176\n",
            "Iteration 69, loss = 1.59498153\n",
            "Iteration 70, loss = 1.59491773\n",
            "Iteration 71, loss = 1.59485680\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.63283811\n",
            "Iteration 2, loss = 1.61418122\n",
            "Iteration 3, loss = 1.60657925\n",
            "Iteration 4, loss = 1.59960957\n",
            "Iteration 5, loss = 1.59804833\n",
            "Iteration 6, loss = 1.59835360\n",
            "Iteration 7, loss = 1.59828400\n",
            "Iteration 8, loss = 1.59793177"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9, loss = 1.59735367\n",
            "Iteration 10, loss = 1.59659125\n",
            "Iteration 11, loss = 1.59511343\n",
            "Iteration 12, loss = 1.59347953\n",
            "Iteration 13, loss = 1.59282728\n",
            "Iteration 14, loss = 1.59205038\n",
            "Iteration 15, loss = 1.59116711\n",
            "Iteration 16, loss = 1.59099571\n",
            "Iteration 17, loss = 1.59054925\n",
            "Iteration 18, loss = 1.58990356\n",
            "Iteration 19, loss = 1.58906750\n",
            "Iteration 20, loss = 1.58779524\n",
            "Iteration 21, loss = 1.58688371\n",
            "Iteration 22, loss = 1.58615804\n",
            "Iteration 23, loss = 1.58485326\n",
            "Iteration 24, loss = 1.58396332\n",
            "Iteration 25, loss = 1.58301629\n",
            "Iteration 26, loss = 1.58222195\n",
            "Iteration 27, loss = 1.58067800\n",
            "Iteration 28, loss = 1.57945578\n",
            "Iteration 29, loss = 1.57836972\n",
            "Iteration 30, loss = 1.57674803\n",
            "Iteration 31, loss = 1.57630906\n",
            "Iteration 32, loss = 1.57385849\n",
            "Iteration 33, loss = 1.57239122\n",
            "Iteration 34, loss = 1.57080231\n",
            "Iteration 35, loss = 1.56867932\n",
            "Iteration 36, loss = 1.56714837\n",
            "Iteration 37, loss = 1.56501211\n",
            "Iteration 38, loss = 1.56287838\n",
            "Iteration 39, loss = 1.56053197\n",
            "Iteration 40, loss = 1.55828627\n",
            "Iteration 41, loss = 1.55596609\n",
            "Iteration 42, loss = 1.55325058\n",
            "Iteration 43, loss = 1.55072289\n",
            "Iteration 44, loss = 1.54774957\n",
            "Iteration 45, loss = 1.54471678\n",
            "Iteration 46, loss = 1.54172392\n",
            "Iteration 47, loss = 1.53830193\n",
            "Iteration 48, loss = 1.53509265\n",
            "Iteration 49, loss = 1.53120214\n",
            "Iteration 50, loss = 1.52756053\n",
            "Iteration 51, loss = 1.52429989\n",
            "Iteration 52, loss = 1.52011391\n",
            "Iteration 53, loss = 1.51555985\n",
            "Iteration 54, loss = 1.51121027\n",
            "Iteration 55, loss = 1.50695599\n",
            "Iteration 56, loss = 1.50226955\n",
            "Iteration 57, loss = 1.49826531\n",
            "Iteration 58, loss = 1.49265171\n",
            "Iteration 59, loss = 1.48788696\n",
            "Iteration 60, loss = 1.48256222\n",
            "Iteration 61, loss = 1.47745760\n",
            "Iteration 62, loss = 1.47180694\n",
            "Iteration 63, loss = 1.46646998\n",
            "Iteration 64, loss = 1.46105305\n",
            "Iteration 65, loss = 1.45554836\n",
            "Iteration 66, loss = 1.44926411\n",
            "Iteration 67, loss = 1.44339104\n",
            "Iteration 68, loss = 1.43750330\n",
            "Iteration 69, loss = 1.43185680\n",
            "Iteration 70, loss = 1.42560417\n",
            "Iteration 71, loss = 1.41968305\n",
            "Iteration 72, loss = 1.41318507\n",
            "Iteration 73, loss = 1.40690636\n",
            "Iteration 74, loss = 1.40073190\n",
            "Iteration 75, loss = 1.39494247\n",
            "Iteration 76, loss = 1.38833400\n",
            "Iteration 77, loss = 1.38244917\n",
            "Iteration 78, loss = 1.37616690\n",
            "Iteration 79, loss = 1.37004899\n",
            "Iteration 80, loss = 1.36432975\n",
            "Iteration 81, loss = 1.35861606\n",
            "Iteration 82, loss = 1.35234141\n",
            "Iteration 83, loss = 1.34649356\n",
            "Iteration 84, loss = 1.34050356\n",
            "Iteration 85, loss = 1.33486863\n",
            "Iteration 86, loss = 1.32953898\n",
            "Iteration 87, loss = 1.32448962\n",
            "Iteration 88, loss = 1.31856244\n",
            "Iteration 89, loss = 1.31359099\n",
            "Iteration 90, loss = 1.30838300\n",
            "Iteration 91, loss = 1.30338560\n",
            "Iteration 92, loss = 1.29886822\n",
            "Iteration 93, loss = 1.29374404\n",
            "Iteration 94, loss = 1.28937925\n",
            "Iteration 95, loss = 1.28479474\n",
            "Iteration 96, loss = 1.28025187\n",
            "Iteration 97, loss = 1.27596790\n",
            "Iteration 98, loss = 1.27212843\n",
            "Iteration 99, loss = 1.26790325\n",
            "Iteration 100, loss = 1.26387289\n",
            "Iteration 1, loss = 1.70220391\n",
            "Iteration 2, loss = 1.62050323\n",
            "Iteration 3, loss = 1.63355519\n",
            "Iteration 4, loss = 1.62707696\n",
            "Iteration 5, loss = 1.60304372\n",
            "Iteration 6, loss = 1.58610766\n",
            "Iteration 7, loss = 1.58612280\n",
            "Iteration 8, loss = 1.58180711\n",
            "Iteration 9, loss = 1.56954462\n",
            "Iteration 10, loss = 1.55623168\n",
            "Iteration 11, loss = 1.53551032\n",
            "Iteration 12, loss = 1.51643142\n",
            "Iteration 13, loss = 1.49235554\n",
            "Iteration 14, loss = 1.46496332\n",
            "Iteration 15, loss = 1.42597609\n",
            "Iteration 16, loss = 1.38777727\n",
            "Iteration 17, loss = 1.35351568\n",
            "Iteration 18, loss = 1.32166843\n",
            "Iteration 19, loss = 1.28789606\n",
            "Iteration 20, loss = 1.25962016\n",
            "Iteration 21, loss = 1.23652889\n",
            "Iteration 22, loss = 1.21694754\n",
            "Iteration 23, loss = 1.19710627\n",
            "Iteration 24, loss = 1.18093310\n",
            "Iteration 25, loss = 1.17009949\n",
            "Iteration 26, loss = 1.16321714\n",
            "Iteration 27, loss = 1.15083228\n",
            "Iteration 28, loss = 1.14651472\n",
            "Iteration 29, loss = 1.14253747\n",
            "Iteration 30, loss = 1.13539103\n",
            "Iteration 31, loss = 1.13326919\n",
            "Iteration 32, loss = 1.12865430\n",
            "Iteration 33, loss = 1.12756311\n",
            "Iteration 34, loss = 1.12472188\n",
            "Iteration 35, loss = 1.12180471\n",
            "Iteration 36, loss = 1.12166386\n",
            "Iteration 37, loss = 1.11540440\n",
            "Iteration 38, loss = 1.11556391\n",
            "Iteration 39, loss = 1.11260579\n",
            "Iteration 40, loss = 1.11380169\n",
            "Iteration 41, loss = 1.11321305\n",
            "Iteration 42, loss = 1.11016544\n",
            "Iteration 43, loss = 1.11496685\n",
            "Iteration 44, loss = 1.11138915\n",
            "Iteration 45, loss = 1.11331815\n",
            "Iteration 46, loss = 1.11063414\n",
            "Iteration 47, loss = 1.11122206\n",
            "Iteration 48, loss = 1.11369035\n",
            "Iteration 49, loss = 1.10603506\n",
            "Iteration 50, loss = 1.11055927\n",
            "Iteration 51, loss = 1.11291631\n",
            "Iteration 52, loss = 1.11205195\n",
            "Iteration 53, loss = 1.10613953\n",
            "Iteration 54, loss = 1.10382249\n",
            "Iteration 55, loss = 1.10403124\n",
            "Iteration 56, loss = 1.10065414\n",
            "Iteration 57, loss = 1.10529699\n",
            "Iteration 58, loss = 1.10006527\n",
            "Iteration 59, loss = 1.10352353\n",
            "Iteration 60, loss = 1.10190957\n",
            "Iteration 61, loss = 1.10314443\n",
            "Iteration 62, loss = 1.09848673\n",
            "Iteration 63, loss = 1.09965908\n",
            "Iteration 64, loss = 1.10125822\n",
            "Iteration 65, loss = 1.09933457\n",
            "Iteration 66, loss = 1.09577363\n",
            "Iteration 67, loss = 1.09608334\n",
            "Iteration 68, loss = 1.09431038\n",
            "Iteration 69, loss = 1.09849872\n",
            "Iteration 70, loss = 1.09557949\n",
            "Iteration 71, loss = 1.10261770\n",
            "Iteration 72, loss = 1.09266069\n",
            "Iteration 73, loss = 1.09087822\n",
            "Iteration 74, loss = 1.08926256\n",
            "Iteration 75, loss = 1.09754571\n",
            "Iteration 76, loss = 1.09051226\n",
            "Iteration 77, loss = 1.09172639\n",
            "Iteration 78, loss = 1.08450251\n",
            "Iteration 79, loss = 1.08872454\n",
            "Iteration 80, loss = 1.09405751\n",
            "Iteration 81, loss = 1.09319620\n",
            "Iteration 82, loss = 1.08278255\n",
            "Iteration 83, loss = 1.08230107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 84, loss = 1.07694574\n",
            "Iteration 85, loss = 1.07905998\n",
            "Iteration 86, loss = 1.06984324\n",
            "Iteration 87, loss = 1.07874150\n",
            "Iteration 88, loss = 1.06877738\n",
            "Iteration 89, loss = 1.06870296\n",
            "Iteration 90, loss = 1.07015377\n",
            "Iteration 91, loss = 1.06664608\n",
            "Iteration 92, loss = 1.06085063\n",
            "Iteration 93, loss = 1.05554744\n",
            "Iteration 94, loss = 1.05693331\n",
            "Iteration 95, loss = 1.05135664\n",
            "Iteration 96, loss = 1.04693532\n",
            "Iteration 97, loss = 1.04417437\n",
            "Iteration 98, loss = 1.04340068\n",
            "Iteration 99, loss = 1.04075415\n",
            "Iteration 100, loss = 1.03454127\n",
            "Iteration 1, loss = 1.64003911\n",
            "Iteration 2, loss = 1.63633561\n",
            "Iteration 3, loss = 1.63314819\n",
            "Iteration 4, loss = 1.62968401\n",
            "Iteration 5, loss = 1.62660740\n",
            "Iteration 6, loss = 1.62379267\n",
            "Iteration 7, loss = 1.62052247\n",
            "Iteration 8, loss = 1.61793948\n",
            "Iteration 9, loss = 1.61565696\n",
            "Iteration 10, loss = 1.61308188\n",
            "Iteration 11, loss = 1.61122940\n",
            "Iteration 12, loss = 1.60898294\n",
            "Iteration 13, loss = 1.60716813\n",
            "Iteration 14, loss = 1.60516624\n",
            "Iteration 15, loss = 1.60355659\n",
            "Iteration 16, loss = 1.60196687\n",
            "Iteration 17, loss = 1.60036547\n",
            "Iteration 18, loss = 1.59893911\n",
            "Iteration 19, loss = 1.59757802\n",
            "Iteration 20, loss = 1.59640331\n",
            "Iteration 21, loss = 1.59535132\n",
            "Iteration 22, loss = 1.59421867\n",
            "Iteration 23, loss = 1.59307606\n",
            "Iteration 24, loss = 1.59214154\n",
            "Iteration 25, loss = 1.59118824\n",
            "Iteration 26, loss = 1.59043228\n",
            "Iteration 27, loss = 1.58956298\n",
            "Iteration 28, loss = 1.58871782\n",
            "Iteration 29, loss = 1.58810686\n",
            "Iteration 30, loss = 1.58728611\n",
            "Iteration 31, loss = 1.58662035\n",
            "Iteration 32, loss = 1.58592091\n",
            "Iteration 33, loss = 1.58537933\n",
            "Iteration 34, loss = 1.58469407\n",
            "Iteration 35, loss = 1.58409385\n",
            "Iteration 36, loss = 1.58345477\n",
            "Iteration 37, loss = 1.58295346\n",
            "Iteration 38, loss = 1.58240817\n",
            "Iteration 39, loss = 1.58182462\n",
            "Iteration 40, loss = 1.58128469\n",
            "Iteration 41, loss = 1.58077435\n",
            "Iteration 42, loss = 1.58021936\n",
            "Iteration 43, loss = 1.57966324\n",
            "Iteration 44, loss = 1.57916555\n",
            "Iteration 45, loss = 1.57867735\n",
            "Iteration 46, loss = 1.57815482\n",
            "Iteration 47, loss = 1.57762176\n",
            "Iteration 48, loss = 1.57714217\n",
            "Iteration 49, loss = 1.57659856\n",
            "Iteration 50, loss = 1.57607940\n",
            "Iteration 51, loss = 1.57561910\n",
            "Iteration 52, loss = 1.57509679\n",
            "Iteration 53, loss = 1.57455612\n",
            "Iteration 54, loss = 1.57405918\n",
            "Iteration 55, loss = 1.57356876\n",
            "Iteration 56, loss = 1.57304745\n",
            "Iteration 57, loss = 1.57254699"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 58, loss = 1.57200715\n",
            "Iteration 59, loss = 1.57150689\n",
            "Iteration 60, loss = 1.57098165\n",
            "Iteration 61, loss = 1.57045734\n",
            "Iteration 62, loss = 1.56992813\n",
            "Iteration 63, loss = 1.56941471\n",
            "Iteration 64, loss = 1.56889877\n",
            "Iteration 65, loss = 1.56838899\n",
            "Iteration 66, loss = 1.56781678\n",
            "Iteration 67, loss = 1.56729076\n",
            "Iteration 68, loss = 1.56676287\n",
            "Iteration 69, loss = 1.56623198\n",
            "Iteration 70, loss = 1.56568297\n",
            "Iteration 71, loss = 1.56516690\n",
            "Iteration 72, loss = 1.56458160\n",
            "Iteration 73, loss = 1.56401933\n",
            "Iteration 74, loss = 1.56345985\n",
            "Iteration 75, loss = 1.56293570\n",
            "Iteration 76, loss = 1.56234779\n",
            "Iteration 77, loss = 1.56179575\n",
            "Iteration 78, loss = 1.56119979\n",
            "Iteration 79, loss = 1.56064187\n",
            "Iteration 80, loss = 1.56006624\n",
            "Iteration 81, loss = 1.55955154\n",
            "Iteration 82, loss = 1.55891999\n",
            "Iteration 83, loss = 1.55832518\n",
            "Iteration 84, loss = 1.55772810\n",
            "Iteration 85, loss = 1.55712605\n",
            "Iteration 86, loss = 1.55654812\n",
            "Iteration 87, loss = 1.55594842\n",
            "Iteration 88, loss = 1.55531646\n",
            "Iteration 89, loss = 1.55472401\n",
            "Iteration 90, loss = 1.55410845\n",
            "Iteration 91, loss = 1.55348298\n",
            "Iteration 92, loss = 1.55288379\n",
            "Iteration 93, loss = 1.55222396\n",
            "Iteration 94, loss = 1.55161704\n",
            "Iteration 95, loss = 1.55097299\n",
            "Iteration 96, loss = 1.55034171\n",
            "Iteration 97, loss = 1.54967440\n",
            "Iteration 98, loss = 1.54906062\n",
            "Iteration 99, loss = 1.54838106\n",
            "Iteration 100, loss = 1.54770749\n",
            "Iteration 1, loss = 1.63283001\n",
            "Iteration 2, loss = 1.60590193\n",
            "Iteration 3, loss = 1.59464455\n",
            "Iteration 4, loss = 1.58773002\n",
            "Iteration 5, loss = 1.58456072\n",
            "Iteration 6, loss = 1.58247880\n",
            "Iteration 7, loss = 1.57817376\n",
            "Iteration 8, loss = 1.57265554\n",
            "Iteration 9, loss = 1.56665669\n",
            "Iteration 10, loss = 1.56226153\n",
            "Iteration 11, loss = 1.55623506\n",
            "Iteration 12, loss = 1.55100009\n",
            "Iteration 13, loss = 1.54564776\n",
            "Iteration 14, loss = 1.54069455\n",
            "Iteration 15, loss = 1.53424288\n",
            "Iteration 16, loss = 1.52825441\n",
            "Iteration 17, loss = 1.52200765\n",
            "Iteration 18, loss = 1.51507371\n",
            "Iteration 19, loss = 1.50762819\n",
            "Iteration 20, loss = 1.49967772\n",
            "Iteration 21, loss = 1.49157319\n",
            "Iteration 22, loss = 1.48298570\n",
            "Iteration 23, loss = 1.47392877\n",
            "Iteration 24, loss = 1.46480071\n",
            "Iteration 25, loss = 1.45490488\n",
            "Iteration 26, loss = 1.44500756\n",
            "Iteration 27, loss = 1.43426631\n",
            "Iteration 28, loss = 1.42376494\n",
            "Iteration 29, loss = 1.41294409\n",
            "Iteration 30, loss = 1.40172906\n",
            "Iteration 31, loss = 1.39172754\n",
            "Iteration 32, loss = 1.37930193\n",
            "Iteration 33, loss = 1.36814841\n",
            "Iteration 34, loss = 1.35718415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 35, loss = 1.34600379\n",
            "Iteration 36, loss = 1.33505413\n",
            "Iteration 37, loss = 1.32474965\n",
            "Iteration 38, loss = 1.31409855\n",
            "Iteration 39, loss = 1.30393202\n",
            "Iteration 40, loss = 1.29420565\n",
            "Iteration 41, loss = 1.28465917\n",
            "Iteration 42, loss = 1.27559783\n",
            "Iteration 43, loss = 1.26717869\n",
            "Iteration 44, loss = 1.25892169\n",
            "Iteration 45, loss = 1.25159198\n",
            "Iteration 46, loss = 1.24392843\n",
            "Iteration 47, loss = 1.23669549\n",
            "Iteration 48, loss = 1.23020364\n",
            "Iteration 49, loss = 1.22374319\n",
            "Iteration 50, loss = 1.21782989\n",
            "Iteration 51, loss = 1.21292165\n",
            "Iteration 52, loss = 1.20742951\n",
            "Iteration 53, loss = 1.20212279\n",
            "Iteration 54, loss = 1.19738788\n",
            "Iteration 55, loss = 1.19324263\n",
            "Iteration 56, loss = 1.18898138\n",
            "Iteration 57, loss = 1.18543758\n",
            "Iteration 58, loss = 1.18126265\n",
            "Iteration 59, loss = 1.17779600\n",
            "Iteration 60, loss = 1.17439518\n",
            "Iteration 61, loss = 1.17129160\n",
            "Iteration 62, loss = 1.16821478\n",
            "Iteration 63, loss = 1.16528622\n",
            "Iteration 64, loss = 1.16271761\n",
            "Iteration 65, loss = 1.16042839\n",
            "Iteration 66, loss = 1.15751546\n",
            "Iteration 67, loss = 1.15536268\n",
            "Iteration 68, loss = 1.15319502\n",
            "Iteration 69, loss = 1.15141027\n",
            "Iteration 70, loss = 1.14895557\n",
            "Iteration 71, loss = 1.14743574\n",
            "Iteration 72, loss = 1.14514417\n",
            "Iteration 73, loss = 1.14321631\n",
            "Iteration 74, loss = 1.14188621\n",
            "Iteration 75, loss = 1.14019894\n",
            "Iteration 76, loss = 1.13860193\n",
            "Iteration 77, loss = 1.13710295\n",
            "Iteration 78, loss = 1.13568987\n",
            "Iteration 79, loss = 1.13407647\n",
            "Iteration 80, loss = 1.13361304\n",
            "Iteration 81, loss = 1.13238099\n",
            "Iteration 82, loss = 1.13052278\n",
            "Iteration 83, loss = 1.12943495\n",
            "Iteration 84, loss = 1.12780618\n",
            "Iteration 85, loss = 1.12672435\n",
            "Iteration 86, loss = 1.12570959\n",
            "Iteration 87, loss = 1.12583023\n",
            "Iteration 88, loss = 1.12363479\n",
            "Iteration 89, loss = 1.12277042\n",
            "Iteration 90, loss = 1.12186767\n",
            "Iteration 91, loss = 1.12066746\n",
            "Iteration 92, loss = 1.11980307\n",
            "Iteration 93, loss = 1.11892453\n",
            "Iteration 94, loss = 1.11820272\n",
            "Iteration 95, loss = 1.11722272\n",
            "Iteration 96, loss = 1.11610967\n",
            "Iteration 97, loss = 1.11530761\n",
            "Iteration 98, loss = 1.11469318\n",
            "Iteration 99, loss = 1.11400219\n",
            "Iteration 100, loss = 1.11305181\n",
            "Iteration 1, loss = 1.64359132\n",
            "Iteration 2, loss = 1.59373821\n",
            "Iteration 3, loss = 1.55074609\n",
            "Iteration 4, loss = 1.50471534\n",
            "Iteration 5, loss = 1.43608826\n",
            "Iteration 6, loss = 1.36101549\n",
            "Iteration 7, loss = 1.29016159\n",
            "Iteration 8, loss = 1.22556998"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9, loss = 1.18269787\n",
            "Iteration 10, loss = 1.16343339\n",
            "Iteration 11, loss = 1.13652085\n",
            "Iteration 12, loss = 1.12853845\n",
            "Iteration 13, loss = 1.11576500\n",
            "Iteration 14, loss = 1.12207874\n",
            "Iteration 15, loss = 1.10928971\n",
            "Iteration 16, loss = 1.11118371\n",
            "Iteration 17, loss = 1.10519042\n",
            "Iteration 18, loss = 1.12027854\n",
            "Iteration 19, loss = 1.11296645\n",
            "Iteration 20, loss = 1.10561654\n",
            "Iteration 21, loss = 1.11166455\n",
            "Iteration 22, loss = 1.10157268\n",
            "Iteration 23, loss = 1.09549380\n",
            "Iteration 24, loss = 1.09292923\n",
            "Iteration 25, loss = 1.08878584\n",
            "Iteration 26, loss = 1.09736511\n",
            "Iteration 27, loss = 1.07839695\n",
            "Iteration 28, loss = 1.08231185\n",
            "Iteration 29, loss = 1.08676517\n",
            "Iteration 30, loss = 1.06937486\n",
            "Iteration 31, loss = 1.07007144\n",
            "Iteration 32, loss = 1.06488975\n",
            "Iteration 33, loss = 1.05268231\n",
            "Iteration 34, loss = 1.05326085\n",
            "Iteration 35, loss = 1.05447201\n",
            "Iteration 36, loss = 1.06312917\n",
            "Iteration 37, loss = 1.04435930\n",
            "Iteration 38, loss = 1.02961812\n",
            "Iteration 39, loss = 1.03691587\n",
            "Iteration 40, loss = 1.01877360\n",
            "Iteration 41, loss = 1.01443727\n",
            "Iteration 42, loss = 0.99454884\n",
            "Iteration 43, loss = 1.00116880\n",
            "Iteration 44, loss = 1.00972653\n",
            "Iteration 45, loss = 1.00541027\n",
            "Iteration 46, loss = 0.96815934\n",
            "Iteration 47, loss = 0.97116294\n",
            "Iteration 48, loss = 0.96465042\n",
            "Iteration 49, loss = 0.95844761\n",
            "Iteration 50, loss = 0.96468453\n",
            "Iteration 51, loss = 0.94351564\n",
            "Iteration 52, loss = 0.95578607\n",
            "Iteration 53, loss = 0.98228718\n",
            "Iteration 54, loss = 0.96189498\n",
            "Iteration 55, loss = 0.92391597\n",
            "Iteration 56, loss = 0.93019651\n",
            "Iteration 57, loss = 0.93504477\n",
            "Iteration 58, loss = 0.90156896\n",
            "Iteration 59, loss = 0.89495878\n",
            "Iteration 60, loss = 0.88312664\n",
            "Iteration 61, loss = 0.88399318\n",
            "Iteration 62, loss = 0.88741635\n",
            "Iteration 63, loss = 0.88356703\n",
            "Iteration 64, loss = 0.86789977\n",
            "Iteration 65, loss = 0.86418915\n",
            "Iteration 66, loss = 0.85635174\n",
            "Iteration 67, loss = 0.85427182\n",
            "Iteration 68, loss = 0.89812597\n",
            "Iteration 69, loss = 0.87718896\n",
            "Iteration 70, loss = 0.86995659\n",
            "Iteration 71, loss = 0.91435988\n",
            "Iteration 72, loss = 0.90182069\n",
            "Iteration 73, loss = 0.87531178\n",
            "Iteration 74, loss = 0.87684439\n",
            "Iteration 75, loss = 0.90095682\n",
            "Iteration 76, loss = 0.86139386\n",
            "Iteration 77, loss = 0.85562552\n",
            "Iteration 78, loss = 0.84651931\n",
            "Iteration 79, loss = 0.87545706\n",
            "Iteration 80, loss = 0.86580723\n",
            "Iteration 81, loss = 0.88586448\n",
            "Iteration 82, loss = 0.96419844\n",
            "Iteration 83, loss = 0.95511548\n",
            "Iteration 84, loss = 0.86942819\n",
            "Iteration 85, loss = 0.86452925\n",
            "Iteration 86, loss = 0.83887873\n",
            "Iteration 87, loss = 0.83534896\n",
            "Iteration 88, loss = 0.82369188\n",
            "Iteration 89, loss = 0.81680444\n",
            "Iteration 90, loss = 0.81743580\n",
            "Iteration 91, loss = 0.81323864\n",
            "Iteration 92, loss = 0.81641119\n",
            "Iteration 93, loss = 0.79455188\n",
            "Iteration 94, loss = 0.79497499\n",
            "Iteration 95, loss = 0.79856098\n",
            "Iteration 96, loss = 0.79352122\n",
            "Iteration 97, loss = 0.79532787\n",
            "Iteration 98, loss = 0.80211368\n",
            "Iteration 99, loss = 0.80398204\n",
            "Iteration 100, loss = 0.81736314\n",
            "Iteration 1, loss = 1.63664499\n",
            "Iteration 2, loss = 1.63526004\n",
            "Iteration 3, loss = 1.63407126\n",
            "Iteration 4, loss = 1.63271138\n",
            "Iteration 5, loss = 1.63154584\n",
            "Iteration 6, loss = 1.63039437\n",
            "Iteration 7, loss = 1.62911138\n",
            "Iteration 8, loss = 1.62807658\n",
            "Iteration 9, loss = 1.62711244\n",
            "Iteration 10, loss = 1.62601122\n",
            "Iteration 11, loss = 1.62517896\n",
            "Iteration 12, loss = 1.62416707\n",
            "Iteration 13, loss = 1.62329608\n",
            "Iteration 14, loss = 1.62241907\n",
            "Iteration 15, loss = 1.62153334\n",
            "Iteration 16, loss = 1.62069083\n",
            "Iteration 17, loss = 1.61984260\n",
            "Iteration 18, loss = 1.61912307\n",
            "Iteration 19, loss = 1.61835676\n",
            "Iteration 20, loss = 1.61765299\n",
            "Iteration 21, loss = 1.61693351\n",
            "Iteration 22, loss = 1.61631750\n",
            "Iteration 23, loss = 1.61554776\n",
            "Iteration 24, loss = 1.61496025\n",
            "Iteration 25, loss = 1.61432730\n",
            "Iteration 26, loss = 1.61379142\n",
            "Iteration 27, loss = 1.61316176\n",
            "Iteration 28, loss = 1.61253564\n",
            "Iteration 29, loss = 1.61205388\n",
            "Iteration 30, loss = 1.61146086\n",
            "Iteration 31, loss = 1.61089502\n",
            "Iteration 32, loss = 1.61039804\n",
            "Iteration 33, loss = 1.60995901\n",
            "Iteration 34, loss = 1.60935602\n",
            "Iteration 35, loss = 1.60889580\n",
            "Iteration 36, loss = 1.60840671\n",
            "Iteration 37, loss = 1.60795090\n",
            "Iteration 38, loss = 1.60750623\n",
            "Iteration 39, loss = 1.60704077\n",
            "Iteration 40, loss = 1.60656556\n",
            "Iteration 41, loss = 1.60616094\n",
            "Iteration 42, loss = 1.60570889\n",
            "Iteration 43, loss = 1.60523141\n",
            "Iteration 44, loss = 1.60486721\n",
            "Iteration 45, loss = 1.60451018\n",
            "Iteration 46, loss = 1.60406999\n",
            "Iteration 47, loss = 1.60368136\n",
            "Iteration 48, loss = 1.60329346\n",
            "Iteration 49, loss = 1.60290675\n",
            "Iteration 50, loss = 1.60249833\n",
            "Iteration 51, loss = 1.60215561\n",
            "Iteration 52, loss = 1.60173771\n",
            "Iteration 53, loss = 1.60142004\n",
            "Iteration 54, loss = 1.60109425\n",
            "Iteration 55, loss = 1.60073289\n",
            "Iteration 56, loss = 1.60040262\n",
            "Iteration 57, loss = 1.60000898\n",
            "Iteration 58, loss = 1.59967512\n",
            "Iteration 59, loss = 1.59939136\n",
            "Iteration 60, loss = 1.59903607\n",
            "Iteration 61, loss = 1.59868542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 62, loss = 1.59838195\n",
            "Iteration 63, loss = 1.59808823\n",
            "Iteration 64, loss = 1.59777985\n",
            "Iteration 65, loss = 1.59747402\n",
            "Iteration 66, loss = 1.59714645\n",
            "Iteration 67, loss = 1.59686734\n",
            "Iteration 68, loss = 1.59655003\n",
            "Iteration 69, loss = 1.59623469\n",
            "Iteration 70, loss = 1.59597390\n",
            "Iteration 71, loss = 1.59568137\n",
            "Iteration 72, loss = 1.59537479\n",
            "Iteration 73, loss = 1.59509590\n",
            "Iteration 74, loss = 1.59479089\n",
            "Iteration 75, loss = 1.59451421\n",
            "Iteration 76, loss = 1.59423227\n",
            "Iteration 77, loss = 1.59394036\n",
            "Iteration 78, loss = 1.59360973\n",
            "Iteration 79, loss = 1.59334573\n",
            "Iteration 80, loss = 1.59299597\n",
            "Iteration 81, loss = 1.59273693\n",
            "Iteration 82, loss = 1.59227998\n",
            "Iteration 83, loss = 1.59184060\n",
            "Iteration 84, loss = 1.59141051\n",
            "Iteration 85, loss = 1.59099911\n",
            "Iteration 86, loss = 1.59063577\n",
            "Iteration 87, loss = 1.59035693\n",
            "Iteration 88, loss = 1.59016930\n",
            "Iteration 89, loss = 1.58998933\n",
            "Iteration 90, loss = 1.58976523\n",
            "Iteration 91, loss = 1.58957820\n",
            "Iteration 92, loss = 1.58936455\n",
            "Iteration 93, loss = 1.58916136\n",
            "Iteration 94, loss = 1.58898022\n",
            "Iteration 95, loss = 1.58875132\n",
            "Iteration 96, loss = 1.58857295\n",
            "Iteration 97, loss = 1.58835834\n",
            "Iteration 98, loss = 1.58817006\n",
            "Iteration 99, loss = 1.58795876\n",
            "Iteration 100, loss = 1.58776265\n",
            "Iteration 1, loss = 1.63482124\n",
            "Iteration 2, loss = 1.62349900\n",
            "Iteration 3, loss = 1.61667775\n",
            "Iteration 4, loss = 1.60976733\n",
            "Iteration 5, loss = 1.60519524\n",
            "Iteration 6, loss = 1.60131612\n",
            "Iteration 7, loss = 1.59616336\n",
            "Iteration 8, loss = 1.59385246\n",
            "Iteration 9, loss = 1.59203730\n",
            "Iteration 10, loss = 1.59037885\n",
            "Iteration 11, loss = 1.58858317\n",
            "Iteration 12, loss = 1.58714729\n",
            "Iteration 13, loss = 1.58575799\n",
            "Iteration 14, loss = 1.58436657\n",
            "Iteration 15, loss = 1.58270568\n",
            "Iteration 16, loss = 1.58132548\n",
            "Iteration 17, loss = 1.57967136\n",
            "Iteration 18, loss = 1.57817449\n",
            "Iteration 19, loss = 1.57651741\n",
            "Iteration 20, loss = 1.57461485\n",
            "Iteration 21, loss = 1.57261507\n",
            "Iteration 22, loss = 1.57073745\n",
            "Iteration 23, loss = 1.56850804\n",
            "Iteration 24, loss = 1.56633664\n",
            "Iteration 25, loss = 1.56401802\n",
            "Iteration 26, loss = 1.56157479\n",
            "Iteration 27, loss = 1.55887500\n",
            "Iteration 28, loss = 1.55615854\n",
            "Iteration 29, loss = 1.55338025\n",
            "Iteration 30, loss = 1.55034262\n",
            "Iteration 31, loss = 1.54768144\n",
            "Iteration 32, loss = 1.54397605\n",
            "Iteration 33, loss = 1.54061057\n",
            "Iteration 34, loss = 1.53713664\n",
            "Iteration 35, loss = 1.53355595\n",
            "Iteration 36, loss = 1.52977529\n",
            "Iteration 37, loss = 1.52604315\n",
            "Iteration 38, loss = 1.52197834\n",
            "Iteration 39, loss = 1.51783591\n",
            "Iteration 40, loss = 1.51382047\n",
            "Iteration 41, loss = 1.50956723\n",
            "Iteration 42, loss = 1.50515334\n",
            "Iteration 43, loss = 1.50083844\n",
            "Iteration 44, loss = 1.49652890\n",
            "Iteration 45, loss = 1.49175765\n",
            "Iteration 46, loss = 1.48739591\n",
            "Iteration 47, loss = 1.48253936\n",
            "Iteration 48, loss = 1.47804773\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 1.47314239\n",
            "Iteration 50, loss = 1.46837753\n",
            "Iteration 51, loss = 1.46371556\n",
            "Iteration 52, loss = 1.45874289\n",
            "Iteration 53, loss = 1.45378223\n",
            "Iteration 54, loss = 1.44873188\n",
            "Iteration 55, loss = 1.44394022\n",
            "Iteration 56, loss = 1.43882314\n",
            "Iteration 57, loss = 1.43404232\n",
            "Iteration 58, loss = 1.42894914\n",
            "Iteration 59, loss = 1.42398957\n",
            "Iteration 60, loss = 1.41896824\n",
            "Iteration 61, loss = 1.41394335\n",
            "Iteration 62, loss = 1.40897445\n",
            "Iteration 63, loss = 1.40402301\n",
            "Iteration 64, loss = 1.39914449\n",
            "Iteration 65, loss = 1.39452708\n",
            "Iteration 66, loss = 1.38935204\n",
            "Iteration 67, loss = 1.38460180\n",
            "Iteration 68, loss = 1.37992646\n",
            "Iteration 69, loss = 1.37518033\n",
            "Iteration 70, loss = 1.37045682\n",
            "Iteration 71, loss = 1.36605174\n",
            "Iteration 72, loss = 1.36141302\n",
            "Iteration 73, loss = 1.35682733\n",
            "Iteration 74, loss = 1.35241579\n",
            "Iteration 75, loss = 1.34807837\n",
            "Iteration 76, loss = 1.34382393\n",
            "Iteration 77, loss = 1.33972756\n",
            "Iteration 78, loss = 1.33549671\n",
            "Iteration 79, loss = 1.33139555\n",
            "Iteration 80, loss = 1.32761013\n",
            "Iteration 81, loss = 1.32357275\n",
            "Iteration 82, loss = 1.31976416\n",
            "Iteration 83, loss = 1.31590469\n",
            "Iteration 84, loss = 1.31214191\n",
            "Iteration 85, loss = 1.30850797\n",
            "Iteration 86, loss = 1.30508532\n",
            "Iteration 87, loss = 1.30188439\n",
            "Iteration 88, loss = 1.29812535\n",
            "Iteration 89, loss = 1.29489775\n",
            "Iteration 90, loss = 1.29151277\n",
            "Iteration 91, loss = 1.28835276\n",
            "Iteration 92, loss = 1.28538511\n",
            "Iteration 93, loss = 1.28219839\n",
            "Iteration 94, loss = 1.27924025\n",
            "Iteration 95, loss = 1.27634299\n",
            "Iteration 96, loss = 1.27345932\n",
            "Iteration 97, loss = 1.27068832\n",
            "Iteration 98, loss = 1.26798170\n",
            "Iteration 99, loss = 1.26525017\n",
            "Iteration 100, loss = 1.26271303\n",
            "Iteration 1, loss = 1.63358091\n",
            "Iteration 2, loss = 1.59717937\n",
            "Iteration 3, loss = 1.59327327\n",
            "Iteration 4, loss = 1.58366185\n",
            "Iteration 5, loss = 1.56443768\n",
            "Iteration 6, loss = 1.54139275\n",
            "Iteration 7, loss = 1.51958635\n",
            "Iteration 8, loss = 1.48931408\n",
            "Iteration 9, loss = 1.45771260\n",
            "Iteration 10, loss = 1.42279497\n",
            "Iteration 11, loss = 1.39030992\n",
            "Iteration 12, loss = 1.35696111\n",
            "Iteration 13, loss = 1.32794378\n",
            "Iteration 14, loss = 1.29806961\n",
            "Iteration 15, loss = 1.27173848\n",
            "Iteration 16, loss = 1.24796930\n",
            "Iteration 17, loss = 1.22910395\n",
            "Iteration 18, loss = 1.21147696\n",
            "Iteration 19, loss = 1.19891014\n",
            "Iteration 20, loss = 1.18506638\n",
            "Iteration 21, loss = 1.17498578\n",
            "Iteration 22, loss = 1.16644949\n",
            "Iteration 23, loss = 1.15872886\n",
            "Iteration 24, loss = 1.15221414\n",
            "Iteration 25, loss = 1.14690291\n",
            "Iteration 26, loss = 1.14490377\n",
            "Iteration 27, loss = 1.13701613\n",
            "Iteration 28, loss = 1.13436750\n",
            "Iteration 29, loss = 1.13037027\n",
            "Iteration 30, loss = 1.12737000\n",
            "Iteration 31, loss = 1.12592163\n",
            "Iteration 32, loss = 1.12293708\n",
            "Iteration 33, loss = 1.12092766\n",
            "Iteration 34, loss = 1.11873590\n",
            "Iteration 35, loss = 1.11902553\n",
            "Iteration 36, loss = 1.12069043\n",
            "Iteration 37, loss = 1.11393213\n",
            "Iteration 38, loss = 1.11540196\n",
            "Iteration 39, loss = 1.11095724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 40, loss = 1.11250035\n",
            "Iteration 41, loss = 1.10845942\n",
            "Iteration 42, loss = 1.10951382\n",
            "Iteration 43, loss = 1.10994699\n",
            "Iteration 44, loss = 1.10800390\n",
            "Iteration 45, loss = 1.10873580\n",
            "Iteration 46, loss = 1.10597612\n",
            "Iteration 47, loss = 1.10708796\n",
            "Iteration 48, loss = 1.10846655\n",
            "Iteration 49, loss = 1.10438616\n",
            "Iteration 50, loss = 1.10984279\n",
            "Iteration 51, loss = 1.10586272\n",
            "Iteration 52, loss = 1.10607495\n",
            "Iteration 53, loss = 1.10234116\n",
            "Iteration 54, loss = 1.10294959\n",
            "Iteration 55, loss = 1.10149074\n",
            "Iteration 56, loss = 1.10216669\n",
            "Iteration 57, loss = 1.10225815\n",
            "Iteration 58, loss = 1.10178571\n",
            "Iteration 59, loss = 1.10076526\n",
            "Iteration 60, loss = 1.10055641\n",
            "Iteration 61, loss = 1.10013870\n",
            "Iteration 62, loss = 1.09946314\n",
            "Iteration 63, loss = 1.09917879\n",
            "Iteration 64, loss = 1.10142316\n",
            "Iteration 65, loss = 1.09986442\n",
            "Iteration 66, loss = 1.09881913\n",
            "Iteration 67, loss = 1.09846305\n",
            "Iteration 68, loss = 1.09856631\n",
            "Iteration 69, loss = 1.10104576\n",
            "Iteration 70, loss = 1.09915568\n",
            "Iteration 71, loss = 1.10500984\n",
            "Iteration 72, loss = 1.09672291\n",
            "Iteration 73, loss = 1.09786714\n",
            "Iteration 74, loss = 1.09736794\n",
            "Iteration 75, loss = 1.10128020\n",
            "Iteration 76, loss = 1.09960362\n",
            "Iteration 77, loss = 1.09968211\n",
            "Iteration 78, loss = 1.09793331\n",
            "Iteration 79, loss = 1.09830821\n",
            "Iteration 80, loss = 1.10347137\n",
            "Iteration 81, loss = 1.09799313\n",
            "Iteration 82, loss = 1.09850872\n",
            "Iteration 83, loss = 1.09979825\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.75059136\n",
            "Iteration 2, loss = 1.74428748\n",
            "Iteration 3, loss = 1.73777372\n",
            "Iteration 4, loss = 1.73172512\n",
            "Iteration 5, loss = 1.72561446\n",
            "Iteration 6, loss = 1.71981210\n",
            "Iteration 7, loss = 1.71434836\n",
            "Iteration 8, loss = 1.70892985\n",
            "Iteration 9, loss = 1.70380451\n",
            "Iteration 10, loss = 1.69860476\n",
            "Iteration 11, loss = 1.69375547\n",
            "Iteration 12, loss = 1.68907375\n",
            "Iteration 13, loss = 1.68455955\n",
            "Iteration 14, loss = 1.68005153\n",
            "Iteration 15, loss = 1.67585488\n",
            "Iteration 16, loss = 1.67205640\n",
            "Iteration 17, loss = 1.66818983\n",
            "Iteration 18, loss = 1.66445831\n",
            "Iteration 19, loss = 1.66082895\n",
            "Iteration 20, loss = 1.65739436\n",
            "Iteration 21, loss = 1.65422331\n",
            "Iteration 22, loss = 1.65089346\n",
            "Iteration 23, loss = 1.64808646\n",
            "Iteration 24, loss = 1.64501670\n",
            "Iteration 25, loss = 1.64207977\n",
            "Iteration 26, loss = 1.63948888\n",
            "Iteration 27, loss = 1.63680983\n",
            "Iteration 28, loss = 1.63417055\n",
            "Iteration 29, loss = 1.63180164\n",
            "Iteration 30, loss = 1.62968958\n",
            "Iteration 31, loss = 1.62732305\n",
            "Iteration 32, loss = 1.62511553\n",
            "Iteration 33, loss = 1.62317257\n",
            "Iteration 34, loss = 1.62120559\n",
            "Iteration 35, loss = 1.61936677\n",
            "Iteration 36, loss = 1.61753799\n",
            "Iteration 37, loss = 1.61557567\n",
            "Iteration 38, loss = 1.61393427\n",
            "Iteration 39, loss = 1.61215689\n",
            "Iteration 40, loss = 1.61060387\n",
            "Iteration 41, loss = 1.60912051\n",
            "Iteration 42, loss = 1.60752777\n",
            "Iteration 43, loss = 1.60613795\n",
            "Iteration 44, loss = 1.60479592\n",
            "Iteration 45, loss = 1.60327944\n",
            "Iteration 46, loss = 1.60219557\n",
            "Iteration 47, loss = 1.60071288\n",
            "Iteration 48, loss = 1.59966314\n",
            "Iteration 49, loss = 1.59855037\n",
            "Iteration 50, loss = 1.59727804\n",
            "Iteration 51, loss = 1.59610719\n",
            "Iteration 52, loss = 1.59512383\n",
            "Iteration 53, loss = 1.59409684\n",
            "Iteration 54, loss = 1.59309419\n",
            "Iteration 55, loss = 1.59215704\n",
            "Iteration 56, loss = 1.59122177\n",
            "Iteration 57, loss = 1.59021834\n",
            "Iteration 58, loss = 1.58929395\n",
            "Iteration 59, loss = 1.58844025\n",
            "Iteration 60, loss = 1.58751286\n",
            "Iteration 61, loss = 1.58681546\n",
            "Iteration 62, loss = 1.58591396\n",
            "Iteration 63, loss = 1.58522502\n",
            "Iteration 64, loss = 1.58440061\n",
            "Iteration 65, loss = 1.58358337\n",
            "Iteration 66, loss = 1.58280985\n",
            "Iteration 67, loss = 1.58214146\n",
            "Iteration 68, loss = 1.58142641\n",
            "Iteration 69, loss = 1.58067249\n",
            "Iteration 70, loss = 1.58001857\n",
            "Iteration 71, loss = 1.57934780\n",
            "Iteration 72, loss = 1.57860795\n",
            "Iteration 73, loss = 1.57795500\n",
            "Iteration 74, loss = 1.57728214\n",
            "Iteration 75, loss = 1.57667122\n",
            "Iteration 76, loss = 1.57601425\n",
            "Iteration 77, loss = 1.57532666\n",
            "Iteration 78, loss = 1.57470744\n",
            "Iteration 79, loss = 1.57412205\n",
            "Iteration 80, loss = 1.57341775\n",
            "Iteration 81, loss = 1.57281192\n",
            "Iteration 82, loss = 1.57215800\n",
            "Iteration 83, loss = 1.57155813\n",
            "Iteration 84, loss = 1.57092729\n",
            "Iteration 85, loss = 1.57030594\n",
            "Iteration 86, loss = 1.56969235\n",
            "Iteration 87, loss = 1.56911185\n",
            "Iteration 88, loss = 1.56844703\n",
            "Iteration 89, loss = 1.56784922\n",
            "Iteration 90, loss = 1.56722604\n",
            "Iteration 91, loss = 1.56658550\n",
            "Iteration 92, loss = 1.56601920\n",
            "Iteration 93, loss = 1.56535235\n",
            "Iteration 94, loss = 1.56471320\n",
            "Iteration 95, loss = 1.56412578\n",
            "Iteration 96, loss = 1.56348974\n",
            "Iteration 97, loss = 1.56286224\n",
            "Iteration 98, loss = 1.56223946\n",
            "Iteration 99, loss = 1.56162537\n",
            "Iteration 100, loss = 1.56096731\n",
            "Iteration 1, loss = 1.73771854\n",
            "Iteration 2, loss = 1.68605476\n",
            "Iteration 3, loss = 1.64583841\n",
            "Iteration 4, loss = 1.62085299\n",
            "Iteration 5, loss = 1.60195348\n",
            "Iteration 6, loss = 1.59174718\n",
            "Iteration 7, loss = 1.58569462\n",
            "Iteration 8, loss = 1.58129075\n",
            "Iteration 9, loss = 1.57772703\n",
            "Iteration 10, loss = 1.57478991\n",
            "Iteration 11, loss = 1.56991164\n",
            "Iteration 12, loss = 1.56459555\n",
            "Iteration 13, loss = 1.55782145\n",
            "Iteration 14, loss = 1.55116529\n",
            "Iteration 15, loss = 1.54451424\n",
            "Iteration 16, loss = 1.53750320\n",
            "Iteration 17, loss = 1.53050671\n",
            "Iteration 18, loss = 1.52421404\n",
            "Iteration 19, loss = 1.51709905\n",
            "Iteration 20, loss = 1.51008539\n",
            "Iteration 21, loss = 1.50259663\n",
            "Iteration 22, loss = 1.49481932\n",
            "Iteration 23, loss = 1.48662342\n",
            "Iteration 24, loss = 1.47798343\n",
            "Iteration 25, loss = 1.46892643\n",
            "Iteration 26, loss = 1.46009230\n",
            "Iteration 27, loss = 1.45017295\n",
            "Iteration 28, loss = 1.44040919\n",
            "Iteration 29, loss = 1.43077844\n",
            "Iteration 30, loss = 1.42111649\n",
            "Iteration 31, loss = 1.41029954\n",
            "Iteration 32, loss = 1.40010508\n",
            "Iteration 33, loss = 1.39004062\n",
            "Iteration 34, loss = 1.37976699\n",
            "Iteration 35, loss = 1.36953758\n",
            "Iteration 36, loss = 1.35908621\n",
            "Iteration 37, loss = 1.34922131\n",
            "Iteration 38, loss = 1.33905905\n",
            "Iteration 39, loss = 1.32992644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 40, loss = 1.32040694\n",
            "Iteration 41, loss = 1.31055804\n",
            "Iteration 42, loss = 1.30212568\n",
            "Iteration 43, loss = 1.29351259\n",
            "Iteration 44, loss = 1.28551260\n",
            "Iteration 45, loss = 1.27744360\n",
            "Iteration 46, loss = 1.26999738\n",
            "Iteration 47, loss = 1.26311817\n",
            "Iteration 48, loss = 1.25581643\n",
            "Iteration 49, loss = 1.24946063\n",
            "Iteration 50, loss = 1.24316122\n",
            "Iteration 51, loss = 1.23747970\n",
            "Iteration 52, loss = 1.23176031\n",
            "Iteration 53, loss = 1.22634552\n",
            "Iteration 54, loss = 1.22158682\n",
            "Iteration 55, loss = 1.21665016\n",
            "Iteration 56, loss = 1.21211781\n",
            "Iteration 57, loss = 1.20804349\n",
            "Iteration 58, loss = 1.20381062\n",
            "Iteration 59, loss = 1.20004412\n",
            "Iteration 60, loss = 1.19651441\n",
            "Iteration 61, loss = 1.19288221\n",
            "Iteration 62, loss = 1.18976810\n",
            "Iteration 63, loss = 1.18646294\n",
            "Iteration 64, loss = 1.18373796\n",
            "Iteration 65, loss = 1.18039517\n",
            "Iteration 66, loss = 1.17757278\n",
            "Iteration 67, loss = 1.17500271\n",
            "Iteration 68, loss = 1.17288391\n",
            "Iteration 69, loss = 1.17004706\n",
            "Iteration 70, loss = 1.16769219\n",
            "Iteration 71, loss = 1.16550266\n",
            "Iteration 72, loss = 1.16337775\n",
            "Iteration 73, loss = 1.16184428\n",
            "Iteration 74, loss = 1.15934770\n",
            "Iteration 75, loss = 1.15747006\n",
            "Iteration 76, loss = 1.15560882\n",
            "Iteration 77, loss = 1.15408890\n",
            "Iteration 78, loss = 1.15224572\n",
            "Iteration 79, loss = 1.15096455\n",
            "Iteration 80, loss = 1.14903996\n",
            "Iteration 81, loss = 1.14745356\n",
            "Iteration 82, loss = 1.14586357\n",
            "Iteration 83, loss = 1.14463314\n",
            "Iteration 84, loss = 1.14320595\n",
            "Iteration 85, loss = 1.14189613\n",
            "Iteration 86, loss = 1.14053694\n",
            "Iteration 87, loss = 1.13945515\n",
            "Iteration 88, loss = 1.13804854\n",
            "Iteration 89, loss = 1.13668873\n",
            "Iteration 90, loss = 1.13588160\n",
            "Iteration 91, loss = 1.13440475\n",
            "Iteration 92, loss = 1.13364954\n",
            "Iteration 93, loss = 1.13223949\n",
            "Iteration 94, loss = 1.13154270\n",
            "Iteration 95, loss = 1.13105633\n",
            "Iteration 96, loss = 1.12925015\n",
            "Iteration 97, loss = 1.12855219\n",
            "Iteration 98, loss = 1.12738018\n",
            "Iteration 99, loss = 1.12679971\n",
            "Iteration 100, loss = 1.12550415\n",
            "Iteration 1, loss = 1.69862303\n",
            "Iteration 2, loss = 1.60560774\n",
            "Iteration 3, loss = 1.53719988\n",
            "Iteration 4, loss = 1.49173028\n",
            "Iteration 5, loss = 1.44684865\n",
            "Iteration 6, loss = 1.36477738\n",
            "Iteration 7, loss = 1.29787013\n",
            "Iteration 8, loss = 1.25772219\n",
            "Iteration 9, loss = 1.22161178\n",
            "Iteration 10, loss = 1.19380786\n",
            "Iteration 11, loss = 1.16706228\n",
            "Iteration 12, loss = 1.15585651\n",
            "Iteration 13, loss = 1.15187597\n",
            "Iteration 14, loss = 1.13760426\n",
            "Iteration 15, loss = 1.13431488\n",
            "Iteration 16, loss = 1.12858715\n",
            "Iteration 17, loss = 1.12447733\n",
            "Iteration 18, loss = 1.12038911\n",
            "Iteration 19, loss = 1.12318085"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 20, loss = 1.12237879\n",
            "Iteration 21, loss = 1.11215796\n",
            "Iteration 22, loss = 1.12095268\n",
            "Iteration 23, loss = 1.11668702\n",
            "Iteration 24, loss = 1.11121491\n",
            "Iteration 25, loss = 1.11308564\n",
            "Iteration 26, loss = 1.11029365\n",
            "Iteration 27, loss = 1.10492551\n",
            "Iteration 28, loss = 1.10658031\n",
            "Iteration 29, loss = 1.10361372\n",
            "Iteration 30, loss = 1.10475517\n",
            "Iteration 31, loss = 1.10223109\n",
            "Iteration 32, loss = 1.10042598\n",
            "Iteration 33, loss = 1.09775917\n",
            "Iteration 34, loss = 1.09661637\n",
            "Iteration 35, loss = 1.09637817\n",
            "Iteration 36, loss = 1.09176001\n",
            "Iteration 37, loss = 1.09044099\n",
            "Iteration 38, loss = 1.09172073\n",
            "Iteration 39, loss = 1.08762916\n",
            "Iteration 40, loss = 1.09290582\n",
            "Iteration 41, loss = 1.07650792\n",
            "Iteration 42, loss = 1.07883855\n",
            "Iteration 43, loss = 1.07050817\n",
            "Iteration 44, loss = 1.06795847\n",
            "Iteration 45, loss = 1.07982307\n",
            "Iteration 46, loss = 1.09313873\n",
            "Iteration 47, loss = 1.07242804\n",
            "Iteration 48, loss = 1.07501178\n",
            "Iteration 49, loss = 1.06556176\n",
            "Iteration 50, loss = 1.06149918\n",
            "Iteration 51, loss = 1.03989180\n",
            "Iteration 52, loss = 1.04364170\n",
            "Iteration 53, loss = 1.03127969\n",
            "Iteration 54, loss = 1.02219200\n",
            "Iteration 55, loss = 1.01088966\n",
            "Iteration 56, loss = 1.01372525\n",
            "Iteration 57, loss = 1.00295534\n",
            "Iteration 58, loss = 0.99477940\n",
            "Iteration 59, loss = 0.98501750\n",
            "Iteration 60, loss = 1.00294631\n",
            "Iteration 61, loss = 1.01274097\n",
            "Iteration 62, loss = 1.00716575\n",
            "Iteration 63, loss = 0.99209784\n",
            "Iteration 64, loss = 0.99820452\n",
            "Iteration 65, loss = 0.96685029\n",
            "Iteration 66, loss = 0.95671281\n",
            "Iteration 67, loss = 0.95026674\n",
            "Iteration 68, loss = 0.93932906\n",
            "Iteration 69, loss = 0.93829899\n",
            "Iteration 70, loss = 0.93456016\n",
            "Iteration 71, loss = 0.92367994\n",
            "Iteration 72, loss = 0.90754672\n",
            "Iteration 73, loss = 0.90026495\n",
            "Iteration 74, loss = 0.89407917\n",
            "Iteration 75, loss = 0.90361467\n",
            "Iteration 76, loss = 0.87606801\n",
            "Iteration 77, loss = 0.89564511\n",
            "Iteration 78, loss = 0.88185420\n",
            "Iteration 79, loss = 0.89558225\n",
            "Iteration 80, loss = 0.90648623\n",
            "Iteration 81, loss = 0.93170075\n",
            "Iteration 82, loss = 0.91962709\n",
            "Iteration 83, loss = 0.95000941\n",
            "Iteration 84, loss = 0.89980951\n",
            "Iteration 85, loss = 0.89756746\n",
            "Iteration 86, loss = 0.86165052\n",
            "Iteration 87, loss = 0.85252587\n",
            "Iteration 88, loss = 0.85910457\n",
            "Iteration 89, loss = 0.88070401\n",
            "Iteration 90, loss = 0.90689996\n",
            "Iteration 91, loss = 0.88086658\n",
            "Iteration 92, loss = 0.88425197\n",
            "Iteration 93, loss = 0.84574579\n",
            "Iteration 94, loss = 0.83467948\n",
            "Iteration 95, loss = 0.83461844\n",
            "Iteration 96, loss = 0.82999165\n",
            "Iteration 97, loss = 0.83033088\n",
            "Iteration 98, loss = 0.80973810\n",
            "Iteration 99, loss = 0.81559451\n",
            "Iteration 100, loss = 0.81304380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.65564049\n",
            "Iteration 2, loss = 1.65240132\n",
            "Iteration 3, loss = 1.64934198\n",
            "Iteration 4, loss = 1.64642908\n",
            "Iteration 5, loss = 1.64317646\n",
            "Iteration 6, loss = 1.64082504\n",
            "Iteration 7, loss = 1.63836759\n",
            "Iteration 8, loss = 1.63583640\n",
            "Iteration 9, loss = 1.63332937\n",
            "Iteration 10, loss = 1.63124250\n",
            "Iteration 11, loss = 1.62895076\n",
            "Iteration 12, loss = 1.62657084\n",
            "Iteration 13, loss = 1.62477339\n",
            "Iteration 14, loss = 1.62286500\n",
            "Iteration 15, loss = 1.62121047\n",
            "Iteration 16, loss = 1.61951292\n",
            "Iteration 17, loss = 1.61796236\n",
            "Iteration 18, loss = 1.61629486\n",
            "Iteration 19, loss = 1.61501581\n",
            "Iteration 20, loss = 1.61352386\n",
            "Iteration 21, loss = 1.61236738\n",
            "Iteration 22, loss = 1.61125165\n",
            "Iteration 23, loss = 1.61030188\n",
            "Iteration 24, loss = 1.60914881\n",
            "Iteration 25, loss = 1.60807030\n",
            "Iteration 26, loss = 1.60723660\n",
            "Iteration 27, loss = 1.60637044\n",
            "Iteration 28, loss = 1.60544969\n",
            "Iteration 29, loss = 1.60476435\n",
            "Iteration 30, loss = 1.60403838\n",
            "Iteration 31, loss = 1.60337325\n",
            "Iteration 32, loss = 1.60277128\n",
            "Iteration 33, loss = 1.60218887\n",
            "Iteration 34, loss = 1.60162030\n",
            "Iteration 35, loss = 1.60118299\n",
            "Iteration 36, loss = 1.60084624\n",
            "Iteration 37, loss = 1.60025048\n",
            "Iteration 38, loss = 1.59987992\n",
            "Iteration 39, loss = 1.59947322\n",
            "Iteration 40, loss = 1.59909469\n",
            "Iteration 41, loss = 1.59884020\n",
            "Iteration 42, loss = 1.59847871\n",
            "Iteration 43, loss = 1.59823457\n",
            "Iteration 44, loss = 1.59802189\n",
            "Iteration 45, loss = 1.59769112\n",
            "Iteration 46, loss = 1.59759293\n",
            "Iteration 47, loss = 1.59734084\n",
            "Iteration 48, loss = 1.59712041\n",
            "Iteration 49, loss = 1.59699994\n",
            "Iteration 50, loss = 1.59679229\n",
            "Iteration 51, loss = 1.59661225\n",
            "Iteration 52, loss = 1.59646325\n",
            "Iteration 53, loss = 1.59633843\n",
            "Iteration 54, loss = 1.59624205\n",
            "Iteration 55, loss = 1.59611955\n",
            "Iteration 56, loss = 1.59602907\n",
            "Iteration 57, loss = 1.59591741\n",
            "Iteration 58, loss = 1.59574153\n",
            "Iteration 59, loss = 1.59565554\n",
            "Iteration 60, loss = 1.59555277\n",
            "Iteration 61, loss = 1.59548751\n",
            "Iteration 62, loss = 1.59541133\n",
            "Iteration 63, loss = 1.59530843\n",
            "Iteration 64, loss = 1.59520793\n",
            "Iteration 65, loss = 1.59508937\n",
            "Iteration 66, loss = 1.59502951\n",
            "Iteration 67, loss = 1.59494914\n",
            "Iteration 68, loss = 1.59486384\n",
            "Iteration 69, loss = 1.59478479\n",
            "Iteration 70, loss = 1.59469204\n",
            "Iteration 71, loss = 1.59460755\n",
            "Iteration 72, loss = 1.59451241\n",
            "Iteration 73, loss = 1.59444066\n",
            "Iteration 74, loss = 1.59435129\n",
            "Iteration 75, loss = 1.59428473\n",
            "Iteration 76, loss = 1.59420173\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.64964922\n",
            "Iteration 2, loss = 1.62358926\n",
            "Iteration 3, loss = 1.60765799\n",
            "Iteration 4, loss = 1.60161504\n",
            "Iteration 5, loss = 1.59673246\n",
            "Iteration 6, loss = 1.59789798\n",
            "Iteration 7, loss = 1.59984963\n",
            "Iteration 8, loss = 1.60022721\n",
            "Iteration 9, loss = 1.59933092\n",
            "Iteration 10, loss = 1.59667706\n",
            "Iteration 11, loss = 1.59458830\n",
            "Iteration 12, loss = 1.59363912\n",
            "Iteration 13, loss = 1.59193079\n",
            "Iteration 14, loss = 1.59094476\n",
            "Iteration 15, loss = 1.59037203\n",
            "Iteration 16, loss = 1.58953748\n",
            "Iteration 17, loss = 1.58850675\n",
            "Iteration 18, loss = 1.58759622\n",
            "Iteration 19, loss = 1.58650831\n",
            "Iteration 20, loss = 1.58531827\n",
            "Iteration 21, loss = 1.58416612\n",
            "Iteration 22, loss = 1.58283253\n",
            "Iteration 23, loss = 1.58194170\n",
            "Iteration 24, loss = 1.58027235\n",
            "Iteration 25, loss = 1.57889930\n",
            "Iteration 26, loss = 1.57757672\n",
            "Iteration 27, loss = 1.57594347\n",
            "Iteration 28, loss = 1.57448493\n",
            "Iteration 29, loss = 1.57331091\n",
            "Iteration 30, loss = 1.57143856\n",
            "Iteration 31, loss = 1.56904511\n",
            "Iteration 32, loss = 1.56720180\n",
            "Iteration 33, loss = 1.56521638\n",
            "Iteration 34, loss = 1.56323366\n",
            "Iteration 35, loss = 1.56115328\n",
            "Iteration 36, loss = 1.55880205\n",
            "Iteration 37, loss = 1.55614324\n",
            "Iteration 38, loss = 1.55339383\n",
            "Iteration 39, loss = 1.55108772\n",
            "Iteration 40, loss = 1.54842030\n",
            "Iteration 41, loss = 1.54504169\n",
            "Iteration 42, loss = 1.54219287\n",
            "Iteration 43, loss = 1.53905068\n",
            "Iteration 44, loss = 1.53597233\n",
            "Iteration 45, loss = 1.53228625\n",
            "Iteration 46, loss = 1.52868266\n",
            "Iteration 47, loss = 1.52544755\n",
            "Iteration 48, loss = 1.52086023\n",
            "Iteration 49, loss = 1.51710432\n",
            "Iteration 50, loss = 1.51282607\n",
            "Iteration 51, loss = 1.50877582\n",
            "Iteration 52, loss = 1.50392356\n",
            "Iteration 53, loss = 1.49929486\n",
            "Iteration 54, loss = 1.49493616\n",
            "Iteration 55, loss = 1.48980819\n",
            "Iteration 56, loss = 1.48500858\n",
            "Iteration 57, loss = 1.48019160\n",
            "Iteration 58, loss = 1.47445850\n",
            "Iteration 59, loss = 1.46901732\n",
            "Iteration 60, loss = 1.46380103\n",
            "Iteration 61, loss = 1.45854232\n",
            "Iteration 62, loss = 1.45280262\n",
            "Iteration 63, loss = 1.44706269\n",
            "Iteration 64, loss = 1.44135765\n",
            "Iteration 65, loss = 1.43535788\n",
            "Iteration 66, loss = 1.42945957\n",
            "Iteration 67, loss = 1.42354596\n",
            "Iteration 68, loss = 1.41774502\n",
            "Iteration 69, loss = 1.41162394\n",
            "Iteration 70, loss = 1.40589420\n",
            "Iteration 71, loss = 1.39997835\n",
            "Iteration 72, loss = 1.39394645\n",
            "Iteration 73, loss = 1.38800741\n",
            "Iteration 74, loss = 1.38209117\n",
            "Iteration 75, loss = 1.37634331\n",
            "Iteration 76, loss = 1.37058830\n",
            "Iteration 77, loss = 1.36471909\n",
            "Iteration 78, loss = 1.35914796\n",
            "Iteration 79, loss = 1.35377245\n",
            "Iteration 80, loss = 1.34785410\n",
            "Iteration 81, loss = 1.34241917\n",
            "Iteration 82, loss = 1.33699824\n",
            "Iteration 83, loss = 1.33188265\n",
            "Iteration 84, loss = 1.32661815\n",
            "Iteration 85, loss = 1.32157607\n",
            "Iteration 86, loss = 1.31656641\n",
            "Iteration 87, loss = 1.31171476\n",
            "Iteration 88, loss = 1.30674703\n",
            "Iteration 89, loss = 1.30190888\n",
            "Iteration 90, loss = 1.29767763\n",
            "Iteration 91, loss = 1.29279652\n",
            "Iteration 92, loss = 1.28896288\n",
            "Iteration 93, loss = 1.28427531\n",
            "Iteration 94, loss = 1.28009061\n",
            "Iteration 95, loss = 1.27646257\n",
            "Iteration 96, loss = 1.27235514\n",
            "Iteration 97, loss = 1.26870578\n",
            "Iteration 98, loss = 1.26495832\n",
            "Iteration 99, loss = 1.26124985\n",
            "Iteration 100, loss = 1.25775745\n",
            "Iteration 1, loss = 1.63882444\n",
            "Iteration 2, loss = 1.64838555\n",
            "Iteration 3, loss = 1.59629125\n",
            "Iteration 4, loss = 1.60870637\n",
            "Iteration 5, loss = 1.58810196\n",
            "Iteration 6, loss = 1.56635804\n",
            "Iteration 7, loss = 1.56391181\n",
            "Iteration 8, loss = 1.55099334\n",
            "Iteration 9, loss = 1.52140694\n",
            "Iteration 10, loss = 1.49141338\n",
            "Iteration 11, loss = 1.46352688\n",
            "Iteration 12, loss = 1.42717491\n",
            "Iteration 13, loss = 1.38259722\n",
            "Iteration 14, loss = 1.34024611\n",
            "Iteration 15, loss = 1.30458642\n",
            "Iteration 16, loss = 1.26859209\n",
            "Iteration 17, loss = 1.23395637\n",
            "Iteration 18, loss = 1.21246295\n",
            "Iteration 19, loss = 1.19368263\n",
            "Iteration 20, loss = 1.17620104\n",
            "Iteration 21, loss = 1.16523996\n",
            "Iteration 22, loss = 1.16035262\n",
            "Iteration 23, loss = 1.15245403\n",
            "Iteration 24, loss = 1.14350195\n",
            "Iteration 25, loss = 1.14172732\n",
            "Iteration 26, loss = 1.13224255\n",
            "Iteration 27, loss = 1.13349701\n",
            "Iteration 28, loss = 1.12638970\n",
            "Iteration 29, loss = 1.13174976\n",
            "Iteration 30, loss = 1.12512366\n",
            "Iteration 31, loss = 1.12343334\n",
            "Iteration 32, loss = 1.12330380\n",
            "Iteration 33, loss = 1.11985132\n",
            "Iteration 34, loss = 1.12220390\n",
            "Iteration 35, loss = 1.11471808\n",
            "Iteration 36, loss = 1.11781028\n",
            "Iteration 37, loss = 1.11693002\n",
            "Iteration 38, loss = 1.11616433\n",
            "Iteration 39, loss = 1.11071436\n",
            "Iteration 40, loss = 1.11373629\n",
            "Iteration 41, loss = 1.11061574\n",
            "Iteration 42, loss = 1.11250146\n",
            "Iteration 43, loss = 1.11101215\n",
            "Iteration 44, loss = 1.11200707\n",
            "Iteration 45, loss = 1.11404240\n",
            "Iteration 46, loss = 1.11804975\n",
            "Iteration 47, loss = 1.12378515\n",
            "Iteration 48, loss = 1.11563898\n",
            "Iteration 49, loss = 1.11983163\n",
            "Iteration 50, loss = 1.11087111\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.74864823\n",
            "Iteration 2, loss = 1.74271306\n",
            "Iteration 3, loss = 1.73656514\n",
            "Iteration 4, loss = 1.73084245\n",
            "Iteration 5, loss = 1.72505484\n",
            "Iteration 6, loss = 1.71956158\n",
            "Iteration 7, loss = 1.71437417\n",
            "Iteration 8, loss = 1.70922032\n",
            "Iteration 9, loss = 1.70433772\n",
            "Iteration 10, loss = 1.69938488\n",
            "Iteration 11, loss = 1.69475061\n",
            "Iteration 12, loss = 1.69026810\n",
            "Iteration 13, loss = 1.68594954\n",
            "Iteration 14, loss = 1.68162182\n",
            "Iteration 15, loss = 1.67759041\n",
            "Iteration 16, loss = 1.67393729"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 17, loss = 1.67021413\n",
            "Iteration 18, loss = 1.66661367\n",
            "Iteration 19, loss = 1.66310915\n",
            "Iteration 20, loss = 1.65978727\n",
            "Iteration 21, loss = 1.65671597\n",
            "Iteration 22, loss = 1.65349381\n",
            "Iteration 23, loss = 1.65076591\n",
            "Iteration 24, loss = 1.64778594\n",
            "Iteration 25, loss = 1.64493427\n",
            "Iteration 26, loss = 1.64240530\n",
            "Iteration 27, loss = 1.63979803\n",
            "Iteration 28, loss = 1.63722652\n",
            "Iteration 29, loss = 1.63491822\n",
            "Iteration 30, loss = 1.63284462\n",
            "Iteration 31, loss = 1.63053518\n",
            "Iteration 32, loss = 1.62837588\n",
            "Iteration 33, loss = 1.62646622\n",
            "Iteration 34, loss = 1.62453867\n",
            "Iteration 35, loss = 1.62273139\n",
            "Iteration 36, loss = 1.62093779\n",
            "Iteration 37, loss = 1.61900649\n",
            "Iteration 38, loss = 1.61739324\n",
            "Iteration 39, loss = 1.61564284\n",
            "Iteration 40, loss = 1.61410733\n",
            "Iteration 41, loss = 1.61264753\n",
            "Iteration 42, loss = 1.61107561\n",
            "Iteration 43, loss = 1.60970285\n",
            "Iteration 44, loss = 1.60836835\n",
            "Iteration 45, loss = 1.60687333\n",
            "Iteration 46, loss = 1.60579414\n",
            "Iteration 47, loss = 1.60432674\n",
            "Iteration 48, loss = 1.60328366\n",
            "Iteration 49, loss = 1.60217643\n",
            "Iteration 50, loss = 1.60091751\n",
            "Iteration 51, loss = 1.59975539\n",
            "Iteration 52, loss = 1.59877501\n",
            "Iteration 53, loss = 1.59775458\n",
            "Iteration 54, loss = 1.59675574\n",
            "Iteration 55, loss = 1.59582510\n",
            "Iteration 56, loss = 1.59489474\n",
            "Iteration 57, loss = 1.59389795\n",
            "Iteration 58, loss = 1.59297783\n",
            "Iteration 59, loss = 1.59213083\n",
            "Iteration 60, loss = 1.59121227\n",
            "Iteration 61, loss = 1.59051749\n",
            "Iteration 62, loss = 1.58962603\n",
            "Iteration 63, loss = 1.58894163\n",
            "Iteration 64, loss = 1.58812420\n",
            "Iteration 65, loss = 1.58731315\n",
            "Iteration 66, loss = 1.58655751\n",
            "Iteration 67, loss = 1.58589508\n",
            "Iteration 68, loss = 1.58519385\n",
            "Iteration 69, loss = 1.58444964\n",
            "Iteration 70, loss = 1.58380860\n",
            "Iteration 71, loss = 1.58314755\n",
            "Iteration 72, loss = 1.58242195\n",
            "Iteration 73, loss = 1.58178425\n",
            "Iteration 74, loss = 1.58112769\n",
            "Iteration 75, loss = 1.58053290\n",
            "Iteration 76, loss = 1.57989403\n",
            "Iteration 77, loss = 1.57922684\n",
            "Iteration 78, loss = 1.57862038\n",
            "Iteration 79, loss = 1.57805853\n",
            "Iteration 80, loss = 1.57737203\n",
            "Iteration 81, loss = 1.57678899\n",
            "Iteration 82, loss = 1.57615645\n",
            "Iteration 83, loss = 1.57558459\n",
            "Iteration 84, loss = 1.57497032\n",
            "Iteration 85, loss = 1.57437036\n",
            "Iteration 86, loss = 1.57378889\n",
            "Iteration 87, loss = 1.57323485\n",
            "Iteration 88, loss = 1.57259382\n",
            "Iteration 89, loss = 1.57202359\n",
            "Iteration 90, loss = 1.57142765\n",
            "Iteration 91, loss = 1.57081409\n",
            "Iteration 92, loss = 1.57027306\n",
            "Iteration 93, loss = 1.56963565\n",
            "Iteration 94, loss = 1.56902361\n",
            "Iteration 95, loss = 1.56846336\n",
            "Iteration 96, loss = 1.56785712\n",
            "Iteration 97, loss = 1.56725879\n",
            "Iteration 98, loss = 1.56666855\n",
            "Iteration 99, loss = 1.56607965\n",
            "Iteration 100, loss = 1.56545593\n",
            "Iteration 1, loss = 1.73642349\n",
            "Iteration 2, loss = 1.68741556\n",
            "Iteration 3, loss = 1.64871153\n",
            "Iteration 4, loss = 1.62415436\n",
            "Iteration 5, loss = 1.60537263\n",
            "Iteration 6, loss = 1.59484336\n",
            "Iteration 7, loss = 1.58838739\n",
            "Iteration 8, loss = 1.58365079\n",
            "Iteration 9, loss = 1.58005576\n",
            "Iteration 10, loss = 1.57741007\n",
            "Iteration 11, loss = 1.57312921\n",
            "Iteration 12, loss = 1.56827101\n",
            "Iteration 13, loss = 1.56208519\n",
            "Iteration 14, loss = 1.55569839\n",
            "Iteration 15, loss = 1.54917544\n",
            "Iteration 16, loss = 1.54210873\n",
            "Iteration 17, loss = 1.53520079\n",
            "Iteration 18, loss = 1.52897023\n",
            "Iteration 19, loss = 1.52192293\n",
            "Iteration 20, loss = 1.51503427\n",
            "Iteration 21, loss = 1.50756182\n",
            "Iteration 22, loss = 1.49989622\n",
            "Iteration 23, loss = 1.49167415\n",
            "Iteration 24, loss = 1.48320680\n",
            "Iteration 25, loss = 1.47425012\n",
            "Iteration 26, loss = 1.46533329\n",
            "Iteration 27, loss = 1.45535496\n",
            "Iteration 28, loss = 1.44546173\n",
            "Iteration 29, loss = 1.43561454\n",
            "Iteration 30, loss = 1.42581995\n",
            "Iteration 31, loss = 1.41483455\n",
            "Iteration 32, loss = 1.40441806\n",
            "Iteration 33, loss = 1.39414318\n",
            "Iteration 34, loss = 1.38359298\n",
            "Iteration 35, loss = 1.37313791\n",
            "Iteration 36, loss = 1.36242491\n",
            "Iteration 37, loss = 1.35231439\n",
            "Iteration 38, loss = 1.34188439\n",
            "Iteration 39, loss = 1.33245773\n",
            "Iteration 40, loss = 1.32262346\n",
            "Iteration 41, loss = 1.31254246\n",
            "Iteration 42, loss = 1.30386619\n",
            "Iteration 43, loss = 1.29498021\n",
            "Iteration 44, loss = 1.28674794\n",
            "Iteration 45, loss = 1.27850109\n",
            "Iteration 46, loss = 1.27086724\n",
            "Iteration 47, loss = 1.26381077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 48, loss = 1.25639279\n",
            "Iteration 49, loss = 1.24988442\n",
            "Iteration 50, loss = 1.24349075\n",
            "Iteration 51, loss = 1.23766083\n",
            "Iteration 52, loss = 1.23191111\n",
            "Iteration 53, loss = 1.22643458\n",
            "Iteration 54, loss = 1.22159786\n",
            "Iteration 55, loss = 1.21664936\n",
            "Iteration 56, loss = 1.21208865\n",
            "Iteration 57, loss = 1.20793956\n",
            "Iteration 58, loss = 1.20375416\n",
            "Iteration 59, loss = 1.19997476\n",
            "Iteration 60, loss = 1.19642869\n",
            "Iteration 61, loss = 1.19282897\n",
            "Iteration 62, loss = 1.18967082\n",
            "Iteration 63, loss = 1.18641248\n",
            "Iteration 64, loss = 1.18372754\n",
            "Iteration 65, loss = 1.18040746\n",
            "Iteration 66, loss = 1.17758222\n",
            "Iteration 67, loss = 1.17504698\n",
            "Iteration 68, loss = 1.17291609\n",
            "Iteration 69, loss = 1.17013827\n",
            "Iteration 70, loss = 1.16780046\n",
            "Iteration 71, loss = 1.16566199\n",
            "Iteration 72, loss = 1.16354431\n",
            "Iteration 73, loss = 1.16200844\n",
            "Iteration 74, loss = 1.15955465\n",
            "Iteration 75, loss = 1.15768551\n",
            "Iteration 76, loss = 1.15582955\n",
            "Iteration 77, loss = 1.15431096\n",
            "Iteration 78, loss = 1.15248026\n",
            "Iteration 79, loss = 1.15120915\n",
            "Iteration 80, loss = 1.14930371\n",
            "Iteration 81, loss = 1.14772745\n",
            "Iteration 82, loss = 1.14612453\n",
            "Iteration 83, loss = 1.14486918\n",
            "Iteration 84, loss = 1.14343908\n",
            "Iteration 85, loss = 1.14215173\n",
            "Iteration 86, loss = 1.14080427\n",
            "Iteration 87, loss = 1.13970841\n",
            "Iteration 88, loss = 1.13830910\n",
            "Iteration 89, loss = 1.13695412\n",
            "Iteration 90, loss = 1.13606910\n",
            "Iteration 91, loss = 1.13465582\n",
            "Iteration 92, loss = 1.13389910\n",
            "Iteration 93, loss = 1.13246849\n",
            "Iteration 94, loss = 1.13175585\n",
            "Iteration 95, loss = 1.13124669\n",
            "Iteration 96, loss = 1.12945050\n",
            "Iteration 97, loss = 1.12871330\n",
            "Iteration 98, loss = 1.12753201\n",
            "Iteration 99, loss = 1.12693948\n",
            "Iteration 100, loss = 1.12562606\n",
            "Iteration 1, loss = 1.69633908\n",
            "Iteration 2, loss = 1.60570056\n",
            "Iteration 3, loss = 1.54125931\n",
            "Iteration 4, loss = 1.49299659\n",
            "Iteration 5, loss = 1.45100445\n",
            "Iteration 6, loss = 1.36838555\n",
            "Iteration 7, loss = 1.30099315\n",
            "Iteration 8, loss = 1.26183870\n",
            "Iteration 9, loss = 1.22378063\n",
            "Iteration 10, loss = 1.19301903\n",
            "Iteration 11, loss = 1.16706882\n",
            "Iteration 12, loss = 1.15810715\n",
            "Iteration 13, loss = 1.15241208\n",
            "Iteration 14, loss = 1.13634026\n",
            "Iteration 15, loss = 1.13344860\n",
            "Iteration 16, loss = 1.12892703\n",
            "Iteration 17, loss = 1.12354447\n",
            "Iteration 18, loss = 1.11820557\n",
            "Iteration 19, loss = 1.12084811\n",
            "Iteration 20, loss = 1.12145479\n",
            "Iteration 21, loss = 1.10914025\n",
            "Iteration 22, loss = 1.11725117\n",
            "Iteration 23, loss = 1.11522399\n",
            "Iteration 24, loss = 1.10818565\n",
            "Iteration 25, loss = 1.10865307\n",
            "Iteration 26, loss = 1.10825530\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 27, loss = 1.10210759\n",
            "Iteration 28, loss = 1.10198966\n",
            "Iteration 29, loss = 1.10085898\n",
            "Iteration 30, loss = 1.10197358\n",
            "Iteration 31, loss = 1.09948942\n",
            "Iteration 32, loss = 1.09918646\n",
            "Iteration 33, loss = 1.09570556\n",
            "Iteration 34, loss = 1.09429445\n",
            "Iteration 35, loss = 1.09485807\n",
            "Iteration 36, loss = 1.08865936\n",
            "Iteration 37, loss = 1.08955312\n",
            "Iteration 38, loss = 1.08836173\n",
            "Iteration 39, loss = 1.08423735\n",
            "Iteration 40, loss = 1.08953775\n",
            "Iteration 41, loss = 1.07493807\n",
            "Iteration 42, loss = 1.07595193\n",
            "Iteration 43, loss = 1.06834443\n",
            "Iteration 44, loss = 1.06633231\n",
            "Iteration 45, loss = 1.07271971\n",
            "Iteration 46, loss = 1.08620639\n",
            "Iteration 47, loss = 1.06462348\n",
            "Iteration 48, loss = 1.06903845\n",
            "Iteration 49, loss = 1.06059008\n",
            "Iteration 50, loss = 1.05324151\n",
            "Iteration 51, loss = 1.03859356\n",
            "Iteration 52, loss = 1.03921206\n",
            "Iteration 53, loss = 1.02573359\n",
            "Iteration 54, loss = 1.01934908\n",
            "Iteration 55, loss = 1.00561649\n",
            "Iteration 56, loss = 1.00920621\n",
            "Iteration 57, loss = 1.00016344\n",
            "Iteration 58, loss = 0.98891084\n",
            "Iteration 59, loss = 0.97543386\n",
            "Iteration 60, loss = 0.98734434\n",
            "Iteration 61, loss = 0.99781184\n",
            "Iteration 62, loss = 0.99265173\n",
            "Iteration 63, loss = 0.97210561\n",
            "Iteration 64, loss = 0.99396967\n",
            "Iteration 65, loss = 0.95790530\n",
            "Iteration 66, loss = 0.94079847\n",
            "Iteration 67, loss = 0.94461900\n",
            "Iteration 68, loss = 0.93870994\n",
            "Iteration 69, loss = 0.94551367\n",
            "Iteration 70, loss = 0.95199055\n",
            "Iteration 71, loss = 0.92418814\n",
            "Iteration 72, loss = 0.90123003\n",
            "Iteration 73, loss = 0.90145309\n",
            "Iteration 74, loss = 0.89906400\n",
            "Iteration 75, loss = 0.90428331\n",
            "Iteration 76, loss = 0.88787706\n",
            "Iteration 77, loss = 0.90993315\n",
            "Iteration 78, loss = 0.89282640\n",
            "Iteration 79, loss = 0.92916066\n",
            "Iteration 80, loss = 0.90010016\n",
            "Iteration 81, loss = 0.88770350\n",
            "Iteration 82, loss = 0.89263403\n",
            "Iteration 83, loss = 0.87309300\n",
            "Iteration 84, loss = 0.86547216\n",
            "Iteration 85, loss = 0.84842086\n",
            "Iteration 86, loss = 0.84274102\n",
            "Iteration 87, loss = 0.85171832\n",
            "Iteration 88, loss = 0.85889299\n",
            "Iteration 89, loss = 0.87142759\n",
            "Iteration 90, loss = 0.89526926\n",
            "Iteration 91, loss = 0.86474884\n",
            "Iteration 92, loss = 0.87096355\n",
            "Iteration 93, loss = 0.83511854\n",
            "Iteration 94, loss = 0.82569369\n",
            "Iteration 95, loss = 0.82129677\n",
            "Iteration 96, loss = 0.82357721\n",
            "Iteration 97, loss = 0.82290455\n",
            "Iteration 98, loss = 0.81178524\n",
            "Iteration 99, loss = 0.80686663\n",
            "Iteration 100, loss = 0.80563052\n",
            "Iteration 1, loss = 1.71715648\n",
            "Iteration 2, loss = 1.71391584\n",
            "Iteration 3, loss = 1.71068051\n",
            "Iteration 4, loss = 1.70757594\n",
            "Iteration 5, loss = 1.70437653\n",
            "Iteration 6, loss = 1.70149738\n",
            "Iteration 7, loss = 1.69865246\n",
            "Iteration 8, loss = 1.69576899\n",
            "Iteration 9, loss = 1.69291668\n",
            "Iteration 10, loss = 1.69017207\n",
            "Iteration 11, loss = 1.68740248\n",
            "Iteration 12, loss = 1.68462515\n",
            "Iteration 13, loss = 1.68208545\n",
            "Iteration 14, loss = 1.67939536\n",
            "Iteration 15, loss = 1.67687139\n",
            "Iteration 16, loss = 1.67449374\n",
            "Iteration 17, loss = 1.67205194\n",
            "Iteration 18, loss = 1.66959084\n",
            "Iteration 19, loss = 1.66724399\n",
            "Iteration 20, loss = 1.66488588\n",
            "Iteration 21, loss = 1.66268845\n",
            "Iteration 22, loss = 1.66046605\n",
            "Iteration 23, loss = 1.65839539\n",
            "Iteration 24, loss = 1.65618612\n",
            "Iteration 25, loss = 1.65403690\n",
            "Iteration 26, loss = 1.65200173\n",
            "Iteration 27, loss = 1.64997141\n",
            "Iteration 28, loss = 1.64790499\n",
            "Iteration 29, loss = 1.64606494\n",
            "Iteration 30, loss = 1.64420677\n",
            "Iteration 31, loss = 1.64229033\n",
            "Iteration 32, loss = 1.64046774\n",
            "Iteration 33, loss = 1.63871489\n",
            "Iteration 34, loss = 1.63698794\n",
            "Iteration 35, loss = 1.63533486\n",
            "Iteration 36, loss = 1.63375355\n",
            "Iteration 37, loss = 1.63191828\n",
            "Iteration 38, loss = 1.63038577\n",
            "Iteration 39, loss = 1.62869035\n",
            "Iteration 40, loss = 1.62718869\n",
            "Iteration 41, loss = 1.62577321\n",
            "Iteration 42, loss = 1.62420074\n",
            "Iteration 43, loss = 1.62283822\n",
            "Iteration 44, loss = 1.62144448\n",
            "Iteration 45, loss = 1.61997369\n",
            "Iteration 46, loss = 1.61883722\n",
            "Iteration 47, loss = 1.61738593\n",
            "Iteration 48, loss = 1.61629655\n",
            "Iteration 49, loss = 1.61515100\n",
            "Iteration 50, loss = 1.61390355\n",
            "Iteration 51, loss = 1.61267000\n",
            "Iteration 52, loss = 1.61163654\n",
            "Iteration 53, loss = 1.61054911\n",
            "Iteration 54, loss = 1.60946418\n",
            "Iteration 55, loss = 1.60842759\n",
            "Iteration 56, loss = 1.60738360\n",
            "Iteration 57, loss = 1.60621805\n",
            "Iteration 58, loss = 1.60506033\n",
            "Iteration 59, loss = 1.60390917\n",
            "Iteration 60, loss = 1.60255848\n",
            "Iteration 61, loss = 1.60128282\n",
            "Iteration 62, loss = 1.59974131\n",
            "Iteration 63, loss = 1.59825006\n",
            "Iteration 64, loss = 1.59658826\n",
            "Iteration 65, loss = 1.59497100\n",
            "Iteration 66, loss = 1.59350145\n",
            "Iteration 67, loss = 1.59218767\n",
            "Iteration 68, loss = 1.59100499\n",
            "Iteration 69, loss = 1.58980676\n",
            "Iteration 70, loss = 1.58879816\n",
            "Iteration 71, loss = 1.58783437\n",
            "Iteration 72, loss = 1.58684232\n",
            "Iteration 73, loss = 1.58598540\n",
            "Iteration 74, loss = 1.58521262\n",
            "Iteration 75, loss = 1.58450402\n",
            "Iteration 76, loss = 1.58381599\n",
            "Iteration 77, loss = 1.58313948\n",
            "Iteration 78, loss = 1.58248274\n",
            "Iteration 79, loss = 1.58192056\n",
            "Iteration 80, loss = 1.58121795\n",
            "Iteration 81, loss = 1.58063704\n",
            "Iteration 82, loss = 1.58000361\n",
            "Iteration 83, loss = 1.57947697\n",
            "Iteration 84, loss = 1.57882870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 1.57823226\n",
            "Iteration 86, loss = 1.57771781\n",
            "Iteration 87, loss = 1.57720182\n",
            "Iteration 88, loss = 1.57657199\n",
            "Iteration 89, loss = 1.57605539\n",
            "Iteration 90, loss = 1.57550214\n",
            "Iteration 91, loss = 1.57492687\n",
            "Iteration 92, loss = 1.57443048\n",
            "Iteration 93, loss = 1.57385584\n",
            "Iteration 94, loss = 1.57329345\n",
            "Iteration 95, loss = 1.57279081\n",
            "Iteration 96, loss = 1.57226463\n",
            "Iteration 97, loss = 1.57174830\n",
            "Iteration 98, loss = 1.57127458\n",
            "Iteration 99, loss = 1.57072316\n",
            "Iteration 100, loss = 1.57022960\n",
            "Iteration 1, loss = 1.71077970\n",
            "Iteration 2, loss = 1.68237666\n",
            "Iteration 3, loss = 1.65776784\n",
            "Iteration 4, loss = 1.63796948\n",
            "Iteration 5, loss = 1.61995561\n",
            "Iteration 6, loss = 1.60678375\n",
            "Iteration 7, loss = 1.59564910\n",
            "Iteration 8, loss = 1.58800928\n",
            "Iteration 9, loss = 1.58316339\n",
            "Iteration 10, loss = 1.57979898\n",
            "Iteration 11, loss = 1.57639641\n",
            "Iteration 12, loss = 1.57289034\n",
            "Iteration 13, loss = 1.56952975\n",
            "Iteration 14, loss = 1.56620411\n",
            "Iteration 15, loss = 1.56269187\n",
            "Iteration 16, loss = 1.55868706\n",
            "Iteration 17, loss = 1.55418205\n",
            "Iteration 18, loss = 1.54985711\n",
            "Iteration 19, loss = 1.54480017\n",
            "Iteration 20, loss = 1.53978769\n",
            "Iteration 21, loss = 1.53424834\n",
            "Iteration 22, loss = 1.52880133\n",
            "Iteration 23, loss = 1.52274110\n",
            "Iteration 24, loss = 1.51671566\n",
            "Iteration 25, loss = 1.51038116\n",
            "Iteration 26, loss = 1.50369974\n",
            "Iteration 27, loss = 1.49616835\n",
            "Iteration 28, loss = 1.48865587\n",
            "Iteration 29, loss = 1.48098406\n",
            "Iteration 30, loss = 1.47336668\n",
            "Iteration 31, loss = 1.46452417\n",
            "Iteration 32, loss = 1.45608921\n",
            "Iteration 33, loss = 1.44762405\n",
            "Iteration 34, loss = 1.43873878\n",
            "Iteration 35, loss = 1.42982603\n",
            "Iteration 36, loss = 1.42047976\n",
            "Iteration 37, loss = 1.41141476\n",
            "Iteration 38, loss = 1.40193667\n",
            "Iteration 39, loss = 1.39300503\n",
            "Iteration 40, loss = 1.38353930\n",
            "Iteration 41, loss = 1.37369017\n",
            "Iteration 42, loss = 1.36488646\n",
            "Iteration 43, loss = 1.35560813\n",
            "Iteration 44, loss = 1.34691681\n",
            "Iteration 45, loss = 1.33792215\n",
            "Iteration 46, loss = 1.32936999\n",
            "Iteration 47, loss = 1.32126693\n",
            "Iteration 48, loss = 1.31267602\n",
            "Iteration 49, loss = 1.30492604\n",
            "Iteration 50, loss = 1.29713038\n",
            "Iteration 51, loss = 1.28996174\n",
            "Iteration 52, loss = 1.28275972\n",
            "Iteration 53, loss = 1.27583054\n",
            "Iteration 54, loss = 1.26948691\n",
            "Iteration 55, loss = 1.26307645\n",
            "Iteration 56, loss = 1.25713471\n",
            "Iteration 57, loss = 1.25146150\n",
            "Iteration 58, loss = 1.24581125\n",
            "Iteration 59, loss = 1.24057705\n",
            "Iteration 60, loss = 1.23570444\n",
            "Iteration 61, loss = 1.23085504\n",
            "Iteration 62, loss = 1.22637606\n",
            "Iteration 63, loss = 1.22189795\n",
            "Iteration 64, loss = 1.21796515\n",
            "Iteration 65, loss = 1.21355959\n",
            "Iteration 66, loss = 1.20975141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 67, loss = 1.20601420\n",
            "Iteration 68, loss = 1.20271410\n",
            "Iteration 69, loss = 1.19922814\n",
            "Iteration 70, loss = 1.19604564\n",
            "Iteration 71, loss = 1.19287458\n",
            "Iteration 72, loss = 1.18997721\n",
            "Iteration 73, loss = 1.18742733\n",
            "Iteration 74, loss = 1.18435100\n",
            "Iteration 75, loss = 1.18185966\n",
            "Iteration 76, loss = 1.17921855\n",
            "Iteration 77, loss = 1.17692701\n",
            "Iteration 78, loss = 1.17448272\n",
            "Iteration 79, loss = 1.17249744\n",
            "Iteration 80, loss = 1.17004497\n",
            "Iteration 81, loss = 1.16788379\n",
            "Iteration 82, loss = 1.16577893\n",
            "Iteration 83, loss = 1.16389866\n",
            "Iteration 84, loss = 1.16196463\n",
            "Iteration 85, loss = 1.16021258\n",
            "Iteration 86, loss = 1.15831947\n",
            "Iteration 87, loss = 1.15670548\n",
            "Iteration 88, loss = 1.15500573\n",
            "Iteration 89, loss = 1.15318904\n",
            "Iteration 90, loss = 1.15180485\n",
            "Iteration 91, loss = 1.15004045\n",
            "Iteration 92, loss = 1.14885250\n",
            "Iteration 93, loss = 1.14709460\n",
            "Iteration 94, loss = 1.14583228\n",
            "Iteration 95, loss = 1.14500454\n",
            "Iteration 96, loss = 1.14311753\n",
            "Iteration 97, loss = 1.14188794\n",
            "Iteration 98, loss = 1.14044786\n",
            "Iteration 99, loss = 1.13951692\n",
            "Iteration 100, loss = 1.13801989\n",
            "Iteration 1, loss = 1.67913323\n",
            "Iteration 2, loss = 1.60745040\n",
            "Iteration 3, loss = 1.56733267\n",
            "Iteration 4, loss = 1.54626810\n",
            "Iteration 5, loss = 1.50624719\n",
            "Iteration 6, loss = 1.44879445\n",
            "Iteration 7, loss = 1.38726955\n",
            "Iteration 8, loss = 1.32714968\n",
            "Iteration 9, loss = 1.27417810\n",
            "Iteration 10, loss = 1.23455651\n",
            "Iteration 11, loss = 1.19519174\n",
            "Iteration 12, loss = 1.17321052\n",
            "Iteration 13, loss = 1.15935396\n",
            "Iteration 14, loss = 1.14043995\n",
            "Iteration 15, loss = 1.13759912\n",
            "Iteration 16, loss = 1.12599613\n",
            "Iteration 17, loss = 1.11855114\n",
            "Iteration 18, loss = 1.11393262\n",
            "Iteration 19, loss = 1.11427055\n",
            "Iteration 20, loss = 1.11303965\n",
            "Iteration 21, loss = 1.10356022\n",
            "Iteration 22, loss = 1.10972380\n",
            "Iteration 23, loss = 1.10931324\n",
            "Iteration 24, loss = 1.10286155\n",
            "Iteration 25, loss = 1.10337876\n",
            "Iteration 26, loss = 1.10292339\n",
            "Iteration 27, loss = 1.09536411\n",
            "Iteration 28, loss = 1.09717895\n",
            "Iteration 29, loss = 1.09578131\n",
            "Iteration 30, loss = 1.09402961\n",
            "Iteration 31, loss = 1.09141204\n",
            "Iteration 32, loss = 1.09116817\n",
            "Iteration 33, loss = 1.08571888\n",
            "Iteration 34, loss = 1.08757688\n",
            "Iteration 35, loss = 1.08477183\n",
            "Iteration 36, loss = 1.08545863\n",
            "Iteration 37, loss = 1.07882956\n",
            "Iteration 38, loss = 1.07852082\n",
            "Iteration 39, loss = 1.07777098\n",
            "Iteration 40, loss = 1.07423168\n",
            "Iteration 41, loss = 1.07183374\n",
            "Iteration 42, loss = 1.06720702\n",
            "Iteration 43, loss = 1.06347823\n",
            "Iteration 44, loss = 1.06369061\n",
            "Iteration 45, loss = 1.07406955\n",
            "Iteration 46, loss = 1.06626186\n",
            "Iteration 47, loss = 1.07026654\n",
            "Iteration 48, loss = 1.05068909\n",
            "Iteration 49, loss = 1.05143300\n",
            "Iteration 50, loss = 1.04265616\n",
            "Iteration 51, loss = 1.04056998\n",
            "Iteration 52, loss = 1.02908198\n",
            "Iteration 53, loss = 1.02585749\n",
            "Iteration 54, loss = 1.01954084\n",
            "Iteration 55, loss = 1.01473150\n",
            "Iteration 56, loss = 1.00904385\n",
            "Iteration 57, loss = 1.00926859\n",
            "Iteration 58, loss = 0.99816273\n",
            "Iteration 59, loss = 0.99283491\n",
            "Iteration 60, loss = 0.99483687\n",
            "Iteration 61, loss = 1.00538505\n",
            "Iteration 62, loss = 0.99459060\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 63, loss = 1.00524560\n",
            "Iteration 64, loss = 0.98178305\n",
            "Iteration 65, loss = 0.96610996\n",
            "Iteration 66, loss = 0.98729456\n",
            "Iteration 67, loss = 0.96786942\n",
            "Iteration 68, loss = 0.95911024\n",
            "Iteration 69, loss = 0.95045582\n",
            "Iteration 70, loss = 0.96228906\n",
            "Iteration 71, loss = 0.96444432\n",
            "Iteration 72, loss = 0.93662345\n",
            "Iteration 73, loss = 0.93652216\n",
            "Iteration 74, loss = 0.92427532\n",
            "Iteration 75, loss = 0.91389282\n",
            "Iteration 76, loss = 0.91468710\n",
            "Iteration 77, loss = 0.90122213\n",
            "Iteration 78, loss = 0.90186541\n",
            "Iteration 79, loss = 0.89895740\n",
            "Iteration 80, loss = 0.90495943\n",
            "Iteration 81, loss = 0.90516405\n",
            "Iteration 82, loss = 0.90542359\n",
            "Iteration 83, loss = 0.89400639\n",
            "Iteration 84, loss = 0.88436943\n",
            "Iteration 85, loss = 0.87321345\n",
            "Iteration 86, loss = 0.86606354\n",
            "Iteration 87, loss = 0.87213872\n",
            "Iteration 88, loss = 0.87518853\n",
            "Iteration 89, loss = 0.87537621\n",
            "Iteration 90, loss = 0.88088243\n",
            "Iteration 91, loss = 0.86715142\n",
            "Iteration 92, loss = 0.84811868\n",
            "Iteration 93, loss = 0.84383421\n",
            "Iteration 94, loss = 0.84972854\n",
            "Iteration 95, loss = 0.85081040\n",
            "Iteration 96, loss = 0.84185088\n",
            "Iteration 97, loss = 0.82983336\n",
            "Iteration 98, loss = 0.83802727\n",
            "Iteration 99, loss = 0.83206438\n",
            "Iteration 100, loss = 0.82571981\n",
            "Iteration 1, loss = 1.77430788\n",
            "Iteration 2, loss = 1.76842307\n",
            "Iteration 3, loss = 1.76263182\n",
            "Iteration 4, loss = 1.75732254\n",
            "Iteration 5, loss = 1.75178424\n",
            "Iteration 6, loss = 1.74662063\n",
            "Iteration 7, loss = 1.74131595\n",
            "Iteration 8, loss = 1.73643608\n",
            "Iteration 9, loss = 1.73147868\n",
            "Iteration 10, loss = 1.72691054\n",
            "Iteration 11, loss = 1.72219924\n",
            "Iteration 12, loss = 1.71789164\n",
            "Iteration 13, loss = 1.71349117\n",
            "Iteration 14, loss = 1.70942896\n",
            "Iteration 15, loss = 1.70530165\n",
            "Iteration 16, loss = 1.70101028\n",
            "Iteration 17, loss = 1.69724424\n",
            "Iteration 18, loss = 1.69362411\n",
            "Iteration 19, loss = 1.68973667\n",
            "Iteration 20, loss = 1.68609478\n",
            "Iteration 21, loss = 1.68275173\n",
            "Iteration 22, loss = 1.67928919\n",
            "Iteration 23, loss = 1.67580481\n",
            "Iteration 24, loss = 1.67266710\n",
            "Iteration 25, loss = 1.66940273\n",
            "Iteration 26, loss = 1.66611677\n",
            "Iteration 27, loss = 1.66318349\n",
            "Iteration 28, loss = 1.66039205\n",
            "Iteration 29, loss = 1.65733674\n",
            "Iteration 30, loss = 1.65476084\n",
            "Iteration 31, loss = 1.65188124\n",
            "Iteration 32, loss = 1.64918877\n",
            "Iteration 33, loss = 1.64658452\n",
            "Iteration 34, loss = 1.64397364\n",
            "Iteration 35, loss = 1.64151493\n",
            "Iteration 36, loss = 1.63900938\n",
            "Iteration 37, loss = 1.63668184\n",
            "Iteration 38, loss = 1.63461416\n",
            "Iteration 39, loss = 1.63219542\n",
            "Iteration 40, loss = 1.62997810\n",
            "Iteration 41, loss = 1.62785879\n",
            "Iteration 42, loss = 1.62572047\n",
            "Iteration 43, loss = 1.62375000\n",
            "Iteration 44, loss = 1.62172096\n",
            "Iteration 45, loss = 1.61970906\n",
            "Iteration 46, loss = 1.61804321\n",
            "Iteration 47, loss = 1.61620373\n",
            "Iteration 48, loss = 1.61462345\n",
            "Iteration 49, loss = 1.61266148\n",
            "Iteration 50, loss = 1.61105188\n",
            "Iteration 51, loss = 1.60931350\n",
            "Iteration 52, loss = 1.60783952\n",
            "Iteration 53, loss = 1.60632238\n",
            "Iteration 54, loss = 1.60476494\n",
            "Iteration 55, loss = 1.60340322\n",
            "Iteration 56, loss = 1.60205939\n",
            "Iteration 57, loss = 1.60053065\n",
            "Iteration 58, loss = 1.59920565\n",
            "Iteration 59, loss = 1.59797395\n",
            "Iteration 60, loss = 1.59689712\n",
            "Iteration 61, loss = 1.59557998\n",
            "Iteration 62, loss = 1.59444419\n",
            "Iteration 63, loss = 1.59338661\n",
            "Iteration 64, loss = 1.59230946\n",
            "Iteration 65, loss = 1.59111969\n",
            "Iteration 66, loss = 1.59020803\n",
            "Iteration 67, loss = 1.58920237\n",
            "Iteration 68, loss = 1.58827937\n",
            "Iteration 69, loss = 1.58722453\n",
            "Iteration 70, loss = 1.58651899\n",
            "Iteration 71, loss = 1.58557966\n",
            "Iteration 72, loss = 1.58470841\n",
            "Iteration 73, loss = 1.58386547\n",
            "Iteration 74, loss = 1.58303445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 75, loss = 1.58228394\n",
            "Iteration 76, loss = 1.58145670\n",
            "Iteration 77, loss = 1.58081301\n",
            "Iteration 78, loss = 1.58007013\n",
            "Iteration 79, loss = 1.57930723\n",
            "Iteration 80, loss = 1.57869354\n",
            "Iteration 81, loss = 1.57790905\n",
            "Iteration 82, loss = 1.57725793\n",
            "Iteration 83, loss = 1.57664240\n",
            "Iteration 84, loss = 1.57589476\n",
            "Iteration 85, loss = 1.57529280\n",
            "Iteration 86, loss = 1.57470865\n",
            "Iteration 87, loss = 1.57413343\n",
            "Iteration 88, loss = 1.57343817\n",
            "Iteration 89, loss = 1.57282774\n",
            "Iteration 90, loss = 1.57220201\n",
            "Iteration 91, loss = 1.57165205\n",
            "Iteration 92, loss = 1.57104100\n",
            "Iteration 93, loss = 1.57042681\n",
            "Iteration 94, loss = 1.56980287\n",
            "Iteration 95, loss = 1.56930799\n",
            "Iteration 96, loss = 1.56870874\n",
            "Iteration 97, loss = 1.56809336\n",
            "Iteration 98, loss = 1.56749204\n",
            "Iteration 99, loss = 1.56691340\n",
            "Iteration 100, loss = 1.56634633\n",
            "Iteration 1, loss = 1.76350130\n",
            "Iteration 2, loss = 1.71278439\n",
            "Iteration 3, loss = 1.67257904\n",
            "Iteration 4, loss = 1.64393247\n",
            "Iteration 5, loss = 1.61967722\n",
            "Iteration 6, loss = 1.60286269\n",
            "Iteration 7, loss = 1.58910768\n",
            "Iteration 8, loss = 1.58089083\n",
            "Iteration 9, loss = 1.57605494\n",
            "Iteration 10, loss = 1.57411594\n",
            "Iteration 11, loss = 1.57102699\n",
            "Iteration 12, loss = 1.56669263\n",
            "Iteration 13, loss = 1.56202222\n",
            "Iteration 14, loss = 1.55640431\n",
            "Iteration 15, loss = 1.54976277\n",
            "Iteration 16, loss = 1.54402795\n",
            "Iteration 17, loss = 1.53639790\n",
            "Iteration 18, loss = 1.52957518\n",
            "Iteration 19, loss = 1.52319116\n",
            "Iteration 20, loss = 1.51647632\n",
            "Iteration 21, loss = 1.50935272\n",
            "Iteration 22, loss = 1.50196650\n",
            "Iteration 23, loss = 1.49431309\n",
            "Iteration 24, loss = 1.48615330\n",
            "Iteration 25, loss = 1.47777171\n",
            "Iteration 26, loss = 1.46913718\n",
            "Iteration 27, loss = 1.45977253\n",
            "Iteration 28, loss = 1.45057343\n",
            "Iteration 29, loss = 1.44158334\n",
            "Iteration 30, loss = 1.43197453\n",
            "Iteration 31, loss = 1.42203295\n",
            "Iteration 32, loss = 1.41206407\n",
            "Iteration 33, loss = 1.40167426\n",
            "Iteration 34, loss = 1.39157203\n",
            "Iteration 35, loss = 1.38140794\n",
            "Iteration 36, loss = 1.37193885\n",
            "Iteration 37, loss = 1.36141684\n",
            "Iteration 38, loss = 1.35136021\n",
            "Iteration 39, loss = 1.34167146\n",
            "Iteration 40, loss = 1.33189923\n",
            "Iteration 41, loss = 1.32260569\n",
            "Iteration 42, loss = 1.31325465\n",
            "Iteration 43, loss = 1.30434829\n",
            "Iteration 44, loss = 1.29573438\n",
            "Iteration 45, loss = 1.28779363\n",
            "Iteration 46, loss = 1.27969726\n",
            "Iteration 47, loss = 1.27166210\n",
            "Iteration 48, loss = 1.26455449\n",
            "Iteration 49, loss = 1.25719066\n",
            "Iteration 50, loss = 1.25042656\n",
            "Iteration 51, loss = 1.24444200\n",
            "Iteration 52, loss = 1.23819623\n",
            "Iteration 53, loss = 1.23219780\n",
            "Iteration 54, loss = 1.22721630\n",
            "Iteration 55, loss = 1.22144539\n",
            "Iteration 56, loss = 1.21641680\n",
            "Iteration 57, loss = 1.21183748\n",
            "Iteration 58, loss = 1.20750049\n",
            "Iteration 59, loss = 1.20332637\n",
            "Iteration 60, loss = 1.19957955\n",
            "Iteration 61, loss = 1.19530117\n",
            "Iteration 62, loss = 1.19182827\n",
            "Iteration 63, loss = 1.18811987\n",
            "Iteration 64, loss = 1.18508065\n",
            "Iteration 65, loss = 1.18140105\n",
            "Iteration 66, loss = 1.17864732\n",
            "Iteration 67, loss = 1.17575426\n",
            "Iteration 68, loss = 1.17299053\n",
            "Iteration 69, loss = 1.17041697\n",
            "Iteration 70, loss = 1.16791643\n",
            "Iteration 71, loss = 1.16569522\n",
            "Iteration 72, loss = 1.16316631\n",
            "Iteration 73, loss = 1.16123603\n",
            "Iteration 74, loss = 1.15887839\n",
            "Iteration 75, loss = 1.15672080\n",
            "Iteration 76, loss = 1.15487801\n",
            "Iteration 77, loss = 1.15348600\n",
            "Iteration 78, loss = 1.15139950\n",
            "Iteration 79, loss = 1.14934953\n",
            "Iteration 80, loss = 1.14789128\n",
            "Iteration 81, loss = 1.14625752\n",
            "Iteration 82, loss = 1.14476419\n",
            "Iteration 83, loss = 1.14373737\n",
            "Iteration 84, loss = 1.14217907\n",
            "Iteration 85, loss = 1.14028872\n",
            "Iteration 86, loss = 1.13894572\n",
            "Iteration 87, loss = 1.13824490\n",
            "Iteration 88, loss = 1.13661477\n",
            "Iteration 89, loss = 1.13545143\n",
            "Iteration 90, loss = 1.13428801\n",
            "Iteration 91, loss = 1.13360616\n",
            "Iteration 92, loss = 1.13252240\n",
            "Iteration 93, loss = 1.13071728\n",
            "Iteration 94, loss = 1.12999076\n",
            "Iteration 95, loss = 1.12900189\n",
            "Iteration 96, loss = 1.12809985\n",
            "Iteration 97, loss = 1.12699023\n",
            "Iteration 98, loss = 1.12617676\n",
            "Iteration 99, loss = 1.12520731\n",
            "Iteration 100, loss = 1.12405769\n",
            "Iteration 1, loss = 1.70878257\n",
            "Iteration 2, loss = 1.57739088\n",
            "Iteration 3, loss = 1.53626868\n",
            "Iteration 4, loss = 1.48032018\n",
            "Iteration 5, loss = 1.42243250\n",
            "Iteration 6, loss = 1.35224645\n",
            "Iteration 7, loss = 1.28819635\n",
            "Iteration 8, loss = 1.24351109\n",
            "Iteration 9, loss = 1.20458127\n",
            "Iteration 10, loss = 1.18109609\n",
            "Iteration 11, loss = 1.16677885\n",
            "Iteration 12, loss = 1.14747955\n",
            "Iteration 13, loss = 1.13750903\n",
            "Iteration 14, loss = 1.12801768\n",
            "Iteration 15, loss = 1.12407021\n",
            "Iteration 16, loss = 1.11735011\n",
            "Iteration 17, loss = 1.11486802\n",
            "Iteration 18, loss = 1.11173590\n",
            "Iteration 19, loss = 1.10890629\n",
            "Iteration 20, loss = 1.10580052\n",
            "Iteration 21, loss = 1.13780023\n",
            "Iteration 22, loss = 1.11944910\n",
            "Iteration 23, loss = 1.11943065\n",
            "Iteration 24, loss = 1.12477813\n",
            "Iteration 25, loss = 1.11379189\n",
            "Iteration 26, loss = 1.11617138\n",
            "Iteration 27, loss = 1.10467776\n",
            "Iteration 28, loss = 1.10372586\n",
            "Iteration 29, loss = 1.10616434\n",
            "Iteration 30, loss = 1.09199933\n",
            "Iteration 31, loss = 1.10121462\n",
            "Iteration 32, loss = 1.09114256\n",
            "Iteration 33, loss = 1.09209569\n",
            "Iteration 34, loss = 1.08607184\n",
            "Iteration 35, loss = 1.08426109\n",
            "Iteration 36, loss = 1.08740788\n",
            "Iteration 37, loss = 1.07433244\n",
            "Iteration 38, loss = 1.07418629\n",
            "Iteration 39, loss = 1.06962462\n",
            "Iteration 40, loss = 1.06411846\n",
            "Iteration 41, loss = 1.06476285\n",
            "Iteration 42, loss = 1.05926740\n",
            "Iteration 43, loss = 1.04798964\n",
            "Iteration 44, loss = 1.04988212\n",
            "Iteration 45, loss = 1.05166977\n",
            "Iteration 46, loss = 1.03764660\n",
            "Iteration 47, loss = 1.03639212\n",
            "Iteration 48, loss = 1.03947759\n",
            "Iteration 49, loss = 1.01887988\n",
            "Iteration 50, loss = 1.01482838\n",
            "Iteration 51, loss = 1.01803447\n",
            "Iteration 52, loss = 1.01767571\n",
            "Iteration 53, loss = 0.99835471\n",
            "Iteration 54, loss = 0.97539565\n",
            "Iteration 55, loss = 0.99137991\n",
            "Iteration 56, loss = 1.01401856\n",
            "Iteration 57, loss = 0.98887421\n",
            "Iteration 58, loss = 0.99502200\n",
            "Iteration 59, loss = 0.96646329\n",
            "Iteration 60, loss = 0.95787426\n",
            "Iteration 61, loss = 0.94250606\n",
            "Iteration 62, loss = 0.93914211\n",
            "Iteration 63, loss = 0.91924042\n",
            "Iteration 64, loss = 0.91929159\n",
            "Iteration 65, loss = 0.91064758\n",
            "Iteration 66, loss = 0.90105272\n",
            "Iteration 67, loss = 0.89806051\n",
            "Iteration 68, loss = 0.92480182\n",
            "Iteration 69, loss = 0.90516497\n",
            "Iteration 70, loss = 0.88475828\n",
            "Iteration 71, loss = 0.87586935\n",
            "Iteration 72, loss = 0.91991671\n",
            "Iteration 73, loss = 0.94219353\n",
            "Iteration 74, loss = 0.90218829\n",
            "Iteration 75, loss = 0.89977822\n",
            "Iteration 76, loss = 0.91224572\n",
            "Iteration 77, loss = 0.92601585\n",
            "Iteration 78, loss = 0.92767236\n",
            "Iteration 79, loss = 0.92081742\n",
            "Iteration 80, loss = 0.88885912\n",
            "Iteration 81, loss = 0.86430033\n",
            "Iteration 82, loss = 0.84585271\n",
            "Iteration 83, loss = 0.84493262\n",
            "Iteration 84, loss = 0.83681167\n",
            "Iteration 85, loss = 0.83392762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 86, loss = 0.83207554\n",
            "Iteration 87, loss = 0.84277508\n",
            "Iteration 88, loss = 0.85830218\n",
            "Iteration 89, loss = 0.90647099\n",
            "Iteration 90, loss = 0.93878329\n",
            "Iteration 91, loss = 0.93996192\n",
            "Iteration 92, loss = 0.87440241\n",
            "Iteration 93, loss = 0.88972726\n",
            "Iteration 94, loss = 0.85241635\n",
            "Iteration 95, loss = 0.84802160\n",
            "Iteration 96, loss = 0.83688189\n",
            "Iteration 97, loss = 0.85866448\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.70636551\n",
            "Iteration 2, loss = 1.70194460\n",
            "Iteration 3, loss = 1.69764613\n",
            "Iteration 4, loss = 1.69372004\n",
            "Iteration 5, loss = 1.68958794\n",
            "Iteration 6, loss = 1.68585477\n",
            "Iteration 7, loss = 1.68187861\n",
            "Iteration 8, loss = 1.67833871\n",
            "Iteration 9, loss = 1.67479957\n",
            "Iteration 10, loss = 1.67158247\n",
            "Iteration 11, loss = 1.66787862\n",
            "Iteration 12, loss = 1.66514241\n",
            "Iteration 13, loss = 1.66219440\n",
            "Iteration 14, loss = 1.65943130\n",
            "Iteration 15, loss = 1.65659476\n",
            "Iteration 16, loss = 1.65403294\n",
            "Iteration 17, loss = 1.65140883\n",
            "Iteration 18, loss = 1.64918026\n",
            "Iteration 19, loss = 1.64662721\n",
            "Iteration 20, loss = 1.64457543\n",
            "Iteration 21, loss = 1.64251776\n",
            "Iteration 22, loss = 1.64053509\n",
            "Iteration 23, loss = 1.63822955\n",
            "Iteration 24, loss = 1.63666075\n",
            "Iteration 25, loss = 1.63488052\n",
            "Iteration 26, loss = 1.63295122\n",
            "Iteration 27, loss = 1.63135961\n",
            "Iteration 28, loss = 1.62980344\n",
            "Iteration 29, loss = 1.62840743\n",
            "Iteration 30, loss = 1.62703481\n",
            "Iteration 31, loss = 1.62558233\n",
            "Iteration 32, loss = 1.62424038\n",
            "Iteration 33, loss = 1.62285656\n",
            "Iteration 34, loss = 1.62165716\n",
            "Iteration 35, loss = 1.62044564\n",
            "Iteration 36, loss = 1.61937808\n",
            "Iteration 37, loss = 1.61824561\n",
            "Iteration 38, loss = 1.61715304\n",
            "Iteration 39, loss = 1.61623499\n",
            "Iteration 40, loss = 1.61525157\n",
            "Iteration 41, loss = 1.61428810\n",
            "Iteration 42, loss = 1.61340462\n",
            "Iteration 43, loss = 1.61259912\n",
            "Iteration 44, loss = 1.61174127\n",
            "Iteration 45, loss = 1.61094267\n",
            "Iteration 46, loss = 1.61034332\n",
            "Iteration 47, loss = 1.60953675\n",
            "Iteration 48, loss = 1.60898342\n",
            "Iteration 49, loss = 1.60822453\n",
            "Iteration 50, loss = 1.60760900\n",
            "Iteration 51, loss = 1.60707850\n",
            "Iteration 52, loss = 1.60656745\n",
            "Iteration 53, loss = 1.60598605\n",
            "Iteration 54, loss = 1.60549561\n",
            "Iteration 55, loss = 1.60501424\n",
            "Iteration 56, loss = 1.60461939\n",
            "Iteration 57, loss = 1.60414020\n",
            "Iteration 58, loss = 1.60373537\n",
            "Iteration 59, loss = 1.60337876\n",
            "Iteration 60, loss = 1.60309439\n",
            "Iteration 61, loss = 1.60262807\n",
            "Iteration 62, loss = 1.60230474\n",
            "Iteration 63, loss = 1.60203266\n",
            "Iteration 64, loss = 1.60171521\n",
            "Iteration 65, loss = 1.60142846\n",
            "Iteration 66, loss = 1.60120470\n",
            "Iteration 67, loss = 1.60095720\n",
            "Iteration 68, loss = 1.60081200\n",
            "Iteration 69, loss = 1.60057100\n",
            "Iteration 70, loss = 1.60039144\n",
            "Iteration 71, loss = 1.60010678\n",
            "Iteration 72, loss = 1.59999384\n",
            "Iteration 73, loss = 1.59980423\n",
            "Iteration 74, loss = 1.59962055\n",
            "Iteration 75, loss = 1.59950061\n",
            "Iteration 76, loss = 1.59934384\n",
            "Iteration 77, loss = 1.59926459\n",
            "Iteration 78, loss = 1.59909899\n",
            "Iteration 79, loss = 1.59895108\n",
            "Iteration 80, loss = 1.59888314\n",
            "Iteration 81, loss = 1.59876388\n",
            "Iteration 82, loss = 1.59868733\n",
            "Iteration 83, loss = 1.59857193\n",
            "Iteration 84, loss = 1.59848986\n",
            "Iteration 85, loss = 1.59840944\n",
            "Iteration 86, loss = 1.59830327\n",
            "Iteration 87, loss = 1.59828239\n",
            "Iteration 88, loss = 1.59818721\n",
            "Iteration 89, loss = 1.59812748\n",
            "Iteration 90, loss = 1.59803176\n",
            "Iteration 91, loss = 1.59798420\n",
            "Iteration 92, loss = 1.59794550\n",
            "Iteration 93, loss = 1.59787338\n",
            "Iteration 94, loss = 1.59781852\n",
            "Iteration 95, loss = 1.59781260\n",
            "Iteration 96, loss = 1.59773547\n",
            "Iteration 97, loss = 1.59765806\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.69713942\n",
            "Iteration 2, loss = 1.66038492\n",
            "Iteration 3, loss = 1.63429516\n",
            "Iteration 4, loss = 1.62010565\n",
            "Iteration 5, loss = 1.60903094\n",
            "Iteration 6, loss = 1.60475400\n",
            "Iteration 7, loss = 1.60153543\n",
            "Iteration 8, loss = 1.60173750\n",
            "Iteration 9, loss = 1.60094724\n",
            "Iteration 10, loss = 1.60196808\n",
            "Iteration 11, loss = 1.60301794\n",
            "Iteration 12, loss = 1.60225498\n",
            "Iteration 13, loss = 1.60131917\n",
            "Iteration 14, loss = 1.59988447\n",
            "Iteration 15, loss = 1.59875477\n",
            "Iteration 16, loss = 1.59772368\n",
            "Iteration 17, loss = 1.59650934\n",
            "Iteration 18, loss = 1.59597592\n",
            "Iteration 19, loss = 1.59573606\n",
            "Iteration 20, loss = 1.59551652\n",
            "Iteration 21, loss = 1.59541229\n",
            "Iteration 22, loss = 1.59517998\n",
            "Iteration 23, loss = 1.59495442\n",
            "Iteration 24, loss = 1.59437255\n",
            "Iteration 25, loss = 1.59390815\n",
            "Iteration 26, loss = 1.59337089\n",
            "Iteration 27, loss = 1.59271160\n",
            "Iteration 28, loss = 1.59244211\n",
            "Iteration 29, loss = 1.59239489\n",
            "Iteration 30, loss = 1.59190292\n",
            "Iteration 31, loss = 1.59131309\n",
            "Iteration 32, loss = 1.59065962\n",
            "Iteration 33, loss = 1.59008411\n",
            "Iteration 34, loss = 1.58938316\n",
            "Iteration 35, loss = 1.58880625\n",
            "Iteration 36, loss = 1.58849953\n",
            "Iteration 37, loss = 1.58743466\n",
            "Iteration 38, loss = 1.58723204\n",
            "Iteration 39, loss = 1.58608253\n",
            "Iteration 40, loss = 1.58533554\n",
            "Iteration 41, loss = 1.58467167\n",
            "Iteration 42, loss = 1.58375992\n",
            "Iteration 43, loss = 1.58275412\n",
            "Iteration 44, loss = 1.58180801\n",
            "Iteration 45, loss = 1.58104920\n",
            "Iteration 46, loss = 1.57984105\n",
            "Iteration 47, loss = 1.57859716\n",
            "Iteration 48, loss = 1.57776384\n",
            "Iteration 49, loss = 1.57614493\n",
            "Iteration 50, loss = 1.57486791\n",
            "Iteration 51, loss = 1.57365908\n",
            "Iteration 52, loss = 1.57214462\n",
            "Iteration 53, loss = 1.57046366\n",
            "Iteration 54, loss = 1.56905461\n",
            "Iteration 55, loss = 1.56724256\n",
            "Iteration 56, loss = 1.56545849\n",
            "Iteration 57, loss = 1.56347795\n",
            "Iteration 58, loss = 1.56196376\n",
            "Iteration 59, loss = 1.55992155\n",
            "Iteration 60, loss = 1.55764204\n",
            "Iteration 61, loss = 1.55564620\n",
            "Iteration 62, loss = 1.55320766\n",
            "Iteration 63, loss = 1.55049064\n",
            "Iteration 64, loss = 1.54806164\n",
            "Iteration 65, loss = 1.54506485\n",
            "Iteration 66, loss = 1.54228978\n",
            "Iteration 67, loss = 1.53959971\n",
            "Iteration 68, loss = 1.53677010\n",
            "Iteration 69, loss = 1.53375667\n",
            "Iteration 70, loss = 1.53071597\n",
            "Iteration 71, loss = 1.52735115\n",
            "Iteration 72, loss = 1.52380945\n",
            "Iteration 73, loss = 1.52034470\n",
            "Iteration 74, loss = 1.51638662\n",
            "Iteration 75, loss = 1.51261456\n",
            "Iteration 76, loss = 1.50884979\n",
            "Iteration 77, loss = 1.50511767\n",
            "Iteration 78, loss = 1.50084974\n",
            "Iteration 79, loss = 1.49631089\n",
            "Iteration 80, loss = 1.49210938\n",
            "Iteration 81, loss = 1.48752406\n",
            "Iteration 82, loss = 1.48305702\n",
            "Iteration 83, loss = 1.47840768\n",
            "Iteration 84, loss = 1.47381759\n",
            "Iteration 85, loss = 1.46873320\n",
            "Iteration 86, loss = 1.46368133\n",
            "Iteration 87, loss = 1.45899480\n",
            "Iteration 88, loss = 1.45349900\n",
            "Iteration 89, loss = 1.44857129\n",
            "Iteration 90, loss = 1.44301858\n",
            "Iteration 91, loss = 1.43790833\n",
            "Iteration 92, loss = 1.43239676\n",
            "Iteration 93, loss = 1.42678557\n",
            "Iteration 94, loss = 1.42163083\n",
            "Iteration 95, loss = 1.41644861\n",
            "Iteration 96, loss = 1.41070605\n",
            "Iteration 97, loss = 1.40484281\n",
            "Iteration 98, loss = 1.39926362\n",
            "Iteration 99, loss = 1.39383599\n",
            "Iteration 100, loss = 1.38829106\n",
            "Iteration 1, loss = 1.66701683\n",
            "Iteration 2, loss = 1.64075506\n",
            "Iteration 3, loss = 1.62024198\n",
            "Iteration 4, loss = 1.60277753\n",
            "Iteration 5, loss = 1.60392288\n",
            "Iteration 6, loss = 1.60316746\n",
            "Iteration 7, loss = 1.59717044\n",
            "Iteration 8, loss = 1.59304671\n",
            "Iteration 9, loss = 1.58661915\n",
            "Iteration 10, loss = 1.58302549\n",
            "Iteration 11, loss = 1.57922571\n",
            "Iteration 12, loss = 1.56655052\n",
            "Iteration 13, loss = 1.55509772\n",
            "Iteration 14, loss = 1.53977679\n",
            "Iteration 15, loss = 1.52075177\n",
            "Iteration 16, loss = 1.49781751\n",
            "Iteration 17, loss = 1.46413622\n",
            "Iteration 18, loss = 1.42895491\n",
            "Iteration 19, loss = 1.39335870\n",
            "Iteration 20, loss = 1.35532237\n",
            "Iteration 21, loss = 1.32261156\n",
            "Iteration 22, loss = 1.28831689\n",
            "Iteration 23, loss = 1.25711823\n",
            "Iteration 24, loss = 1.23103902\n",
            "Iteration 25, loss = 1.21140276\n",
            "Iteration 26, loss = 1.19404672\n",
            "Iteration 27, loss = 1.17557563\n",
            "Iteration 28, loss = 1.16724262\n",
            "Iteration 29, loss = 1.15888455\n",
            "Iteration 30, loss = 1.15261585\n",
            "Iteration 31, loss = 1.14478350\n",
            "Iteration 32, loss = 1.14420401\n",
            "Iteration 33, loss = 1.13754264\n",
            "Iteration 34, loss = 1.13221875\n",
            "Iteration 35, loss = 1.12499963\n",
            "Iteration 36, loss = 1.13302413\n",
            "Iteration 37, loss = 1.12123190\n",
            "Iteration 38, loss = 1.12428118\n",
            "Iteration 39, loss = 1.11945350\n",
            "Iteration 40, loss = 1.11844791\n",
            "Iteration 41, loss = 1.11279585\n",
            "Iteration 42, loss = 1.11337047\n",
            "Iteration 43, loss = 1.11077790\n",
            "Iteration 44, loss = 1.11262370\n",
            "Iteration 45, loss = 1.11882945\n",
            "Iteration 46, loss = 1.10987073\n",
            "Iteration 47, loss = 1.10713025\n",
            "Iteration 48, loss = 1.10873301\n",
            "Iteration 49, loss = 1.10646578\n",
            "Iteration 50, loss = 1.10555456\n",
            "Iteration 51, loss = 1.10960589\n",
            "Iteration 52, loss = 1.10519628\n",
            "Iteration 53, loss = 1.10600937\n",
            "Iteration 54, loss = 1.10675325\n",
            "Iteration 55, loss = 1.10459782\n",
            "Iteration 56, loss = 1.10619677\n",
            "Iteration 57, loss = 1.10094970\n",
            "Iteration 58, loss = 1.10498709\n",
            "Iteration 59, loss = 1.10622959\n",
            "Iteration 60, loss = 1.10445093\n",
            "Iteration 61, loss = 1.10883662\n",
            "Iteration 62, loss = 1.10615826\n",
            "Iteration 63, loss = 1.09966784\n",
            "Iteration 64, loss = 1.10296426\n",
            "Iteration 65, loss = 1.09737872\n",
            "Iteration 66, loss = 1.09677768\n",
            "Iteration 67, loss = 1.09767420\n",
            "Iteration 68, loss = 1.09659144\n",
            "Iteration 69, loss = 1.09576654\n",
            "Iteration 70, loss = 1.09661329\n",
            "Iteration 71, loss = 1.09779196\n",
            "Iteration 72, loss = 1.09414238\n",
            "Iteration 73, loss = 1.09792310\n",
            "Iteration 74, loss = 1.09122103\n",
            "Iteration 75, loss = 1.08940173\n",
            "Iteration 76, loss = 1.09068469\n",
            "Iteration 77, loss = 1.08978938\n",
            "Iteration 78, loss = 1.09045571\n",
            "Iteration 79, loss = 1.08430018\n",
            "Iteration 80, loss = 1.08427110\n",
            "Iteration 81, loss = 1.08273036\n",
            "Iteration 82, loss = 1.07953906\n",
            "Iteration 83, loss = 1.07906760\n",
            "Iteration 84, loss = 1.07957288\n",
            "Iteration 85, loss = 1.06894507\n",
            "Iteration 86, loss = 1.06404577\n",
            "Iteration 87, loss = 1.07231527\n",
            "Iteration 88, loss = 1.06965844\n",
            "Iteration 89, loss = 1.06950959\n",
            "Iteration 90, loss = 1.05799758\n",
            "Iteration 91, loss = 1.05886489\n",
            "Iteration 92, loss = 1.05559631\n",
            "Iteration 93, loss = 1.04980991\n",
            "Iteration 94, loss = 1.04971916\n",
            "Iteration 95, loss = 1.04599253\n",
            "Iteration 96, loss = 1.04695138\n",
            "Iteration 97, loss = 1.04643705\n",
            "Iteration 98, loss = 1.03719762\n",
            "Iteration 99, loss = 1.03852683\n",
            "Iteration 100, loss = 1.03655732\n",
            "Iteration 1, loss = 1.76759164\n",
            "Iteration 2, loss = 1.76215471\n",
            "Iteration 3, loss = 1.75679936\n",
            "Iteration 4, loss = 1.75188590\n",
            "Iteration 5, loss = 1.74674884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.74195043\n",
            "Iteration 7, loss = 1.73701050\n",
            "Iteration 8, loss = 1.73246148\n",
            "Iteration 9, loss = 1.72783378\n",
            "Iteration 10, loss = 1.72356829\n",
            "Iteration 11, loss = 1.71915554\n",
            "Iteration 12, loss = 1.71511534\n",
            "Iteration 13, loss = 1.71098223\n",
            "Iteration 14, loss = 1.70716616\n",
            "Iteration 15, loss = 1.70328029\n",
            "Iteration 16, loss = 1.69922834\n",
            "Iteration 17, loss = 1.69567734\n",
            "Iteration 18, loss = 1.69225883\n",
            "Iteration 19, loss = 1.68858008\n",
            "Iteration 20, loss = 1.68512869\n",
            "Iteration 21, loss = 1.68196269\n",
            "Iteration 22, loss = 1.67867523\n",
            "Iteration 23, loss = 1.67536675\n",
            "Iteration 24, loss = 1.67238415\n",
            "Iteration 25, loss = 1.66927869\n",
            "Iteration 26, loss = 1.66614942\n",
            "Iteration 27, loss = 1.66335672\n",
            "Iteration 28, loss = 1.66069823\n",
            "Iteration 29, loss = 1.65778423\n",
            "Iteration 30, loss = 1.65532305\n",
            "Iteration 31, loss = 1.65257206\n",
            "Iteration 32, loss = 1.65000301\n",
            "Iteration 33, loss = 1.64751177\n",
            "Iteration 34, loss = 1.64501200\n",
            "Iteration 35, loss = 1.64265620\n",
            "Iteration 36, loss = 1.64025757\n",
            "Iteration 37, loss = 1.63802263\n",
            "Iteration 38, loss = 1.63603444\n",
            "Iteration 39, loss = 1.63371499\n",
            "Iteration 40, loss = 1.63158277\n",
            "Iteration 41, loss = 1.62954373\n",
            "Iteration 42, loss = 1.62748465\n",
            "Iteration 43, loss = 1.62558402\n",
            "Iteration 44, loss = 1.62362658\n",
            "Iteration 45, loss = 1.62168376\n",
            "Iteration 46, loss = 1.62007074\n",
            "Iteration 47, loss = 1.61828855\n",
            "Iteration 48, loss = 1.61675232\n",
            "Iteration 49, loss = 1.61485088\n",
            "Iteration 50, loss = 1.61328339\n",
            "Iteration 51, loss = 1.61159240\n",
            "Iteration 52, loss = 1.61015118\n",
            "Iteration 53, loss = 1.60866671\n",
            "Iteration 54, loss = 1.60714266\n",
            "Iteration 55, loss = 1.60580339\n",
            "Iteration 56, loss = 1.60447942\n",
            "Iteration 57, loss = 1.60297795\n",
            "Iteration 58, loss = 1.60166957\n",
            "Iteration 59, loss = 1.60044666\n",
            "Iteration 60, loss = 1.59937123\n",
            "Iteration 61, loss = 1.59806809\n",
            "Iteration 62, loss = 1.59693016\n",
            "Iteration 63, loss = 1.59587208\n",
            "Iteration 64, loss = 1.59479146\n",
            "Iteration 65, loss = 1.59360091\n",
            "Iteration 66, loss = 1.59267815\n",
            "Iteration 67, loss = 1.59166442\n",
            "Iteration 68, loss = 1.59072693\n",
            "Iteration 69, loss = 1.58966573\n",
            "Iteration 70, loss = 1.58893789\n",
            "Iteration 71, loss = 1.58798874\n",
            "Iteration 72, loss = 1.58710322\n",
            "Iteration 73, loss = 1.58624375\n",
            "Iteration 74, loss = 1.58539602\n",
            "Iteration 75, loss = 1.58462551\n",
            "Iteration 76, loss = 1.58378181\n",
            "Iteration 77, loss = 1.58312327\n",
            "Iteration 78, loss = 1.58235972\n",
            "Iteration 79, loss = 1.58158301\n",
            "Iteration 80, loss = 1.58095457\n",
            "Iteration 81, loss = 1.58015168\n",
            "Iteration 82, loss = 1.57948560\n",
            "Iteration 83, loss = 1.57886306\n",
            "Iteration 84, loss = 1.57809591\n",
            "Iteration 85, loss = 1.57748703\n",
            "Iteration 86, loss = 1.57689673\n",
            "Iteration 87, loss = 1.57631507\n",
            "Iteration 88, loss = 1.57560793\n",
            "Iteration 89, loss = 1.57499083\n",
            "Iteration 90, loss = 1.57436229\n",
            "Iteration 91, loss = 1.57381692\n",
            "Iteration 92, loss = 1.57320126\n",
            "Iteration 93, loss = 1.57258677\n",
            "Iteration 94, loss = 1.57195963\n",
            "Iteration 95, loss = 1.57147181\n",
            "Iteration 96, loss = 1.57088557\n",
            "Iteration 97, loss = 1.57027308\n",
            "Iteration 98, loss = 1.56968032\n",
            "Iteration 99, loss = 1.56911232\n",
            "Iteration 100, loss = 1.56855313\n",
            "Iteration 1, loss = 1.75761066\n",
            "Iteration 2, loss = 1.71038004\n",
            "Iteration 3, loss = 1.67241901\n",
            "Iteration 4, loss = 1.64506783\n",
            "Iteration 5, loss = 1.62166877\n",
            "Iteration 6, loss = 1.60525433\n",
            "Iteration 7, loss = 1.59157556\n",
            "Iteration 8, loss = 1.58294769\n",
            "Iteration 9, loss = 1.57733690\n",
            "Iteration 10, loss = 1.57444917\n",
            "Iteration 11, loss = 1.57084474\n",
            "Iteration 12, loss = 1.56658389\n",
            "Iteration 13, loss = 1.56267594\n",
            "Iteration 14, loss = 1.55793355\n",
            "Iteration 15, loss = 1.55206649\n",
            "Iteration 16, loss = 1.54663400\n",
            "Iteration 17, loss = 1.53930789\n",
            "Iteration 18, loss = 1.53250200\n",
            "Iteration 19, loss = 1.52604493\n",
            "Iteration 20, loss = 1.51925439\n",
            "Iteration 21, loss = 1.51220267\n",
            "Iteration 22, loss = 1.50487442\n",
            "Iteration 23, loss = 1.49751126\n",
            "Iteration 24, loss = 1.48947696\n",
            "Iteration 25, loss = 1.48135942\n",
            "Iteration 26, loss = 1.47305023\n",
            "Iteration 27, loss = 1.46387072\n",
            "Iteration 28, loss = 1.45479208\n",
            "Iteration 29, loss = 1.44592712\n",
            "Iteration 30, loss = 1.43638326\n",
            "Iteration 31, loss = 1.42657187\n",
            "Iteration 32, loss = 1.41666325\n",
            "Iteration 33, loss = 1.40636281\n",
            "Iteration 34, loss = 1.39634652\n",
            "Iteration 35, loss = 1.38626442\n",
            "Iteration 36, loss = 1.37673775\n",
            "Iteration 37, loss = 1.36633379\n",
            "Iteration 38, loss = 1.35628167\n",
            "Iteration 39, loss = 1.34658621\n",
            "Iteration 40, loss = 1.33677148\n",
            "Iteration 41, loss = 1.32741132\n",
            "Iteration 42, loss = 1.31800308\n",
            "Iteration 43, loss = 1.30901181\n",
            "Iteration 44, loss = 1.30029523\n",
            "Iteration 45, loss = 1.29223981\n",
            "Iteration 46, loss = 1.28399631\n",
            "Iteration 47, loss = 1.27583213\n",
            "Iteration 48, loss = 1.26858489\n",
            "Iteration 49, loss = 1.26106969\n",
            "Iteration 50, loss = 1.25414626\n",
            "Iteration 51, loss = 1.24801682\n",
            "Iteration 52, loss = 1.24162922\n",
            "Iteration 53, loss = 1.23547819\n",
            "Iteration 54, loss = 1.23028651\n",
            "Iteration 55, loss = 1.22441684\n",
            "Iteration 56, loss = 1.21925786\n",
            "Iteration 57, loss = 1.21452049\n",
            "Iteration 58, loss = 1.21000419\n",
            "Iteration 59, loss = 1.20571495\n",
            "Iteration 60, loss = 1.20187030\n",
            "Iteration 61, loss = 1.19740814\n",
            "Iteration 62, loss = 1.19383998\n",
            "Iteration 63, loss = 1.19004117\n",
            "Iteration 64, loss = 1.18689124\n",
            "Iteration 65, loss = 1.18313343\n",
            "Iteration 66, loss = 1.18027350\n",
            "Iteration 67, loss = 1.17728820\n",
            "Iteration 68, loss = 1.17446203\n",
            "Iteration 69, loss = 1.17179404\n",
            "Iteration 70, loss = 1.16920437\n",
            "Iteration 71, loss = 1.16692618\n",
            "Iteration 72, loss = 1.16431758\n",
            "Iteration 73, loss = 1.16231334\n",
            "Iteration 74, loss = 1.15992365\n",
            "Iteration 75, loss = 1.15771113\n",
            "Iteration 76, loss = 1.15578877\n",
            "Iteration 77, loss = 1.15432411\n",
            "Iteration 78, loss = 1.15221674\n",
            "Iteration 79, loss = 1.15011904\n",
            "Iteration 80, loss = 1.14861922\n",
            "Iteration 81, loss = 1.14693593\n",
            "Iteration 82, loss = 1.14539479\n",
            "Iteration 83, loss = 1.14429579\n",
            "Iteration 84, loss = 1.14273511\n",
            "Iteration 85, loss = 1.14079964\n",
            "Iteration 86, loss = 1.13939702\n",
            "Iteration 87, loss = 1.13864709\n",
            "Iteration 88, loss = 1.13698044\n",
            "Iteration 89, loss = 1.13575030\n",
            "Iteration 90, loss = 1.13455854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 91, loss = 1.13381501\n",
            "Iteration 92, loss = 1.13268085\n",
            "Iteration 93, loss = 1.13088202\n",
            "Iteration 94, loss = 1.13011166\n",
            "Iteration 95, loss = 1.12908188\n",
            "Iteration 96, loss = 1.12812246\n",
            "Iteration 97, loss = 1.12697049\n",
            "Iteration 98, loss = 1.12611091\n",
            "Iteration 99, loss = 1.12507694\n",
            "Iteration 100, loss = 1.12391525\n",
            "Iteration 1, loss = 1.70614434\n",
            "Iteration 2, loss = 1.57451667\n",
            "Iteration 3, loss = 1.53928675\n",
            "Iteration 4, loss = 1.48050853\n",
            "Iteration 5, loss = 1.42307186\n",
            "Iteration 6, loss = 1.35526868\n",
            "Iteration 7, loss = 1.28882368\n",
            "Iteration 8, loss = 1.24279990\n",
            "Iteration 9, loss = 1.20322744\n",
            "Iteration 10, loss = 1.17784831\n",
            "Iteration 11, loss = 1.16695410\n",
            "Iteration 12, loss = 1.14777046\n",
            "Iteration 13, loss = 1.13952205\n",
            "Iteration 14, loss = 1.12821632\n",
            "Iteration 15, loss = 1.12103870\n",
            "Iteration 16, loss = 1.11573547\n",
            "Iteration 17, loss = 1.11272084\n",
            "Iteration 18, loss = 1.11038888\n",
            "Iteration 19, loss = 1.10623146\n",
            "Iteration 20, loss = 1.10500882\n",
            "Iteration 21, loss = 1.13686369\n",
            "Iteration 22, loss = 1.11610220\n",
            "Iteration 23, loss = 1.11837144\n",
            "Iteration 24, loss = 1.12155719\n",
            "Iteration 25, loss = 1.11048364\n",
            "Iteration 26, loss = 1.11357988\n",
            "Iteration 27, loss = 1.09986284\n",
            "Iteration 28, loss = 1.10110616\n",
            "Iteration 29, loss = 1.10205308\n",
            "Iteration 30, loss = 1.08968948\n",
            "Iteration 31, loss = 1.09784083\n",
            "Iteration 32, loss = 1.08868514\n",
            "Iteration 33, loss = 1.09077474\n",
            "Iteration 34, loss = 1.08267938\n",
            "Iteration 35, loss = 1.08328695\n",
            "Iteration 36, loss = 1.08404884\n",
            "Iteration 37, loss = 1.07152173\n",
            "Iteration 38, loss = 1.07093383\n",
            "Iteration 39, loss = 1.06620956\n",
            "Iteration 40, loss = 1.05992241\n",
            "Iteration 41, loss = 1.06197523\n",
            "Iteration 42, loss = 1.05743331\n",
            "Iteration 43, loss = 1.04412757\n",
            "Iteration 44, loss = 1.04777750\n",
            "Iteration 45, loss = 1.04717227\n",
            "Iteration 46, loss = 1.03345904\n",
            "Iteration 47, loss = 1.03352773\n",
            "Iteration 48, loss = 1.03589322\n",
            "Iteration 49, loss = 1.01621361\n",
            "Iteration 50, loss = 1.01093110\n",
            "Iteration 51, loss = 1.01274180\n",
            "Iteration 52, loss = 1.01562419\n",
            "Iteration 53, loss = 0.99117162\n",
            "Iteration 54, loss = 0.97224980\n",
            "Iteration 55, loss = 0.99044905\n",
            "Iteration 56, loss = 1.01135351\n",
            "Iteration 57, loss = 0.98781578\n",
            "Iteration 58, loss = 0.98843473\n",
            "Iteration 59, loss = 0.95900468\n",
            "Iteration 60, loss = 0.95132847\n",
            "Iteration 61, loss = 0.94059630\n",
            "Iteration 62, loss = 0.93092713\n",
            "Iteration 63, loss = 0.91815039\n",
            "Iteration 64, loss = 0.91505740\n",
            "Iteration 65, loss = 0.90725373\n",
            "Iteration 66, loss = 0.89730881\n",
            "Iteration 67, loss = 0.89584148\n",
            "Iteration 68, loss = 0.92463460\n",
            "Iteration 69, loss = 0.90484551\n",
            "Iteration 70, loss = 0.88195624\n",
            "Iteration 71, loss = 0.87074817\n",
            "Iteration 72, loss = 0.91490416\n",
            "Iteration 73, loss = 0.93668358\n",
            "Iteration 74, loss = 0.89947171\n",
            "Iteration 75, loss = 0.89505145\n",
            "Iteration 76, loss = 0.90960519\n",
            "Iteration 77, loss = 0.92272522\n",
            "Iteration 78, loss = 0.91898948\n",
            "Iteration 79, loss = 0.91212064\n",
            "Iteration 80, loss = 0.88725428\n",
            "Iteration 81, loss = 0.85993021\n",
            "Iteration 82, loss = 0.84520283\n",
            "Iteration 83, loss = 0.83894634\n",
            "Iteration 84, loss = 0.83072004\n",
            "Iteration 85, loss = 0.82927744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 86, loss = 0.82507558\n",
            "Iteration 87, loss = 0.83760730\n",
            "Iteration 88, loss = 0.84807203\n",
            "Iteration 89, loss = 0.89973303\n",
            "Iteration 90, loss = 0.93246667\n",
            "Iteration 91, loss = 0.93301833\n",
            "Iteration 92, loss = 0.87697922\n",
            "Iteration 93, loss = 0.87236696\n",
            "Iteration 94, loss = 0.86503040\n",
            "Iteration 95, loss = 0.82577052\n",
            "Iteration 96, loss = 0.83143091\n",
            "Iteration 97, loss = 0.84657679\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.72278760\n",
            "Iteration 2, loss = 1.71981909\n",
            "Iteration 3, loss = 1.71674031\n",
            "Iteration 4, loss = 1.71392623\n",
            "Iteration 5, loss = 1.71094082\n",
            "Iteration 6, loss = 1.70815204\n",
            "Iteration 7, loss = 1.70524089\n",
            "Iteration 8, loss = 1.70257909\n",
            "Iteration 9, loss = 1.69981810\n",
            "Iteration 10, loss = 1.69721570\n",
            "Iteration 11, loss = 1.69451816\n",
            "Iteration 12, loss = 1.69214728\n",
            "Iteration 13, loss = 1.68963464\n",
            "Iteration 14, loss = 1.68731115\n",
            "Iteration 15, loss = 1.68496077\n",
            "Iteration 16, loss = 1.68255445\n",
            "Iteration 17, loss = 1.68038823\n",
            "Iteration 18, loss = 1.67838584\n",
            "Iteration 19, loss = 1.67617523\n",
            "Iteration 20, loss = 1.67414158\n",
            "Iteration 21, loss = 1.67229364\n",
            "Iteration 22, loss = 1.67032808\n",
            "Iteration 23, loss = 1.66835454\n",
            "Iteration 24, loss = 1.66661748\n",
            "Iteration 25, loss = 1.66476038\n",
            "Iteration 26, loss = 1.66291287\n",
            "Iteration 27, loss = 1.66124383\n",
            "Iteration 28, loss = 1.65964663\n",
            "Iteration 29, loss = 1.65790763\n",
            "Iteration 30, loss = 1.65643259\n",
            "Iteration 31, loss = 1.65478018\n",
            "Iteration 32, loss = 1.65321472\n",
            "Iteration 33, loss = 1.65168044\n",
            "Iteration 34, loss = 1.65014720\n",
            "Iteration 35, loss = 1.64867261\n",
            "Iteration 36, loss = 1.64719643\n",
            "Iteration 37, loss = 1.64580505\n",
            "Iteration 38, loss = 1.64452373\n",
            "Iteration 39, loss = 1.64309566\n",
            "Iteration 40, loss = 1.64173186\n",
            "Iteration 41, loss = 1.64043482\n",
            "Iteration 42, loss = 1.63909627\n",
            "Iteration 43, loss = 1.63787724\n",
            "Iteration 44, loss = 1.63660052\n",
            "Iteration 45, loss = 1.63532940\n",
            "Iteration 46, loss = 1.63430649\n",
            "Iteration 47, loss = 1.63309245\n",
            "Iteration 48, loss = 1.63211380\n",
            "Iteration 49, loss = 1.63083366\n",
            "Iteration 50, loss = 1.62976900\n",
            "Iteration 51, loss = 1.62865754\n",
            "Iteration 52, loss = 1.62770322\n",
            "Iteration 53, loss = 1.62667441\n",
            "Iteration 54, loss = 1.62561718\n",
            "Iteration 55, loss = 1.62472988\n",
            "Iteration 56, loss = 1.62382248\n",
            "Iteration 57, loss = 1.62277806\n",
            "Iteration 58, loss = 1.62188717\n",
            "Iteration 59, loss = 1.62098866\n",
            "Iteration 60, loss = 1.62030280\n",
            "Iteration 61, loss = 1.61935664\n",
            "Iteration 62, loss = 1.61855613\n",
            "Iteration 63, loss = 1.61782471\n",
            "Iteration 64, loss = 1.61703771\n",
            "Iteration 65, loss = 1.61621893\n",
            "Iteration 66, loss = 1.61557729\n",
            "Iteration 67, loss = 1.61488192\n",
            "Iteration 68, loss = 1.61423767\n",
            "Iteration 69, loss = 1.61348299\n",
            "Iteration 70, loss = 1.61297811\n",
            "Iteration 71, loss = 1.61229624\n",
            "Iteration 72, loss = 1.61170215\n",
            "Iteration 73, loss = 1.61107437\n",
            "Iteration 74, loss = 1.61050336\n",
            "Iteration 75, loss = 1.61000360\n",
            "Iteration 76, loss = 1.60942092\n",
            "Iteration 77, loss = 1.60900112\n",
            "Iteration 78, loss = 1.60849168\n",
            "Iteration 79, loss = 1.60794980\n",
            "Iteration 80, loss = 1.60759677\n",
            "Iteration 81, loss = 1.60706164\n",
            "Iteration 82, loss = 1.60664164\n",
            "Iteration 83, loss = 1.60627200\n",
            "Iteration 84, loss = 1.60575607\n",
            "Iteration 85, loss = 1.60542658\n",
            "Iteration 86, loss = 1.60508898\n",
            "Iteration 87, loss = 1.60480445\n",
            "Iteration 88, loss = 1.60437299\n",
            "Iteration 89, loss = 1.60405710\n",
            "Iteration 90, loss = 1.60370237\n",
            "Iteration 91, loss = 1.60344708\n",
            "Iteration 92, loss = 1.60315262\n",
            "Iteration 93, loss = 1.60283856\n",
            "Iteration 94, loss = 1.60250005\n",
            "Iteration 95, loss = 1.60232647\n",
            "Iteration 96, loss = 1.60209230\n",
            "Iteration 97, loss = 1.60180048\n",
            "Iteration 98, loss = 1.60154817\n",
            "Iteration 99, loss = 1.60132806\n",
            "Iteration 100, loss = 1.60110917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.71673152\n",
            "Iteration 2, loss = 1.68934777\n",
            "Iteration 3, loss = 1.66687497\n",
            "Iteration 4, loss = 1.65071176\n",
            "Iteration 5, loss = 1.63561128\n",
            "Iteration 6, loss = 1.62483995\n",
            "Iteration 7, loss = 1.61480580\n",
            "Iteration 8, loss = 1.60864694\n",
            "Iteration 9, loss = 1.60404636\n",
            "Iteration 10, loss = 1.60194775\n",
            "Iteration 11, loss = 1.59927963\n",
            "Iteration 12, loss = 1.59885919\n",
            "Iteration 13, loss = 1.59880121\n",
            "Iteration 14, loss = 1.59895821\n",
            "Iteration 15, loss = 1.59868132\n",
            "Iteration 16, loss = 1.59810482\n",
            "Iteration 17, loss = 1.59714860\n",
            "Iteration 18, loss = 1.59570075\n",
            "Iteration 19, loss = 1.59494771\n",
            "Iteration 20, loss = 1.59362457\n",
            "Iteration 21, loss = 1.59218013\n",
            "Iteration 22, loss = 1.59103177\n",
            "Iteration 23, loss = 1.59018605\n",
            "Iteration 24, loss = 1.58870691\n",
            "Iteration 25, loss = 1.58748907\n",
            "Iteration 26, loss = 1.58631497\n",
            "Iteration 27, loss = 1.58463810\n",
            "Iteration 28, loss = 1.58302718\n",
            "Iteration 29, loss = 1.58136554\n",
            "Iteration 30, loss = 1.57944779\n",
            "Iteration 31, loss = 1.57729265\n",
            "Iteration 32, loss = 1.57490348\n",
            "Iteration 33, loss = 1.57238901\n",
            "Iteration 34, loss = 1.56973349\n",
            "Iteration 35, loss = 1.56688348\n",
            "Iteration 36, loss = 1.56395401\n",
            "Iteration 37, loss = 1.56057561\n",
            "Iteration 38, loss = 1.55721906\n",
            "Iteration 39, loss = 1.55343277\n",
            "Iteration 40, loss = 1.54944336\n",
            "Iteration 41, loss = 1.54534809\n",
            "Iteration 42, loss = 1.54087084\n",
            "Iteration 43, loss = 1.53621544\n",
            "Iteration 44, loss = 1.53122410\n",
            "Iteration 45, loss = 1.52624353\n",
            "Iteration 46, loss = 1.52087733\n",
            "Iteration 47, loss = 1.51496405\n",
            "Iteration 48, loss = 1.50934988\n",
            "Iteration 49, loss = 1.50286453\n",
            "Iteration 50, loss = 1.49649739\n",
            "Iteration 51, loss = 1.49014028\n",
            "Iteration 52, loss = 1.48332279\n",
            "Iteration 53, loss = 1.47611225\n",
            "Iteration 54, loss = 1.46886208\n",
            "Iteration 55, loss = 1.46141878\n",
            "Iteration 56, loss = 1.45391619\n",
            "Iteration 57, loss = 1.44595873\n",
            "Iteration 58, loss = 1.43828524\n",
            "Iteration 59, loss = 1.43024210\n",
            "Iteration 60, loss = 1.42247103\n",
            "Iteration 61, loss = 1.41410870\n",
            "Iteration 62, loss = 1.40588863\n",
            "Iteration 63, loss = 1.39753813\n",
            "Iteration 64, loss = 1.38945440\n",
            "Iteration 65, loss = 1.38118675\n",
            "Iteration 66, loss = 1.37320616\n",
            "Iteration 67, loss = 1.36540527\n",
            "Iteration 68, loss = 1.35791143\n",
            "Iteration 69, loss = 1.35002894\n",
            "Iteration 70, loss = 1.34247391\n",
            "Iteration 71, loss = 1.33497040\n",
            "Iteration 72, loss = 1.32772962\n",
            "Iteration 73, loss = 1.32061292\n",
            "Iteration 74, loss = 1.31348613\n",
            "Iteration 75, loss = 1.30671764\n",
            "Iteration 76, loss = 1.30025018\n",
            "Iteration 77, loss = 1.29405311\n",
            "Iteration 78, loss = 1.28780157\n",
            "Iteration 79, loss = 1.28156692\n",
            "Iteration 80, loss = 1.27577634\n",
            "Iteration 81, loss = 1.27017017\n",
            "Iteration 82, loss = 1.26476028\n",
            "Iteration 83, loss = 1.25953952\n",
            "Iteration 84, loss = 1.25456620\n",
            "Iteration 85, loss = 1.24943196\n",
            "Iteration 86, loss = 1.24476953\n",
            "Iteration 87, loss = 1.24040879\n",
            "Iteration 88, loss = 1.23604930\n",
            "Iteration 89, loss = 1.23203802\n",
            "Iteration 90, loss = 1.22804343\n",
            "Iteration 91, loss = 1.22454926\n",
            "Iteration 92, loss = 1.22092687\n",
            "Iteration 93, loss = 1.21701978\n",
            "Iteration 94, loss = 1.21378915\n",
            "Iteration 95, loss = 1.21093486\n",
            "Iteration 96, loss = 1.20756931\n",
            "Iteration 97, loss = 1.20461726\n",
            "Iteration 98, loss = 1.20178022\n",
            "Iteration 99, loss = 1.19898823\n",
            "Iteration 100, loss = 1.19622730\n",
            "Iteration 1, loss = 1.68213051\n",
            "Iteration 2, loss = 1.60494579\n",
            "Iteration 3, loss = 1.60999609\n",
            "Iteration 4, loss = 1.59304837\n",
            "Iteration 5, loss = 1.56872440\n",
            "Iteration 6, loss = 1.56216197\n",
            "Iteration 7, loss = 1.53228306\n",
            "Iteration 8, loss = 1.49155692\n",
            "Iteration 9, loss = 1.46007116\n",
            "Iteration 10, loss = 1.41215162\n",
            "Iteration 11, loss = 1.36803943\n",
            "Iteration 12, loss = 1.32610939\n",
            "Iteration 13, loss = 1.28527462\n",
            "Iteration 14, loss = 1.24924601\n",
            "Iteration 15, loss = 1.21832084\n",
            "Iteration 16, loss = 1.19789182\n",
            "Iteration 17, loss = 1.17863918\n",
            "Iteration 18, loss = 1.16190698\n",
            "Iteration 19, loss = 1.15026468\n",
            "Iteration 20, loss = 1.14025856\n",
            "Iteration 21, loss = 1.14290777\n",
            "Iteration 22, loss = 1.12733694\n",
            "Iteration 23, loss = 1.12461645\n",
            "Iteration 24, loss = 1.12152104\n",
            "Iteration 25, loss = 1.11739727\n",
            "Iteration 26, loss = 1.11470076\n",
            "Iteration 27, loss = 1.10967008\n",
            "Iteration 28, loss = 1.11095817\n",
            "Iteration 29, loss = 1.10863717\n",
            "Iteration 30, loss = 1.10807679\n",
            "Iteration 31, loss = 1.10603944\n",
            "Iteration 32, loss = 1.10800967\n",
            "Iteration 33, loss = 1.10709533\n",
            "Iteration 34, loss = 1.10891348\n",
            "Iteration 35, loss = 1.10327088\n",
            "Iteration 36, loss = 1.10311065\n",
            "Iteration 37, loss = 1.09991492\n",
            "Iteration 38, loss = 1.10093977\n",
            "Iteration 39, loss = 1.10066928\n",
            "Iteration 40, loss = 1.09883180\n",
            "Iteration 41, loss = 1.10029208\n",
            "Iteration 42, loss = 1.09785761\n",
            "Iteration 43, loss = 1.09601431\n",
            "Iteration 44, loss = 1.09589665\n",
            "Iteration 45, loss = 1.10383984\n",
            "Iteration 46, loss = 1.10055955\n",
            "Iteration 47, loss = 1.09723680\n",
            "Iteration 48, loss = 1.09748664\n",
            "Iteration 49, loss = 1.09564986\n",
            "Iteration 50, loss = 1.09399550\n",
            "Iteration 51, loss = 1.10681859\n",
            "Iteration 52, loss = 1.09515814\n",
            "Iteration 53, loss = 1.10005197\n",
            "Iteration 54, loss = 1.09507666\n",
            "Iteration 55, loss = 1.09326992\n",
            "Iteration 56, loss = 1.10039805\n",
            "Iteration 57, loss = 1.09040557\n",
            "Iteration 58, loss = 1.09876635\n",
            "Iteration 59, loss = 1.09009378\n",
            "Iteration 60, loss = 1.09373952\n",
            "Iteration 61, loss = 1.09782076\n",
            "Iteration 62, loss = 1.09785545\n",
            "Iteration 63, loss = 1.08831032\n",
            "Iteration 64, loss = 1.08878737\n",
            "Iteration 65, loss = 1.08462024\n",
            "Iteration 66, loss = 1.08866621\n",
            "Iteration 67, loss = 1.08558920\n",
            "Iteration 68, loss = 1.08653178\n",
            "Iteration 69, loss = 1.08171439\n",
            "Iteration 70, loss = 1.08409209\n",
            "Iteration 71, loss = 1.08197464\n",
            "Iteration 72, loss = 1.08298399\n",
            "Iteration 73, loss = 1.07955187\n",
            "Iteration 74, loss = 1.08034171\n",
            "Iteration 75, loss = 1.07558777\n",
            "Iteration 76, loss = 1.07779621\n",
            "Iteration 77, loss = 1.07588992\n",
            "Iteration 78, loss = 1.07457720\n",
            "Iteration 79, loss = 1.07646489\n",
            "Iteration 80, loss = 1.07576482\n",
            "Iteration 81, loss = 1.07264054\n",
            "Iteration 82, loss = 1.07293080\n",
            "Iteration 83, loss = 1.07183093\n",
            "Iteration 84, loss = 1.07117176\n",
            "Iteration 85, loss = 1.06805746\n",
            "Iteration 86, loss = 1.06423078\n",
            "Iteration 87, loss = 1.06929166\n",
            "Iteration 88, loss = 1.06473284\n",
            "Iteration 89, loss = 1.06127095\n",
            "Iteration 90, loss = 1.05963297\n",
            "Iteration 91, loss = 1.06341606\n",
            "Iteration 92, loss = 1.06030745\n",
            "Iteration 93, loss = 1.05268328\n",
            "Iteration 94, loss = 1.05662187\n",
            "Iteration 95, loss = 1.04802243\n",
            "Iteration 96, loss = 1.05557346\n",
            "Iteration 97, loss = 1.04852226\n",
            "Iteration 98, loss = 1.05003151\n",
            "Iteration 99, loss = 1.04337615\n",
            "Iteration 100, loss = 1.04275574\n",
            "Iteration 1, loss = 1.63828142\n",
            "Iteration 2, loss = 1.63627610\n",
            "Iteration 3, loss = 1.63401808\n",
            "Iteration 4, loss = 1.63209046\n",
            "Iteration 5, loss = 1.63018408\n",
            "Iteration 6, loss = 1.62824012\n",
            "Iteration 7, loss = 1.62619591\n",
            "Iteration 8, loss = 1.62439188\n",
            "Iteration 9, loss = 1.62289384\n",
            "Iteration 10, loss = 1.62098578\n",
            "Iteration 11, loss = 1.61956194\n",
            "Iteration 12, loss = 1.61805085\n",
            "Iteration 13, loss = 1.61662134\n",
            "Iteration 14, loss = 1.61524139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 1.61398448\n",
            "Iteration 16, loss = 1.61266386\n",
            "Iteration 17, loss = 1.61130837\n",
            "Iteration 18, loss = 1.61037361\n",
            "Iteration 19, loss = 1.60918362\n",
            "Iteration 20, loss = 1.60824909\n",
            "Iteration 21, loss = 1.60721221\n",
            "Iteration 22, loss = 1.60643802\n",
            "Iteration 23, loss = 1.60555112\n",
            "Iteration 24, loss = 1.60465042\n",
            "Iteration 25, loss = 1.60389519\n",
            "Iteration 26, loss = 1.60327067\n",
            "Iteration 27, loss = 1.60243552\n",
            "Iteration 28, loss = 1.60181201\n",
            "Iteration 29, loss = 1.60129955\n",
            "Iteration 30, loss = 1.60061356\n",
            "Iteration 31, loss = 1.60013827\n",
            "Iteration 32, loss = 1.59956540\n",
            "Iteration 33, loss = 1.59905626\n",
            "Iteration 34, loss = 1.59849913\n",
            "Iteration 35, loss = 1.59801387\n",
            "Iteration 36, loss = 1.59761930\n",
            "Iteration 37, loss = 1.59716368\n",
            "Iteration 38, loss = 1.59678271\n",
            "Iteration 39, loss = 1.59628363\n",
            "Iteration 40, loss = 1.59589044\n",
            "Iteration 41, loss = 1.59559313\n",
            "Iteration 42, loss = 1.59514172\n",
            "Iteration 43, loss = 1.59475067\n",
            "Iteration 44, loss = 1.59438391\n",
            "Iteration 45, loss = 1.59402809\n",
            "Iteration 46, loss = 1.59368499\n",
            "Iteration 47, loss = 1.59331815\n",
            "Iteration 48, loss = 1.59302616\n",
            "Iteration 49, loss = 1.59260225\n",
            "Iteration 50, loss = 1.59230457\n",
            "Iteration 51, loss = 1.59191608\n",
            "Iteration 52, loss = 1.59158275\n",
            "Iteration 53, loss = 1.59124376\n",
            "Iteration 54, loss = 1.59095442\n",
            "Iteration 55, loss = 1.59059007\n",
            "Iteration 56, loss = 1.59026667\n",
            "Iteration 57, loss = 1.58993438\n",
            "Iteration 58, loss = 1.58961706\n",
            "Iteration 59, loss = 1.58926999\n",
            "Iteration 60, loss = 1.58899845\n",
            "Iteration 61, loss = 1.58862327\n",
            "Iteration 62, loss = 1.58829467\n",
            "Iteration 63, loss = 1.58795608\n",
            "Iteration 64, loss = 1.58766351\n",
            "Iteration 65, loss = 1.58728464\n",
            "Iteration 66, loss = 1.58695992\n",
            "Iteration 67, loss = 1.58661928\n",
            "Iteration 68, loss = 1.58630604\n",
            "Iteration 69, loss = 1.58597460\n",
            "Iteration 70, loss = 1.58564056\n",
            "Iteration 71, loss = 1.58530673\n",
            "Iteration 72, loss = 1.58496104\n",
            "Iteration 73, loss = 1.58461142\n",
            "Iteration 74, loss = 1.58427841\n",
            "Iteration 75, loss = 1.58393609\n",
            "Iteration 76, loss = 1.58360589\n",
            "Iteration 77, loss = 1.58323571\n",
            "Iteration 78, loss = 1.58288911\n",
            "Iteration 79, loss = 1.58253044\n",
            "Iteration 80, loss = 1.58220997\n",
            "Iteration 81, loss = 1.58182887\n",
            "Iteration 82, loss = 1.58146345\n",
            "Iteration 83, loss = 1.58112521\n",
            "Iteration 84, loss = 1.58074981\n",
            "Iteration 85, loss = 1.58037656\n",
            "Iteration 86, loss = 1.58003482\n",
            "Iteration 87, loss = 1.57964162\n",
            "Iteration 88, loss = 1.57925811\n",
            "Iteration 89, loss = 1.57888077\n",
            "Iteration 90, loss = 1.57850746\n",
            "Iteration 91, loss = 1.57812343\n",
            "Iteration 92, loss = 1.57773771\n",
            "Iteration 93, loss = 1.57732536\n",
            "Iteration 94, loss = 1.57693937\n",
            "Iteration 95, loss = 1.57653018\n",
            "Iteration 96, loss = 1.57615530\n",
            "Iteration 97, loss = 1.57574126\n",
            "Iteration 98, loss = 1.57532176\n",
            "Iteration 99, loss = 1.57490136\n",
            "Iteration 100, loss = 1.57449701\n",
            "Iteration 1, loss = 1.63338764\n",
            "Iteration 2, loss = 1.61705634\n",
            "Iteration 3, loss = 1.60371771\n",
            "Iteration 4, loss = 1.59818117\n",
            "Iteration 5, loss = 1.59601815\n",
            "Iteration 6, loss = 1.59407554\n",
            "Iteration 7, loss = 1.59192466\n",
            "Iteration 8, loss = 1.58872076\n",
            "Iteration 9, loss = 1.58430808\n",
            "Iteration 10, loss = 1.58158539\n",
            "Iteration 11, loss = 1.57741886\n",
            "Iteration 12, loss = 1.57405522\n",
            "Iteration 13, loss = 1.57055674\n",
            "Iteration 14, loss = 1.56696915\n",
            "Iteration 15, loss = 1.56329011\n",
            "Iteration 16, loss = 1.55883851\n",
            "Iteration 17, loss = 1.55412278\n",
            "Iteration 18, loss = 1.54882755\n",
            "Iteration 19, loss = 1.54347104\n",
            "Iteration 20, loss = 1.53722187\n",
            "Iteration 21, loss = 1.53051491\n",
            "Iteration 22, loss = 1.52365252\n",
            "Iteration 23, loss = 1.51592938\n",
            "Iteration 24, loss = 1.50751574\n",
            "Iteration 25, loss = 1.49906918\n",
            "Iteration 26, loss = 1.48914872\n",
            "Iteration 27, loss = 1.47949288\n",
            "Iteration 28, loss = 1.46857371\n",
            "Iteration 29, loss = 1.45812327\n",
            "Iteration 30, loss = 1.44628677\n",
            "Iteration 31, loss = 1.43431319\n",
            "Iteration 32, loss = 1.42222386\n",
            "Iteration 33, loss = 1.40983370\n",
            "Iteration 34, loss = 1.39732011\n",
            "Iteration 35, loss = 1.38395137\n",
            "Iteration 36, loss = 1.37079809\n",
            "Iteration 37, loss = 1.35806915\n",
            "Iteration 38, loss = 1.34532959\n",
            "Iteration 39, loss = 1.33258119\n",
            "Iteration 40, loss = 1.32027741\n",
            "Iteration 41, loss = 1.30877046\n",
            "Iteration 42, loss = 1.29684796\n",
            "Iteration 43, loss = 1.28586160\n",
            "Iteration 44, loss = 1.27574535\n",
            "Iteration 45, loss = 1.26556173\n",
            "Iteration 46, loss = 1.25576044\n",
            "Iteration 47, loss = 1.24685516\n",
            "Iteration 48, loss = 1.23883993\n",
            "Iteration 49, loss = 1.23114833\n",
            "Iteration 50, loss = 1.22323263\n",
            "Iteration 51, loss = 1.21643457\n",
            "Iteration 52, loss = 1.20990203\n",
            "Iteration 53, loss = 1.20360292\n",
            "Iteration 54, loss = 1.19987903\n",
            "Iteration 55, loss = 1.19320349\n",
            "Iteration 56, loss = 1.18791421\n",
            "Iteration 57, loss = 1.18339816\n",
            "Iteration 58, loss = 1.17982325\n",
            "Iteration 59, loss = 1.17582552\n",
            "Iteration 60, loss = 1.17172303\n",
            "Iteration 61, loss = 1.16784694\n",
            "Iteration 62, loss = 1.16508795\n",
            "Iteration 63, loss = 1.16138166\n",
            "Iteration 64, loss = 1.15939928\n",
            "Iteration 65, loss = 1.15604326\n",
            "Iteration 66, loss = 1.15310051\n",
            "Iteration 67, loss = 1.15040734\n",
            "Iteration 68, loss = 1.14827222\n",
            "Iteration 69, loss = 1.14591302\n",
            "Iteration 70, loss = 1.14385642\n",
            "Iteration 71, loss = 1.14192386\n",
            "Iteration 72, loss = 1.13964143\n",
            "Iteration 73, loss = 1.13775075\n",
            "Iteration 74, loss = 1.13599175\n",
            "Iteration 75, loss = 1.13507081\n",
            "Iteration 76, loss = 1.13274885\n",
            "Iteration 77, loss = 1.13113395\n",
            "Iteration 78, loss = 1.12975400\n",
            "Iteration 79, loss = 1.12818377\n",
            "Iteration 80, loss = 1.12766133\n",
            "Iteration 81, loss = 1.12542476\n",
            "Iteration 82, loss = 1.12428211\n",
            "Iteration 83, loss = 1.12297080\n",
            "Iteration 84, loss = 1.12169180\n",
            "Iteration 85, loss = 1.12029743\n",
            "Iteration 86, loss = 1.11987380\n",
            "Iteration 87, loss = 1.11853762\n",
            "Iteration 88, loss = 1.11711642\n",
            "Iteration 89, loss = 1.11590736\n",
            "Iteration 90, loss = 1.11502902\n",
            "Iteration 91, loss = 1.11413578\n",
            "Iteration 92, loss = 1.11314539\n",
            "Iteration 93, loss = 1.11197343\n",
            "Iteration 94, loss = 1.11124943\n",
            "Iteration 95, loss = 1.11034443\n",
            "Iteration 96, loss = 1.10972565\n",
            "Iteration 97, loss = 1.10893656\n",
            "Iteration 98, loss = 1.10778321\n",
            "Iteration 99, loss = 1.10658342\n",
            "Iteration 100, loss = 1.10622984\n",
            "Iteration 1, loss = 1.61459544\n",
            "Iteration 2, loss = 1.60590949\n",
            "Iteration 3, loss = 1.56366695\n",
            "Iteration 4, loss = 1.53121203\n",
            "Iteration 5, loss = 1.45939694\n",
            "Iteration 6, loss = 1.39083817\n",
            "Iteration 7, loss = 1.32508036\n",
            "Iteration 8, loss = 1.25817575\n",
            "Iteration 9, loss = 1.20675273\n",
            "Iteration 10, loss = 1.17156577\n",
            "Iteration 11, loss = 1.15124016\n",
            "Iteration 12, loss = 1.13636197\n",
            "Iteration 13, loss = 1.12527513\n",
            "Iteration 14, loss = 1.11997428\n",
            "Iteration 15, loss = 1.12197281\n",
            "Iteration 16, loss = 1.11944012\n",
            "Iteration 17, loss = 1.12256365\n",
            "Iteration 18, loss = 1.11032201\n",
            "Iteration 19, loss = 1.12046539\n",
            "Iteration 20, loss = 1.10618966\n",
            "Iteration 21, loss = 1.10514217\n",
            "Iteration 22, loss = 1.10752149\n",
            "Iteration 23, loss = 1.10291556\n",
            "Iteration 24, loss = 1.10060905\n",
            "Iteration 25, loss = 1.10023309\n",
            "Iteration 26, loss = 1.09657420\n",
            "Iteration 27, loss = 1.09190071\n",
            "Iteration 28, loss = 1.08657366\n",
            "Iteration 29, loss = 1.09313606\n",
            "Iteration 30, loss = 1.10343817\n",
            "Iteration 31, loss = 1.07565996\n",
            "Iteration 32, loss = 1.07737498\n",
            "Iteration 33, loss = 1.08303980\n",
            "Iteration 34, loss = 1.08024989\n",
            "Iteration 35, loss = 1.06587061\n",
            "Iteration 36, loss = 1.06291564\n",
            "Iteration 37, loss = 1.06516238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 1.05958375\n",
            "Iteration 39, loss = 1.06328396\n",
            "Iteration 40, loss = 1.03370305\n",
            "Iteration 41, loss = 1.03226049\n",
            "Iteration 42, loss = 1.05011248\n",
            "Iteration 43, loss = 1.04040757\n",
            "Iteration 44, loss = 1.02553845\n",
            "Iteration 45, loss = 1.01100286\n",
            "Iteration 46, loss = 1.01057418\n",
            "Iteration 47, loss = 1.00248599\n",
            "Iteration 48, loss = 1.00390741\n",
            "Iteration 49, loss = 1.00542923\n",
            "Iteration 50, loss = 1.00663583\n",
            "Iteration 51, loss = 0.99718136\n",
            "Iteration 52, loss = 0.96831355\n",
            "Iteration 53, loss = 0.96636445\n",
            "Iteration 54, loss = 0.96509835\n",
            "Iteration 55, loss = 0.97847152\n",
            "Iteration 56, loss = 0.95574044\n",
            "Iteration 57, loss = 0.92812210\n",
            "Iteration 58, loss = 0.93687872\n",
            "Iteration 59, loss = 0.92270128\n",
            "Iteration 60, loss = 0.92788801\n",
            "Iteration 61, loss = 0.95532373\n",
            "Iteration 62, loss = 0.92460549\n",
            "Iteration 63, loss = 0.91432641\n",
            "Iteration 64, loss = 0.95090841\n",
            "Iteration 65, loss = 0.90991189\n",
            "Iteration 66, loss = 0.90321395\n",
            "Iteration 67, loss = 0.90330048\n",
            "Iteration 68, loss = 0.89348384\n",
            "Iteration 69, loss = 0.89050585\n",
            "Iteration 70, loss = 0.87981560\n",
            "Iteration 71, loss = 0.88697218\n",
            "Iteration 72, loss = 0.88680495\n",
            "Iteration 73, loss = 0.87680112\n",
            "Iteration 74, loss = 0.86934425\n",
            "Iteration 75, loss = 0.85901822\n",
            "Iteration 76, loss = 0.87323915\n",
            "Iteration 77, loss = 0.87938891\n",
            "Iteration 78, loss = 0.84702626\n",
            "Iteration 79, loss = 0.84028681\n",
            "Iteration 80, loss = 0.87869570\n",
            "Iteration 81, loss = 0.86277713\n",
            "Iteration 82, loss = 0.87983908\n",
            "Iteration 83, loss = 0.85008810\n",
            "Iteration 84, loss = 0.83869764\n",
            "Iteration 85, loss = 0.82630689\n",
            "Iteration 86, loss = 0.83336474\n",
            "Iteration 87, loss = 0.90633792\n",
            "Iteration 88, loss = 0.84414817\n",
            "Iteration 89, loss = 0.83300678\n",
            "Iteration 90, loss = 0.84032731\n",
            "Iteration 91, loss = 0.86745949\n",
            "Iteration 92, loss = 0.82363595\n",
            "Iteration 93, loss = 0.84743700\n",
            "Iteration 94, loss = 0.87627624\n",
            "Iteration 95, loss = 0.88526806\n",
            "Iteration 96, loss = 0.88260073\n",
            "Iteration 97, loss = 0.84242706\n",
            "Iteration 98, loss = 0.85402604\n",
            "Iteration 99, loss = 0.86142060\n",
            "Iteration 100, loss = 0.90265567\n",
            "Iteration 1, loss = 1.68650388\n",
            "Iteration 2, loss = 1.68307552\n",
            "Iteration 3, loss = 1.67955034\n",
            "Iteration 4, loss = 1.67650348\n",
            "Iteration 5, loss = 1.67354588\n",
            "Iteration 6, loss = 1.67044893\n",
            "Iteration 7, loss = 1.66753803\n",
            "Iteration 8, loss = 1.66465109\n",
            "Iteration 9, loss = 1.66191416\n",
            "Iteration 10, loss = 1.65908455\n",
            "Iteration 11, loss = 1.65664408\n",
            "Iteration 12, loss = 1.65424502\n",
            "Iteration 13, loss = 1.65197016\n",
            "Iteration 14, loss = 1.64970172\n",
            "Iteration 15, loss = 1.64767718\n",
            "Iteration 16, loss = 1.64538593\n",
            "Iteration 17, loss = 1.64337861\n",
            "Iteration 18, loss = 1.64157363\n",
            "Iteration 19, loss = 1.63976111\n",
            "Iteration 20, loss = 1.63782382\n",
            "Iteration 21, loss = 1.63622445\n",
            "Iteration 22, loss = 1.63459252\n",
            "Iteration 23, loss = 1.63307914\n",
            "Iteration 24, loss = 1.63160785\n",
            "Iteration 25, loss = 1.63000998\n",
            "Iteration 26, loss = 1.62872561\n",
            "Iteration 27, loss = 1.62732515\n",
            "Iteration 28, loss = 1.62614508\n",
            "Iteration 29, loss = 1.62514416\n",
            "Iteration 30, loss = 1.62371169\n",
            "Iteration 31, loss = 1.62272894\n",
            "Iteration 32, loss = 1.62163108\n",
            "Iteration 33, loss = 1.62054926\n",
            "Iteration 34, loss = 1.61955808\n",
            "Iteration 35, loss = 1.61856665\n",
            "Iteration 36, loss = 1.61765854\n",
            "Iteration 37, loss = 1.61682625\n",
            "Iteration 38, loss = 1.61597445\n",
            "Iteration 39, loss = 1.61512200\n",
            "Iteration 40, loss = 1.61436181\n",
            "Iteration 41, loss = 1.61358193\n",
            "Iteration 42, loss = 1.61284574\n",
            "Iteration 43, loss = 1.61214264\n",
            "Iteration 44, loss = 1.61151593\n",
            "Iteration 45, loss = 1.61088708\n",
            "Iteration 46, loss = 1.61037377\n",
            "Iteration 47, loss = 1.60969729\n",
            "Iteration 48, loss = 1.60914345\n",
            "Iteration 49, loss = 1.60857769\n",
            "Iteration 50, loss = 1.60809926\n",
            "Iteration 51, loss = 1.60755228\n",
            "Iteration 52, loss = 1.60708170\n",
            "Iteration 53, loss = 1.60658637\n",
            "Iteration 54, loss = 1.60610490\n",
            "Iteration 55, loss = 1.60575750\n",
            "Iteration 56, loss = 1.60533121\n",
            "Iteration 57, loss = 1.60490627\n",
            "Iteration 58, loss = 1.60462512\n",
            "Iteration 59, loss = 1.60418867\n",
            "Iteration 60, loss = 1.60398390\n",
            "Iteration 61, loss = 1.60357934\n",
            "Iteration 62, loss = 1.60320442\n",
            "Iteration 63, loss = 1.60290473\n",
            "Iteration 64, loss = 1.60263689\n",
            "Iteration 65, loss = 1.60233526\n",
            "Iteration 66, loss = 1.60207753\n",
            "Iteration 67, loss = 1.60182988\n",
            "Iteration 68, loss = 1.60159729\n",
            "Iteration 69, loss = 1.60141121\n",
            "Iteration 70, loss = 1.60117470\n",
            "Iteration 71, loss = 1.60090858\n",
            "Iteration 72, loss = 1.60072819\n",
            "Iteration 73, loss = 1.60051596\n",
            "Iteration 74, loss = 1.60036300\n",
            "Iteration 75, loss = 1.60018602\n",
            "Iteration 76, loss = 1.59999059\n",
            "Iteration 77, loss = 1.59980853\n",
            "Iteration 78, loss = 1.59970019\n",
            "Iteration 79, loss = 1.59952915\n",
            "Iteration 80, loss = 1.59940415\n",
            "Iteration 81, loss = 1.59923992\n",
            "Iteration 82, loss = 1.59910073\n",
            "Iteration 83, loss = 1.59903247\n",
            "Iteration 84, loss = 1.59883758\n",
            "Iteration 85, loss = 1.59876747\n",
            "Iteration 86, loss = 1.59864090\n",
            "Iteration 87, loss = 1.59854153\n",
            "Iteration 88, loss = 1.59839100\n",
            "Iteration 89, loss = 1.59833101\n",
            "Iteration 90, loss = 1.59821545\n",
            "Iteration 91, loss = 1.59815403\n",
            "Iteration 92, loss = 1.59803749\n",
            "Iteration 93, loss = 1.59795055\n",
            "Iteration 94, loss = 1.59787637\n",
            "Iteration 95, loss = 1.59779641\n",
            "Iteration 96, loss = 1.59772198\n",
            "Iteration 97, loss = 1.59765276\n",
            "Iteration 98, loss = 1.59754004\n",
            "Iteration 99, loss = 1.59749828\n",
            "Iteration 100, loss = 1.59741874\n",
            "Iteration 1, loss = 1.67935821\n",
            "Iteration 2, loss = 1.65183396\n",
            "Iteration 3, loss = 1.63046761\n",
            "Iteration 4, loss = 1.61863378\n",
            "Iteration 5, loss = 1.61104797\n",
            "Iteration 6, loss = 1.60474975\n",
            "Iteration 7, loss = 1.60215026\n",
            "Iteration 8, loss = 1.60038739\n",
            "Iteration 9, loss = 1.59868929\n",
            "Iteration 10, loss = 1.60019612\n",
            "Iteration 11, loss = 1.59998565\n",
            "Iteration 12, loss = 1.60035330\n",
            "Iteration 13, loss = 1.60017107\n",
            "Iteration 14, loss = 1.59952510\n",
            "Iteration 15, loss = 1.59882112\n",
            "Iteration 16, loss = 1.59750504\n",
            "Iteration 17, loss = 1.59650302\n",
            "Iteration 18, loss = 1.59570626\n",
            "Iteration 19, loss = 1.59465864\n",
            "Iteration 20, loss = 1.59441544\n",
            "Iteration 21, loss = 1.59384795\n",
            "Iteration 22, loss = 1.59357805\n",
            "Iteration 23, loss = 1.59309091\n",
            "Iteration 24, loss = 1.59260716\n",
            "Iteration 25, loss = 1.59229629\n",
            "Iteration 26, loss = 1.59143981\n",
            "Iteration 27, loss = 1.59080601\n",
            "Iteration 28, loss = 1.59002717\n",
            "Iteration 29, loss = 1.58959582\n",
            "Iteration 30, loss = 1.58826760\n",
            "Iteration 31, loss = 1.58760582\n",
            "Iteration 32, loss = 1.58664408\n",
            "Iteration 33, loss = 1.58584963\n",
            "Iteration 34, loss = 1.58503119\n",
            "Iteration 35, loss = 1.58379748\n",
            "Iteration 36, loss = 1.58249514\n",
            "Iteration 37, loss = 1.58125313\n",
            "Iteration 38, loss = 1.58004141\n",
            "Iteration 39, loss = 1.57872012\n",
            "Iteration 40, loss = 1.57724859\n",
            "Iteration 41, loss = 1.57590520\n",
            "Iteration 42, loss = 1.57396304\n",
            "Iteration 43, loss = 1.57236943\n",
            "Iteration 44, loss = 1.57042354\n",
            "Iteration 45, loss = 1.56856563\n",
            "Iteration 46, loss = 1.56638032\n",
            "Iteration 47, loss = 1.56417730\n",
            "Iteration 48, loss = 1.56217003\n",
            "Iteration 49, loss = 1.55974859\n",
            "Iteration 50, loss = 1.55712477\n",
            "Iteration 51, loss = 1.55447554\n",
            "Iteration 52, loss = 1.55155141\n",
            "Iteration 53, loss = 1.54855381\n",
            "Iteration 54, loss = 1.54645416\n",
            "Iteration 55, loss = 1.54257770\n",
            "Iteration 56, loss = 1.53945904\n",
            "Iteration 57, loss = 1.53610804\n",
            "Iteration 58, loss = 1.53256304\n",
            "Iteration 59, loss = 1.52878632\n",
            "Iteration 60, loss = 1.52547021\n",
            "Iteration 61, loss = 1.52122064\n",
            "Iteration 62, loss = 1.51716867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 63, loss = 1.51299205\n",
            "Iteration 64, loss = 1.50916979\n",
            "Iteration 65, loss = 1.50428483\n",
            "Iteration 66, loss = 1.50002333\n",
            "Iteration 67, loss = 1.49528313\n",
            "Iteration 68, loss = 1.49099265\n",
            "Iteration 69, loss = 1.48589989\n",
            "Iteration 70, loss = 1.48110944\n",
            "Iteration 71, loss = 1.47607238\n",
            "Iteration 72, loss = 1.47099547\n",
            "Iteration 73, loss = 1.46567285\n",
            "Iteration 74, loss = 1.46050936\n",
            "Iteration 75, loss = 1.45512028\n",
            "Iteration 76, loss = 1.44992137\n",
            "Iteration 77, loss = 1.44417002\n",
            "Iteration 78, loss = 1.43866383\n",
            "Iteration 79, loss = 1.43307932\n",
            "Iteration 80, loss = 1.42787691\n",
            "Iteration 81, loss = 1.42196847\n",
            "Iteration 82, loss = 1.41605681\n",
            "Iteration 83, loss = 1.41076303\n",
            "Iteration 84, loss = 1.40487387\n",
            "Iteration 85, loss = 1.39906819\n",
            "Iteration 86, loss = 1.39383360\n",
            "Iteration 87, loss = 1.38787802\n",
            "Iteration 88, loss = 1.38207677\n",
            "Iteration 89, loss = 1.37655295\n",
            "Iteration 90, loss = 1.37110097\n",
            "Iteration 91, loss = 1.36573832\n",
            "Iteration 92, loss = 1.36023414\n",
            "Iteration 93, loss = 1.35476311\n",
            "Iteration 94, loss = 1.34953357\n",
            "Iteration 95, loss = 1.34404956\n",
            "Iteration 96, loss = 1.33919260\n",
            "Iteration 97, loss = 1.33406640\n",
            "Iteration 98, loss = 1.32894021\n",
            "Iteration 99, loss = 1.32387835\n",
            "Iteration 100, loss = 1.31933789\n",
            "Iteration 1, loss = 1.66027240\n",
            "Iteration 2, loss = 1.63382365\n",
            "Iteration 3, loss = 1.63092591\n",
            "Iteration 4, loss = 1.60648406\n",
            "Iteration 5, loss = 1.59722384\n",
            "Iteration 6, loss = 1.60240048\n",
            "Iteration 7, loss = 1.59716958\n",
            "Iteration 8, loss = 1.58700201\n",
            "Iteration 9, loss = 1.57700754\n",
            "Iteration 10, loss = 1.57279707\n",
            "Iteration 11, loss = 1.55871986\n",
            "Iteration 12, loss = 1.54172706\n",
            "Iteration 13, loss = 1.52206186\n",
            "Iteration 14, loss = 1.49442189\n",
            "Iteration 15, loss = 1.45851440\n",
            "Iteration 16, loss = 1.42561736\n",
            "Iteration 17, loss = 1.38401958\n",
            "Iteration 18, loss = 1.34420296\n",
            "Iteration 19, loss = 1.30967197\n",
            "Iteration 20, loss = 1.27515349\n",
            "Iteration 21, loss = 1.24481773\n",
            "Iteration 22, loss = 1.22203330\n",
            "Iteration 23, loss = 1.20417101\n",
            "Iteration 24, loss = 1.18547472\n",
            "Iteration 25, loss = 1.17149785\n",
            "Iteration 26, loss = 1.16417976\n",
            "Iteration 27, loss = 1.15581521\n",
            "Iteration 28, loss = 1.14817274\n",
            "Iteration 29, loss = 1.14757594\n",
            "Iteration 30, loss = 1.14020555\n",
            "Iteration 31, loss = 1.13382802\n",
            "Iteration 32, loss = 1.12829028\n",
            "Iteration 33, loss = 1.12779189\n",
            "Iteration 34, loss = 1.12735841\n",
            "Iteration 35, loss = 1.12169146\n",
            "Iteration 36, loss = 1.12105170\n",
            "Iteration 37, loss = 1.11822949\n",
            "Iteration 38, loss = 1.11434208\n",
            "Iteration 39, loss = 1.11691155\n",
            "Iteration 40, loss = 1.11504920\n",
            "Iteration 41, loss = 1.11798585\n",
            "Iteration 42, loss = 1.11293119\n",
            "Iteration 43, loss = 1.11084532\n",
            "Iteration 44, loss = 1.11852847\n",
            "Iteration 45, loss = 1.10851538\n",
            "Iteration 46, loss = 1.11225280\n",
            "Iteration 47, loss = 1.11141109\n",
            "Iteration 48, loss = 1.11388968\n",
            "Iteration 49, loss = 1.11566188\n",
            "Iteration 50, loss = 1.11570369\n",
            "Iteration 51, loss = 1.10374914\n",
            "Iteration 52, loss = 1.10634226\n",
            "Iteration 53, loss = 1.10099164\n",
            "Iteration 54, loss = 1.11293305\n",
            "Iteration 55, loss = 1.10441291\n",
            "Iteration 56, loss = 1.11087948\n",
            "Iteration 57, loss = 1.10435968\n",
            "Iteration 58, loss = 1.10366621\n",
            "Iteration 59, loss = 1.09891953\n",
            "Iteration 60, loss = 1.09898784\n",
            "Iteration 61, loss = 1.09283898\n",
            "Iteration 62, loss = 1.10124085\n",
            "Iteration 63, loss = 1.09054442\n",
            "Iteration 64, loss = 1.09872708\n",
            "Iteration 65, loss = 1.09402126\n",
            "Iteration 66, loss = 1.09075606\n",
            "Iteration 67, loss = 1.08733500\n",
            "Iteration 68, loss = 1.08937444\n",
            "Iteration 69, loss = 1.08484365\n",
            "Iteration 70, loss = 1.08416988\n",
            "Iteration 71, loss = 1.08310726\n",
            "Iteration 72, loss = 1.07951182\n",
            "Iteration 73, loss = 1.07896665\n",
            "Iteration 74, loss = 1.07731843\n",
            "Iteration 75, loss = 1.07893627\n",
            "Iteration 76, loss = 1.07316467\n",
            "Iteration 77, loss = 1.07047997\n",
            "Iteration 78, loss = 1.07152460\n",
            "Iteration 79, loss = 1.06717374\n",
            "Iteration 80, loss = 1.06873887\n",
            "Iteration 81, loss = 1.06510763\n",
            "Iteration 82, loss = 1.05896977\n",
            "Iteration 83, loss = 1.06442931\n",
            "Iteration 84, loss = 1.06058687\n",
            "Iteration 85, loss = 1.05762242\n",
            "Iteration 86, loss = 1.05937556\n",
            "Iteration 87, loss = 1.04900830\n",
            "Iteration 88, loss = 1.05588668\n",
            "Iteration 89, loss = 1.04495984\n",
            "Iteration 90, loss = 1.04513504\n",
            "Iteration 91, loss = 1.04435550\n",
            "Iteration 92, loss = 1.04272014\n",
            "Iteration 93, loss = 1.03611024\n",
            "Iteration 94, loss = 1.03788797\n",
            "Iteration 95, loss = 1.03252329\n",
            "Iteration 96, loss = 1.03653062\n",
            "Iteration 97, loss = 1.02911341\n",
            "Iteration 98, loss = 1.02457211\n",
            "Iteration 99, loss = 1.02352284\n",
            "Iteration 100, loss = 1.02212480\n",
            "Iteration 1, loss = 1.63713906\n",
            "Iteration 2, loss = 1.63522771\n",
            "Iteration 3, loss = 1.63306807\n",
            "Iteration 4, loss = 1.63122957\n",
            "Iteration 5, loss = 1.62940697\n",
            "Iteration 6, loss = 1.62754353\n",
            "Iteration 7, loss = 1.62558345\n",
            "Iteration 8, loss = 1.62385037\n",
            "Iteration 9, loss = 1.62240677\n",
            "Iteration 10, loss = 1.62056986\n",
            "Iteration 11, loss = 1.61919320\n",
            "Iteration 12, loss = 1.61773362\n",
            "Iteration 13, loss = 1.61635010\n",
            "Iteration 14, loss = 1.61501134\n",
            "Iteration 15, loss = 1.61379262\n",
            "Iteration 16, loss = 1.61250765\n",
            "Iteration 17, loss = 1.61118801\n",
            "Iteration 18, loss = 1.61027496\n",
            "Iteration 19, loss = 1.60911535\n",
            "Iteration 20, loss = 1.60819999\n",
            "Iteration 21, loss = 1.60718508\n",
            "Iteration 22, loss = 1.60642591\n",
            "Iteration 23, loss = 1.60555771\n",
            "Iteration 24, loss = 1.60467308\n",
            "Iteration 25, loss = 1.60393113\n",
            "Iteration 26, loss = 1.60331673\n",
            "Iteration 27, loss = 1.60249569\n",
            "Iteration 28, loss = 1.60188275\n",
            "Iteration 29, loss = 1.60137756\n",
            "Iteration 30, loss = 1.60070373\n",
            "Iteration 31, loss = 1.60023769\n",
            "Iteration 32, loss = 1.59967393\n",
            "Iteration 33, loss = 1.59917445\n",
            "Iteration 34, loss = 1.59862568\n",
            "Iteration 35, loss = 1.59815119\n",
            "Iteration 36, loss = 1.59776728\n",
            "Iteration 37, loss = 1.59732179\n",
            "Iteration 38, loss = 1.59695106\n",
            "Iteration 39, loss = 1.59646303\n",
            "Iteration 40, loss = 1.59608155\n",
            "Iteration 41, loss = 1.59579600\n",
            "Iteration 42, loss = 1.59535718\n",
            "Iteration 43, loss = 1.59497909\n",
            "Iteration 44, loss = 1.59462595\n",
            "Iteration 45, loss = 1.59428222\n",
            "Iteration 46, loss = 1.59395472\n",
            "Iteration 47, loss = 1.59360219\n",
            "Iteration 48, loss = 1.59332501\n",
            "Iteration 49, loss = 1.59291480\n",
            "Iteration 50, loss = 1.59263531\n",
            "Iteration 51, loss = 1.59226209\n",
            "Iteration 52, loss = 1.59194522\n",
            "Iteration 53, loss = 1.59162280\n",
            "Iteration 54, loss = 1.59134425\n",
            "Iteration 55, loss = 1.59100275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 56, loss = 1.59069582\n",
            "Iteration 57, loss = 1.59038030\n",
            "Iteration 58, loss = 1.59008265\n",
            "Iteration 59, loss = 1.58975204\n",
            "Iteration 60, loss = 1.58949731\n",
            "Iteration 61, loss = 1.58914205\n",
            "Iteration 62, loss = 1.58883260\n",
            "Iteration 63, loss = 1.58851230\n",
            "Iteration 64, loss = 1.58823762\n",
            "Iteration 65, loss = 1.58787884\n",
            "Iteration 66, loss = 1.58757226\n",
            "Iteration 67, loss = 1.58725140\n",
            "Iteration 68, loss = 1.58695585\n",
            "Iteration 69, loss = 1.58664556\n",
            "Iteration 70, loss = 1.58633123\n",
            "Iteration 71, loss = 1.58601717\n",
            "Iteration 72, loss = 1.58569188\n",
            "Iteration 73, loss = 1.58536282\n",
            "Iteration 74, loss = 1.58504946\n",
            "Iteration 75, loss = 1.58472844\n",
            "Iteration 76, loss = 1.58441921\n",
            "Iteration 77, loss = 1.58407056\n",
            "Iteration 78, loss = 1.58374462\n",
            "Iteration 79, loss = 1.58340860\n",
            "Iteration 80, loss = 1.58310949\n",
            "Iteration 81, loss = 1.58275032\n",
            "Iteration 82, loss = 1.58240765\n",
            "Iteration 83, loss = 1.58209142\n",
            "Iteration 84, loss = 1.58173906\n",
            "Iteration 85, loss = 1.58138869\n",
            "Iteration 86, loss = 1.58106991\n",
            "Iteration 87, loss = 1.58070165\n",
            "Iteration 88, loss = 1.58034238\n",
            "Iteration 89, loss = 1.57998863\n",
            "Iteration 90, loss = 1.57963999\n",
            "Iteration 91, loss = 1.57927977\n",
            "Iteration 92, loss = 1.57892030\n",
            "Iteration 93, loss = 1.57853489\n",
            "Iteration 94, loss = 1.57817415\n",
            "Iteration 95, loss = 1.57779163\n",
            "Iteration 96, loss = 1.57744298\n",
            "Iteration 97, loss = 1.57705544\n",
            "Iteration 98, loss = 1.57666420\n",
            "Iteration 99, loss = 1.57627143\n",
            "Iteration 100, loss = 1.57589425\n",
            "Iteration 1, loss = 1.63243812\n",
            "Iteration 2, loss = 1.61676444\n",
            "Iteration 3, loss = 1.60375716\n",
            "Iteration 4, loss = 1.59825067\n",
            "Iteration 5, loss = 1.59601163\n",
            "Iteration 6, loss = 1.59412950\n",
            "Iteration 7, loss = 1.59216726\n",
            "Iteration 8, loss = 1.58927991\n",
            "Iteration 9, loss = 1.58519446\n",
            "Iteration 10, loss = 1.58259165\n",
            "Iteration 11, loss = 1.57856399\n",
            "Iteration 12, loss = 1.57528612\n",
            "Iteration 13, loss = 1.57188483\n",
            "Iteration 14, loss = 1.56845771\n",
            "Iteration 15, loss = 1.56493096\n",
            "Iteration 16, loss = 1.56074543\n",
            "Iteration 17, loss = 1.55629081\n",
            "Iteration 18, loss = 1.55119708\n",
            "Iteration 19, loss = 1.54607294\n",
            "Iteration 20, loss = 1.54005682\n",
            "Iteration 21, loss = 1.53360136\n",
            "Iteration 22, loss = 1.52696709\n",
            "Iteration 23, loss = 1.51951826\n",
            "Iteration 24, loss = 1.51137936\n",
            "Iteration 25, loss = 1.50314516\n",
            "Iteration 26, loss = 1.49348867\n",
            "Iteration 27, loss = 1.48401402\n",
            "Iteration 28, loss = 1.47327479\n",
            "Iteration 29, loss = 1.46288728\n",
            "Iteration 30, loss = 1.45116911\n",
            "Iteration 31, loss = 1.43921991\n",
            "Iteration 32, loss = 1.42711984\n",
            "Iteration 33, loss = 1.41462970\n",
            "Iteration 34, loss = 1.40200950\n",
            "Iteration 35, loss = 1.38848059\n",
            "Iteration 36, loss = 1.37511832\n",
            "Iteration 37, loss = 1.36214224\n",
            "Iteration 38, loss = 1.34911463\n",
            "Iteration 39, loss = 1.33604654\n",
            "Iteration 40, loss = 1.32344098\n",
            "Iteration 41, loss = 1.31159876\n",
            "Iteration 42, loss = 1.29937038\n",
            "Iteration 43, loss = 1.28803782\n",
            "Iteration 44, loss = 1.27761717\n",
            "Iteration 45, loss = 1.26716709\n",
            "Iteration 46, loss = 1.25708291\n",
            "Iteration 47, loss = 1.24792877\n",
            "Iteration 48, loss = 1.23965489\n",
            "Iteration 49, loss = 1.23172901\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 50, loss = 1.22370860\n",
            "Iteration 51, loss = 1.21672605\n",
            "Iteration 52, loss = 1.21009628\n",
            "Iteration 53, loss = 1.20370758\n",
            "Iteration 54, loss = 1.19973250\n",
            "Iteration 55, loss = 1.19310913\n",
            "Iteration 56, loss = 1.18778054\n",
            "Iteration 57, loss = 1.18321713\n",
            "Iteration 58, loss = 1.17959238\n",
            "Iteration 59, loss = 1.17562903\n",
            "Iteration 60, loss = 1.17145469\n",
            "Iteration 61, loss = 1.16765556\n",
            "Iteration 62, loss = 1.16487737\n",
            "Iteration 63, loss = 1.16121723\n",
            "Iteration 64, loss = 1.15921106\n",
            "Iteration 65, loss = 1.15595356\n",
            "Iteration 66, loss = 1.15303873\n",
            "Iteration 67, loss = 1.15038943\n",
            "Iteration 68, loss = 1.14822134\n",
            "Iteration 69, loss = 1.14595198\n",
            "Iteration 70, loss = 1.14390649\n",
            "Iteration 71, loss = 1.14200772\n",
            "Iteration 72, loss = 1.13981498\n",
            "Iteration 73, loss = 1.13795636\n",
            "Iteration 74, loss = 1.13620292\n",
            "Iteration 75, loss = 1.13531244\n",
            "Iteration 76, loss = 1.13299468\n",
            "Iteration 77, loss = 1.13143619\n",
            "Iteration 78, loss = 1.13006297\n",
            "Iteration 79, loss = 1.12854410\n",
            "Iteration 80, loss = 1.12793346\n",
            "Iteration 81, loss = 1.12575845\n",
            "Iteration 82, loss = 1.12465683\n",
            "Iteration 83, loss = 1.12330546\n",
            "Iteration 84, loss = 1.12204745\n",
            "Iteration 85, loss = 1.12066832\n",
            "Iteration 86, loss = 1.12020520\n",
            "Iteration 87, loss = 1.11891439\n",
            "Iteration 88, loss = 1.11747749\n",
            "Iteration 89, loss = 1.11627534\n",
            "Iteration 90, loss = 1.11538350\n",
            "Iteration 91, loss = 1.11446651\n",
            "Iteration 92, loss = 1.11344384\n",
            "Iteration 93, loss = 1.11225041\n",
            "Iteration 94, loss = 1.11145807\n",
            "Iteration 95, loss = 1.11058711\n",
            "Iteration 96, loss = 1.10992108\n",
            "Iteration 97, loss = 1.10904957\n",
            "Iteration 98, loss = 1.10790992\n",
            "Iteration 99, loss = 1.10668500\n",
            "Iteration 100, loss = 1.10623998\n",
            "Iteration 1, loss = 1.61365523\n",
            "Iteration 2, loss = 1.60549670\n",
            "Iteration 3, loss = 1.56398460\n",
            "Iteration 4, loss = 1.53072133\n",
            "Iteration 5, loss = 1.45932820\n",
            "Iteration 6, loss = 1.38887048\n",
            "Iteration 7, loss = 1.32310490\n",
            "Iteration 8, loss = 1.25650970\n",
            "Iteration 9, loss = 1.20493325\n",
            "Iteration 10, loss = 1.16989472\n",
            "Iteration 11, loss = 1.15007075\n",
            "Iteration 12, loss = 1.13603359\n",
            "Iteration 13, loss = 1.12511655\n",
            "Iteration 14, loss = 1.11949445\n",
            "Iteration 15, loss = 1.12071315\n",
            "Iteration 16, loss = 1.11765277\n",
            "Iteration 17, loss = 1.12088135\n",
            "Iteration 18, loss = 1.10823277\n",
            "Iteration 19, loss = 1.11688666\n",
            "Iteration 20, loss = 1.10449896\n",
            "Iteration 21, loss = 1.10260798\n",
            "Iteration 22, loss = 1.10598330\n",
            "Iteration 23, loss = 1.10048552\n",
            "Iteration 24, loss = 1.09846090\n",
            "Iteration 25, loss = 1.09828079\n",
            "Iteration 26, loss = 1.09484341\n",
            "Iteration 27, loss = 1.09085578\n",
            "Iteration 28, loss = 1.08554024\n",
            "Iteration 29, loss = 1.09502751\n",
            "Iteration 30, loss = 1.10472029\n",
            "Iteration 31, loss = 1.07754755\n",
            "Iteration 32, loss = 1.08291953\n",
            "Iteration 33, loss = 1.08779642\n",
            "Iteration 34, loss = 1.07660824\n",
            "Iteration 35, loss = 1.06819913\n",
            "Iteration 36, loss = 1.07281604\n",
            "Iteration 37, loss = 1.06916142\n",
            "Iteration 38, loss = 1.05896930\n",
            "Iteration 39, loss = 1.05699470\n",
            "Iteration 40, loss = 1.03902951\n",
            "Iteration 41, loss = 1.04958743\n",
            "Iteration 42, loss = 1.04514450\n",
            "Iteration 43, loss = 1.03654681\n",
            "Iteration 44, loss = 1.02655431\n",
            "Iteration 45, loss = 1.01937712\n",
            "Iteration 46, loss = 1.01514151\n",
            "Iteration 47, loss = 1.00055936\n",
            "Iteration 48, loss = 1.02270356\n",
            "Iteration 49, loss = 1.00098002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 50, loss = 1.00645817\n",
            "Iteration 51, loss = 0.99994655\n",
            "Iteration 52, loss = 0.97016975\n",
            "Iteration 53, loss = 0.98207339\n",
            "Iteration 54, loss = 0.98831748\n",
            "Iteration 55, loss = 0.98220377\n",
            "Iteration 56, loss = 0.95680113\n",
            "Iteration 57, loss = 0.95267434\n",
            "Iteration 58, loss = 0.94673316\n",
            "Iteration 59, loss = 0.93784544\n",
            "Iteration 60, loss = 0.96396767\n",
            "Iteration 61, loss = 0.93727970\n",
            "Iteration 62, loss = 0.92917428\n",
            "Iteration 63, loss = 0.94363679\n",
            "Iteration 64, loss = 0.91718538\n",
            "Iteration 65, loss = 0.90871228\n",
            "Iteration 66, loss = 0.90058886\n",
            "Iteration 67, loss = 0.88662786\n",
            "Iteration 68, loss = 0.88920399\n",
            "Iteration 69, loss = 0.88339549\n",
            "Iteration 70, loss = 0.90057418\n",
            "Iteration 71, loss = 0.87023989\n",
            "Iteration 72, loss = 0.89332071\n",
            "Iteration 73, loss = 0.88317198\n",
            "Iteration 74, loss = 0.87295064\n",
            "Iteration 75, loss = 0.85983442\n",
            "Iteration 76, loss = 0.85537654\n",
            "Iteration 77, loss = 0.86086252\n",
            "Iteration 78, loss = 0.87532602\n",
            "Iteration 79, loss = 0.85383374\n",
            "Iteration 80, loss = 0.84861780\n",
            "Iteration 81, loss = 0.84626227\n",
            "Iteration 82, loss = 0.86932765\n",
            "Iteration 83, loss = 0.84938263\n",
            "Iteration 84, loss = 0.84584640\n",
            "Iteration 85, loss = 0.84957654\n",
            "Iteration 86, loss = 0.88661976\n",
            "Iteration 87, loss = 0.88522930\n",
            "Iteration 88, loss = 0.82555578\n",
            "Iteration 89, loss = 0.81631510\n",
            "Iteration 90, loss = 0.83400543\n",
            "Iteration 91, loss = 0.83067252\n",
            "Iteration 92, loss = 0.81717046\n",
            "Iteration 93, loss = 0.82683687\n",
            "Iteration 94, loss = 0.85178219\n",
            "Iteration 95, loss = 0.86519561\n",
            "Iteration 96, loss = 0.86938067\n",
            "Iteration 97, loss = 0.81983803\n",
            "Iteration 98, loss = 0.82511655\n",
            "Iteration 99, loss = 0.81709734\n",
            "Iteration 100, loss = 0.84242908\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.64639474\n",
            "Iteration 2, loss = 1.64559723\n",
            "Iteration 3, loss = 1.64479788\n",
            "Iteration 4, loss = 1.64407168\n",
            "Iteration 5, loss = 1.64336991\n",
            "Iteration 6, loss = 1.64258859\n",
            "Iteration 7, loss = 1.64188858\n",
            "Iteration 8, loss = 1.64112910\n",
            "Iteration 9, loss = 1.64035704\n",
            "Iteration 10, loss = 1.63963033\n",
            "Iteration 11, loss = 1.63889842\n",
            "Iteration 12, loss = 1.63819859\n",
            "Iteration 13, loss = 1.63749500\n",
            "Iteration 14, loss = 1.63676580\n",
            "Iteration 15, loss = 1.63612036\n",
            "Iteration 16, loss = 1.63541104\n",
            "Iteration 17, loss = 1.63480746\n",
            "Iteration 18, loss = 1.63424932\n",
            "Iteration 19, loss = 1.63372976\n",
            "Iteration 20, loss = 1.63320938\n",
            "Iteration 21, loss = 1.63278720\n",
            "Iteration 22, loss = 1.63237300\n",
            "Iteration 23, loss = 1.63196752\n",
            "Iteration 24, loss = 1.63154483\n",
            "Iteration 25, loss = 1.63110471\n",
            "Iteration 26, loss = 1.63073105\n",
            "Iteration 27, loss = 1.63029518\n",
            "Iteration 28, loss = 1.62991989\n",
            "Iteration 29, loss = 1.62957983\n",
            "Iteration 30, loss = 1.62913614\n",
            "Iteration 31, loss = 1.62880225\n",
            "Iteration 32, loss = 1.62840953\n",
            "Iteration 33, loss = 1.62803278\n",
            "Iteration 34, loss = 1.62763958\n",
            "Iteration 35, loss = 1.62727884\n",
            "Iteration 36, loss = 1.62694360\n",
            "Iteration 37, loss = 1.62659142\n",
            "Iteration 38, loss = 1.62624754\n",
            "Iteration 39, loss = 1.62587155\n",
            "Iteration 40, loss = 1.62553595\n",
            "Iteration 41, loss = 1.62522424\n",
            "Iteration 42, loss = 1.62486495\n",
            "Iteration 43, loss = 1.62453016\n",
            "Iteration 44, loss = 1.62420383\n",
            "Iteration 45, loss = 1.62386545\n",
            "Iteration 46, loss = 1.62358212\n",
            "Iteration 47, loss = 1.62324449\n",
            "Iteration 48, loss = 1.62295173\n",
            "Iteration 49, loss = 1.62259674\n",
            "Iteration 50, loss = 1.62233032\n",
            "Iteration 51, loss = 1.62199447\n",
            "Iteration 52, loss = 1.62168846\n",
            "Iteration 53, loss = 1.62139186\n",
            "Iteration 54, loss = 1.62106438\n",
            "Iteration 55, loss = 1.62080020\n",
            "Iteration 56, loss = 1.62052232\n",
            "Iteration 57, loss = 1.62022791\n",
            "Iteration 58, loss = 1.61997737\n",
            "Iteration 59, loss = 1.61964880\n",
            "Iteration 60, loss = 1.61939458\n",
            "Iteration 61, loss = 1.61910849\n",
            "Iteration 62, loss = 1.61884548\n",
            "Iteration 63, loss = 1.61857820\n",
            "Iteration 64, loss = 1.61833499\n",
            "Iteration 65, loss = 1.61803547\n",
            "Iteration 66, loss = 1.61775839\n",
            "Iteration 67, loss = 1.61749929\n",
            "Iteration 68, loss = 1.61723664\n",
            "Iteration 69, loss = 1.61701084\n",
            "Iteration 70, loss = 1.61674801\n",
            "Iteration 71, loss = 1.61646808\n",
            "Iteration 72, loss = 1.61623906\n",
            "Iteration 73, loss = 1.61600090\n",
            "Iteration 74, loss = 1.61574162\n",
            "Iteration 75, loss = 1.61550380\n",
            "Iteration 76, loss = 1.61525540\n",
            "Iteration 77, loss = 1.61503348\n",
            "Iteration 78, loss = 1.61480932\n",
            "Iteration 79, loss = 1.61454406\n",
            "Iteration 80, loss = 1.61429684\n",
            "Iteration 81, loss = 1.61407610\n",
            "Iteration 82, loss = 1.61387442\n",
            "Iteration 83, loss = 1.61364092\n",
            "Iteration 84, loss = 1.61339277\n",
            "Iteration 85, loss = 1.61319649\n",
            "Iteration 86, loss = 1.61292463\n",
            "Iteration 87, loss = 1.61272412\n",
            "Iteration 88, loss = 1.61251132\n",
            "Iteration 89, loss = 1.61231422\n",
            "Iteration 90, loss = 1.61210096\n",
            "Iteration 91, loss = 1.61189373\n",
            "Iteration 92, loss = 1.61168508\n",
            "Iteration 93, loss = 1.61143989\n",
            "Iteration 94, loss = 1.61123070\n",
            "Iteration 95, loss = 1.61103976\n",
            "Iteration 96, loss = 1.61080960\n",
            "Iteration 97, loss = 1.61063895\n",
            "Iteration 98, loss = 1.61041596\n",
            "Iteration 99, loss = 1.61022267\n",
            "Iteration 100, loss = 1.61004310\n",
            "Iteration 1, loss = 1.64501199\n",
            "Iteration 2, loss = 1.63738192\n",
            "Iteration 3, loss = 1.63139558\n",
            "Iteration 4, loss = 1.62777902\n",
            "Iteration 5, loss = 1.62450307\n",
            "Iteration 6, loss = 1.62117715\n",
            "Iteration 7, loss = 1.61813986\n",
            "Iteration 8, loss = 1.61529803\n",
            "Iteration 9, loss = 1.61293225\n",
            "Iteration 10, loss = 1.61020363\n",
            "Iteration 11, loss = 1.60829279\n",
            "Iteration 12, loss = 1.60644640\n",
            "Iteration 13, loss = 1.60479726\n",
            "Iteration 14, loss = 1.60328145\n",
            "Iteration 15, loss = 1.60215819\n",
            "Iteration 16, loss = 1.60089126\n",
            "Iteration 17, loss = 1.59967928\n",
            "Iteration 18, loss = 1.59886156\n",
            "Iteration 19, loss = 1.59811041\n",
            "Iteration 20, loss = 1.59705443\n",
            "Iteration 21, loss = 1.59623996\n",
            "Iteration 22, loss = 1.59552535\n",
            "Iteration 23, loss = 1.59466348\n",
            "Iteration 24, loss = 1.59371220\n",
            "Iteration 25, loss = 1.59262714\n",
            "Iteration 26, loss = 1.59070350\n",
            "Iteration 27, loss = 1.58941989\n",
            "Iteration 28, loss = 1.58754511\n",
            "Iteration 29, loss = 1.58562042\n",
            "Iteration 30, loss = 1.58285058\n",
            "Iteration 31, loss = 1.57994297\n",
            "Iteration 32, loss = 1.57652156\n",
            "Iteration 33, loss = 1.57255230\n",
            "Iteration 34, loss = 1.56834977\n",
            "Iteration 35, loss = 1.56361057\n",
            "Iteration 36, loss = 1.55821223\n",
            "Iteration 37, loss = 1.55254287\n",
            "Iteration 38, loss = 1.54647081\n",
            "Iteration 39, loss = 1.53977884\n",
            "Iteration 40, loss = 1.53298368\n",
            "Iteration 41, loss = 1.52612779\n",
            "Iteration 42, loss = 1.51810528\n",
            "Iteration 43, loss = 1.51018557\n",
            "Iteration 44, loss = 1.50211157\n",
            "Iteration 45, loss = 1.49407660\n",
            "Iteration 46, loss = 1.48509651\n",
            "Iteration 47, loss = 1.47666207\n",
            "Iteration 48, loss = 1.46806918\n",
            "Iteration 49, loss = 1.45902031\n",
            "Iteration 50, loss = 1.44977920\n",
            "Iteration 51, loss = 1.44087197\n",
            "Iteration 52, loss = 1.43165670\n",
            "Iteration 53, loss = 1.42284651\n",
            "Iteration 54, loss = 1.41485954\n",
            "Iteration 55, loss = 1.40563079\n",
            "Iteration 56, loss = 1.39683386\n",
            "Iteration 57, loss = 1.38845873\n",
            "Iteration 58, loss = 1.37999605\n",
            "Iteration 59, loss = 1.37236783\n",
            "Iteration 60, loss = 1.36444293\n",
            "Iteration 61, loss = 1.35657664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 62, loss = 1.34883637\n",
            "Iteration 63, loss = 1.34165957\n",
            "Iteration 64, loss = 1.33506304\n",
            "Iteration 65, loss = 1.32805745\n",
            "Iteration 66, loss = 1.32148903\n",
            "Iteration 67, loss = 1.31511320\n",
            "Iteration 68, loss = 1.30917765\n",
            "Iteration 69, loss = 1.30312818\n",
            "Iteration 70, loss = 1.29749199\n",
            "Iteration 71, loss = 1.29205300\n",
            "Iteration 72, loss = 1.28700444\n",
            "Iteration 73, loss = 1.28181231\n",
            "Iteration 74, loss = 1.27694258\n",
            "Iteration 75, loss = 1.27254552\n",
            "Iteration 76, loss = 1.26801624\n",
            "Iteration 77, loss = 1.26348690\n",
            "Iteration 78, loss = 1.25937181\n",
            "Iteration 79, loss = 1.25545943\n",
            "Iteration 80, loss = 1.25171593\n",
            "Iteration 81, loss = 1.24794994\n",
            "Iteration 82, loss = 1.24437318\n",
            "Iteration 83, loss = 1.24107989\n",
            "Iteration 84, loss = 1.23776371\n",
            "Iteration 85, loss = 1.23444919\n",
            "Iteration 86, loss = 1.23177362\n",
            "Iteration 87, loss = 1.22863820\n",
            "Iteration 88, loss = 1.22570686\n",
            "Iteration 89, loss = 1.22291290\n",
            "Iteration 90, loss = 1.22037411\n",
            "Iteration 91, loss = 1.21791001\n",
            "Iteration 92, loss = 1.21538787\n",
            "Iteration 93, loss = 1.21294503\n",
            "Iteration 94, loss = 1.21074295\n",
            "Iteration 95, loss = 1.20851239\n",
            "Iteration 96, loss = 1.20653630\n",
            "Iteration 97, loss = 1.20439595\n",
            "Iteration 98, loss = 1.20240022\n",
            "Iteration 99, loss = 1.20010604\n",
            "Iteration 100, loss = 1.19822429\n",
            "Iteration 1, loss = 1.63612698\n",
            "Iteration 2, loss = 1.60904965\n",
            "Iteration 3, loss = 1.59820927\n",
            "Iteration 4, loss = 1.60126441\n",
            "Iteration 5, loss = 1.60231218\n",
            "Iteration 6, loss = 1.59580561\n",
            "Iteration 7, loss = 1.58540719\n",
            "Iteration 8, loss = 1.57132102\n",
            "Iteration 9, loss = 1.54939589\n",
            "Iteration 10, loss = 1.52344882\n",
            "Iteration 11, loss = 1.48856014\n",
            "Iteration 12, loss = 1.45126005\n",
            "Iteration 13, loss = 1.40927723\n",
            "Iteration 14, loss = 1.36777089\n",
            "Iteration 15, loss = 1.33063031\n",
            "Iteration 16, loss = 1.29802859\n",
            "Iteration 17, loss = 1.26319768\n",
            "Iteration 18, loss = 1.24290561\n",
            "Iteration 19, loss = 1.21879158\n",
            "Iteration 20, loss = 1.20432798\n",
            "Iteration 21, loss = 1.18943528\n",
            "Iteration 22, loss = 1.17954448\n",
            "Iteration 23, loss = 1.16945582\n",
            "Iteration 24, loss = 1.15968485\n",
            "Iteration 25, loss = 1.15334667\n",
            "Iteration 26, loss = 1.14693753\n",
            "Iteration 27, loss = 1.14259134\n",
            "Iteration 28, loss = 1.13731373\n",
            "Iteration 29, loss = 1.13583710\n",
            "Iteration 30, loss = 1.13357364\n",
            "Iteration 31, loss = 1.12736415\n",
            "Iteration 32, loss = 1.12381448\n",
            "Iteration 33, loss = 1.12398708\n",
            "Iteration 34, loss = 1.12226670\n",
            "Iteration 35, loss = 1.12037567\n",
            "Iteration 36, loss = 1.12027242\n",
            "Iteration 37, loss = 1.11551865\n",
            "Iteration 38, loss = 1.11240023\n",
            "Iteration 39, loss = 1.11172483\n",
            "Iteration 40, loss = 1.11137710\n",
            "Iteration 41, loss = 1.10968975\n",
            "Iteration 42, loss = 1.10893607\n",
            "Iteration 43, loss = 1.10806792\n",
            "Iteration 44, loss = 1.11369219\n",
            "Iteration 45, loss = 1.10615668\n",
            "Iteration 46, loss = 1.11067841\n",
            "Iteration 47, loss = 1.11201055\n",
            "Iteration 48, loss = 1.10918163\n",
            "Iteration 49, loss = 1.11658280\n",
            "Iteration 50, loss = 1.11018315\n",
            "Iteration 51, loss = 1.10908988\n",
            "Iteration 52, loss = 1.10394212\n",
            "Iteration 53, loss = 1.10526464\n",
            "Iteration 54, loss = 1.10513793\n",
            "Iteration 55, loss = 1.10651580\n",
            "Iteration 56, loss = 1.10737406\n",
            "Iteration 57, loss = 1.10327177\n",
            "Iteration 58, loss = 1.10652394\n",
            "Iteration 59, loss = 1.10210243\n",
            "Iteration 60, loss = 1.10219177\n",
            "Iteration 61, loss = 1.10080519\n",
            "Iteration 62, loss = 1.10319563\n",
            "Iteration 63, loss = 1.09998678\n",
            "Iteration 64, loss = 1.10546266\n",
            "Iteration 65, loss = 1.10691579\n",
            "Iteration 66, loss = 1.10299193\n",
            "Iteration 67, loss = 1.09962272\n",
            "Iteration 68, loss = 1.10367811\n",
            "Iteration 69, loss = 1.10506315\n",
            "Iteration 70, loss = 1.10320262\n",
            "Iteration 71, loss = 1.10052381\n",
            "Iteration 72, loss = 1.10130983\n",
            "Iteration 73, loss = 1.09799430\n",
            "Iteration 74, loss = 1.09908982\n",
            "Iteration 75, loss = 1.10399389\n",
            "Iteration 76, loss = 1.09802425\n",
            "Iteration 77, loss = 1.09937073\n",
            "Iteration 78, loss = 1.09961512\n",
            "Iteration 79, loss = 1.09873767\n",
            "Iteration 80, loss = 1.09764253\n",
            "Iteration 81, loss = 1.09847863\n",
            "Iteration 82, loss = 1.09621866\n",
            "Iteration 83, loss = 1.09904472\n",
            "Iteration 84, loss = 1.09972545\n",
            "Iteration 85, loss = 1.09771927\n",
            "Iteration 86, loss = 1.09636718\n",
            "Iteration 87, loss = 1.09985201\n",
            "Iteration 88, loss = 1.09727194\n",
            "Iteration 89, loss = 1.09859885\n",
            "Iteration 90, loss = 1.09647340\n",
            "Iteration 91, loss = 1.09824608\n",
            "Iteration 92, loss = 1.09772628\n",
            "Iteration 93, loss = 1.09490882\n",
            "Iteration 94, loss = 1.09999867\n",
            "Iteration 95, loss = 1.09482313\n",
            "Iteration 96, loss = 1.09946264\n",
            "Iteration 97, loss = 1.09418905\n",
            "Iteration 98, loss = 1.10265608\n",
            "Iteration 99, loss = 1.09490077\n",
            "Iteration 100, loss = 1.10567347\n",
            "Iteration 1, loss = 1.66511662\n",
            "Iteration 2, loss = 1.66140265\n",
            "Iteration 3, loss = 1.65789005\n",
            "Iteration 4, loss = 1.65426088\n",
            "Iteration 5, loss = 1.65132232\n",
            "Iteration 6, loss = 1.64822979\n",
            "Iteration 7, loss = 1.64499658\n",
            "Iteration 8, loss = 1.64198682\n",
            "Iteration 9, loss = 1.63969040\n",
            "Iteration 10, loss = 1.63701228\n",
            "Iteration 11, loss = 1.63448962\n",
            "Iteration 12, loss = 1.63199778\n",
            "Iteration 13, loss = 1.63009705\n",
            "Iteration 14, loss = 1.62781950\n",
            "Iteration 15, loss = 1.62591680\n",
            "Iteration 16, loss = 1.62379773\n",
            "Iteration 17, loss = 1.62213254\n",
            "Iteration 18, loss = 1.62040801\n",
            "Iteration 19, loss = 1.61864865\n",
            "Iteration 20, loss = 1.61714791\n",
            "Iteration 21, loss = 1.61572269\n",
            "Iteration 22, loss = 1.61435818\n",
            "Iteration 23, loss = 1.61300944\n",
            "Iteration 24, loss = 1.61166598\n",
            "Iteration 25, loss = 1.61054218\n",
            "Iteration 26, loss = 1.60934386\n",
            "Iteration 27, loss = 1.60855396\n",
            "Iteration 28, loss = 1.60735407\n",
            "Iteration 29, loss = 1.60645455\n",
            "Iteration 30, loss = 1.60558785\n",
            "Iteration 31, loss = 1.60470499\n",
            "Iteration 32, loss = 1.60405289\n",
            "Iteration 33, loss = 1.60333666\n",
            "Iteration 34, loss = 1.60238945\n",
            "Iteration 35, loss = 1.60179300\n",
            "Iteration 36, loss = 1.60113174\n",
            "Iteration 37, loss = 1.60053096\n",
            "Iteration 38, loss = 1.60003618\n",
            "Iteration 39, loss = 1.59934734\n",
            "Iteration 40, loss = 1.59885993\n",
            "Iteration 41, loss = 1.59842385\n",
            "Iteration 42, loss = 1.59789075\n",
            "Iteration 43, loss = 1.59741419\n",
            "Iteration 44, loss = 1.59699209\n",
            "Iteration 45, loss = 1.59653207\n",
            "Iteration 46, loss = 1.59617608\n",
            "Iteration 47, loss = 1.59584293\n",
            "Iteration 48, loss = 1.59542639\n",
            "Iteration 49, loss = 1.59506314\n",
            "Iteration 50, loss = 1.59468122\n",
            "Iteration 51, loss = 1.59431474\n",
            "Iteration 52, loss = 1.59398866\n",
            "Iteration 53, loss = 1.59367414\n",
            "Iteration 54, loss = 1.59338813\n",
            "Iteration 55, loss = 1.59304747\n",
            "Iteration 56, loss = 1.59274067\n",
            "Iteration 57, loss = 1.59247272\n",
            "Iteration 58, loss = 1.59218293\n",
            "Iteration 59, loss = 1.59188946\n",
            "Iteration 60, loss = 1.59160685\n",
            "Iteration 61, loss = 1.59129917\n",
            "Iteration 62, loss = 1.59103349\n",
            "Iteration 63, loss = 1.59076215\n",
            "Iteration 64, loss = 1.59050966\n",
            "Iteration 65, loss = 1.59021652\n",
            "Iteration 66, loss = 1.58992701\n",
            "Iteration 67, loss = 1.58968991\n",
            "Iteration 68, loss = 1.58935877\n",
            "Iteration 69, loss = 1.58908231\n",
            "Iteration 70, loss = 1.58882289\n",
            "Iteration 71, loss = 1.58853904\n",
            "Iteration 72, loss = 1.58827376\n",
            "Iteration 73, loss = 1.58796435\n",
            "Iteration 74, loss = 1.58769190\n",
            "Iteration 75, loss = 1.58741744\n",
            "Iteration 76, loss = 1.58712903\n",
            "Iteration 77, loss = 1.58681950\n",
            "Iteration 78, loss = 1.58653919\n",
            "Iteration 79, loss = 1.58624636\n",
            "Iteration 80, loss = 1.58592990\n",
            "Iteration 81, loss = 1.58562092\n",
            "Iteration 82, loss = 1.58538461\n",
            "Iteration 83, loss = 1.58502987\n",
            "Iteration 84, loss = 1.58471456\n",
            "Iteration 85, loss = 1.58439683\n",
            "Iteration 86, loss = 1.58409777\n",
            "Iteration 87, loss = 1.58375816\n",
            "Iteration 88, loss = 1.58343941\n",
            "Iteration 89, loss = 1.58310582\n",
            "Iteration 90, loss = 1.58277790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 91, loss = 1.58247391\n",
            "Iteration 92, loss = 1.58208977\n",
            "Iteration 93, loss = 1.58174470\n",
            "Iteration 94, loss = 1.58141360\n",
            "Iteration 95, loss = 1.58106460\n",
            "Iteration 96, loss = 1.58071613\n",
            "Iteration 97, loss = 1.58035580\n",
            "Iteration 98, loss = 1.57997802\n",
            "Iteration 99, loss = 1.57962416\n",
            "Iteration 100, loss = 1.57928169\n",
            "Iteration 1, loss = 1.65866941\n",
            "Iteration 2, loss = 1.63002745\n",
            "Iteration 3, loss = 1.61201825\n",
            "Iteration 4, loss = 1.60046634\n",
            "Iteration 5, loss = 1.59823775\n",
            "Iteration 6, loss = 1.59687743\n",
            "Iteration 7, loss = 1.59567313\n",
            "Iteration 8, loss = 1.59444529\n",
            "Iteration 9, loss = 1.59230364\n",
            "Iteration 10, loss = 1.58949887\n",
            "Iteration 11, loss = 1.58615667\n",
            "Iteration 12, loss = 1.58241464\n",
            "Iteration 13, loss = 1.57783266\n",
            "Iteration 14, loss = 1.57462655\n",
            "Iteration 15, loss = 1.57100903\n",
            "Iteration 16, loss = 1.56767568\n",
            "Iteration 17, loss = 1.56340704\n",
            "Iteration 18, loss = 1.55861213\n",
            "Iteration 19, loss = 1.55387448\n",
            "Iteration 20, loss = 1.54811360\n",
            "Iteration 21, loss = 1.54257418\n",
            "Iteration 22, loss = 1.53644218\n",
            "Iteration 23, loss = 1.52979050\n",
            "Iteration 24, loss = 1.52260692\n",
            "Iteration 25, loss = 1.51541939\n",
            "Iteration 26, loss = 1.50771248\n",
            "Iteration 27, loss = 1.49969503\n",
            "Iteration 28, loss = 1.49129336\n",
            "Iteration 29, loss = 1.48193329\n",
            "Iteration 30, loss = 1.47284289\n",
            "Iteration 31, loss = 1.46341011\n",
            "Iteration 32, loss = 1.45404394\n",
            "Iteration 33, loss = 1.44417309\n",
            "Iteration 34, loss = 1.43381575\n",
            "Iteration 35, loss = 1.42337010\n",
            "Iteration 36, loss = 1.41269702\n",
            "Iteration 37, loss = 1.40252767\n",
            "Iteration 38, loss = 1.39170474\n",
            "Iteration 39, loss = 1.38085890\n",
            "Iteration 40, loss = 1.37019042\n",
            "Iteration 41, loss = 1.36014128\n",
            "Iteration 42, loss = 1.34958315\n",
            "Iteration 43, loss = 1.33946409\n",
            "Iteration 44, loss = 1.32950553\n",
            "Iteration 45, loss = 1.32015775\n",
            "Iteration 46, loss = 1.31055384\n",
            "Iteration 47, loss = 1.30156859\n",
            "Iteration 48, loss = 1.29292723\n",
            "Iteration 49, loss = 1.28483652\n",
            "Iteration 50, loss = 1.27632230\n",
            "Iteration 51, loss = 1.26887634\n",
            "Iteration 52, loss = 1.26147042\n",
            "Iteration 53, loss = 1.25458701\n",
            "Iteration 54, loss = 1.24795171\n",
            "Iteration 55, loss = 1.24169560\n",
            "Iteration 56, loss = 1.23567802\n",
            "Iteration 57, loss = 1.23016534\n",
            "Iteration 58, loss = 1.22498279\n",
            "Iteration 59, loss = 1.22007036\n",
            "Iteration 60, loss = 1.21519684\n",
            "Iteration 61, loss = 1.21025518\n",
            "Iteration 62, loss = 1.20650511\n",
            "Iteration 63, loss = 1.20206957\n",
            "Iteration 64, loss = 1.19875139\n",
            "Iteration 65, loss = 1.19465969\n",
            "Iteration 66, loss = 1.19113762\n",
            "Iteration 67, loss = 1.18804869\n",
            "Iteration 68, loss = 1.18476486\n",
            "Iteration 69, loss = 1.18181738\n",
            "Iteration 70, loss = 1.17882070\n",
            "Iteration 71, loss = 1.17669966\n",
            "Iteration 72, loss = 1.17416726\n",
            "Iteration 73, loss = 1.17108835\n",
            "Iteration 74, loss = 1.16880372\n",
            "Iteration 75, loss = 1.16670468\n",
            "Iteration 76, loss = 1.16447366\n",
            "Iteration 77, loss = 1.16205686\n",
            "Iteration 78, loss = 1.16041471\n",
            "Iteration 79, loss = 1.15862507\n",
            "Iteration 80, loss = 1.15645081\n",
            "Iteration 81, loss = 1.15466054\n",
            "Iteration 82, loss = 1.15355906\n",
            "Iteration 83, loss = 1.15156520\n",
            "Iteration 84, loss = 1.14996877\n",
            "Iteration 85, loss = 1.14872600\n",
            "Iteration 86, loss = 1.14700145\n",
            "Iteration 87, loss = 1.14586435\n",
            "Iteration 88, loss = 1.14443066\n",
            "Iteration 89, loss = 1.14302102\n",
            "Iteration 90, loss = 1.14160474\n",
            "Iteration 91, loss = 1.14083244\n",
            "Iteration 92, loss = 1.13939257\n",
            "Iteration 93, loss = 1.13805755\n",
            "Iteration 94, loss = 1.13757268\n",
            "Iteration 95, loss = 1.13597749\n",
            "Iteration 96, loss = 1.13521913\n",
            "Iteration 97, loss = 1.13411478\n",
            "Iteration 98, loss = 1.13285312\n",
            "Iteration 99, loss = 1.13291198\n",
            "Iteration 100, loss = 1.13220376\n",
            "Iteration 1, loss = 1.66560742\n",
            "Iteration 2, loss = 1.62357786\n",
            "Iteration 3, loss = 1.57598796\n",
            "Iteration 4, loss = 1.55367003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 1.49591599\n",
            "Iteration 6, loss = 1.42936637\n",
            "Iteration 7, loss = 1.36995766\n",
            "Iteration 8, loss = 1.30637129\n",
            "Iteration 9, loss = 1.25359744\n",
            "Iteration 10, loss = 1.21224985\n",
            "Iteration 11, loss = 1.17760620\n",
            "Iteration 12, loss = 1.15891997\n",
            "Iteration 13, loss = 1.14673801\n",
            "Iteration 14, loss = 1.13656225\n",
            "Iteration 15, loss = 1.13944035\n",
            "Iteration 16, loss = 1.12346656\n",
            "Iteration 17, loss = 1.12327569\n",
            "Iteration 18, loss = 1.11973859\n",
            "Iteration 19, loss = 1.12199184\n",
            "Iteration 20, loss = 1.11479417\n",
            "Iteration 21, loss = 1.11370475\n",
            "Iteration 22, loss = 1.11460688\n",
            "Iteration 23, loss = 1.11210692\n",
            "Iteration 24, loss = 1.11261942\n",
            "Iteration 25, loss = 1.11149660\n",
            "Iteration 26, loss = 1.11222546\n",
            "Iteration 27, loss = 1.11229739\n",
            "Iteration 28, loss = 1.11085991\n",
            "Iteration 29, loss = 1.11153214\n",
            "Iteration 30, loss = 1.10690917\n",
            "Iteration 31, loss = 1.10629087\n",
            "Iteration 32, loss = 1.10707100\n",
            "Iteration 33, loss = 1.10632725\n",
            "Iteration 34, loss = 1.10880044\n",
            "Iteration 35, loss = 1.11534768\n",
            "Iteration 36, loss = 1.10242009\n",
            "Iteration 37, loss = 1.10969317\n",
            "Iteration 38, loss = 1.10787052\n",
            "Iteration 39, loss = 1.10747038\n",
            "Iteration 40, loss = 1.10261165\n",
            "Iteration 41, loss = 1.10495149\n",
            "Iteration 42, loss = 1.10213855\n",
            "Iteration 43, loss = 1.10139737\n",
            "Iteration 44, loss = 1.09824784\n",
            "Iteration 45, loss = 1.09821629\n",
            "Iteration 46, loss = 1.09385395\n",
            "Iteration 47, loss = 1.09370714\n",
            "Iteration 48, loss = 1.09149104\n",
            "Iteration 49, loss = 1.09084165\n",
            "Iteration 50, loss = 1.08521285\n",
            "Iteration 51, loss = 1.08447385\n",
            "Iteration 52, loss = 1.08294755\n",
            "Iteration 53, loss = 1.07639575\n",
            "Iteration 54, loss = 1.07965207\n",
            "Iteration 55, loss = 1.07241246\n",
            "Iteration 56, loss = 1.06985487\n",
            "Iteration 57, loss = 1.06111072\n",
            "Iteration 58, loss = 1.06490893\n",
            "Iteration 59, loss = 1.06038933\n",
            "Iteration 60, loss = 1.04933672\n",
            "Iteration 61, loss = 1.04612407\n",
            "Iteration 62, loss = 1.03807948\n",
            "Iteration 63, loss = 1.02289337\n",
            "Iteration 64, loss = 1.02481721\n",
            "Iteration 65, loss = 1.02154463\n",
            "Iteration 66, loss = 1.02645780\n",
            "Iteration 67, loss = 1.00702391\n",
            "Iteration 68, loss = 1.02095908\n",
            "Iteration 69, loss = 1.03018795\n",
            "Iteration 70, loss = 0.98935162\n",
            "Iteration 71, loss = 0.97660335\n",
            "Iteration 72, loss = 0.95792878\n",
            "Iteration 73, loss = 0.95139606\n",
            "Iteration 74, loss = 0.95293103\n",
            "Iteration 75, loss = 0.94850094\n",
            "Iteration 76, loss = 0.93502621\n",
            "Iteration 77, loss = 0.92341333\n",
            "Iteration 78, loss = 0.92209263\n",
            "Iteration 79, loss = 0.91732000\n",
            "Iteration 80, loss = 0.93666935\n",
            "Iteration 81, loss = 0.89198828\n",
            "Iteration 82, loss = 0.89014566\n",
            "Iteration 83, loss = 0.89403815\n",
            "Iteration 84, loss = 0.90507537\n",
            "Iteration 85, loss = 0.89464196\n",
            "Iteration 86, loss = 0.87955579\n",
            "Iteration 87, loss = 0.86226798\n",
            "Iteration 88, loss = 0.85875061\n",
            "Iteration 89, loss = 0.86429418\n",
            "Iteration 90, loss = 0.84696572\n",
            "Iteration 91, loss = 0.85363422\n",
            "Iteration 92, loss = 0.84583142\n",
            "Iteration 93, loss = 0.85687816\n",
            "Iteration 94, loss = 0.86178701\n",
            "Iteration 95, loss = 0.83969927\n",
            "Iteration 96, loss = 0.84831488\n",
            "Iteration 97, loss = 0.82265194\n",
            "Iteration 98, loss = 0.83198590\n",
            "Iteration 99, loss = 0.86113580\n",
            "Iteration 100, loss = 0.83386642\n",
            "Iteration 1, loss = 1.62567234\n",
            "Iteration 2, loss = 1.62401610\n",
            "Iteration 3, loss = 1.62210172\n",
            "Iteration 4, loss = 1.62039192\n",
            "Iteration 5, loss = 1.61882955\n",
            "Iteration 6, loss = 1.61743206\n",
            "Iteration 7, loss = 1.61586714\n",
            "Iteration 8, loss = 1.61455841\n",
            "Iteration 9, loss = 1.61350390\n",
            "Iteration 10, loss = 1.61218133\n",
            "Iteration 11, loss = 1.61107891\n",
            "Iteration 12, loss = 1.61008436\n",
            "Iteration 13, loss = 1.60926634\n",
            "Iteration 14, loss = 1.60822771\n",
            "Iteration 15, loss = 1.60742392\n",
            "Iteration 16, loss = 1.60661453\n",
            "Iteration 17, loss = 1.60586493\n",
            "Iteration 18, loss = 1.60530342\n",
            "Iteration 19, loss = 1.60459869\n",
            "Iteration 20, loss = 1.60403216\n",
            "Iteration 21, loss = 1.60353777\n",
            "Iteration 22, loss = 1.60303971\n",
            "Iteration 23, loss = 1.60248908\n",
            "Iteration 24, loss = 1.60216815\n",
            "Iteration 25, loss = 1.60183194\n",
            "Iteration 26, loss = 1.60134679\n",
            "Iteration 27, loss = 1.60120968\n",
            "Iteration 28, loss = 1.60090201\n",
            "Iteration 29, loss = 1.60059336\n",
            "Iteration 30, loss = 1.60037340\n",
            "Iteration 31, loss = 1.60014718\n",
            "Iteration 32, loss = 1.60006562\n",
            "Iteration 33, loss = 1.59990611\n",
            "Iteration 34, loss = 1.59966234\n",
            "Iteration 35, loss = 1.59948804\n",
            "Iteration 36, loss = 1.59942352\n",
            "Iteration 37, loss = 1.59926632\n",
            "Iteration 38, loss = 1.59926133\n",
            "Iteration 39, loss = 1.59907701\n",
            "Iteration 40, loss = 1.59902248\n",
            "Iteration 41, loss = 1.59898571\n",
            "Iteration 42, loss = 1.59887895\n",
            "Iteration 43, loss = 1.59884442\n",
            "Iteration 44, loss = 1.59876006\n",
            "Iteration 45, loss = 1.59868929\n",
            "Iteration 46, loss = 1.59868710\n",
            "Iteration 47, loss = 1.59866018\n",
            "Iteration 48, loss = 1.59860629\n",
            "Iteration 49, loss = 1.59857432\n",
            "Iteration 50, loss = 1.59852458\n",
            "Iteration 51, loss = 1.59848578\n",
            "Iteration 52, loss = 1.59845634\n",
            "Iteration 53, loss = 1.59843168\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.62138999\n",
            "Iteration 2, loss = 1.60922626\n",
            "Iteration 3, loss = 1.60072107\n",
            "Iteration 4, loss = 1.59868794\n",
            "Iteration 5, loss = 1.59918057\n",
            "Iteration 6, loss = 1.60066944\n",
            "Iteration 7, loss = 1.60151042\n",
            "Iteration 8, loss = 1.60133562\n",
            "Iteration 9, loss = 1.60017074\n",
            "Iteration 10, loss = 1.59931128\n",
            "Iteration 11, loss = 1.59877515\n",
            "Iteration 12, loss = 1.59794263\n",
            "Iteration 13, loss = 1.59731613\n",
            "Iteration 14, loss = 1.59727168\n",
            "Iteration 15, loss = 1.59720708\n",
            "Iteration 16, loss = 1.59703623\n",
            "Iteration 17, loss = 1.59669813\n",
            "Iteration 18, loss = 1.59650913\n",
            "Iteration 19, loss = 1.59600695\n",
            "Iteration 20, loss = 1.59583484\n",
            "Iteration 21, loss = 1.59521586\n",
            "Iteration 22, loss = 1.59488045\n",
            "Iteration 23, loss = 1.59451432\n",
            "Iteration 24, loss = 1.59402516\n",
            "Iteration 25, loss = 1.59374244\n",
            "Iteration 26, loss = 1.59349825\n",
            "Iteration 27, loss = 1.59273952\n",
            "Iteration 28, loss = 1.59236720\n",
            "Iteration 29, loss = 1.59121557\n",
            "Iteration 30, loss = 1.59053999\n",
            "Iteration 31, loss = 1.58969858\n",
            "Iteration 32, loss = 1.58910384\n",
            "Iteration 33, loss = 1.58843571\n",
            "Iteration 34, loss = 1.58726896\n",
            "Iteration 35, loss = 1.58622250\n",
            "Iteration 36, loss = 1.58508852\n",
            "Iteration 37, loss = 1.58407461\n",
            "Iteration 38, loss = 1.58282222\n",
            "Iteration 39, loss = 1.58130646\n",
            "Iteration 40, loss = 1.57973186\n",
            "Iteration 41, loss = 1.57834711\n",
            "Iteration 42, loss = 1.57646157\n",
            "Iteration 43, loss = 1.57475677\n",
            "Iteration 44, loss = 1.57273449\n",
            "Iteration 45, loss = 1.57091672\n",
            "Iteration 46, loss = 1.56840610\n",
            "Iteration 47, loss = 1.56621494\n",
            "Iteration 48, loss = 1.56364886\n",
            "Iteration 49, loss = 1.56112813\n",
            "Iteration 50, loss = 1.55806714\n",
            "Iteration 51, loss = 1.55529259\n",
            "Iteration 52, loss = 1.55213382\n",
            "Iteration 53, loss = 1.54868577\n",
            "Iteration 54, loss = 1.54540454\n",
            "Iteration 55, loss = 1.54168550\n",
            "Iteration 56, loss = 1.53789410\n",
            "Iteration 57, loss = 1.53402629\n",
            "Iteration 58, loss = 1.53003984\n",
            "Iteration 59, loss = 1.52556270\n",
            "Iteration 60, loss = 1.52115579\n",
            "Iteration 61, loss = 1.51633594\n",
            "Iteration 62, loss = 1.51167431\n",
            "Iteration 63, loss = 1.50649214\n",
            "Iteration 64, loss = 1.50176183\n",
            "Iteration 65, loss = 1.49617447\n",
            "Iteration 66, loss = 1.49060169\n",
            "Iteration 67, loss = 1.48527474\n",
            "Iteration 68, loss = 1.47951446\n",
            "Iteration 69, loss = 1.47372226\n",
            "Iteration 70, loss = 1.46770452\n",
            "Iteration 71, loss = 1.46188678\n",
            "Iteration 72, loss = 1.45579785\n",
            "Iteration 73, loss = 1.44933793\n",
            "Iteration 74, loss = 1.44334346\n",
            "Iteration 75, loss = 1.43722059\n",
            "Iteration 76, loss = 1.43100680\n",
            "Iteration 77, loss = 1.42433977\n",
            "Iteration 78, loss = 1.41816035\n",
            "Iteration 79, loss = 1.41208133\n",
            "Iteration 80, loss = 1.40562458\n",
            "Iteration 81, loss = 1.39923546\n",
            "Iteration 82, loss = 1.39389874\n",
            "Iteration 83, loss = 1.38719560\n",
            "Iteration 84, loss = 1.38115364\n",
            "Iteration 85, loss = 1.37513120\n",
            "Iteration 86, loss = 1.36926507\n",
            "Iteration 87, loss = 1.36332028\n",
            "Iteration 88, loss = 1.35758684\n",
            "Iteration 89, loss = 1.35191773\n",
            "Iteration 90, loss = 1.34620330\n",
            "Iteration 91, loss = 1.34128192\n",
            "Iteration 92, loss = 1.33533728\n",
            "Iteration 93, loss = 1.33000215\n",
            "Iteration 94, loss = 1.32505766\n",
            "Iteration 95, loss = 1.31978174\n",
            "Iteration 96, loss = 1.31505694\n",
            "Iteration 97, loss = 1.31037399\n",
            "Iteration 98, loss = 1.30544100\n",
            "Iteration 99, loss = 1.30133113\n",
            "Iteration 100, loss = 1.29685841\n",
            "Iteration 1, loss = 1.62239434\n",
            "Iteration 2, loss = 1.61782618\n",
            "Iteration 3, loss = 1.62161136\n",
            "Iteration 4, loss = 1.61200443\n",
            "Iteration 5, loss = 1.60098382\n",
            "Iteration 6, loss = 1.60503361\n",
            "Iteration 7, loss = 1.60421878\n",
            "Iteration 8, loss = 1.59692002\n",
            "Iteration 9, loss = 1.58886808\n",
            "Iteration 10, loss = 1.58575396\n",
            "Iteration 11, loss = 1.57660859\n",
            "Iteration 12, loss = 1.56305309\n",
            "Iteration 13, loss = 1.55011995\n",
            "Iteration 14, loss = 1.52928841\n",
            "Iteration 15, loss = 1.50207585\n",
            "Iteration 16, loss = 1.46815005\n",
            "Iteration 17, loss = 1.42706995\n",
            "Iteration 18, loss = 1.38504779\n",
            "Iteration 19, loss = 1.34275772\n",
            "Iteration 20, loss = 1.30273353\n",
            "Iteration 21, loss = 1.26871675\n",
            "Iteration 22, loss = 1.23776859\n",
            "Iteration 23, loss = 1.21291338\n",
            "Iteration 24, loss = 1.19259493\n",
            "Iteration 25, loss = 1.17852069\n",
            "Iteration 26, loss = 1.16724607\n",
            "Iteration 27, loss = 1.15825563\n",
            "Iteration 28, loss = 1.15438617\n",
            "Iteration 29, loss = 1.13972882\n",
            "Iteration 30, loss = 1.13614330\n",
            "Iteration 31, loss = 1.13062728\n",
            "Iteration 32, loss = 1.13084484\n",
            "Iteration 33, loss = 1.12696845\n",
            "Iteration 34, loss = 1.12545122\n",
            "Iteration 35, loss = 1.12357405\n",
            "Iteration 36, loss = 1.12106026\n",
            "Iteration 37, loss = 1.11866462\n",
            "Iteration 38, loss = 1.11711598\n",
            "Iteration 39, loss = 1.11550999\n",
            "Iteration 40, loss = 1.11385270\n",
            "Iteration 41, loss = 1.11211156\n",
            "Iteration 42, loss = 1.11014220\n",
            "Iteration 43, loss = 1.11061409\n",
            "Iteration 44, loss = 1.10883955\n",
            "Iteration 45, loss = 1.11320438\n",
            "Iteration 46, loss = 1.10855046\n",
            "Iteration 47, loss = 1.10934198\n",
            "Iteration 48, loss = 1.10679832\n",
            "Iteration 49, loss = 1.10732947\n",
            "Iteration 50, loss = 1.10623489\n",
            "Iteration 51, loss = 1.10551973\n",
            "Iteration 52, loss = 1.10405012\n",
            "Iteration 53, loss = 1.10663914\n",
            "Iteration 54, loss = 1.10210230\n",
            "Iteration 55, loss = 1.10509879\n",
            "Iteration 56, loss = 1.10437239\n",
            "Iteration 57, loss = 1.10220078\n",
            "Iteration 58, loss = 1.10284941\n",
            "Iteration 59, loss = 1.10350027\n",
            "Iteration 60, loss = 1.10100518\n",
            "Iteration 61, loss = 1.10150308\n",
            "Iteration 62, loss = 1.10310794\n",
            "Iteration 63, loss = 1.09998140\n",
            "Iteration 64, loss = 1.10574995\n",
            "Iteration 65, loss = 1.09862323\n",
            "Iteration 66, loss = 1.09905354\n",
            "Iteration 67, loss = 1.10053705\n",
            "Iteration 68, loss = 1.09869142\n",
            "Iteration 69, loss = 1.09575606\n",
            "Iteration 70, loss = 1.09759991\n",
            "Iteration 71, loss = 1.09814902\n",
            "Iteration 72, loss = 1.09578025\n",
            "Iteration 73, loss = 1.09238676\n",
            "Iteration 74, loss = 1.09326121\n",
            "Iteration 75, loss = 1.09286044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 76, loss = 1.09682777\n",
            "Iteration 77, loss = 1.08877722\n",
            "Iteration 78, loss = 1.09293850\n",
            "Iteration 79, loss = 1.08848427\n",
            "Iteration 80, loss = 1.08586757\n",
            "Iteration 81, loss = 1.08458312\n",
            "Iteration 82, loss = 1.08806719\n",
            "Iteration 83, loss = 1.08485655\n",
            "Iteration 84, loss = 1.08590400\n",
            "Iteration 85, loss = 1.08515737\n",
            "Iteration 86, loss = 1.07961662\n",
            "Iteration 87, loss = 1.08127521\n",
            "Iteration 88, loss = 1.07936391\n",
            "Iteration 89, loss = 1.07818646\n",
            "Iteration 90, loss = 1.07663308\n",
            "Iteration 91, loss = 1.07278977\n",
            "Iteration 92, loss = 1.06944470\n",
            "Iteration 93, loss = 1.07010878\n",
            "Iteration 94, loss = 1.07167851\n",
            "Iteration 95, loss = 1.06623078\n",
            "Iteration 96, loss = 1.06373295\n",
            "Iteration 97, loss = 1.06661346\n",
            "Iteration 98, loss = 1.06532602\n",
            "Iteration 99, loss = 1.06206095\n",
            "Iteration 100, loss = 1.06263080\n",
            "Iteration 1, loss = 1.66286559\n",
            "Iteration 2, loss = 1.65959407\n",
            "Iteration 3, loss = 1.65650671\n",
            "Iteration 4, loss = 1.65329721\n",
            "Iteration 5, loss = 1.65069924\n",
            "Iteration 6, loss = 1.64794817\n",
            "Iteration 7, loss = 1.64506800\n",
            "Iteration 8, loss = 1.64237351\n",
            "Iteration 9, loss = 1.64030510\n",
            "Iteration 10, loss = 1.63789317\n",
            "Iteration 11, loss = 1.63560576\n",
            "Iteration 12, loss = 1.63333462\n",
            "Iteration 13, loss = 1.63158356\n",
            "Iteration 14, loss = 1.62949104\n",
            "Iteration 15, loss = 1.62772473\n",
            "Iteration 16, loss = 1.62575632\n",
            "Iteration 17, loss = 1.62419107\n",
            "Iteration 18, loss = 1.62256075\n",
            "Iteration 19, loss = 1.62089883\n",
            "Iteration 20, loss = 1.61946117\n",
            "Iteration 21, loss = 1.61808834\n",
            "Iteration 22, loss = 1.61676479\n",
            "Iteration 23, loss = 1.61545383\n",
            "Iteration 24, loss = 1.61413723\n",
            "Iteration 25, loss = 1.61302016\n",
            "Iteration 26, loss = 1.61183521\n",
            "Iteration 27, loss = 1.61101874\n",
            "Iteration 28, loss = 1.60981718\n",
            "Iteration 29, loss = 1.60889768\n",
            "Iteration 30, loss = 1.60800376\n",
            "Iteration 31, loss = 1.60709242\n",
            "Iteration 32, loss = 1.60639417\n",
            "Iteration 33, loss = 1.60563537\n",
            "Iteration 34, loss = 1.60465743\n",
            "Iteration 35, loss = 1.60401356\n",
            "Iteration 36, loss = 1.60330107\n",
            "Iteration 37, loss = 1.60265061\n",
            "Iteration 38, loss = 1.60209928\n",
            "Iteration 39, loss = 1.60136241\n",
            "Iteration 40, loss = 1.60082078\n",
            "Iteration 41, loss = 1.60032901\n",
            "Iteration 42, loss = 1.59974275\n",
            "Iteration 43, loss = 1.59920964\n",
            "Iteration 44, loss = 1.59873680\n",
            "Iteration 45, loss = 1.59822262\n",
            "Iteration 46, loss = 1.59781520\n",
            "Iteration 47, loss = 1.59743250\n",
            "Iteration 48, loss = 1.59696692\n",
            "Iteration 49, loss = 1.59655595\n",
            "Iteration 50, loss = 1.59612748\n",
            "Iteration 51, loss = 1.59571503\n",
            "Iteration 52, loss = 1.59534734\n",
            "Iteration 53, loss = 1.59499526\n",
            "Iteration 54, loss = 1.59467472\n",
            "Iteration 55, loss = 1.59429388\n",
            "Iteration 56, loss = 1.59395637\n",
            "Iteration 57, loss = 1.59366028\n",
            "Iteration 58, loss = 1.59334235\n",
            "Iteration 59, loss = 1.59302147\n",
            "Iteration 60, loss = 1.59271970\n",
            "Iteration 61, loss = 1.59238719\n",
            "Iteration 62, loss = 1.59210417\n",
            "Iteration 63, loss = 1.59182225\n",
            "Iteration 64, loss = 1.59154781\n",
            "Iteration 65, loss = 1.59125025\n",
            "Iteration 66, loss = 1.59095456\n",
            "Iteration 67, loss = 1.59071548\n",
            "Iteration 68, loss = 1.59036524\n",
            "Iteration 69, loss = 1.59008466\n",
            "Iteration 70, loss = 1.58983447\n",
            "Iteration 71, loss = 1.58954528\n",
            "Iteration 72, loss = 1.58929253\n",
            "Iteration 73, loss = 1.58898682\n",
            "Iteration 74, loss = 1.58871461\n",
            "Iteration 75, loss = 1.58845019\n",
            "Iteration 76, loss = 1.58816666\n",
            "Iteration 77, loss = 1.58787648\n",
            "Iteration 78, loss = 1.58760948\n",
            "Iteration 79, loss = 1.58732555\n",
            "Iteration 80, loss = 1.58701867\n",
            "Iteration 81, loss = 1.58672668\n",
            "Iteration 82, loss = 1.58650986\n",
            "Iteration 83, loss = 1.58617397\n",
            "Iteration 84, loss = 1.58586268\n",
            "Iteration 85, loss = 1.58557209\n",
            "Iteration 86, loss = 1.58529066\n",
            "Iteration 87, loss = 1.58497268\n",
            "Iteration 88, loss = 1.58467390\n",
            "Iteration 89, loss = 1.58435986\n",
            "Iteration 90, loss = 1.58405256\n",
            "Iteration 91, loss = 1.58376954\n",
            "Iteration 92, loss = 1.58341139\n",
            "Iteration 93, loss = 1.58308760\n",
            "Iteration 94, loss = 1.58278113\n",
            "Iteration 95, loss = 1.58245249\n",
            "Iteration 96, loss = 1.58212655\n",
            "Iteration 97, loss = 1.58179202\n",
            "Iteration 98, loss = 1.58144080\n",
            "Iteration 99, loss = 1.58110433\n",
            "Iteration 100, loss = 1.58079006\n",
            "Iteration 1, loss = 1.65720545\n",
            "Iteration 2, loss = 1.63145393\n",
            "Iteration 3, loss = 1.61452627\n",
            "Iteration 4, loss = 1.60269862\n",
            "Iteration 5, loss = 1.59912852\n",
            "Iteration 6, loss = 1.59647564\n",
            "Iteration 7, loss = 1.59439949\n",
            "Iteration 8, loss = 1.59300227\n",
            "Iteration 9, loss = 1.59210106\n",
            "Iteration 10, loss = 1.59031988\n",
            "Iteration 11, loss = 1.58767842\n",
            "Iteration 12, loss = 1.58440295\n",
            "Iteration 13, loss = 1.58004557\n",
            "Iteration 14, loss = 1.57660400\n",
            "Iteration 15, loss = 1.57271430\n",
            "Iteration 16, loss = 1.56942254\n",
            "Iteration 17, loss = 1.56519630\n",
            "Iteration 18, loss = 1.56069372\n",
            "Iteration 19, loss = 1.55653395\n",
            "Iteration 20, loss = 1.55110291\n",
            "Iteration 21, loss = 1.54593749\n",
            "Iteration 22, loss = 1.54012397\n",
            "Iteration 23, loss = 1.53381256\n",
            "Iteration 24, loss = 1.52694123\n",
            "Iteration 25, loss = 1.52008595\n",
            "Iteration 26, loss = 1.51279217\n",
            "Iteration 27, loss = 1.50532281\n",
            "Iteration 28, loss = 1.49726383\n",
            "Iteration 29, loss = 1.48848599\n",
            "Iteration 30, loss = 1.47990881\n",
            "Iteration 31, loss = 1.47096886\n",
            "Iteration 32, loss = 1.46209061\n",
            "Iteration 33, loss = 1.45264741\n",
            "Iteration 34, loss = 1.44283226\n",
            "Iteration 35, loss = 1.43278172\n",
            "Iteration 36, loss = 1.42254997\n",
            "Iteration 37, loss = 1.41275696\n",
            "Iteration 38, loss = 1.40225002\n",
            "Iteration 39, loss = 1.39180130\n",
            "Iteration 40, loss = 1.38137749\n",
            "Iteration 41, loss = 1.37151656\n",
            "Iteration 42, loss = 1.36113401\n",
            "Iteration 43, loss = 1.35110139\n",
            "Iteration 44, loss = 1.34117722\n",
            "Iteration 45, loss = 1.33175171\n",
            "Iteration 46, loss = 1.32208472\n",
            "Iteration 47, loss = 1.31294559\n",
            "Iteration 48, loss = 1.30410347\n",
            "Iteration 49, loss = 1.29578252\n",
            "Iteration 50, loss = 1.28700757\n",
            "Iteration 51, loss = 1.27923480\n",
            "Iteration 52, loss = 1.27145491\n",
            "Iteration 53, loss = 1.26420703\n",
            "Iteration 54, loss = 1.25722739\n",
            "Iteration 55, loss = 1.25057458\n",
            "Iteration 56, loss = 1.24415482\n",
            "Iteration 57, loss = 1.23824311\n",
            "Iteration 58, loss = 1.23266504\n",
            "Iteration 59, loss = 1.22730884\n",
            "Iteration 60, loss = 1.22207871\n",
            "Iteration 61, loss = 1.21681855\n",
            "Iteration 62, loss = 1.21267408\n",
            "Iteration 63, loss = 1.20790420\n",
            "Iteration 64, loss = 1.20417878\n",
            "Iteration 65, loss = 1.19980443\n",
            "Iteration 66, loss = 1.19602359\n",
            "Iteration 67, loss = 1.19263743\n",
            "Iteration 68, loss = 1.18906798\n",
            "Iteration 69, loss = 1.18583089\n",
            "Iteration 70, loss = 1.18259909\n",
            "Iteration 71, loss = 1.18022439\n",
            "Iteration 72, loss = 1.17750554\n",
            "Iteration 73, loss = 1.17427496\n",
            "Iteration 74, loss = 1.17173544\n",
            "Iteration 75, loss = 1.16947431\n",
            "Iteration 76, loss = 1.16702274\n",
            "Iteration 77, loss = 1.16453057\n",
            "Iteration 78, loss = 1.16269589\n",
            "Iteration 79, loss = 1.16073223\n",
            "Iteration 80, loss = 1.15844736\n",
            "Iteration 81, loss = 1.15654341\n",
            "Iteration 82, loss = 1.15527810\n",
            "Iteration 83, loss = 1.15318917\n",
            "Iteration 84, loss = 1.15150223\n",
            "Iteration 85, loss = 1.15016566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 86, loss = 1.14830660\n",
            "Iteration 87, loss = 1.14712661\n",
            "Iteration 88, loss = 1.14555638\n",
            "Iteration 89, loss = 1.14411977\n",
            "Iteration 90, loss = 1.14261479\n",
            "Iteration 91, loss = 1.14178038\n",
            "Iteration 92, loss = 1.14028902\n",
            "Iteration 93, loss = 1.13888279\n",
            "Iteration 94, loss = 1.13834229\n",
            "Iteration 95, loss = 1.13672434\n",
            "Iteration 96, loss = 1.13585147\n",
            "Iteration 97, loss = 1.13469516\n",
            "Iteration 98, loss = 1.13340270\n",
            "Iteration 99, loss = 1.13336388\n",
            "Iteration 100, loss = 1.13255083\n",
            "Iteration 1, loss = 1.65903918\n",
            "Iteration 2, loss = 1.62179800\n",
            "Iteration 3, loss = 1.57891694\n",
            "Iteration 4, loss = 1.55320389\n",
            "Iteration 5, loss = 1.49973726\n",
            "Iteration 6, loss = 1.43158102\n",
            "Iteration 7, loss = 1.37062350\n",
            "Iteration 8, loss = 1.30682408\n",
            "Iteration 9, loss = 1.25229314\n",
            "Iteration 10, loss = 1.21187937\n",
            "Iteration 11, loss = 1.17891503\n",
            "Iteration 12, loss = 1.15908091\n",
            "Iteration 13, loss = 1.14667868\n",
            "Iteration 14, loss = 1.13653246\n",
            "Iteration 15, loss = 1.14032996\n",
            "Iteration 16, loss = 1.12366780\n",
            "Iteration 17, loss = 1.12506045\n",
            "Iteration 18, loss = 1.11979107\n",
            "Iteration 19, loss = 1.12245390\n",
            "Iteration 20, loss = 1.11424269\n",
            "Iteration 21, loss = 1.11275272\n",
            "Iteration 22, loss = 1.11252566\n",
            "Iteration 23, loss = 1.11057256\n",
            "Iteration 24, loss = 1.10988266\n",
            "Iteration 25, loss = 1.10965153\n",
            "Iteration 26, loss = 1.11007431\n",
            "Iteration 27, loss = 1.10884131\n",
            "Iteration 28, loss = 1.10878371\n",
            "Iteration 29, loss = 1.10796710\n",
            "Iteration 30, loss = 1.10442678\n",
            "Iteration 31, loss = 1.10401673\n",
            "Iteration 32, loss = 1.10461480\n",
            "Iteration 33, loss = 1.10389844\n",
            "Iteration 34, loss = 1.10690735\n",
            "Iteration 35, loss = 1.11468327\n",
            "Iteration 36, loss = 1.10069432\n",
            "Iteration 37, loss = 1.10873777\n",
            "Iteration 38, loss = 1.10645361\n",
            "Iteration 39, loss = 1.10599280\n",
            "Iteration 40, loss = 1.10165015\n",
            "Iteration 41, loss = 1.10455368\n",
            "Iteration 42, loss = 1.10034243\n",
            "Iteration 43, loss = 1.10028962\n",
            "Iteration 44, loss = 1.09784618\n",
            "Iteration 45, loss = 1.09769456\n",
            "Iteration 46, loss = 1.09464880\n",
            "Iteration 47, loss = 1.09432211\n",
            "Iteration 48, loss = 1.09168448\n",
            "Iteration 49, loss = 1.09222880\n",
            "Iteration 50, loss = 1.08810336\n",
            "Iteration 51, loss = 1.08797779\n",
            "Iteration 52, loss = 1.08722529\n",
            "Iteration 53, loss = 1.08320268\n",
            "Iteration 54, loss = 1.08481517\n",
            "Iteration 55, loss = 1.08042166\n",
            "Iteration 56, loss = 1.07697303\n",
            "Iteration 57, loss = 1.07399853\n",
            "Iteration 58, loss = 1.07665774\n",
            "Iteration 59, loss = 1.07680152\n",
            "Iteration 60, loss = 1.06823087\n",
            "Iteration 61, loss = 1.06553466\n",
            "Iteration 62, loss = 1.05624651\n",
            "Iteration 63, loss = 1.05294716\n",
            "Iteration 64, loss = 1.05433282\n",
            "Iteration 65, loss = 1.05902712\n",
            "Iteration 66, loss = 1.03610639\n",
            "Iteration 67, loss = 1.04406074\n",
            "Iteration 68, loss = 1.04038677\n",
            "Iteration 69, loss = 1.01743289\n",
            "Iteration 70, loss = 1.01706215\n",
            "Iteration 71, loss = 1.02863543\n",
            "Iteration 72, loss = 1.01497663\n",
            "Iteration 73, loss = 1.00361720\n",
            "Iteration 74, loss = 0.98176705\n",
            "Iteration 75, loss = 0.99816056\n",
            "Iteration 76, loss = 0.97401396\n",
            "Iteration 77, loss = 0.97059088\n",
            "Iteration 78, loss = 0.96687157\n",
            "Iteration 79, loss = 0.97199616\n",
            "Iteration 80, loss = 0.93708273\n",
            "Iteration 81, loss = 0.95223425\n",
            "Iteration 82, loss = 0.95887636\n",
            "Iteration 83, loss = 0.95178003\n",
            "Iteration 84, loss = 0.94818013\n",
            "Iteration 85, loss = 0.94423583\n",
            "Iteration 86, loss = 0.93259290\n",
            "Iteration 87, loss = 0.95131842\n",
            "Iteration 88, loss = 0.92229320\n",
            "Iteration 89, loss = 0.90973324\n",
            "Iteration 90, loss = 0.90428321\n",
            "Iteration 91, loss = 0.89661871\n",
            "Iteration 92, loss = 0.88495321\n",
            "Iteration 93, loss = 0.86978860\n",
            "Iteration 94, loss = 0.86711662\n",
            "Iteration 95, loss = 0.87012276\n",
            "Iteration 96, loss = 0.86744166\n",
            "Iteration 97, loss = 0.85277897\n",
            "Iteration 98, loss = 0.85079902\n",
            "Iteration 99, loss = 0.87220173\n",
            "Iteration 100, loss = 0.86038319\n",
            "Iteration 1, loss = 1.62651660\n",
            "Iteration 2, loss = 1.62565885\n",
            "Iteration 3, loss = 1.62485731\n",
            "Iteration 4, loss = 1.62396984\n",
            "Iteration 5, loss = 1.62329270\n",
            "Iteration 6, loss = 1.62254029\n",
            "Iteration 7, loss = 1.62170302\n",
            "Iteration 8, loss = 1.62090069\n",
            "Iteration 9, loss = 1.62032916\n",
            "Iteration 10, loss = 1.61960025\n",
            "Iteration 11, loss = 1.61885330\n",
            "Iteration 12, loss = 1.61813685\n",
            "Iteration 13, loss = 1.61758264\n",
            "Iteration 14, loss = 1.61686142\n",
            "Iteration 15, loss = 1.61626518\n",
            "Iteration 16, loss = 1.61553440\n",
            "Iteration 17, loss = 1.61497350\n",
            "Iteration 18, loss = 1.61436618\n",
            "Iteration 19, loss = 1.61369829\n",
            "Iteration 20, loss = 1.61313892\n",
            "Iteration 21, loss = 1.61256834\n",
            "Iteration 22, loss = 1.61200340\n",
            "Iteration 23, loss = 1.61143597\n",
            "Iteration 24, loss = 1.61084521\n",
            "Iteration 25, loss = 1.61032806\n",
            "Iteration 26, loss = 1.60978255\n",
            "Iteration 27, loss = 1.60939896\n",
            "Iteration 28, loss = 1.60877991\n",
            "Iteration 29, loss = 1.60832622\n",
            "Iteration 30, loss = 1.60786306\n",
            "Iteration 31, loss = 1.60738657\n",
            "Iteration 32, loss = 1.60700656\n",
            "Iteration 33, loss = 1.60658961\n",
            "Iteration 34, loss = 1.60602643\n",
            "Iteration 35, loss = 1.60565917\n",
            "Iteration 36, loss = 1.60523026\n",
            "Iteration 37, loss = 1.60482900\n",
            "Iteration 38, loss = 1.60449478\n",
            "Iteration 39, loss = 1.60402741\n",
            "Iteration 40, loss = 1.60368400\n",
            "Iteration 41, loss = 1.60336244\n",
            "Iteration 42, loss = 1.60296965\n",
            "Iteration 43, loss = 1.60261014\n",
            "Iteration 44, loss = 1.60228141\n",
            "Iteration 45, loss = 1.60192198\n",
            "Iteration 46, loss = 1.60163425\n",
            "Iteration 47, loss = 1.60135532\n",
            "Iteration 48, loss = 1.60102411\n",
            "Iteration 49, loss = 1.60072124\n",
            "Iteration 50, loss = 1.60040193\n",
            "Iteration 51, loss = 1.60009433\n",
            "Iteration 52, loss = 1.59981893\n",
            "Iteration 53, loss = 1.59955271\n",
            "Iteration 54, loss = 1.59930971\n",
            "Iteration 55, loss = 1.59901732\n",
            "Iteration 56, loss = 1.59875975\n",
            "Iteration 57, loss = 1.59854007\n",
            "Iteration 58, loss = 1.59829610\n",
            "Iteration 59, loss = 1.59804279\n",
            "Iteration 60, loss = 1.59782769\n",
            "Iteration 61, loss = 1.59756431\n",
            "Iteration 62, loss = 1.59736116\n",
            "Iteration 63, loss = 1.59715603\n",
            "Iteration 64, loss = 1.59695412\n",
            "Iteration 65, loss = 1.59673644\n",
            "Iteration 66, loss = 1.59653322\n",
            "Iteration 67, loss = 1.59637333\n",
            "Iteration 68, loss = 1.59610197\n",
            "Iteration 69, loss = 1.59590825\n",
            "Iteration 70, loss = 1.59576675\n",
            "Iteration 71, loss = 1.59556914\n",
            "Iteration 72, loss = 1.59541129\n",
            "Iteration 73, loss = 1.59521744\n",
            "Iteration 74, loss = 1.59503714\n",
            "Iteration 75, loss = 1.59488947\n",
            "Iteration 76, loss = 1.59470725\n",
            "Iteration 77, loss = 1.59455167\n",
            "Iteration 78, loss = 1.59440620\n",
            "Iteration 79, loss = 1.59423450\n",
            "Iteration 80, loss = 1.59405191\n",
            "Iteration 81, loss = 1.59390109\n",
            "Iteration 82, loss = 1.59381476\n",
            "Iteration 83, loss = 1.59363115\n",
            "Iteration 84, loss = 1.59344417\n",
            "Iteration 85, loss = 1.59331471\n",
            "Iteration 86, loss = 1.59318080\n",
            "Iteration 87, loss = 1.59302430\n",
            "Iteration 88, loss = 1.59288628\n",
            "Iteration 89, loss = 1.59273166\n",
            "Iteration 90, loss = 1.59259285\n",
            "Iteration 91, loss = 1.59247014\n",
            "Iteration 92, loss = 1.59230661\n",
            "Iteration 93, loss = 1.59215973\n",
            "Iteration 94, loss = 1.59203358\n",
            "Iteration 95, loss = 1.59188355\n",
            "Iteration 96, loss = 1.59174602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 97, loss = 1.59160151\n",
            "Iteration 98, loss = 1.59146587\n",
            "Iteration 99, loss = 1.59131154\n",
            "Iteration 100, loss = 1.59120575\n",
            "Iteration 1, loss = 1.62552038\n",
            "Iteration 2, loss = 1.61783480\n",
            "Iteration 3, loss = 1.61144565\n",
            "Iteration 4, loss = 1.60541862\n",
            "Iteration 5, loss = 1.60224694\n",
            "Iteration 6, loss = 1.59932993\n",
            "Iteration 7, loss = 1.59654178\n",
            "Iteration 8, loss = 1.59462011\n",
            "Iteration 9, loss = 1.59454333\n",
            "Iteration 10, loss = 1.59385906\n",
            "Iteration 11, loss = 1.59339492\n",
            "Iteration 12, loss = 1.59265951\n",
            "Iteration 13, loss = 1.59207247\n",
            "Iteration 14, loss = 1.59107674\n",
            "Iteration 15, loss = 1.58984623\n",
            "Iteration 16, loss = 1.58859650\n",
            "Iteration 17, loss = 1.58697447\n",
            "Iteration 18, loss = 1.58529741\n",
            "Iteration 19, loss = 1.58391003\n",
            "Iteration 20, loss = 1.58214947\n",
            "Iteration 21, loss = 1.58026717\n",
            "Iteration 22, loss = 1.57846381\n",
            "Iteration 23, loss = 1.57671972\n",
            "Iteration 24, loss = 1.57449031\n",
            "Iteration 25, loss = 1.57231592\n",
            "Iteration 26, loss = 1.57016374\n",
            "Iteration 27, loss = 1.56765042\n",
            "Iteration 28, loss = 1.56504689\n",
            "Iteration 29, loss = 1.56215561\n",
            "Iteration 30, loss = 1.55936761\n",
            "Iteration 31, loss = 1.55637401\n",
            "Iteration 32, loss = 1.55344515\n",
            "Iteration 33, loss = 1.55027028\n",
            "Iteration 34, loss = 1.54674935\n",
            "Iteration 35, loss = 1.54324982\n",
            "Iteration 36, loss = 1.53955254\n",
            "Iteration 37, loss = 1.53592052\n",
            "Iteration 38, loss = 1.53200163\n",
            "Iteration 39, loss = 1.52794757\n",
            "Iteration 40, loss = 1.52375476\n",
            "Iteration 41, loss = 1.51962229\n",
            "Iteration 42, loss = 1.51521710\n",
            "Iteration 43, loss = 1.51081983\n",
            "Iteration 44, loss = 1.50617876\n",
            "Iteration 45, loss = 1.50170276\n",
            "Iteration 46, loss = 1.49678087\n",
            "Iteration 47, loss = 1.49197720\n",
            "Iteration 48, loss = 1.48707351\n",
            "Iteration 49, loss = 1.48216359\n",
            "Iteration 50, loss = 1.47687075\n",
            "Iteration 51, loss = 1.47187089\n",
            "Iteration 52, loss = 1.46653795\n",
            "Iteration 53, loss = 1.46113897\n",
            "Iteration 54, loss = 1.45586525\n",
            "Iteration 55, loss = 1.45045015\n",
            "Iteration 56, loss = 1.44478800\n",
            "Iteration 57, loss = 1.43939285\n",
            "Iteration 58, loss = 1.43390773\n",
            "Iteration 59, loss = 1.42821217\n",
            "Iteration 60, loss = 1.42248978\n",
            "Iteration 61, loss = 1.41660213\n",
            "Iteration 62, loss = 1.41106944\n",
            "Iteration 63, loss = 1.40498122\n",
            "Iteration 64, loss = 1.39950939\n",
            "Iteration 65, loss = 1.39348330\n",
            "Iteration 66, loss = 1.38772490\n",
            "Iteration 67, loss = 1.38236751\n",
            "Iteration 68, loss = 1.37677412\n",
            "Iteration 69, loss = 1.37142934\n",
            "Iteration 70, loss = 1.36624979\n",
            "Iteration 71, loss = 1.36146569\n",
            "Iteration 72, loss = 1.35627972\n",
            "Iteration 73, loss = 1.35105700\n",
            "Iteration 74, loss = 1.34623590\n",
            "Iteration 75, loss = 1.34150090\n",
            "Iteration 76, loss = 1.33677929\n",
            "Iteration 77, loss = 1.33186280\n",
            "Iteration 78, loss = 1.32742249\n",
            "Iteration 79, loss = 1.32280805\n",
            "Iteration 80, loss = 1.31847198\n",
            "Iteration 81, loss = 1.31408136\n",
            "Iteration 82, loss = 1.31026979\n",
            "Iteration 83, loss = 1.30577066\n",
            "Iteration 84, loss = 1.30179214\n",
            "Iteration 85, loss = 1.29774570\n",
            "Iteration 86, loss = 1.29387765\n",
            "Iteration 87, loss = 1.29006092\n",
            "Iteration 88, loss = 1.28620992\n",
            "Iteration 89, loss = 1.28266414\n",
            "Iteration 90, loss = 1.27889890\n",
            "Iteration 91, loss = 1.27566737\n",
            "Iteration 92, loss = 1.27198977\n",
            "Iteration 93, loss = 1.26857323\n",
            "Iteration 94, loss = 1.26534038\n",
            "Iteration 95, loss = 1.26196103\n",
            "Iteration 96, loss = 1.25906499\n",
            "Iteration 97, loss = 1.25617630\n",
            "Iteration 98, loss = 1.25281056\n",
            "Iteration 99, loss = 1.25037209\n",
            "Iteration 100, loss = 1.24727152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.63255064\n",
            "Iteration 2, loss = 1.60588450\n",
            "Iteration 3, loss = 1.60510853\n",
            "Iteration 4, loss = 1.59657831\n",
            "Iteration 5, loss = 1.58678869\n",
            "Iteration 6, loss = 1.57229460\n",
            "Iteration 7, loss = 1.54672130\n",
            "Iteration 8, loss = 1.52295149\n",
            "Iteration 9, loss = 1.49379622\n",
            "Iteration 10, loss = 1.46605910\n",
            "Iteration 11, loss = 1.43012618\n",
            "Iteration 12, loss = 1.39367922\n",
            "Iteration 13, loss = 1.35387573\n",
            "Iteration 14, loss = 1.32146453\n",
            "Iteration 15, loss = 1.28431129\n",
            "Iteration 16, loss = 1.26009538\n",
            "Iteration 17, loss = 1.23332530\n",
            "Iteration 18, loss = 1.21180086\n",
            "Iteration 19, loss = 1.19704681\n",
            "Iteration 20, loss = 1.17855243\n",
            "Iteration 21, loss = 1.16861257\n",
            "Iteration 22, loss = 1.15662318\n",
            "Iteration 23, loss = 1.15093201\n",
            "Iteration 24, loss = 1.14167327\n",
            "Iteration 25, loss = 1.13910231\n",
            "Iteration 26, loss = 1.13254048\n",
            "Iteration 27, loss = 1.13045513\n",
            "Iteration 28, loss = 1.12805459\n",
            "Iteration 29, loss = 1.12291117\n",
            "Iteration 30, loss = 1.11960219\n",
            "Iteration 31, loss = 1.11686727\n",
            "Iteration 32, loss = 1.11627320\n",
            "Iteration 33, loss = 1.11557123\n",
            "Iteration 34, loss = 1.11378068\n",
            "Iteration 35, loss = 1.11500293\n",
            "Iteration 36, loss = 1.11071608\n",
            "Iteration 37, loss = 1.10967043\n",
            "Iteration 38, loss = 1.10876200\n",
            "Iteration 39, loss = 1.10692387\n",
            "Iteration 40, loss = 1.10461800\n",
            "Iteration 41, loss = 1.10361074\n",
            "Iteration 42, loss = 1.10231817\n",
            "Iteration 43, loss = 1.10286748\n",
            "Iteration 44, loss = 1.10185360\n",
            "Iteration 45, loss = 1.10358725\n",
            "Iteration 46, loss = 1.09989528\n",
            "Iteration 47, loss = 1.09949701\n",
            "Iteration 48, loss = 1.10041723\n",
            "Iteration 49, loss = 1.09863076\n",
            "Iteration 50, loss = 1.09799065\n",
            "Iteration 51, loss = 1.09884571\n",
            "Iteration 52, loss = 1.09607441\n",
            "Iteration 53, loss = 1.09850993\n",
            "Iteration 54, loss = 1.09464325\n",
            "Iteration 55, loss = 1.09884471\n",
            "Iteration 56, loss = 1.09571317\n",
            "Iteration 57, loss = 1.09477552\n",
            "Iteration 58, loss = 1.09626662\n",
            "Iteration 59, loss = 1.09485568\n",
            "Iteration 60, loss = 1.09274328\n",
            "Iteration 61, loss = 1.09030247\n",
            "Iteration 62, loss = 1.09125448\n",
            "Iteration 63, loss = 1.09026264\n",
            "Iteration 64, loss = 1.09702818\n",
            "Iteration 65, loss = 1.08987186\n",
            "Iteration 66, loss = 1.09068767\n",
            "Iteration 67, loss = 1.08907621\n",
            "Iteration 68, loss = 1.09038761\n",
            "Iteration 69, loss = 1.08896726\n",
            "Iteration 70, loss = 1.08644074\n",
            "Iteration 71, loss = 1.09436822\n",
            "Iteration 72, loss = 1.08972134\n",
            "Iteration 73, loss = 1.08651322\n",
            "Iteration 74, loss = 1.08732628\n",
            "Iteration 75, loss = 1.08469009\n",
            "Iteration 76, loss = 1.09141166\n",
            "Iteration 77, loss = 1.08569981\n",
            "Iteration 78, loss = 1.09172558\n",
            "Iteration 79, loss = 1.08633407\n",
            "Iteration 80, loss = 1.08535936\n",
            "Iteration 81, loss = 1.08091442\n",
            "Iteration 82, loss = 1.08939067\n",
            "Iteration 83, loss = 1.08031606\n",
            "Iteration 84, loss = 1.08907073\n",
            "Iteration 85, loss = 1.08323476\n",
            "Iteration 86, loss = 1.08782879\n",
            "Iteration 87, loss = 1.08030272\n",
            "Iteration 88, loss = 1.08115230\n",
            "Iteration 89, loss = 1.08495969\n",
            "Iteration 90, loss = 1.07804176\n",
            "Iteration 91, loss = 1.08282455\n",
            "Iteration 92, loss = 1.07678787\n",
            "Iteration 93, loss = 1.07933770\n",
            "Iteration 94, loss = 1.07544572\n",
            "Iteration 95, loss = 1.07546029\n",
            "Iteration 96, loss = 1.07816905\n",
            "Iteration 97, loss = 1.07662030\n",
            "Iteration 98, loss = 1.07414338\n",
            "Iteration 99, loss = 1.07383602\n",
            "Iteration 100, loss = 1.08236944\n",
            "Iteration 1, loss = 1.74300499\n",
            "Iteration 2, loss = 1.73834407\n",
            "Iteration 3, loss = 1.73413253\n",
            "Iteration 4, loss = 1.73020388\n",
            "Iteration 5, loss = 1.72590218\n",
            "Iteration 6, loss = 1.72189529\n",
            "Iteration 7, loss = 1.71821254\n",
            "Iteration 8, loss = 1.71443941\n",
            "Iteration 9, loss = 1.71079323\n",
            "Iteration 10, loss = 1.70726405\n",
            "Iteration 11, loss = 1.70406020\n",
            "Iteration 12, loss = 1.70051114\n",
            "Iteration 13, loss = 1.69721664\n",
            "Iteration 14, loss = 1.69403823\n",
            "Iteration 15, loss = 1.69093588\n",
            "Iteration 16, loss = 1.68797508\n",
            "Iteration 17, loss = 1.68500748\n",
            "Iteration 18, loss = 1.68227411\n",
            "Iteration 19, loss = 1.67939592\n",
            "Iteration 20, loss = 1.67688196\n",
            "Iteration 21, loss = 1.67421483\n",
            "Iteration 22, loss = 1.67168606\n",
            "Iteration 23, loss = 1.66903807\n",
            "Iteration 24, loss = 1.66684087\n",
            "Iteration 25, loss = 1.66447294\n",
            "Iteration 26, loss = 1.66209119\n",
            "Iteration 27, loss = 1.65981038\n",
            "Iteration 28, loss = 1.65775823\n",
            "Iteration 29, loss = 1.65571639\n",
            "Iteration 30, loss = 1.65363838\n",
            "Iteration 31, loss = 1.65173423\n",
            "Iteration 32, loss = 1.64966518\n",
            "Iteration 33, loss = 1.64773494\n",
            "Iteration 34, loss = 1.64595105\n",
            "Iteration 35, loss = 1.64404493\n",
            "Iteration 36, loss = 1.64242522\n",
            "Iteration 37, loss = 1.64072117\n",
            "Iteration 38, loss = 1.63911531\n",
            "Iteration 39, loss = 1.63732371\n",
            "Iteration 40, loss = 1.63585157\n",
            "Iteration 41, loss = 1.63437042\n",
            "Iteration 42, loss = 1.63287099\n",
            "Iteration 43, loss = 1.63149272\n",
            "Iteration 44, loss = 1.62995899\n",
            "Iteration 45, loss = 1.62868890\n",
            "Iteration 46, loss = 1.62740190\n",
            "Iteration 47, loss = 1.62597467\n",
            "Iteration 48, loss = 1.62488366\n",
            "Iteration 49, loss = 1.62369950\n",
            "Iteration 50, loss = 1.62237345\n",
            "Iteration 51, loss = 1.62133245\n",
            "Iteration 52, loss = 1.62005383\n",
            "Iteration 53, loss = 1.61903828\n",
            "Iteration 54, loss = 1.61812026\n",
            "Iteration 55, loss = 1.61703244\n",
            "Iteration 56, loss = 1.61601163\n",
            "Iteration 57, loss = 1.61501878\n",
            "Iteration 58, loss = 1.61407517\n",
            "Iteration 59, loss = 1.61323101\n",
            "Iteration 60, loss = 1.61238404\n",
            "Iteration 61, loss = 1.61142629\n",
            "Iteration 62, loss = 1.61069696\n",
            "Iteration 63, loss = 1.60984574\n",
            "Iteration 64, loss = 1.60912354\n",
            "Iteration 65, loss = 1.60836219\n",
            "Iteration 66, loss = 1.60759677\n",
            "Iteration 67, loss = 1.60692110\n",
            "Iteration 68, loss = 1.60622248\n",
            "Iteration 69, loss = 1.60545759\n",
            "Iteration 70, loss = 1.60480738\n",
            "Iteration 71, loss = 1.60422072\n",
            "Iteration 72, loss = 1.60359426\n",
            "Iteration 73, loss = 1.60298177\n",
            "Iteration 74, loss = 1.60240598\n",
            "Iteration 75, loss = 1.60184859\n",
            "Iteration 76, loss = 1.60125267\n",
            "Iteration 77, loss = 1.60074387\n",
            "Iteration 78, loss = 1.60022725\n",
            "Iteration 79, loss = 1.59968341\n",
            "Iteration 80, loss = 1.59921122\n",
            "Iteration 81, loss = 1.59870471\n",
            "Iteration 82, loss = 1.59824325\n",
            "Iteration 83, loss = 1.59784337\n",
            "Iteration 84, loss = 1.59731025\n",
            "Iteration 85, loss = 1.59683851\n",
            "Iteration 86, loss = 1.59645185\n",
            "Iteration 87, loss = 1.59601923\n",
            "Iteration 88, loss = 1.59570068\n",
            "Iteration 89, loss = 1.59524060\n",
            "Iteration 90, loss = 1.59487981\n",
            "Iteration 91, loss = 1.59442249\n",
            "Iteration 92, loss = 1.59405932\n",
            "Iteration 93, loss = 1.59373593\n",
            "Iteration 94, loss = 1.59334100\n",
            "Iteration 95, loss = 1.59293925\n",
            "Iteration 96, loss = 1.59260721\n",
            "Iteration 97, loss = 1.59228062\n",
            "Iteration 98, loss = 1.59191390\n",
            "Iteration 99, loss = 1.59161162\n",
            "Iteration 100, loss = 1.59126137\n",
            "Iteration 1, loss = 1.73524989\n",
            "Iteration 2, loss = 1.69537234\n",
            "Iteration 3, loss = 1.66684486\n",
            "Iteration 4, loss = 1.64650031\n",
            "Iteration 5, loss = 1.62787977\n",
            "Iteration 6, loss = 1.61601268\n",
            "Iteration 7, loss = 1.60875168\n",
            "Iteration 8, loss = 1.60349051\n",
            "Iteration 9, loss = 1.59941557\n",
            "Iteration 10, loss = 1.59637090\n",
            "Iteration 11, loss = 1.59461846\n",
            "Iteration 12, loss = 1.59147895\n",
            "Iteration 13, loss = 1.58897989\n",
            "Iteration 14, loss = 1.58634093\n",
            "Iteration 15, loss = 1.58443274\n",
            "Iteration 16, loss = 1.58215295\n",
            "Iteration 17, loss = 1.58040156\n",
            "Iteration 18, loss = 1.57813445\n",
            "Iteration 19, loss = 1.57586893\n",
            "Iteration 20, loss = 1.57335101\n",
            "Iteration 21, loss = 1.57020081\n",
            "Iteration 22, loss = 1.56718639\n",
            "Iteration 23, loss = 1.56378810\n",
            "Iteration 24, loss = 1.56018844\n",
            "Iteration 25, loss = 1.55612549\n",
            "Iteration 26, loss = 1.55209469\n",
            "Iteration 27, loss = 1.54788788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 1.54300295\n",
            "Iteration 29, loss = 1.53825674\n",
            "Iteration 30, loss = 1.53293225\n",
            "Iteration 31, loss = 1.52749600\n",
            "Iteration 32, loss = 1.52136647\n",
            "Iteration 33, loss = 1.51563313\n",
            "Iteration 34, loss = 1.50846188\n",
            "Iteration 35, loss = 1.50157296\n",
            "Iteration 36, loss = 1.49423370\n",
            "Iteration 37, loss = 1.48673698\n",
            "Iteration 38, loss = 1.47907068\n",
            "Iteration 39, loss = 1.47060945\n",
            "Iteration 40, loss = 1.46223361\n",
            "Iteration 41, loss = 1.45333202\n",
            "Iteration 42, loss = 1.44481874\n",
            "Iteration 43, loss = 1.43557982\n",
            "Iteration 44, loss = 1.42591772\n",
            "Iteration 45, loss = 1.41653316\n",
            "Iteration 46, loss = 1.40708170\n",
            "Iteration 47, loss = 1.39727183\n",
            "Iteration 48, loss = 1.38772396\n",
            "Iteration 49, loss = 1.37819161\n",
            "Iteration 50, loss = 1.36825593\n",
            "Iteration 51, loss = 1.35899665\n",
            "Iteration 52, loss = 1.34945308\n",
            "Iteration 53, loss = 1.34031504\n",
            "Iteration 54, loss = 1.33125120\n",
            "Iteration 55, loss = 1.32234355\n",
            "Iteration 56, loss = 1.31345695\n",
            "Iteration 57, loss = 1.30521464\n",
            "Iteration 58, loss = 1.29706720\n",
            "Iteration 59, loss = 1.28924063\n",
            "Iteration 60, loss = 1.28187343\n",
            "Iteration 61, loss = 1.27455607\n",
            "Iteration 62, loss = 1.26727657\n",
            "Iteration 63, loss = 1.26084937\n",
            "Iteration 64, loss = 1.25445601\n",
            "Iteration 65, loss = 1.24847837\n",
            "Iteration 66, loss = 1.24284469\n",
            "Iteration 67, loss = 1.23705263\n",
            "Iteration 68, loss = 1.23176152\n",
            "Iteration 69, loss = 1.22655825\n",
            "Iteration 70, loss = 1.22189472\n",
            "Iteration 71, loss = 1.21725674\n",
            "Iteration 72, loss = 1.21281867\n",
            "Iteration 73, loss = 1.20890503\n",
            "Iteration 74, loss = 1.20477206\n",
            "Iteration 75, loss = 1.20108406\n",
            "Iteration 76, loss = 1.19751519\n",
            "Iteration 77, loss = 1.19390366\n",
            "Iteration 78, loss = 1.19060094\n",
            "Iteration 79, loss = 1.18796684\n",
            "Iteration 80, loss = 1.18450191\n",
            "Iteration 81, loss = 1.18175466\n",
            "Iteration 82, loss = 1.17882319\n",
            "Iteration 83, loss = 1.17687986\n",
            "Iteration 84, loss = 1.17395619\n",
            "Iteration 85, loss = 1.17180366\n",
            "Iteration 86, loss = 1.16901720\n",
            "Iteration 87, loss = 1.16676573\n",
            "Iteration 88, loss = 1.16503827\n",
            "Iteration 89, loss = 1.16277140\n",
            "Iteration 90, loss = 1.16069926\n",
            "Iteration 91, loss = 1.15886042\n",
            "Iteration 92, loss = 1.15661797\n",
            "Iteration 93, loss = 1.15503715\n",
            "Iteration 94, loss = 1.15341267\n",
            "Iteration 95, loss = 1.15149286\n",
            "Iteration 96, loss = 1.14984981\n",
            "Iteration 97, loss = 1.14849411\n",
            "Iteration 98, loss = 1.14664367\n",
            "Iteration 99, loss = 1.14526886\n",
            "Iteration 100, loss = 1.14375445\n",
            "Iteration 1, loss = 1.70331862\n",
            "Iteration 2, loss = 1.60468568\n",
            "Iteration 3, loss = 1.58560941\n",
            "Iteration 4, loss = 1.57647977\n",
            "Iteration 5, loss = 1.54713480\n",
            "Iteration 6, loss = 1.51466827\n",
            "Iteration 7, loss = 1.47836053\n",
            "Iteration 8, loss = 1.43199112\n",
            "Iteration 9, loss = 1.37298916\n",
            "Iteration 10, loss = 1.31920370\n",
            "Iteration 11, loss = 1.27333767\n",
            "Iteration 12, loss = 1.22201012\n",
            "Iteration 13, loss = 1.19127869\n",
            "Iteration 14, loss = 1.16498819\n",
            "Iteration 15, loss = 1.14071301\n",
            "Iteration 16, loss = 1.13318532\n",
            "Iteration 17, loss = 1.11588375\n",
            "Iteration 18, loss = 1.11179512\n",
            "Iteration 19, loss = 1.11059205\n",
            "Iteration 20, loss = 1.10405891\n",
            "Iteration 21, loss = 1.10258094\n",
            "Iteration 22, loss = 1.09464546\n",
            "Iteration 23, loss = 1.09235327\n",
            "Iteration 24, loss = 1.09716306\n",
            "Iteration 25, loss = 1.08404315\n",
            "Iteration 26, loss = 1.09114561\n",
            "Iteration 27, loss = 1.08337730\n",
            "Iteration 28, loss = 1.08029631\n",
            "Iteration 29, loss = 1.07851174\n",
            "Iteration 30, loss = 1.07173680\n",
            "Iteration 31, loss = 1.08696900\n",
            "Iteration 32, loss = 1.06081925\n",
            "Iteration 33, loss = 1.07421386\n",
            "Iteration 34, loss = 1.06278255\n",
            "Iteration 35, loss = 1.06565437\n",
            "Iteration 36, loss = 1.05398431\n",
            "Iteration 37, loss = 1.03507769\n",
            "Iteration 38, loss = 1.03958523\n",
            "Iteration 39, loss = 1.02184873\n",
            "Iteration 40, loss = 1.03255338\n",
            "Iteration 41, loss = 1.01427799\n",
            "Iteration 42, loss = 1.00958589\n",
            "Iteration 43, loss = 0.99389321\n",
            "Iteration 44, loss = 1.00371274\n",
            "Iteration 45, loss = 0.99108161\n",
            "Iteration 46, loss = 0.99690764\n",
            "Iteration 47, loss = 0.99611532\n",
            "Iteration 48, loss = 0.98580755\n",
            "Iteration 49, loss = 0.97672745\n",
            "Iteration 50, loss = 0.96846558\n",
            "Iteration 51, loss = 0.96527432\n",
            "Iteration 52, loss = 0.97871396\n",
            "Iteration 53, loss = 0.97204809\n",
            "Iteration 54, loss = 0.99986379\n",
            "Iteration 55, loss = 1.01602784\n",
            "Iteration 56, loss = 0.98135915\n",
            "Iteration 57, loss = 0.97269168\n",
            "Iteration 58, loss = 0.97679701\n",
            "Iteration 59, loss = 0.95547908\n",
            "Iteration 60, loss = 0.95089684\n",
            "Iteration 61, loss = 0.94630781\n",
            "Iteration 62, loss = 0.92189433\n",
            "Iteration 63, loss = 0.93336022\n",
            "Iteration 64, loss = 0.93523726\n",
            "Iteration 65, loss = 0.95196728\n",
            "Iteration 66, loss = 0.94108201\n",
            "Iteration 67, loss = 0.91164548\n",
            "Iteration 68, loss = 0.92732536\n",
            "Iteration 69, loss = 0.92883541\n",
            "Iteration 70, loss = 0.90880392\n",
            "Iteration 71, loss = 0.90769770\n",
            "Iteration 72, loss = 0.89168570\n",
            "Iteration 73, loss = 0.91471226\n",
            "Iteration 74, loss = 0.89075051\n",
            "Iteration 75, loss = 0.86790067\n",
            "Iteration 76, loss = 0.88871226\n",
            "Iteration 77, loss = 0.87537289\n",
            "Iteration 78, loss = 0.86362654\n",
            "Iteration 79, loss = 0.86384650\n",
            "Iteration 80, loss = 0.87184175\n",
            "Iteration 81, loss = 0.86580883\n",
            "Iteration 82, loss = 0.85092043\n",
            "Iteration 83, loss = 0.84947019\n",
            "Iteration 84, loss = 0.86303362\n",
            "Iteration 85, loss = 0.84879987\n",
            "Iteration 86, loss = 0.83862989\n",
            "Iteration 87, loss = 0.83802941\n",
            "Iteration 88, loss = 0.83699357\n",
            "Iteration 89, loss = 0.83609918\n",
            "Iteration 90, loss = 0.83460129\n",
            "Iteration 91, loss = 0.82989347\n",
            "Iteration 92, loss = 0.82495831\n",
            "Iteration 93, loss = 0.82605551\n",
            "Iteration 94, loss = 0.83171885\n",
            "Iteration 95, loss = 0.83535653\n",
            "Iteration 96, loss = 0.83042635\n",
            "Iteration 97, loss = 0.84539721\n",
            "Iteration 98, loss = 0.85656162\n",
            "Iteration 99, loss = 0.84736275\n",
            "Iteration 100, loss = 0.82956954\n",
            "Iteration 1, loss = 1.62514803\n",
            "Iteration 2, loss = 1.62357034\n",
            "Iteration 3, loss = 1.62221531\n",
            "Iteration 4, loss = 1.62082877\n",
            "Iteration 5, loss = 1.61957584\n",
            "Iteration 6, loss = 1.61818087\n",
            "Iteration 7, loss = 1.61721021\n",
            "Iteration 8, loss = 1.61593292\n",
            "Iteration 9, loss = 1.61499557\n",
            "Iteration 10, loss = 1.61393932\n",
            "Iteration 11, loss = 1.61307404\n",
            "Iteration 12, loss = 1.61193666\n",
            "Iteration 13, loss = 1.61120180\n",
            "Iteration 14, loss = 1.61031065\n",
            "Iteration 15, loss = 1.60961058\n",
            "Iteration 16, loss = 1.60876844\n",
            "Iteration 17, loss = 1.60800421\n",
            "Iteration 18, loss = 1.60752973\n",
            "Iteration 19, loss = 1.60678952\n",
            "Iteration 20, loss = 1.60623150\n",
            "Iteration 21, loss = 1.60578555\n",
            "Iteration 22, loss = 1.60520039\n",
            "Iteration 23, loss = 1.60462347\n",
            "Iteration 24, loss = 1.60422779\n",
            "Iteration 25, loss = 1.60380633\n",
            "Iteration 26, loss = 1.60334456\n",
            "Iteration 27, loss = 1.60302055\n",
            "Iteration 28, loss = 1.60266024\n",
            "Iteration 29, loss = 1.60230447\n",
            "Iteration 30, loss = 1.60196720\n",
            "Iteration 31, loss = 1.60177122\n",
            "Iteration 32, loss = 1.60138339\n",
            "Iteration 33, loss = 1.60110599\n",
            "Iteration 34, loss = 1.60085885\n",
            "Iteration 35, loss = 1.60056150\n",
            "Iteration 36, loss = 1.60042994\n",
            "Iteration 37, loss = 1.60020220\n",
            "Iteration 38, loss = 1.60008654\n",
            "Iteration 39, loss = 1.59979569\n",
            "Iteration 40, loss = 1.59960692\n",
            "Iteration 41, loss = 1.59948593\n",
            "Iteration 42, loss = 1.59937257\n",
            "Iteration 43, loss = 1.59923227\n",
            "Iteration 44, loss = 1.59903495\n",
            "Iteration 45, loss = 1.59891913\n",
            "Iteration 46, loss = 1.59882802\n",
            "Iteration 47, loss = 1.59866173\n",
            "Iteration 48, loss = 1.59862761\n",
            "Iteration 49, loss = 1.59853198\n",
            "Iteration 50, loss = 1.59841529\n",
            "Iteration 51, loss = 1.59832002\n",
            "Iteration 52, loss = 1.59820221\n",
            "Iteration 53, loss = 1.59817804\n",
            "Iteration 54, loss = 1.59813556\n",
            "Iteration 55, loss = 1.59799134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 56, loss = 1.59794489\n",
            "Iteration 57, loss = 1.59788551\n",
            "Iteration 58, loss = 1.59778626\n",
            "Iteration 59, loss = 1.59773331\n",
            "Iteration 60, loss = 1.59768252\n",
            "Iteration 61, loss = 1.59761813\n",
            "Iteration 62, loss = 1.59757841\n",
            "Iteration 63, loss = 1.59751377\n",
            "Iteration 64, loss = 1.59749227\n",
            "Iteration 65, loss = 1.59745344\n",
            "Iteration 66, loss = 1.59743563\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.62413825\n",
            "Iteration 2, loss = 1.61130426\n",
            "Iteration 3, loss = 1.60460721\n",
            "Iteration 4, loss = 1.60065308\n",
            "Iteration 5, loss = 1.59873207\n",
            "Iteration 6, loss = 1.59797993\n",
            "Iteration 7, loss = 1.59863936\n",
            "Iteration 8, loss = 1.59910999\n",
            "Iteration 9, loss = 1.59889620\n",
            "Iteration 10, loss = 1.59860874\n",
            "Iteration 11, loss = 1.59823005\n",
            "Iteration 12, loss = 1.59775219\n",
            "Iteration 13, loss = 1.59642589\n",
            "Iteration 14, loss = 1.59582701\n",
            "Iteration 15, loss = 1.59542959\n",
            "Iteration 16, loss = 1.59484127\n",
            "Iteration 17, loss = 1.59498449\n",
            "Iteration 18, loss = 1.59436404\n",
            "Iteration 19, loss = 1.59404276\n",
            "Iteration 20, loss = 1.59380298\n",
            "Iteration 21, loss = 1.59324053\n",
            "Iteration 22, loss = 1.59256929\n",
            "Iteration 23, loss = 1.59199796\n",
            "Iteration 24, loss = 1.59150714\n",
            "Iteration 25, loss = 1.59089017\n",
            "Iteration 26, loss = 1.59024541\n",
            "Iteration 27, loss = 1.58985341\n",
            "Iteration 28, loss = 1.58908630\n",
            "Iteration 29, loss = 1.58828174\n",
            "Iteration 30, loss = 1.58746741\n",
            "Iteration 31, loss = 1.58685989\n",
            "Iteration 32, loss = 1.58578733\n",
            "Iteration 33, loss = 1.58528604\n",
            "Iteration 34, loss = 1.58387776\n",
            "Iteration 35, loss = 1.58308401\n",
            "Iteration 36, loss = 1.58175730\n",
            "Iteration 37, loss = 1.58059903\n",
            "Iteration 38, loss = 1.57960146\n",
            "Iteration 39, loss = 1.57828489\n",
            "Iteration 40, loss = 1.57684009\n",
            "Iteration 41, loss = 1.57527361\n",
            "Iteration 42, loss = 1.57399980\n",
            "Iteration 43, loss = 1.57239463\n",
            "Iteration 44, loss = 1.57046482\n",
            "Iteration 45, loss = 1.56872418\n",
            "Iteration 46, loss = 1.56692082\n",
            "Iteration 47, loss = 1.56499870\n",
            "Iteration 48, loss = 1.56294944\n",
            "Iteration 49, loss = 1.56077964\n",
            "Iteration 50, loss = 1.55837882\n",
            "Iteration 51, loss = 1.55613935\n",
            "Iteration 52, loss = 1.55355608\n",
            "Iteration 53, loss = 1.55107781\n",
            "Iteration 54, loss = 1.54847172\n",
            "Iteration 55, loss = 1.54555404\n",
            "Iteration 56, loss = 1.54240661\n",
            "Iteration 57, loss = 1.53940370\n",
            "Iteration 58, loss = 1.53615154\n",
            "Iteration 59, loss = 1.53288361\n",
            "Iteration 60, loss = 1.52966791\n",
            "Iteration 61, loss = 1.52598095\n",
            "Iteration 62, loss = 1.52229467\n",
            "Iteration 63, loss = 1.51841468\n",
            "Iteration 64, loss = 1.51459440\n",
            "Iteration 65, loss = 1.51076759\n",
            "Iteration 66, loss = 1.50677609\n",
            "Iteration 67, loss = 1.50226598\n",
            "Iteration 68, loss = 1.49791452\n",
            "Iteration 69, loss = 1.49324176\n",
            "Iteration 70, loss = 1.48860117\n",
            "Iteration 71, loss = 1.48394169\n",
            "Iteration 72, loss = 1.47903600\n",
            "Iteration 73, loss = 1.47418356\n",
            "Iteration 74, loss = 1.46911919\n",
            "Iteration 75, loss = 1.46406271\n",
            "Iteration 76, loss = 1.45896925\n",
            "Iteration 77, loss = 1.45373997\n",
            "Iteration 78, loss = 1.44837355\n",
            "Iteration 79, loss = 1.44346548\n",
            "Iteration 80, loss = 1.43804654\n",
            "Iteration 81, loss = 1.43241850\n",
            "Iteration 82, loss = 1.42683419\n",
            "Iteration 83, loss = 1.42178418\n",
            "Iteration 84, loss = 1.41614926\n",
            "Iteration 85, loss = 1.41095974\n",
            "Iteration 86, loss = 1.40509143\n",
            "Iteration 87, loss = 1.39974894\n",
            "Iteration 88, loss = 1.39477447\n",
            "Iteration 89, loss = 1.38922091\n",
            "Iteration 90, loss = 1.38375426\n",
            "Iteration 91, loss = 1.37874461\n",
            "Iteration 92, loss = 1.37306624\n",
            "Iteration 93, loss = 1.36797079\n",
            "Iteration 94, loss = 1.36291607\n",
            "Iteration 95, loss = 1.35766496\n",
            "Iteration 96, loss = 1.35259699\n",
            "Iteration 97, loss = 1.34756799\n",
            "Iteration 98, loss = 1.34256990\n",
            "Iteration 99, loss = 1.33784124\n",
            "Iteration 100, loss = 1.33311352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.64958872\n",
            "Iteration 2, loss = 1.61964117\n",
            "Iteration 3, loss = 1.61767045\n",
            "Iteration 4, loss = 1.61146134\n",
            "Iteration 5, loss = 1.59457579\n",
            "Iteration 6, loss = 1.59172805\n",
            "Iteration 7, loss = 1.59025577\n",
            "Iteration 8, loss = 1.58311074\n",
            "Iteration 9, loss = 1.57422286\n",
            "Iteration 10, loss = 1.56300108\n",
            "Iteration 11, loss = 1.55084430\n",
            "Iteration 12, loss = 1.53517208\n",
            "Iteration 13, loss = 1.51303536\n",
            "Iteration 14, loss = 1.48718840\n",
            "Iteration 15, loss = 1.45969961\n",
            "Iteration 16, loss = 1.42908453\n",
            "Iteration 17, loss = 1.39869809\n",
            "Iteration 18, loss = 1.36561418\n",
            "Iteration 19, loss = 1.33142790\n",
            "Iteration 20, loss = 1.30082288\n",
            "Iteration 21, loss = 1.27125901\n",
            "Iteration 22, loss = 1.24489561\n",
            "Iteration 23, loss = 1.22368607\n",
            "Iteration 24, loss = 1.20561518\n",
            "Iteration 25, loss = 1.18831244\n",
            "Iteration 26, loss = 1.17654179\n",
            "Iteration 27, loss = 1.16833477\n",
            "Iteration 28, loss = 1.15913908\n",
            "Iteration 29, loss = 1.15049198\n",
            "Iteration 30, loss = 1.14782410\n",
            "Iteration 31, loss = 1.15181649\n",
            "Iteration 32, loss = 1.13699986\n",
            "Iteration 33, loss = 1.14363519\n",
            "Iteration 34, loss = 1.13518737\n",
            "Iteration 35, loss = 1.12739132\n",
            "Iteration 36, loss = 1.12850008\n",
            "Iteration 37, loss = 1.12176180\n",
            "Iteration 38, loss = 1.12281344\n",
            "Iteration 39, loss = 1.12082621\n",
            "Iteration 40, loss = 1.11920565\n",
            "Iteration 41, loss = 1.11643245\n",
            "Iteration 42, loss = 1.11579666\n",
            "Iteration 43, loss = 1.11480869\n",
            "Iteration 44, loss = 1.11240326\n",
            "Iteration 45, loss = 1.11116320\n",
            "Iteration 46, loss = 1.11279496\n",
            "Iteration 47, loss = 1.11063768\n",
            "Iteration 48, loss = 1.11028695\n",
            "Iteration 49, loss = 1.11038863\n",
            "Iteration 50, loss = 1.10771273\n",
            "Iteration 51, loss = 1.10804022\n",
            "Iteration 52, loss = 1.10744465\n",
            "Iteration 53, loss = 1.10976739\n",
            "Iteration 54, loss = 1.11148675\n",
            "Iteration 55, loss = 1.11163659\n",
            "Iteration 56, loss = 1.10751483\n",
            "Iteration 57, loss = 1.11027028\n",
            "Iteration 58, loss = 1.10546471\n",
            "Iteration 59, loss = 1.10490269\n",
            "Iteration 60, loss = 1.10469999\n",
            "Iteration 61, loss = 1.10292932\n",
            "Iteration 62, loss = 1.10265451\n",
            "Iteration 63, loss = 1.10039203\n",
            "Iteration 64, loss = 1.10090414\n",
            "Iteration 65, loss = 1.10072924\n",
            "Iteration 66, loss = 1.10345305\n",
            "Iteration 67, loss = 1.09779245\n",
            "Iteration 68, loss = 1.10021440\n",
            "Iteration 69, loss = 1.09759943\n",
            "Iteration 70, loss = 1.09637914\n",
            "Iteration 71, loss = 1.09605803\n",
            "Iteration 72, loss = 1.09513342\n",
            "Iteration 73, loss = 1.09671581\n",
            "Iteration 74, loss = 1.09352047\n",
            "Iteration 75, loss = 1.09296978\n",
            "Iteration 76, loss = 1.09359200\n",
            "Iteration 77, loss = 1.09225603\n",
            "Iteration 78, loss = 1.08951824\n",
            "Iteration 79, loss = 1.09315681\n",
            "Iteration 80, loss = 1.09165877\n",
            "Iteration 81, loss = 1.08678968\n",
            "Iteration 82, loss = 1.08541427\n",
            "Iteration 83, loss = 1.08593830\n",
            "Iteration 84, loss = 1.08564181\n",
            "Iteration 85, loss = 1.08576724\n",
            "Iteration 86, loss = 1.08136546\n",
            "Iteration 87, loss = 1.08201593\n",
            "Iteration 88, loss = 1.08062022\n",
            "Iteration 89, loss = 1.07846430\n",
            "Iteration 90, loss = 1.07905217\n",
            "Iteration 91, loss = 1.07656410\n",
            "Iteration 92, loss = 1.07333506\n",
            "Iteration 93, loss = 1.07521506\n",
            "Iteration 94, loss = 1.07190097\n",
            "Iteration 95, loss = 1.06811755\n",
            "Iteration 96, loss = 1.06564331\n",
            "Iteration 97, loss = 1.06297650\n",
            "Iteration 98, loss = 1.06146308\n",
            "Iteration 99, loss = 1.05940549\n",
            "Iteration 100, loss = 1.05564086\n",
            "Iteration 1, loss = 1.73951492\n",
            "Iteration 2, loss = 1.73513191\n",
            "Iteration 3, loss = 1.73116586\n",
            "Iteration 4, loss = 1.72745703\n",
            "Iteration 5, loss = 1.72338028\n",
            "Iteration 6, loss = 1.71958543\n",
            "Iteration 7, loss = 1.71608894\n",
            "Iteration 8, loss = 1.71250483\n",
            "Iteration 9, loss = 1.70903339\n",
            "Iteration 10, loss = 1.70567198\n",
            "Iteration 11, loss = 1.70261708\n",
            "Iteration 12, loss = 1.69922310\n",
            "Iteration 13, loss = 1.69607252\n",
            "Iteration 14, loss = 1.69302946\n",
            "Iteration 15, loss = 1.69005579\n",
            "Iteration 16, loss = 1.68721481\n",
            "Iteration 17, loss = 1.68436341\n",
            "Iteration 18, loss = 1.68173844\n",
            "Iteration 19, loss = 1.67896795\n",
            "Iteration 20, loss = 1.67654633\n",
            "Iteration 21, loss = 1.67397800\n",
            "Iteration 22, loss = 1.67153876\n",
            "Iteration 23, loss = 1.66898212\n",
            "Iteration 24, loss = 1.66686048\n",
            "Iteration 25, loss = 1.66457367\n",
            "Iteration 26, loss = 1.66227190\n",
            "Iteration 27, loss = 1.66006436\n",
            "Iteration 28, loss = 1.65808061\n",
            "Iteration 29, loss = 1.65610392\n",
            "Iteration 30, loss = 1.65409188\n",
            "Iteration 31, loss = 1.65224927\n",
            "Iteration 32, loss = 1.65024438\n",
            "Iteration 33, loss = 1.64837159\n",
            "Iteration 34, loss = 1.64664338\n",
            "Iteration 35, loss = 1.64479447\n",
            "Iteration 36, loss = 1.64322496\n",
            "Iteration 37, loss = 1.64157007\n",
            "Iteration 38, loss = 1.64001155\n",
            "Iteration 39, loss = 1.63827314\n",
            "Iteration 40, loss = 1.63684033\n",
            "Iteration 41, loss = 1.63540251\n",
            "Iteration 42, loss = 1.63394349\n",
            "Iteration 43, loss = 1.63260069\n",
            "Iteration 44, loss = 1.63110962\n",
            "Iteration 45, loss = 1.62987105\n",
            "Iteration 46, loss = 1.62861659\n",
            "Iteration 47, loss = 1.62722578\n",
            "Iteration 48, loss = 1.62615669\n",
            "Iteration 49, loss = 1.62499980\n",
            "Iteration 50, loss = 1.62370550\n",
            "Iteration 51, loss = 1.62268517\n",
            "Iteration 52, loss = 1.62143516\n",
            "Iteration 53, loss = 1.62043585\n",
            "Iteration 54, loss = 1.61953401\n",
            "Iteration 55, loss = 1.61846614\n",
            "Iteration 56, loss = 1.61746367\n",
            "Iteration 57, loss = 1.61648286\n",
            "Iteration 58, loss = 1.61555420\n",
            "Iteration 59, loss = 1.61472040\n",
            "Iteration 60, loss = 1.61388141\n",
            "Iteration 61, loss = 1.61293545\n",
            "Iteration 62, loss = 1.61220989\n",
            "Iteration 63, loss = 1.61136802\n",
            "Iteration 64, loss = 1.61064652\n",
            "Iteration 65, loss = 1.60989018\n",
            "Iteration 66, loss = 1.60911944\n",
            "Iteration 67, loss = 1.60845633\n",
            "Iteration 68, loss = 1.60775860\n",
            "Iteration 69, loss = 1.60699191\n",
            "Iteration 70, loss = 1.60634264\n",
            "Iteration 71, loss = 1.60575581\n",
            "Iteration 72, loss = 1.60512193\n",
            "Iteration 73, loss = 1.60451184\n",
            "Iteration 74, loss = 1.60393120\n",
            "Iteration 75, loss = 1.60337119\n",
            "Iteration 76, loss = 1.60277588\n",
            "Iteration 77, loss = 1.60225555\n",
            "Iteration 78, loss = 1.60173520\n",
            "Iteration 79, loss = 1.60119194\n",
            "Iteration 80, loss = 1.60071113\n",
            "Iteration 81, loss = 1.60019940\n",
            "Iteration 82, loss = 1.59973109\n",
            "Iteration 83, loss = 1.59932634\n",
            "Iteration 84, loss = 1.59878745\n",
            "Iteration 85, loss = 1.59830858\n",
            "Iteration 86, loss = 1.59791930\n",
            "Iteration 87, loss = 1.59747795\n",
            "Iteration 88, loss = 1.59715370\n",
            "Iteration 89, loss = 1.59668992\n",
            "Iteration 90, loss = 1.59631821\n",
            "Iteration 91, loss = 1.59585032\n",
            "Iteration 92, loss = 1.59548677\n",
            "Iteration 93, loss = 1.59515702\n",
            "Iteration 94, loss = 1.59476165\n",
            "Iteration 95, loss = 1.59434737\n",
            "Iteration 96, loss = 1.59400775\n",
            "Iteration 97, loss = 1.59367567\n",
            "Iteration 98, loss = 1.59331240\n",
            "Iteration 99, loss = 1.59300368\n",
            "Iteration 100, loss = 1.59264538\n",
            "Iteration 1, loss = 1.73221863\n",
            "Iteration 2, loss = 1.69422852\n",
            "Iteration 3, loss = 1.66681444\n",
            "Iteration 4, loss = 1.64713028\n",
            "Iteration 5, loss = 1.62905097\n",
            "Iteration 6, loss = 1.61737133\n",
            "Iteration 7, loss = 1.61005490\n",
            "Iteration 8, loss = 1.60462903\n",
            "Iteration 9, loss = 1.60032548\n",
            "Iteration 10, loss = 1.59722379\n",
            "Iteration 11, loss = 1.59550056\n",
            "Iteration 12, loss = 1.59256373\n",
            "Iteration 13, loss = 1.59006791\n",
            "Iteration 14, loss = 1.58761489\n",
            "Iteration 15, loss = 1.58575801\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 1.58355783\n",
            "Iteration 17, loss = 1.58177299\n",
            "Iteration 18, loss = 1.57959592\n",
            "Iteration 19, loss = 1.57735933\n",
            "Iteration 20, loss = 1.57483560\n",
            "Iteration 21, loss = 1.57188943\n",
            "Iteration 22, loss = 1.56891591\n",
            "Iteration 23, loss = 1.56560673\n",
            "Iteration 24, loss = 1.56197849\n",
            "Iteration 25, loss = 1.55795936\n",
            "Iteration 26, loss = 1.55396620\n",
            "Iteration 27, loss = 1.54974556\n",
            "Iteration 28, loss = 1.54486452\n",
            "Iteration 29, loss = 1.54013284\n",
            "Iteration 30, loss = 1.53481794\n",
            "Iteration 31, loss = 1.52938259\n",
            "Iteration 32, loss = 1.52329862\n",
            "Iteration 33, loss = 1.51759621\n",
            "Iteration 34, loss = 1.51048025\n",
            "Iteration 35, loss = 1.50365146\n",
            "Iteration 36, loss = 1.49637630\n",
            "Iteration 37, loss = 1.48896648\n",
            "Iteration 38, loss = 1.48139111\n",
            "Iteration 39, loss = 1.47306573\n",
            "Iteration 40, loss = 1.46482228\n",
            "Iteration 41, loss = 1.45606276\n",
            "Iteration 42, loss = 1.44769307\n",
            "Iteration 43, loss = 1.43866260\n",
            "Iteration 44, loss = 1.42920518\n",
            "Iteration 45, loss = 1.42003123\n",
            "Iteration 46, loss = 1.41076904\n",
            "Iteration 47, loss = 1.40117847\n",
            "Iteration 48, loss = 1.39186850\n",
            "Iteration 49, loss = 1.38254162\n",
            "Iteration 50, loss = 1.37281612\n",
            "Iteration 51, loss = 1.36376298\n",
            "Iteration 52, loss = 1.35436639\n",
            "Iteration 53, loss = 1.34539104\n",
            "Iteration 54, loss = 1.33646437\n",
            "Iteration 55, loss = 1.32767538\n",
            "Iteration 56, loss = 1.31888348\n",
            "Iteration 57, loss = 1.31071349\n",
            "Iteration 58, loss = 1.30260179\n",
            "Iteration 59, loss = 1.29478538\n",
            "Iteration 60, loss = 1.28738254\n",
            "Iteration 61, loss = 1.28004092\n",
            "Iteration 62, loss = 1.27272892\n",
            "Iteration 63, loss = 1.26619655\n",
            "Iteration 64, loss = 1.25972928\n",
            "Iteration 65, loss = 1.25363893\n",
            "Iteration 66, loss = 1.24782863\n",
            "Iteration 67, loss = 1.24196746\n",
            "Iteration 68, loss = 1.23654119\n",
            "Iteration 69, loss = 1.23120481\n",
            "Iteration 70, loss = 1.22640091\n",
            "Iteration 71, loss = 1.22164099\n",
            "Iteration 72, loss = 1.21706658\n",
            "Iteration 73, loss = 1.21300229\n",
            "Iteration 74, loss = 1.20874312\n",
            "Iteration 75, loss = 1.20492338\n",
            "Iteration 76, loss = 1.20120803\n",
            "Iteration 77, loss = 1.19747687\n",
            "Iteration 78, loss = 1.19405747\n",
            "Iteration 79, loss = 1.19128408\n",
            "Iteration 80, loss = 1.18773856\n",
            "Iteration 81, loss = 1.18488500\n",
            "Iteration 82, loss = 1.18186136\n",
            "Iteration 83, loss = 1.17980237\n",
            "Iteration 84, loss = 1.17681345\n",
            "Iteration 85, loss = 1.17454425\n",
            "Iteration 86, loss = 1.17169227\n",
            "Iteration 87, loss = 1.16939968\n",
            "Iteration 88, loss = 1.16754444\n",
            "Iteration 89, loss = 1.16526141\n",
            "Iteration 90, loss = 1.16313459\n",
            "Iteration 91, loss = 1.16123124\n",
            "Iteration 92, loss = 1.15895032\n",
            "Iteration 93, loss = 1.15729787\n",
            "Iteration 94, loss = 1.15558909\n",
            "Iteration 95, loss = 1.15369054\n",
            "Iteration 96, loss = 1.15200691\n",
            "Iteration 97, loss = 1.15055700\n",
            "Iteration 98, loss = 1.14869155\n",
            "Iteration 99, loss = 1.14729224\n",
            "Iteration 100, loss = 1.14573692\n",
            "Iteration 1, loss = 1.70113514\n",
            "Iteration 2, loss = 1.60480765\n",
            "Iteration 3, loss = 1.58870542\n",
            "Iteration 4, loss = 1.57959997\n",
            "Iteration 5, loss = 1.55323177\n",
            "Iteration 6, loss = 1.52231744\n",
            "Iteration 7, loss = 1.48572038\n",
            "Iteration 8, loss = 1.43923274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9, loss = 1.38101028\n",
            "Iteration 10, loss = 1.32542085\n",
            "Iteration 11, loss = 1.27778746\n",
            "Iteration 12, loss = 1.22567663\n",
            "Iteration 13, loss = 1.19353217\n",
            "Iteration 14, loss = 1.16755702\n",
            "Iteration 15, loss = 1.14294375\n",
            "Iteration 16, loss = 1.13412458\n",
            "Iteration 17, loss = 1.12043194\n",
            "Iteration 18, loss = 1.11060282\n",
            "Iteration 19, loss = 1.11174551\n",
            "Iteration 20, loss = 1.10306214\n",
            "Iteration 21, loss = 1.10290844\n",
            "Iteration 22, loss = 1.09243918\n",
            "Iteration 23, loss = 1.08984797\n",
            "Iteration 24, loss = 1.09352444\n",
            "Iteration 25, loss = 1.08385045\n",
            "Iteration 26, loss = 1.08426576\n",
            "Iteration 27, loss = 1.08412025\n",
            "Iteration 28, loss = 1.07341472\n",
            "Iteration 29, loss = 1.07608218\n",
            "Iteration 30, loss = 1.06815191\n",
            "Iteration 31, loss = 1.07657351\n",
            "Iteration 32, loss = 1.05978088\n",
            "Iteration 33, loss = 1.06189994\n",
            "Iteration 34, loss = 1.06172441\n",
            "Iteration 35, loss = 1.05865650\n",
            "Iteration 36, loss = 1.05091980\n",
            "Iteration 37, loss = 1.03385952\n",
            "Iteration 38, loss = 1.03693036\n",
            "Iteration 39, loss = 1.02357728\n",
            "Iteration 40, loss = 1.02472857\n",
            "Iteration 41, loss = 1.01713971\n",
            "Iteration 42, loss = 1.00443307\n",
            "Iteration 43, loss = 0.99464053\n",
            "Iteration 44, loss = 1.00075275\n",
            "Iteration 45, loss = 0.99096938\n",
            "Iteration 46, loss = 0.99319113\n",
            "Iteration 47, loss = 0.99475896\n",
            "Iteration 48, loss = 0.96799996\n",
            "Iteration 49, loss = 0.97910027\n",
            "Iteration 50, loss = 0.95430273\n",
            "Iteration 51, loss = 0.95884080\n",
            "Iteration 52, loss = 0.95558775\n",
            "Iteration 53, loss = 0.95103525\n",
            "Iteration 54, loss = 0.96182001\n",
            "Iteration 55, loss = 0.99022352\n",
            "Iteration 56, loss = 0.96456627\n",
            "Iteration 57, loss = 0.95890935\n",
            "Iteration 58, loss = 0.95192617\n",
            "Iteration 59, loss = 0.93162794\n",
            "Iteration 60, loss = 0.94853552\n",
            "Iteration 61, loss = 0.94357133\n",
            "Iteration 62, loss = 0.92032233\n",
            "Iteration 63, loss = 0.90693735\n",
            "Iteration 64, loss = 0.91757093\n",
            "Iteration 65, loss = 0.95421663\n",
            "Iteration 66, loss = 0.95797535\n",
            "Iteration 67, loss = 0.90508472\n",
            "Iteration 68, loss = 0.92661539\n",
            "Iteration 69, loss = 0.94068711\n",
            "Iteration 70, loss = 0.90985164\n",
            "Iteration 71, loss = 0.90412174\n",
            "Iteration 72, loss = 0.89126274\n",
            "Iteration 73, loss = 0.91724493\n",
            "Iteration 74, loss = 0.88207171\n",
            "Iteration 75, loss = 0.86747247\n",
            "Iteration 76, loss = 0.89419838\n",
            "Iteration 77, loss = 0.87015354\n",
            "Iteration 78, loss = 0.85388617\n",
            "Iteration 79, loss = 0.86136342\n",
            "Iteration 80, loss = 0.85789723\n",
            "Iteration 81, loss = 0.84373789\n",
            "Iteration 82, loss = 0.84005977\n",
            "Iteration 83, loss = 0.84097686\n",
            "Iteration 84, loss = 0.83784569\n",
            "Iteration 85, loss = 0.83787911\n",
            "Iteration 86, loss = 0.83388159\n",
            "Iteration 87, loss = 0.83018295\n",
            "Iteration 88, loss = 0.82903024\n",
            "Iteration 89, loss = 0.82357357\n",
            "Iteration 90, loss = 0.82406045\n",
            "Iteration 91, loss = 0.82186202\n",
            "Iteration 92, loss = 0.81615331\n",
            "Iteration 93, loss = 0.81867049\n",
            "Iteration 94, loss = 0.82455225\n",
            "Iteration 95, loss = 0.82343578\n",
            "Iteration 96, loss = 0.82088134\n",
            "Iteration 97, loss = 0.83492600\n",
            "Iteration 98, loss = 0.83931153\n",
            "Iteration 99, loss = 0.82270350\n",
            "Iteration 100, loss = 0.80332607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.68375459\n",
            "Iteration 2, loss = 1.68137598\n",
            "Iteration 3, loss = 1.67925603\n",
            "Iteration 4, loss = 1.67717994\n",
            "Iteration 5, loss = 1.67515587\n",
            "Iteration 6, loss = 1.67313126\n",
            "Iteration 7, loss = 1.67137322\n",
            "Iteration 8, loss = 1.66942274\n",
            "Iteration 9, loss = 1.66767582\n",
            "Iteration 10, loss = 1.66598493\n",
            "Iteration 11, loss = 1.66443786\n",
            "Iteration 12, loss = 1.66271656\n",
            "Iteration 13, loss = 1.66117244\n",
            "Iteration 14, loss = 1.65961450\n",
            "Iteration 15, loss = 1.65819558\n",
            "Iteration 16, loss = 1.65667209\n",
            "Iteration 17, loss = 1.65523301\n",
            "Iteration 18, loss = 1.65391773\n",
            "Iteration 19, loss = 1.65244592\n",
            "Iteration 20, loss = 1.65114383\n",
            "Iteration 21, loss = 1.64985314\n",
            "Iteration 22, loss = 1.64854102\n",
            "Iteration 23, loss = 1.64718323\n",
            "Iteration 24, loss = 1.64601992\n",
            "Iteration 25, loss = 1.64477468\n",
            "Iteration 26, loss = 1.64349603\n",
            "Iteration 27, loss = 1.64234270\n",
            "Iteration 28, loss = 1.64119084\n",
            "Iteration 29, loss = 1.64007667\n",
            "Iteration 30, loss = 1.63895038\n",
            "Iteration 31, loss = 1.63789139\n",
            "Iteration 32, loss = 1.63671813\n",
            "Iteration 33, loss = 1.63568230\n",
            "Iteration 34, loss = 1.63461338\n",
            "Iteration 35, loss = 1.63353506\n",
            "Iteration 36, loss = 1.63257759\n",
            "Iteration 37, loss = 1.63158752\n",
            "Iteration 38, loss = 1.63064101\n",
            "Iteration 39, loss = 1.62954007\n",
            "Iteration 40, loss = 1.62866269\n",
            "Iteration 41, loss = 1.62773175\n",
            "Iteration 42, loss = 1.62681510\n",
            "Iteration 43, loss = 1.62596476\n",
            "Iteration 44, loss = 1.62497386\n",
            "Iteration 45, loss = 1.62415815\n",
            "Iteration 46, loss = 1.62330045\n",
            "Iteration 47, loss = 1.62237223\n",
            "Iteration 48, loss = 1.62165956\n",
            "Iteration 49, loss = 1.62084843\n",
            "Iteration 50, loss = 1.61994509\n",
            "Iteration 51, loss = 1.61922245\n",
            "Iteration 52, loss = 1.61833572\n",
            "Iteration 53, loss = 1.61762038\n",
            "Iteration 54, loss = 1.61694094\n",
            "Iteration 55, loss = 1.61617490\n",
            "Iteration 56, loss = 1.61539979\n",
            "Iteration 57, loss = 1.61467459\n",
            "Iteration 58, loss = 1.61395759\n",
            "Iteration 59, loss = 1.61330450\n",
            "Iteration 60, loss = 1.61265125\n",
            "Iteration 61, loss = 1.61189791\n",
            "Iteration 62, loss = 1.61131263\n",
            "Iteration 63, loss = 1.61062905\n",
            "Iteration 64, loss = 1.61003158\n",
            "Iteration 65, loss = 1.60940961\n",
            "Iteration 66, loss = 1.60876493\n",
            "Iteration 67, loss = 1.60817938\n",
            "Iteration 68, loss = 1.60758177\n",
            "Iteration 69, loss = 1.60693249\n",
            "Iteration 70, loss = 1.60635849\n",
            "Iteration 71, loss = 1.60582993\n",
            "Iteration 72, loss = 1.60526129\n",
            "Iteration 73, loss = 1.60470548\n",
            "Iteration 74, loss = 1.60417006\n",
            "Iteration 75, loss = 1.60364428\n",
            "Iteration 76, loss = 1.60309239\n",
            "Iteration 77, loss = 1.60259323\n",
            "Iteration 78, loss = 1.60209107\n",
            "Iteration 79, loss = 1.60157154\n",
            "Iteration 80, loss = 1.60109224\n",
            "Iteration 81, loss = 1.60059307\n",
            "Iteration 82, loss = 1.60011975\n",
            "Iteration 83, loss = 1.59969798\n",
            "Iteration 84, loss = 1.59916008\n",
            "Iteration 85, loss = 1.59867980\n",
            "Iteration 86, loss = 1.59826524\n",
            "Iteration 87, loss = 1.59780516\n",
            "Iteration 88, loss = 1.59745226\n",
            "Iteration 89, loss = 1.59696856\n",
            "Iteration 90, loss = 1.59656189\n",
            "Iteration 91, loss = 1.59606444\n",
            "Iteration 92, loss = 1.59567722\n",
            "Iteration 93, loss = 1.59530703\n",
            "Iteration 94, loss = 1.59487732\n",
            "Iteration 95, loss = 1.59442848\n",
            "Iteration 96, loss = 1.59404786\n",
            "Iteration 97, loss = 1.59367325\n",
            "Iteration 98, loss = 1.59326813\n",
            "Iteration 99, loss = 1.59291510\n",
            "Iteration 100, loss = 1.59251272\n",
            "Iteration 1, loss = 1.68020720\n",
            "Iteration 2, loss = 1.66075503\n",
            "Iteration 3, loss = 1.64706022\n",
            "Iteration 4, loss = 1.63527033\n",
            "Iteration 5, loss = 1.62474360\n",
            "Iteration 6, loss = 1.61586330\n",
            "Iteration 7, loss = 1.60949648\n",
            "Iteration 8, loss = 1.60399488\n",
            "Iteration 9, loss = 1.59915262\n",
            "Iteration 10, loss = 1.59534913\n",
            "Iteration 11, loss = 1.59310685\n",
            "Iteration 12, loss = 1.58988579\n",
            "Iteration 13, loss = 1.58756614\n",
            "Iteration 14, loss = 1.58550323\n",
            "Iteration 15, loss = 1.58365537\n",
            "Iteration 16, loss = 1.58215023\n",
            "Iteration 17, loss = 1.58044514\n",
            "Iteration 18, loss = 1.57896327\n",
            "Iteration 19, loss = 1.57716805\n",
            "Iteration 20, loss = 1.57544565\n",
            "Iteration 21, loss = 1.57348177\n",
            "Iteration 22, loss = 1.57137563\n",
            "Iteration 23, loss = 1.56913814\n",
            "Iteration 24, loss = 1.56670382\n",
            "Iteration 25, loss = 1.56403402\n",
            "Iteration 26, loss = 1.56139736\n",
            "Iteration 27, loss = 1.55877838\n",
            "Iteration 28, loss = 1.55554938\n",
            "Iteration 29, loss = 1.55256421\n",
            "Iteration 30, loss = 1.54929327\n",
            "Iteration 31, loss = 1.54591385\n",
            "Iteration 32, loss = 1.54232821\n",
            "Iteration 33, loss = 1.53900191\n",
            "Iteration 34, loss = 1.53486058\n",
            "Iteration 35, loss = 1.53094493\n",
            "Iteration 36, loss = 1.52674399\n",
            "Iteration 37, loss = 1.52254405\n",
            "Iteration 38, loss = 1.51823429\n",
            "Iteration 39, loss = 1.51361359\n",
            "Iteration 40, loss = 1.50887864\n",
            "Iteration 41, loss = 1.50390655\n",
            "Iteration 42, loss = 1.49918490\n",
            "Iteration 43, loss = 1.49402283\n",
            "Iteration 44, loss = 1.48853179\n",
            "Iteration 45, loss = 1.48323110\n",
            "Iteration 46, loss = 1.47777568\n",
            "Iteration 47, loss = 1.47206491\n",
            "Iteration 48, loss = 1.46657893\n",
            "Iteration 49, loss = 1.46091784\n",
            "Iteration 50, loss = 1.45482722\n",
            "Iteration 51, loss = 1.44914776\n",
            "Iteration 52, loss = 1.44297335\n",
            "Iteration 53, loss = 1.43698211\n",
            "Iteration 54, loss = 1.43090206\n",
            "Iteration 55, loss = 1.42477330\n",
            "Iteration 56, loss = 1.41828331\n",
            "Iteration 57, loss = 1.41212143\n",
            "Iteration 58, loss = 1.40585562\n",
            "Iteration 59, loss = 1.39952998\n",
            "Iteration 60, loss = 1.39338332\n",
            "Iteration 61, loss = 1.38700151\n",
            "Iteration 62, loss = 1.38062064\n",
            "Iteration 63, loss = 1.37436561\n",
            "Iteration 64, loss = 1.36817444\n",
            "Iteration 65, loss = 1.36206430\n",
            "Iteration 66, loss = 1.35603812\n",
            "Iteration 67, loss = 1.34977017\n",
            "Iteration 68, loss = 1.34379504\n",
            "Iteration 69, loss = 1.33778315\n",
            "Iteration 70, loss = 1.33195235\n",
            "Iteration 71, loss = 1.32627486\n",
            "Iteration 72, loss = 1.32058480\n",
            "Iteration 73, loss = 1.31512701\n",
            "Iteration 74, loss = 1.30965905\n",
            "Iteration 75, loss = 1.30438246\n",
            "Iteration 76, loss = 1.29923919\n",
            "Iteration 77, loss = 1.29413616\n",
            "Iteration 78, loss = 1.28911859\n",
            "Iteration 79, loss = 1.28477813\n",
            "Iteration 80, loss = 1.27982067\n",
            "Iteration 81, loss = 1.27534163\n",
            "Iteration 82, loss = 1.27071565\n",
            "Iteration 83, loss = 1.26696473\n",
            "Iteration 84, loss = 1.26248854\n",
            "Iteration 85, loss = 1.25861677\n",
            "Iteration 86, loss = 1.25432394\n",
            "Iteration 87, loss = 1.25061092\n",
            "Iteration 88, loss = 1.24716144\n",
            "Iteration 89, loss = 1.24364152\n",
            "Iteration 90, loss = 1.24008001\n",
            "Iteration 91, loss = 1.23680310\n",
            "Iteration 92, loss = 1.23333032\n",
            "Iteration 93, loss = 1.23028816\n",
            "Iteration 94, loss = 1.22735573\n",
            "Iteration 95, loss = 1.22425704\n",
            "Iteration 96, loss = 1.22140230\n",
            "Iteration 97, loss = 1.21870782\n",
            "Iteration 98, loss = 1.21587423\n",
            "Iteration 99, loss = 1.21332625\n",
            "Iteration 100, loss = 1.21070081\n",
            "Iteration 1, loss = 1.66433393\n",
            "Iteration 2, loss = 1.59493418\n",
            "Iteration 3, loss = 1.58475094\n",
            "Iteration 4, loss = 1.57463264\n",
            "Iteration 5, loss = 1.55570329\n",
            "Iteration 6, loss = 1.52376886\n",
            "Iteration 7, loss = 1.48976792\n",
            "Iteration 8, loss = 1.45692487\n",
            "Iteration 9, loss = 1.41923215\n",
            "Iteration 10, loss = 1.38395531\n",
            "Iteration 11, loss = 1.34633062\n",
            "Iteration 12, loss = 1.31026708\n",
            "Iteration 13, loss = 1.27359452\n",
            "Iteration 14, loss = 1.24338536\n",
            "Iteration 15, loss = 1.21744024\n",
            "Iteration 16, loss = 1.19501049\n",
            "Iteration 17, loss = 1.17906992\n",
            "Iteration 18, loss = 1.15903143\n",
            "Iteration 19, loss = 1.14793953\n",
            "Iteration 20, loss = 1.13974272\n",
            "Iteration 21, loss = 1.12921464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 22, loss = 1.12114594\n",
            "Iteration 23, loss = 1.11796192\n",
            "Iteration 24, loss = 1.11059018\n",
            "Iteration 25, loss = 1.10671221\n",
            "Iteration 26, loss = 1.10794265\n",
            "Iteration 27, loss = 1.10093251\n",
            "Iteration 28, loss = 1.10140145\n",
            "Iteration 29, loss = 1.09831874\n",
            "Iteration 30, loss = 1.09143687\n",
            "Iteration 31, loss = 1.11418184\n",
            "Iteration 32, loss = 1.08492293\n",
            "Iteration 33, loss = 1.11296820\n",
            "Iteration 34, loss = 1.08998198\n",
            "Iteration 35, loss = 1.10468574\n",
            "Iteration 36, loss = 1.08547433\n",
            "Iteration 37, loss = 1.08672208\n",
            "Iteration 38, loss = 1.07649668\n",
            "Iteration 39, loss = 1.08050697\n",
            "Iteration 40, loss = 1.07440834\n",
            "Iteration 41, loss = 1.07382591\n",
            "Iteration 42, loss = 1.07159821\n",
            "Iteration 43, loss = 1.06889206\n",
            "Iteration 44, loss = 1.06163202\n",
            "Iteration 45, loss = 1.06120325\n",
            "Iteration 46, loss = 1.05781700\n",
            "Iteration 47, loss = 1.05637371\n",
            "Iteration 48, loss = 1.04966653\n",
            "Iteration 49, loss = 1.04898278\n",
            "Iteration 50, loss = 1.04378344\n",
            "Iteration 51, loss = 1.03831390\n",
            "Iteration 52, loss = 1.03412987\n",
            "Iteration 53, loss = 1.03326663\n",
            "Iteration 54, loss = 1.03164142\n",
            "Iteration 55, loss = 1.02786640\n",
            "Iteration 56, loss = 1.01891806\n",
            "Iteration 57, loss = 1.01883383\n",
            "Iteration 58, loss = 1.00972202\n",
            "Iteration 59, loss = 1.00524574\n",
            "Iteration 60, loss = 1.00363016\n",
            "Iteration 61, loss = 1.00089682\n",
            "Iteration 62, loss = 0.99312284\n",
            "Iteration 63, loss = 0.98514740\n",
            "Iteration 64, loss = 0.98799118\n",
            "Iteration 65, loss = 1.00405082\n",
            "Iteration 66, loss = 0.99945388\n",
            "Iteration 67, loss = 0.97704617\n",
            "Iteration 68, loss = 0.98693883\n",
            "Iteration 69, loss = 0.99046931\n",
            "Iteration 70, loss = 0.96858861\n",
            "Iteration 71, loss = 0.97122401\n",
            "Iteration 72, loss = 0.97454961\n",
            "Iteration 73, loss = 0.96509418\n",
            "Iteration 74, loss = 0.95476460\n",
            "Iteration 75, loss = 0.95942759\n",
            "Iteration 76, loss = 0.94624577\n",
            "Iteration 77, loss = 0.94139364\n",
            "Iteration 78, loss = 0.93981585\n",
            "Iteration 79, loss = 0.93444495\n",
            "Iteration 80, loss = 0.92353469\n",
            "Iteration 81, loss = 0.92600911\n",
            "Iteration 82, loss = 0.92297839\n",
            "Iteration 83, loss = 0.91832742\n",
            "Iteration 84, loss = 0.92074421\n",
            "Iteration 85, loss = 0.91316485\n",
            "Iteration 86, loss = 0.90589746\n",
            "Iteration 87, loss = 0.90385787\n",
            "Iteration 88, loss = 0.90141002\n",
            "Iteration 89, loss = 0.90026135\n",
            "Iteration 90, loss = 0.89476587\n",
            "Iteration 91, loss = 0.89308848\n",
            "Iteration 92, loss = 0.89159782\n",
            "Iteration 93, loss = 0.89047148\n",
            "Iteration 94, loss = 0.89772749\n",
            "Iteration 95, loss = 0.88743776\n",
            "Iteration 96, loss = 0.88029415\n",
            "Iteration 97, loss = 0.89000933\n",
            "Iteration 98, loss = 0.87804104\n",
            "Iteration 99, loss = 0.87357509\n",
            "Iteration 100, loss = 0.87130422\n",
            "Iteration 1, loss = 1.78275101\n",
            "Iteration 2, loss = 1.77642770\n",
            "Iteration 3, loss = 1.77066818\n",
            "Iteration 4, loss = 1.76468118\n",
            "Iteration 5, loss = 1.75894492\n",
            "Iteration 6, loss = 1.75340416\n",
            "Iteration 7, loss = 1.74788175\n",
            "Iteration 8, loss = 1.74274618\n",
            "Iteration 9, loss = 1.73797849\n",
            "Iteration 10, loss = 1.73297037\n",
            "Iteration 11, loss = 1.72836492\n",
            "Iteration 12, loss = 1.72382635\n",
            "Iteration 13, loss = 1.71930180\n",
            "Iteration 14, loss = 1.71510137\n",
            "Iteration 15, loss = 1.71107377\n",
            "Iteration 16, loss = 1.70699961\n",
            "Iteration 17, loss = 1.70314752\n",
            "Iteration 18, loss = 1.69974130\n",
            "Iteration 19, loss = 1.69606531\n",
            "Iteration 20, loss = 1.69260409\n",
            "Iteration 21, loss = 1.68943659\n",
            "Iteration 22, loss = 1.68631768\n",
            "Iteration 23, loss = 1.68317719\n",
            "Iteration 24, loss = 1.68020310\n",
            "Iteration 25, loss = 1.67727590\n",
            "Iteration 26, loss = 1.67463754\n",
            "Iteration 27, loss = 1.67195942\n",
            "Iteration 28, loss = 1.66964502\n",
            "Iteration 29, loss = 1.66709854\n",
            "Iteration 30, loss = 1.66463523\n",
            "Iteration 31, loss = 1.66243248\n",
            "Iteration 32, loss = 1.66031823\n",
            "Iteration 33, loss = 1.65808414\n",
            "Iteration 34, loss = 1.65616994\n",
            "Iteration 35, loss = 1.65419534\n",
            "Iteration 36, loss = 1.65219790\n",
            "Iteration 37, loss = 1.65037552\n",
            "Iteration 38, loss = 1.64861361\n",
            "Iteration 39, loss = 1.64701493\n",
            "Iteration 40, loss = 1.64524968\n",
            "Iteration 41, loss = 1.64359827\n",
            "Iteration 42, loss = 1.64208907\n",
            "Iteration 43, loss = 1.64068889\n",
            "Iteration 44, loss = 1.63918959\n",
            "Iteration 45, loss = 1.63774793\n",
            "Iteration 46, loss = 1.63659614\n",
            "Iteration 47, loss = 1.63521182\n",
            "Iteration 48, loss = 1.63401142\n",
            "Iteration 49, loss = 1.63273701\n",
            "Iteration 50, loss = 1.63154382\n",
            "Iteration 51, loss = 1.63040150\n",
            "Iteration 52, loss = 1.62935434\n",
            "Iteration 53, loss = 1.62828007\n",
            "Iteration 54, loss = 1.62722729\n",
            "Iteration 55, loss = 1.62624900\n",
            "Iteration 56, loss = 1.62523807\n",
            "Iteration 57, loss = 1.62436458\n",
            "Iteration 58, loss = 1.62347334\n",
            "Iteration 59, loss = 1.62250561\n",
            "Iteration 60, loss = 1.62170770\n",
            "Iteration 61, loss = 1.62085190\n",
            "Iteration 62, loss = 1.62009937\n",
            "Iteration 63, loss = 1.61931106\n",
            "Iteration 64, loss = 1.61853806\n",
            "Iteration 65, loss = 1.61775945\n",
            "Iteration 66, loss = 1.61711319\n",
            "Iteration 67, loss = 1.61644997\n",
            "Iteration 68, loss = 1.61566998\n",
            "Iteration 69, loss = 1.61505985\n",
            "Iteration 70, loss = 1.61445558\n",
            "Iteration 71, loss = 1.61377479\n",
            "Iteration 72, loss = 1.61323436\n",
            "Iteration 73, loss = 1.61259838\n",
            "Iteration 74, loss = 1.61200225\n",
            "Iteration 75, loss = 1.61149327\n",
            "Iteration 76, loss = 1.61085447\n",
            "Iteration 77, loss = 1.61039544\n",
            "Iteration 78, loss = 1.60990293\n",
            "Iteration 79, loss = 1.60937726\n",
            "Iteration 80, loss = 1.60889064\n",
            "Iteration 81, loss = 1.60836204\n",
            "Iteration 82, loss = 1.60788857\n",
            "Iteration 83, loss = 1.60738563\n",
            "Iteration 84, loss = 1.60700103\n",
            "Iteration 85, loss = 1.60657798\n",
            "Iteration 86, loss = 1.60621878\n",
            "Iteration 87, loss = 1.60569332\n",
            "Iteration 88, loss = 1.60528572\n",
            "Iteration 89, loss = 1.60483543\n",
            "Iteration 90, loss = 1.60447074\n",
            "Iteration 91, loss = 1.60401972\n",
            "Iteration 92, loss = 1.60371433\n",
            "Iteration 93, loss = 1.60332332\n",
            "Iteration 94, loss = 1.60295803\n",
            "Iteration 95, loss = 1.60263395\n",
            "Iteration 96, loss = 1.60218603\n",
            "Iteration 97, loss = 1.60189332\n",
            "Iteration 98, loss = 1.60158391\n",
            "Iteration 99, loss = 1.60122957\n",
            "Iteration 100, loss = 1.60083970\n",
            "Iteration 1, loss = 1.77015439\n",
            "Iteration 2, loss = 1.71700498\n",
            "Iteration 3, loss = 1.68069585\n",
            "Iteration 4, loss = 1.65223826\n",
            "Iteration 5, loss = 1.63323615\n",
            "Iteration 6, loss = 1.62095474\n",
            "Iteration 7, loss = 1.61296877\n",
            "Iteration 8, loss = 1.60830328\n",
            "Iteration 9, loss = 1.60616954\n",
            "Iteration 10, loss = 1.60354883\n",
            "Iteration 11, loss = 1.60173941\n",
            "Iteration 12, loss = 1.59948203\n",
            "Iteration 13, loss = 1.59772348\n",
            "Iteration 14, loss = 1.59587612\n",
            "Iteration 15, loss = 1.59448731\n",
            "Iteration 16, loss = 1.59281297\n",
            "Iteration 17, loss = 1.59145075\n",
            "Iteration 18, loss = 1.58989528\n",
            "Iteration 19, loss = 1.58869317\n",
            "Iteration 20, loss = 1.58706581\n",
            "Iteration 21, loss = 1.58532606\n",
            "Iteration 22, loss = 1.58336463\n",
            "Iteration 23, loss = 1.58136532\n",
            "Iteration 24, loss = 1.57920918\n",
            "Iteration 25, loss = 1.57675114\n",
            "Iteration 26, loss = 1.57431081\n",
            "Iteration 27, loss = 1.57165423\n",
            "Iteration 28, loss = 1.56866223\n",
            "Iteration 29, loss = 1.56553165\n",
            "Iteration 30, loss = 1.56216838\n",
            "Iteration 31, loss = 1.55855940\n",
            "Iteration 32, loss = 1.55469292\n",
            "Iteration 33, loss = 1.55049349\n",
            "Iteration 34, loss = 1.54636236\n",
            "Iteration 35, loss = 1.54132114\n",
            "Iteration 36, loss = 1.53636798\n",
            "Iteration 37, loss = 1.53099422\n",
            "Iteration 38, loss = 1.52538093\n",
            "Iteration 39, loss = 1.51937800\n",
            "Iteration 40, loss = 1.51290772\n",
            "Iteration 41, loss = 1.50633621\n",
            "Iteration 42, loss = 1.49931330\n",
            "Iteration 43, loss = 1.49218184\n",
            "Iteration 44, loss = 1.48456031\n",
            "Iteration 45, loss = 1.47675327\n",
            "Iteration 46, loss = 1.46877527\n",
            "Iteration 47, loss = 1.46039251\n",
            "Iteration 48, loss = 1.45249272\n",
            "Iteration 49, loss = 1.44348833\n",
            "Iteration 50, loss = 1.43428890\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 51, loss = 1.42545345\n",
            "Iteration 52, loss = 1.41649178\n",
            "Iteration 53, loss = 1.40750853\n",
            "Iteration 54, loss = 1.39832655\n",
            "Iteration 55, loss = 1.38913626\n",
            "Iteration 56, loss = 1.38012078\n",
            "Iteration 57, loss = 1.37092338\n",
            "Iteration 58, loss = 1.36207510\n",
            "Iteration 59, loss = 1.35322769\n",
            "Iteration 60, loss = 1.34445329\n",
            "Iteration 61, loss = 1.33604063\n",
            "Iteration 62, loss = 1.32766802\n",
            "Iteration 63, loss = 1.31980217\n",
            "Iteration 64, loss = 1.31177435\n",
            "Iteration 65, loss = 1.30395248\n",
            "Iteration 66, loss = 1.29656955\n",
            "Iteration 67, loss = 1.28949553\n",
            "Iteration 68, loss = 1.28242363\n",
            "Iteration 69, loss = 1.27565436\n",
            "Iteration 70, loss = 1.26930727\n",
            "Iteration 71, loss = 1.26341668\n",
            "Iteration 72, loss = 1.25795001\n",
            "Iteration 73, loss = 1.25187746\n",
            "Iteration 74, loss = 1.24625024\n",
            "Iteration 75, loss = 1.24117713\n",
            "Iteration 76, loss = 1.23637353\n",
            "Iteration 77, loss = 1.23151822\n",
            "Iteration 78, loss = 1.22728537\n",
            "Iteration 79, loss = 1.22263934\n",
            "Iteration 80, loss = 1.21857874\n",
            "Iteration 81, loss = 1.21467139\n",
            "Iteration 82, loss = 1.21114365\n",
            "Iteration 83, loss = 1.20788714\n",
            "Iteration 84, loss = 1.20397412\n",
            "Iteration 85, loss = 1.20086110\n",
            "Iteration 86, loss = 1.19788642\n",
            "Iteration 87, loss = 1.19451573\n",
            "Iteration 88, loss = 1.19212731\n",
            "Iteration 89, loss = 1.18891291\n",
            "Iteration 90, loss = 1.18612779\n",
            "Iteration 91, loss = 1.18358772\n",
            "Iteration 92, loss = 1.18123583\n",
            "Iteration 93, loss = 1.17901229\n",
            "Iteration 94, loss = 1.17676424\n",
            "Iteration 95, loss = 1.17504094\n",
            "Iteration 96, loss = 1.17236786\n",
            "Iteration 97, loss = 1.17052521\n",
            "Iteration 98, loss = 1.16875577\n",
            "Iteration 99, loss = 1.16650150\n",
            "Iteration 100, loss = 1.16463434\n",
            "Iteration 1, loss = 1.70810691\n",
            "Iteration 2, loss = 1.62115689\n",
            "Iteration 3, loss = 1.61111258\n",
            "Iteration 4, loss = 1.60159758\n",
            "Iteration 5, loss = 1.60044506\n",
            "Iteration 6, loss = 1.60469851\n",
            "Iteration 7, loss = 1.58945945\n",
            "Iteration 8, loss = 1.57331334\n",
            "Iteration 9, loss = 1.55976037\n",
            "Iteration 10, loss = 1.53283933\n",
            "Iteration 11, loss = 1.49481020\n",
            "Iteration 12, loss = 1.44856241\n",
            "Iteration 13, loss = 1.39836132\n",
            "Iteration 14, loss = 1.34265129\n",
            "Iteration 15, loss = 1.29062957\n",
            "Iteration 16, loss = 1.24241945\n",
            "Iteration 17, loss = 1.20924682\n",
            "Iteration 18, loss = 1.18508738\n",
            "Iteration 19, loss = 1.16394517\n",
            "Iteration 20, loss = 1.15060021\n",
            "Iteration 21, loss = 1.13792705\n",
            "Iteration 22, loss = 1.13616159\n",
            "Iteration 23, loss = 1.12335302\n",
            "Iteration 24, loss = 1.11855164\n",
            "Iteration 25, loss = 1.11605178\n",
            "Iteration 26, loss = 1.11333729\n",
            "Iteration 27, loss = 1.10878123\n",
            "Iteration 28, loss = 1.10911674\n",
            "Iteration 29, loss = 1.10637561\n",
            "Iteration 30, loss = 1.10463295\n",
            "Iteration 31, loss = 1.10320527\n",
            "Iteration 32, loss = 1.09847871\n",
            "Iteration 33, loss = 1.09985210\n",
            "Iteration 34, loss = 1.09811276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 35, loss = 1.09543151\n",
            "Iteration 36, loss = 1.10125500\n",
            "Iteration 37, loss = 1.08839930\n",
            "Iteration 38, loss = 1.09458765\n",
            "Iteration 39, loss = 1.08794821\n",
            "Iteration 40, loss = 1.08765479\n",
            "Iteration 41, loss = 1.08377681\n",
            "Iteration 42, loss = 1.07848466\n",
            "Iteration 43, loss = 1.07509027\n",
            "Iteration 44, loss = 1.07841717\n",
            "Iteration 45, loss = 1.07539453\n",
            "Iteration 46, loss = 1.07025560\n",
            "Iteration 47, loss = 1.06779530\n",
            "Iteration 48, loss = 1.05878220\n",
            "Iteration 49, loss = 1.06512651\n",
            "Iteration 50, loss = 1.05062054\n",
            "Iteration 51, loss = 1.05062027\n",
            "Iteration 52, loss = 1.04543677\n",
            "Iteration 53, loss = 1.03721749\n",
            "Iteration 54, loss = 1.03321156\n",
            "Iteration 55, loss = 1.02396266\n",
            "Iteration 56, loss = 1.03160220\n",
            "Iteration 57, loss = 1.01767805\n",
            "Iteration 58, loss = 1.01434666\n",
            "Iteration 59, loss = 0.99862505\n",
            "Iteration 60, loss = 1.00579662\n",
            "Iteration 61, loss = 1.00106400\n",
            "Iteration 62, loss = 0.98741008\n",
            "Iteration 63, loss = 0.99894967\n",
            "Iteration 64, loss = 1.00456467\n",
            "Iteration 65, loss = 1.01803133\n",
            "Iteration 66, loss = 1.02304720\n",
            "Iteration 67, loss = 1.00135698\n",
            "Iteration 68, loss = 0.97649952\n",
            "Iteration 69, loss = 0.98054015\n",
            "Iteration 70, loss = 0.98303283\n",
            "Iteration 71, loss = 0.96192245\n",
            "Iteration 72, loss = 0.94827672\n",
            "Iteration 73, loss = 0.94012242\n",
            "Iteration 74, loss = 0.94775681\n",
            "Iteration 75, loss = 0.93335233\n",
            "Iteration 76, loss = 0.92180768\n",
            "Iteration 77, loss = 0.91398423\n",
            "Iteration 78, loss = 0.92731814\n",
            "Iteration 79, loss = 0.92532982\n",
            "Iteration 80, loss = 0.90178203\n",
            "Iteration 81, loss = 0.90104193\n",
            "Iteration 82, loss = 0.91403523\n",
            "Iteration 83, loss = 0.89689022\n",
            "Iteration 84, loss = 0.89482784\n",
            "Iteration 85, loss = 0.89560335\n",
            "Iteration 86, loss = 0.88128262\n",
            "Iteration 87, loss = 0.87346766\n",
            "Iteration 88, loss = 0.89641880\n",
            "Iteration 89, loss = 0.87448653\n",
            "Iteration 90, loss = 0.86374734\n",
            "Iteration 91, loss = 0.86105297\n",
            "Iteration 92, loss = 0.86482387\n",
            "Iteration 93, loss = 0.86211390\n",
            "Iteration 94, loss = 0.85929192\n",
            "Iteration 95, loss = 0.85594346\n",
            "Iteration 96, loss = 0.85360108\n",
            "Iteration 97, loss = 0.87039412\n",
            "Iteration 98, loss = 0.90496234\n",
            "Iteration 99, loss = 0.88633899\n",
            "Iteration 100, loss = 0.89191570\n",
            "Iteration 1, loss = 1.62887046\n",
            "Iteration 2, loss = 1.62749004\n",
            "Iteration 3, loss = 1.62600665\n",
            "Iteration 4, loss = 1.62475043\n",
            "Iteration 5, loss = 1.62355695\n",
            "Iteration 6, loss = 1.62241397\n",
            "Iteration 7, loss = 1.62118859\n",
            "Iteration 8, loss = 1.62013754\n",
            "Iteration 9, loss = 1.61899002\n",
            "Iteration 10, loss = 1.61797268\n",
            "Iteration 11, loss = 1.61704506\n",
            "Iteration 12, loss = 1.61603915\n",
            "Iteration 13, loss = 1.61515150\n",
            "Iteration 14, loss = 1.61428794\n",
            "Iteration 15, loss = 1.61342017\n",
            "Iteration 16, loss = 1.61264337\n",
            "Iteration 17, loss = 1.61188468\n",
            "Iteration 18, loss = 1.61101902\n",
            "Iteration 19, loss = 1.61043562\n",
            "Iteration 20, loss = 1.60969128\n",
            "Iteration 21, loss = 1.60898319\n",
            "Iteration 22, loss = 1.60845090\n",
            "Iteration 23, loss = 1.60781983\n",
            "Iteration 24, loss = 1.60732662\n",
            "Iteration 25, loss = 1.60676400\n",
            "Iteration 26, loss = 1.60627278\n",
            "Iteration 27, loss = 1.60575606\n",
            "Iteration 28, loss = 1.60527403\n",
            "Iteration 29, loss = 1.60479797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 30, loss = 1.60446245\n",
            "Iteration 31, loss = 1.60398034\n",
            "Iteration 32, loss = 1.60362088\n",
            "Iteration 33, loss = 1.60331012\n",
            "Iteration 34, loss = 1.60293552\n",
            "Iteration 35, loss = 1.60258873\n",
            "Iteration 36, loss = 1.60232389\n",
            "Iteration 37, loss = 1.60202930\n",
            "Iteration 38, loss = 1.60180941\n",
            "Iteration 39, loss = 1.60147653\n",
            "Iteration 40, loss = 1.60119519\n",
            "Iteration 41, loss = 1.60103976\n",
            "Iteration 42, loss = 1.60075278\n",
            "Iteration 43, loss = 1.60056042\n",
            "Iteration 44, loss = 1.60032199\n",
            "Iteration 45, loss = 1.60019646\n",
            "Iteration 46, loss = 1.59991531\n",
            "Iteration 47, loss = 1.59980038\n",
            "Iteration 48, loss = 1.59965576\n",
            "Iteration 49, loss = 1.59949113\n",
            "Iteration 50, loss = 1.59930354\n",
            "Iteration 51, loss = 1.59916951\n",
            "Iteration 52, loss = 1.59901733\n",
            "Iteration 53, loss = 1.59892764\n",
            "Iteration 54, loss = 1.59881037\n",
            "Iteration 55, loss = 1.59866100\n",
            "Iteration 56, loss = 1.59857439\n",
            "Iteration 57, loss = 1.59844113\n",
            "Iteration 58, loss = 1.59835656\n",
            "Iteration 59, loss = 1.59826620\n",
            "Iteration 60, loss = 1.59817417\n",
            "Iteration 61, loss = 1.59810621\n",
            "Iteration 62, loss = 1.59799482\n",
            "Iteration 63, loss = 1.59792167\n",
            "Iteration 64, loss = 1.59785200\n",
            "Iteration 65, loss = 1.59777791\n",
            "Iteration 66, loss = 1.59768944\n",
            "Iteration 67, loss = 1.59761438\n",
            "Iteration 68, loss = 1.59756561\n",
            "Iteration 69, loss = 1.59749357\n",
            "Iteration 70, loss = 1.59742668\n",
            "Iteration 71, loss = 1.59740873\n",
            "Iteration 72, loss = 1.59733393\n",
            "Iteration 73, loss = 1.59727585\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.62669571\n",
            "Iteration 2, loss = 1.61556205\n",
            "Iteration 3, loss = 1.60608521\n",
            "Iteration 4, loss = 1.60132240\n",
            "Iteration 5, loss = 1.59893925\n",
            "Iteration 6, loss = 1.59843711\n",
            "Iteration 7, loss = 1.59779474\n",
            "Iteration 8, loss = 1.59784876\n",
            "Iteration 9, loss = 1.59783869\n",
            "Iteration 10, loss = 1.59793039\n",
            "Iteration 11, loss = 1.59786382\n",
            "Iteration 12, loss = 1.59761159\n",
            "Iteration 13, loss = 1.59689859\n",
            "Iteration 14, loss = 1.59618439\n",
            "Iteration 15, loss = 1.59554377\n",
            "Iteration 16, loss = 1.59484348\n",
            "Iteration 17, loss = 1.59463676\n",
            "Iteration 18, loss = 1.59398377\n",
            "Iteration 19, loss = 1.59335329\n",
            "Iteration 20, loss = 1.59300343\n",
            "Iteration 21, loss = 1.59259052\n",
            "Iteration 22, loss = 1.59220467\n",
            "Iteration 23, loss = 1.59146315\n",
            "Iteration 24, loss = 1.59092854\n",
            "Iteration 25, loss = 1.59033082\n",
            "Iteration 26, loss = 1.58965694\n",
            "Iteration 27, loss = 1.58902743\n",
            "Iteration 28, loss = 1.58836339\n",
            "Iteration 29, loss = 1.58729897\n",
            "Iteration 30, loss = 1.58657358\n",
            "Iteration 31, loss = 1.58562605\n",
            "Iteration 32, loss = 1.58466350\n",
            "Iteration 33, loss = 1.58373020\n",
            "Iteration 34, loss = 1.58302735\n",
            "Iteration 35, loss = 1.58164812\n",
            "Iteration 36, loss = 1.58054241\n",
            "Iteration 37, loss = 1.57932844\n",
            "Iteration 38, loss = 1.57822599\n",
            "Iteration 39, loss = 1.57692361\n",
            "Iteration 40, loss = 1.57567895\n",
            "Iteration 41, loss = 1.57407685\n",
            "Iteration 42, loss = 1.57245602\n",
            "Iteration 43, loss = 1.57092332\n",
            "Iteration 44, loss = 1.56924983\n",
            "Iteration 45, loss = 1.56745308\n",
            "Iteration 46, loss = 1.56568009\n",
            "Iteration 47, loss = 1.56375536\n",
            "Iteration 48, loss = 1.56210225\n",
            "Iteration 49, loss = 1.55977529\n",
            "Iteration 50, loss = 1.55731072\n",
            "Iteration 51, loss = 1.55504823\n",
            "Iteration 52, loss = 1.55274059\n",
            "Iteration 53, loss = 1.55030616\n",
            "Iteration 54, loss = 1.54771436\n",
            "Iteration 55, loss = 1.54498783\n",
            "Iteration 56, loss = 1.54243049\n",
            "Iteration 57, loss = 1.53933527\n",
            "Iteration 58, loss = 1.53647860\n",
            "Iteration 59, loss = 1.53326441\n",
            "Iteration 60, loss = 1.53009735\n",
            "Iteration 61, loss = 1.52685526\n",
            "Iteration 62, loss = 1.52335464\n",
            "Iteration 63, loss = 1.51998215\n",
            "Iteration 64, loss = 1.51625921\n",
            "Iteration 65, loss = 1.51254164\n",
            "Iteration 66, loss = 1.50869847\n",
            "Iteration 67, loss = 1.50485170\n",
            "Iteration 68, loss = 1.50079208\n",
            "Iteration 69, loss = 1.49659760\n",
            "Iteration 70, loss = 1.49253569\n",
            "Iteration 71, loss = 1.48835722\n",
            "Iteration 72, loss = 1.48414966\n",
            "Iteration 73, loss = 1.47961835\n",
            "Iteration 74, loss = 1.47502072\n",
            "Iteration 75, loss = 1.47044712\n",
            "Iteration 76, loss = 1.46605898\n",
            "Iteration 77, loss = 1.46114773\n",
            "Iteration 78, loss = 1.45672817\n",
            "Iteration 79, loss = 1.45176933\n",
            "Iteration 80, loss = 1.44694024\n",
            "Iteration 81, loss = 1.44204463\n",
            "Iteration 82, loss = 1.43747797\n",
            "Iteration 83, loss = 1.43258666\n",
            "Iteration 84, loss = 1.42755561\n",
            "Iteration 85, loss = 1.42260434\n",
            "Iteration 86, loss = 1.41853775\n",
            "Iteration 87, loss = 1.41299397\n",
            "Iteration 88, loss = 1.40859154\n",
            "Iteration 89, loss = 1.40342843\n",
            "Iteration 90, loss = 1.39849685\n",
            "Iteration 91, loss = 1.39383258\n",
            "Iteration 92, loss = 1.38917387\n",
            "Iteration 93, loss = 1.38436802\n",
            "Iteration 94, loss = 1.37973337\n",
            "Iteration 95, loss = 1.37573309\n",
            "Iteration 96, loss = 1.37055622\n",
            "Iteration 97, loss = 1.36607073\n",
            "Iteration 98, loss = 1.36181396\n",
            "Iteration 99, loss = 1.35716547\n",
            "Iteration 100, loss = 1.35288449\n",
            "Iteration 1, loss = 1.62923131\n",
            "Iteration 2, loss = 1.61520828\n",
            "Iteration 3, loss = 1.61106528\n",
            "Iteration 4, loss = 1.59968396\n",
            "Iteration 5, loss = 1.59157510\n",
            "Iteration 6, loss = 1.58908053\n",
            "Iteration 7, loss = 1.58465022\n",
            "Iteration 8, loss = 1.57251902\n",
            "Iteration 9, loss = 1.55801399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 1.54130826\n",
            "Iteration 11, loss = 1.52324670\n",
            "Iteration 12, loss = 1.49827634\n",
            "Iteration 13, loss = 1.46647924\n",
            "Iteration 14, loss = 1.43032454\n",
            "Iteration 15, loss = 1.39551223\n",
            "Iteration 16, loss = 1.35660040\n",
            "Iteration 17, loss = 1.32078068\n",
            "Iteration 18, loss = 1.28954706\n",
            "Iteration 19, loss = 1.26343426\n",
            "Iteration 20, loss = 1.23844481\n",
            "Iteration 21, loss = 1.21942634\n",
            "Iteration 22, loss = 1.20257894\n",
            "Iteration 23, loss = 1.19113958\n",
            "Iteration 24, loss = 1.17853027\n",
            "Iteration 25, loss = 1.17138172\n",
            "Iteration 26, loss = 1.16268096\n",
            "Iteration 27, loss = 1.15746645\n",
            "Iteration 28, loss = 1.15062999\n",
            "Iteration 29, loss = 1.14752410\n",
            "Iteration 30, loss = 1.14484477\n",
            "Iteration 31, loss = 1.13921308\n",
            "Iteration 32, loss = 1.13570174\n",
            "Iteration 33, loss = 1.13398887\n",
            "Iteration 34, loss = 1.13494220\n",
            "Iteration 35, loss = 1.13057893\n",
            "Iteration 36, loss = 1.12769292\n",
            "Iteration 37, loss = 1.12461356\n",
            "Iteration 38, loss = 1.12652131\n",
            "Iteration 39, loss = 1.12287731\n",
            "Iteration 40, loss = 1.12222477\n",
            "Iteration 41, loss = 1.12188398\n",
            "Iteration 42, loss = 1.11883027\n",
            "Iteration 43, loss = 1.11813532\n",
            "Iteration 44, loss = 1.11881394\n",
            "Iteration 45, loss = 1.11758238\n",
            "Iteration 46, loss = 1.11598474\n",
            "Iteration 47, loss = 1.11564910\n",
            "Iteration 48, loss = 1.11629724\n",
            "Iteration 49, loss = 1.11705338\n",
            "Iteration 50, loss = 1.11236521\n",
            "Iteration 51, loss = 1.11279835\n",
            "Iteration 52, loss = 1.11231148\n",
            "Iteration 53, loss = 1.11179913\n",
            "Iteration 54, loss = 1.11137179\n",
            "Iteration 55, loss = 1.11143681\n",
            "Iteration 56, loss = 1.11500441\n",
            "Iteration 57, loss = 1.10980889\n",
            "Iteration 58, loss = 1.11568118\n",
            "Iteration 59, loss = 1.10750271\n",
            "Iteration 60, loss = 1.11108698\n",
            "Iteration 61, loss = 1.11334272\n",
            "Iteration 62, loss = 1.10828808\n",
            "Iteration 63, loss = 1.10877122\n",
            "Iteration 64, loss = 1.11120451\n",
            "Iteration 65, loss = 1.10891163\n",
            "Iteration 66, loss = 1.10736775\n",
            "Iteration 67, loss = 1.10704227\n",
            "Iteration 68, loss = 1.10857781\n",
            "Iteration 69, loss = 1.10684335\n",
            "Iteration 70, loss = 1.10475774\n",
            "Iteration 71, loss = 1.10685939\n",
            "Iteration 72, loss = 1.10937111\n",
            "Iteration 73, loss = 1.11110306\n",
            "Iteration 74, loss = 1.10708977\n",
            "Iteration 75, loss = 1.10680889\n",
            "Iteration 76, loss = 1.10755603\n",
            "Iteration 77, loss = 1.10416808\n",
            "Iteration 78, loss = 1.10373335\n",
            "Iteration 79, loss = 1.10341314\n",
            "Iteration 80, loss = 1.10205313\n",
            "Iteration 81, loss = 1.10278976\n",
            "Iteration 82, loss = 1.10757311\n",
            "Iteration 83, loss = 1.10407045\n",
            "Iteration 84, loss = 1.10284557\n",
            "Iteration 85, loss = 1.10529051\n",
            "Iteration 86, loss = 1.10294111\n",
            "Iteration 87, loss = 1.10401933\n",
            "Iteration 88, loss = 1.10887921\n",
            "Iteration 89, loss = 1.10284011\n",
            "Iteration 90, loss = 1.10835350\n",
            "Iteration 91, loss = 1.09988371\n",
            "Iteration 92, loss = 1.10394418\n",
            "Iteration 93, loss = 1.10197840\n",
            "Iteration 94, loss = 1.10283960\n",
            "Iteration 95, loss = 1.09589892\n",
            "Iteration 96, loss = 1.09640948\n",
            "Iteration 97, loss = 1.09686578\n",
            "Iteration 98, loss = 1.10018642\n",
            "Iteration 99, loss = 1.09619480\n",
            "Iteration 100, loss = 1.09377925\n",
            "Iteration 1, loss = 1.76260167\n",
            "Iteration 2, loss = 1.75727870\n",
            "Iteration 3, loss = 1.75243252\n",
            "Iteration 4, loss = 1.74736728\n",
            "Iteration 5, loss = 1.74250898\n",
            "Iteration 6, loss = 1.73780007\n",
            "Iteration 7, loss = 1.73309667\n",
            "Iteration 8, loss = 1.72871473\n",
            "Iteration 9, loss = 1.72464864\n",
            "Iteration 10, loss = 1.72034496\n",
            "Iteration 11, loss = 1.71638869\n",
            "Iteration 12, loss = 1.71246629\n",
            "Iteration 13, loss = 1.70854913\n",
            "Iteration 14, loss = 1.70490381\n",
            "Iteration 15, loss = 1.70140483\n",
            "Iteration 16, loss = 1.69783868\n",
            "Iteration 17, loss = 1.69446829\n",
            "Iteration 18, loss = 1.69148680\n",
            "Iteration 19, loss = 1.68825047\n",
            "Iteration 20, loss = 1.68519365\n",
            "Iteration 21, loss = 1.68239480\n",
            "Iteration 22, loss = 1.67963488\n",
            "Iteration 23, loss = 1.67683582\n",
            "Iteration 24, loss = 1.67418681\n",
            "Iteration 25, loss = 1.67156821\n",
            "Iteration 26, loss = 1.66920794\n",
            "Iteration 27, loss = 1.66679983\n",
            "Iteration 28, loss = 1.66473716\n",
            "Iteration 29, loss = 1.66243036\n",
            "Iteration 30, loss = 1.66021017\n",
            "Iteration 31, loss = 1.65821346\n",
            "Iteration 32, loss = 1.65630296\n",
            "Iteration 33, loss = 1.65427117\n",
            "Iteration 34, loss = 1.65253892\n",
            "Iteration 35, loss = 1.65073576\n",
            "Iteration 36, loss = 1.64891212\n",
            "Iteration 37, loss = 1.64724838\n",
            "Iteration 38, loss = 1.64563969\n",
            "Iteration 39, loss = 1.64417515\n",
            "Iteration 40, loss = 1.64254212\n",
            "Iteration 41, loss = 1.64103049\n",
            "Iteration 42, loss = 1.63964018\n",
            "Iteration 43, loss = 1.63835798\n",
            "Iteration 44, loss = 1.63696547\n",
            "Iteration 45, loss = 1.63563833\n",
            "Iteration 46, loss = 1.63457359\n",
            "Iteration 47, loss = 1.63329281\n",
            "Iteration 48, loss = 1.63218171\n",
            "Iteration 49, loss = 1.63099190\n",
            "Iteration 50, loss = 1.62987911\n",
            "Iteration 51, loss = 1.62881786\n",
            "Iteration 52, loss = 1.62784150\n",
            "Iteration 53, loss = 1.62684095\n",
            "Iteration 54, loss = 1.62585533\n",
            "Iteration 55, loss = 1.62493560\n",
            "Iteration 56, loss = 1.62398422\n",
            "Iteration 57, loss = 1.62316770\n",
            "Iteration 58, loss = 1.62233015\n",
            "Iteration 59, loss = 1.62141699\n",
            "Iteration 60, loss = 1.62066514\n",
            "Iteration 61, loss = 1.61985776\n",
            "Iteration 62, loss = 1.61914047\n",
            "Iteration 63, loss = 1.61838986\n",
            "Iteration 64, loss = 1.61765804\n",
            "Iteration 65, loss = 1.61691420\n",
            "Iteration 66, loss = 1.61629549\n",
            "Iteration 67, loss = 1.61566234\n",
            "Iteration 68, loss = 1.61491043\n",
            "Iteration 69, loss = 1.61432912\n",
            "Iteration 70, loss = 1.61374744\n",
            "Iteration 71, loss = 1.61309537\n",
            "Iteration 72, loss = 1.61256631"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 73, loss = 1.61195581\n",
            "Iteration 74, loss = 1.61137545\n",
            "Iteration 75, loss = 1.61087968\n",
            "Iteration 76, loss = 1.61026136\n",
            "Iteration 77, loss = 1.60980587\n",
            "Iteration 78, loss = 1.60933024\n",
            "Iteration 79, loss = 1.60880911\n",
            "Iteration 80, loss = 1.60833079\n",
            "Iteration 81, loss = 1.60781561\n",
            "Iteration 82, loss = 1.60734582\n",
            "Iteration 83, loss = 1.60684864\n",
            "Iteration 84, loss = 1.60647376\n",
            "Iteration 85, loss = 1.60604576\n",
            "Iteration 86, loss = 1.60569344\n",
            "Iteration 87, loss = 1.60517075\n",
            "Iteration 88, loss = 1.60477447\n",
            "Iteration 89, loss = 1.60431249\n",
            "Iteration 90, loss = 1.60394993\n",
            "Iteration 91, loss = 1.60349945\n",
            "Iteration 92, loss = 1.60319843\n",
            "Iteration 93, loss = 1.60280214\n",
            "Iteration 94, loss = 1.60243610\n",
            "Iteration 95, loss = 1.60211266\n",
            "Iteration 96, loss = 1.60166100\n",
            "Iteration 97, loss = 1.60136827\n",
            "Iteration 98, loss = 1.60105246\n",
            "Iteration 99, loss = 1.60069525\n",
            "Iteration 100, loss = 1.60030076\n",
            "Iteration 1, loss = 1.75197879\n",
            "Iteration 2, loss = 1.70659402\n",
            "Iteration 3, loss = 1.67494990\n",
            "Iteration 4, loss = 1.64950496\n",
            "Iteration 5, loss = 1.63218446\n",
            "Iteration 6, loss = 1.62070309\n",
            "Iteration 7, loss = 1.61305792\n",
            "Iteration 8, loss = 1.60834417\n",
            "Iteration 9, loss = 1.60590861\n",
            "Iteration 10, loss = 1.60302508\n",
            "Iteration 11, loss = 1.60096554\n",
            "Iteration 12, loss = 1.59850250\n",
            "Iteration 13, loss = 1.59672125\n",
            "Iteration 14, loss = 1.59491662\n",
            "Iteration 15, loss = 1.59364080\n",
            "Iteration 16, loss = 1.59209224\n",
            "Iteration 17, loss = 1.59079327\n",
            "Iteration 18, loss = 1.58934232\n",
            "Iteration 19, loss = 1.58805011\n",
            "Iteration 20, loss = 1.58632761\n",
            "Iteration 21, loss = 1.58449987\n",
            "Iteration 22, loss = 1.58234591\n",
            "Iteration 23, loss = 1.58016724\n",
            "Iteration 24, loss = 1.57781303\n",
            "Iteration 25, loss = 1.57515441\n",
            "Iteration 26, loss = 1.57252068\n",
            "Iteration 27, loss = 1.56961381\n",
            "Iteration 28, loss = 1.56637961\n",
            "Iteration 29, loss = 1.56300077\n",
            "Iteration 30, loss = 1.55934650\n",
            "Iteration 31, loss = 1.55539746\n",
            "Iteration 32, loss = 1.55117505\n",
            "Iteration 33, loss = 1.54661777\n",
            "Iteration 34, loss = 1.54213786\n",
            "Iteration 35, loss = 1.53665455\n",
            "Iteration 36, loss = 1.53130979\n",
            "Iteration 37, loss = 1.52551993\n",
            "Iteration 38, loss = 1.51947891\n",
            "Iteration 39, loss = 1.51305256\n",
            "Iteration 40, loss = 1.50616961\n",
            "Iteration 41, loss = 1.49917452\n",
            "Iteration 42, loss = 1.49176323\n",
            "Iteration 43, loss = 1.48424126\n",
            "Iteration 44, loss = 1.47625835\n",
            "Iteration 45, loss = 1.46812223\n",
            "Iteration 46, loss = 1.45983334\n",
            "Iteration 47, loss = 1.45117014\n",
            "Iteration 48, loss = 1.44306316\n",
            "Iteration 49, loss = 1.43383318\n",
            "Iteration 50, loss = 1.42449539\n",
            "Iteration 51, loss = 1.41555630\n",
            "Iteration 52, loss = 1.40652846\n",
            "Iteration 53, loss = 1.39752805\n",
            "Iteration 54, loss = 1.38837404\n",
            "Iteration 55, loss = 1.37924883\n",
            "Iteration 56, loss = 1.37034294\n",
            "Iteration 57, loss = 1.36128795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 1.35262525\n",
            "Iteration 59, loss = 1.34400536\n",
            "Iteration 60, loss = 1.33545900\n",
            "Iteration 61, loss = 1.32728324\n",
            "Iteration 62, loss = 1.31921260\n",
            "Iteration 63, loss = 1.31166508\n",
            "Iteration 64, loss = 1.30393716\n",
            "Iteration 65, loss = 1.29640094\n",
            "Iteration 66, loss = 1.28934940\n",
            "Iteration 67, loss = 1.28257547\n",
            "Iteration 68, loss = 1.27582497\n",
            "Iteration 69, loss = 1.26936818\n",
            "Iteration 70, loss = 1.26330457\n",
            "Iteration 71, loss = 1.25772755\n",
            "Iteration 72, loss = 1.25252966\n",
            "Iteration 73, loss = 1.24672608\n",
            "Iteration 74, loss = 1.24134951\n",
            "Iteration 75, loss = 1.23652862\n",
            "Iteration 76, loss = 1.23194470\n",
            "Iteration 77, loss = 1.22732452\n",
            "Iteration 78, loss = 1.22328533\n",
            "Iteration 79, loss = 1.21883725\n",
            "Iteration 80, loss = 1.21497425\n",
            "Iteration 81, loss = 1.21124555\n",
            "Iteration 82, loss = 1.20784172\n",
            "Iteration 83, loss = 1.20478319\n",
            "Iteration 84, loss = 1.20101333\n",
            "Iteration 85, loss = 1.19803672\n",
            "Iteration 86, loss = 1.19517740\n",
            "Iteration 87, loss = 1.19193220\n",
            "Iteration 88, loss = 1.18968192\n",
            "Iteration 89, loss = 1.18655284\n",
            "Iteration 90, loss = 1.18387695\n",
            "Iteration 91, loss = 1.18142277\n",
            "Iteration 92, loss = 1.17916176\n",
            "Iteration 93, loss = 1.17702729\n",
            "Iteration 94, loss = 1.17486434\n",
            "Iteration 95, loss = 1.17317414\n",
            "Iteration 96, loss = 1.17059070\n",
            "Iteration 97, loss = 1.16882935\n",
            "Iteration 98, loss = 1.16709581\n",
            "Iteration 99, loss = 1.16489266\n",
            "Iteration 100, loss = 1.16307786\n",
            "Iteration 1, loss = 1.69767334\n",
            "Iteration 2, loss = 1.61891547\n",
            "Iteration 3, loss = 1.60890743\n",
            "Iteration 4, loss = 1.60309809\n",
            "Iteration 5, loss = 1.59906583\n",
            "Iteration 6, loss = 1.60256323\n",
            "Iteration 7, loss = 1.58917080\n",
            "Iteration 8, loss = 1.57464537\n",
            "Iteration 9, loss = 1.56017630\n",
            "Iteration 10, loss = 1.53473995\n",
            "Iteration 11, loss = 1.49999139\n",
            "Iteration 12, loss = 1.45728362\n",
            "Iteration 13, loss = 1.40733316\n",
            "Iteration 14, loss = 1.35331126\n",
            "Iteration 15, loss = 1.30132480\n",
            "Iteration 16, loss = 1.25242639\n",
            "Iteration 17, loss = 1.21706085\n",
            "Iteration 18, loss = 1.18971682\n",
            "Iteration 19, loss = 1.16698914\n",
            "Iteration 20, loss = 1.15249028\n",
            "Iteration 21, loss = 1.13716358\n",
            "Iteration 22, loss = 1.13398455\n",
            "Iteration 23, loss = 1.12110866\n",
            "Iteration 24, loss = 1.11518037\n",
            "Iteration 25, loss = 1.11097820\n",
            "Iteration 26, loss = 1.10612518\n",
            "Iteration 27, loss = 1.10152550\n",
            "Iteration 28, loss = 1.09924369\n",
            "Iteration 29, loss = 1.09506575\n",
            "Iteration 30, loss = 1.09372487\n",
            "Iteration 31, loss = 1.09073102\n",
            "Iteration 32, loss = 1.08636021\n",
            "Iteration 33, loss = 1.08476935\n",
            "Iteration 34, loss = 1.08476672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 35, loss = 1.08166034\n",
            "Iteration 36, loss = 1.08293557\n",
            "Iteration 37, loss = 1.06793010\n",
            "Iteration 38, loss = 1.07361097\n",
            "Iteration 39, loss = 1.06588896\n",
            "Iteration 40, loss = 1.06259614\n",
            "Iteration 41, loss = 1.05785665\n",
            "Iteration 42, loss = 1.05022181\n",
            "Iteration 43, loss = 1.04547621\n",
            "Iteration 44, loss = 1.04707672\n",
            "Iteration 45, loss = 1.04087381\n",
            "Iteration 46, loss = 1.03280117\n",
            "Iteration 47, loss = 1.03370687\n",
            "Iteration 48, loss = 1.01460103\n",
            "Iteration 49, loss = 1.01450886\n",
            "Iteration 50, loss = 1.00935532\n",
            "Iteration 51, loss = 0.99792838\n",
            "Iteration 52, loss = 0.99196804\n",
            "Iteration 53, loss = 0.98393956\n",
            "Iteration 54, loss = 0.97710208\n",
            "Iteration 55, loss = 0.96820560\n",
            "Iteration 56, loss = 0.97440673\n",
            "Iteration 57, loss = 0.96294775\n",
            "Iteration 58, loss = 0.96176330\n",
            "Iteration 59, loss = 0.93997963\n",
            "Iteration 60, loss = 0.93779390\n",
            "Iteration 61, loss = 0.93544905\n",
            "Iteration 62, loss = 0.91871356\n",
            "Iteration 63, loss = 0.92519863\n",
            "Iteration 64, loss = 0.92767279\n",
            "Iteration 65, loss = 0.92022718\n",
            "Iteration 66, loss = 0.93310968\n",
            "Iteration 67, loss = 0.94878634\n",
            "Iteration 68, loss = 0.95574470\n",
            "Iteration 69, loss = 0.95160551\n",
            "Iteration 70, loss = 0.92224083\n",
            "Iteration 71, loss = 0.92880165\n",
            "Iteration 72, loss = 0.90174007\n",
            "Iteration 73, loss = 0.90558877\n",
            "Iteration 74, loss = 0.88048444\n",
            "Iteration 75, loss = 0.88836248\n",
            "Iteration 76, loss = 0.88046187\n",
            "Iteration 77, loss = 0.89863607\n",
            "Iteration 78, loss = 0.92519144\n",
            "Iteration 79, loss = 0.88726607\n",
            "Iteration 80, loss = 0.85306710\n",
            "Iteration 81, loss = 0.87955490\n",
            "Iteration 82, loss = 0.88052612\n",
            "Iteration 83, loss = 0.84784849\n",
            "Iteration 84, loss = 0.84471570\n",
            "Iteration 85, loss = 0.83642170\n",
            "Iteration 86, loss = 0.84312837\n",
            "Iteration 87, loss = 0.83036155\n",
            "Iteration 88, loss = 0.83834488\n",
            "Iteration 89, loss = 0.83778803\n",
            "Iteration 90, loss = 0.81704960\n",
            "Iteration 91, loss = 0.82038059\n",
            "Iteration 92, loss = 0.82858579\n",
            "Iteration 93, loss = 0.81087910\n",
            "Iteration 94, loss = 0.80903737\n",
            "Iteration 95, loss = 0.82495808\n",
            "Iteration 96, loss = 0.82874013\n",
            "Iteration 97, loss = 0.86161332\n",
            "Iteration 98, loss = 0.84207353\n",
            "Iteration 99, loss = 0.83476612\n",
            "Iteration 100, loss = 0.85037469\n",
            "Iteration 1, loss = 1.63343775\n",
            "Iteration 2, loss = 1.63279187\n",
            "Iteration 3, loss = 1.63223535\n",
            "Iteration 4, loss = 1.63161216\n",
            "Iteration 5, loss = 1.63103893\n",
            "Iteration 6, loss = 1.63050402\n",
            "Iteration 7, loss = 1.62988844\n",
            "Iteration 8, loss = 1.62938739\n",
            "Iteration 9, loss = 1.62893580\n",
            "Iteration 10, loss = 1.62839256\n",
            "Iteration 11, loss = 1.62794062\n",
            "Iteration 12, loss = 1.62745641\n",
            "Iteration 13, loss = 1.62695047\n",
            "Iteration 14, loss = 1.62650148\n",
            "Iteration 15, loss = 1.62606575\n",
            "Iteration 16, loss = 1.62559675\n",
            "Iteration 17, loss = 1.62514188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 1.62478958\n",
            "Iteration 19, loss = 1.62436358\n",
            "Iteration 20, loss = 1.62392597\n",
            "Iteration 21, loss = 1.62355078\n",
            "Iteration 22, loss = 1.62319075\n",
            "Iteration 23, loss = 1.62277746\n",
            "Iteration 24, loss = 1.62241924\n",
            "Iteration 25, loss = 1.62201462\n",
            "Iteration 26, loss = 1.62168391\n",
            "Iteration 27, loss = 1.62130465\n",
            "Iteration 28, loss = 1.62102922\n",
            "Iteration 29, loss = 1.62063184\n",
            "Iteration 30, loss = 1.62031319\n",
            "Iteration 31, loss = 1.61996129\n",
            "Iteration 32, loss = 1.61967449\n",
            "Iteration 33, loss = 1.61933214\n",
            "Iteration 34, loss = 1.61906481\n",
            "Iteration 35, loss = 1.61873616\n",
            "Iteration 36, loss = 1.61842452\n",
            "Iteration 37, loss = 1.61812840\n",
            "Iteration 38, loss = 1.61786548\n",
            "Iteration 39, loss = 1.61756612\n",
            "Iteration 40, loss = 1.61722209\n",
            "Iteration 41, loss = 1.61697782\n",
            "Iteration 42, loss = 1.61667890\n",
            "Iteration 43, loss = 1.61645218\n",
            "Iteration 44, loss = 1.61613783\n",
            "Iteration 45, loss = 1.61589289\n",
            "Iteration 46, loss = 1.61564881\n",
            "Iteration 47, loss = 1.61538274\n",
            "Iteration 48, loss = 1.61516195\n",
            "Iteration 49, loss = 1.61487464\n",
            "Iteration 50, loss = 1.61460735\n",
            "Iteration 51, loss = 1.61436974\n",
            "Iteration 52, loss = 1.61413741\n",
            "Iteration 53, loss = 1.61391648\n",
            "Iteration 54, loss = 1.61367688\n",
            "Iteration 55, loss = 1.61343030\n",
            "Iteration 56, loss = 1.61318695\n",
            "Iteration 57, loss = 1.61298080\n",
            "Iteration 58, loss = 1.61276119\n",
            "Iteration 59, loss = 1.61252261\n",
            "Iteration 60, loss = 1.61232638\n",
            "Iteration 61, loss = 1.61211089\n",
            "Iteration 62, loss = 1.61190024\n",
            "Iteration 63, loss = 1.61168603\n",
            "Iteration 64, loss = 1.61147411\n",
            "Iteration 65, loss = 1.61126163\n",
            "Iteration 66, loss = 1.61107103\n",
            "Iteration 67, loss = 1.61088429\n",
            "Iteration 68, loss = 1.61064386\n",
            "Iteration 69, loss = 1.61047393\n",
            "Iteration 70, loss = 1.61029654\n",
            "Iteration 71, loss = 1.61011340\n",
            "Iteration 72, loss = 1.60992430\n",
            "Iteration 73, loss = 1.60973417\n",
            "Iteration 74, loss = 1.60954392\n",
            "Iteration 75, loss = 1.60937756\n",
            "Iteration 76, loss = 1.60918601\n",
            "Iteration 77, loss = 1.60901110\n",
            "Iteration 78, loss = 1.60887061\n",
            "Iteration 79, loss = 1.60867592\n",
            "Iteration 80, loss = 1.60851289\n",
            "Iteration 81, loss = 1.60834018\n",
            "Iteration 82, loss = 1.60817527\n",
            "Iteration 83, loss = 1.60799037\n",
            "Iteration 84, loss = 1.60788182\n",
            "Iteration 85, loss = 1.60771417\n",
            "Iteration 86, loss = 1.60760807\n",
            "Iteration 87, loss = 1.60740309\n",
            "Iteration 88, loss = 1.60729405\n",
            "Iteration 89, loss = 1.60708597\n",
            "Iteration 90, loss = 1.60695945\n",
            "Iteration 91, loss = 1.60679074\n",
            "Iteration 92, loss = 1.60670396\n",
            "Iteration 93, loss = 1.60654494\n",
            "Iteration 94, loss = 1.60641153\n",
            "Iteration 95, loss = 1.60631195\n",
            "Iteration 96, loss = 1.60612171\n",
            "Iteration 97, loss = 1.60603090\n",
            "Iteration 98, loss = 1.60590776\n",
            "Iteration 99, loss = 1.60577570\n",
            "Iteration 100, loss = 1.60562069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.63227386\n",
            "Iteration 2, loss = 1.62670038\n",
            "Iteration 3, loss = 1.62272750\n",
            "Iteration 4, loss = 1.61886954\n",
            "Iteration 5, loss = 1.61582947\n",
            "Iteration 6, loss = 1.61320151\n",
            "Iteration 7, loss = 1.61067177\n",
            "Iteration 8, loss = 1.60852926\n",
            "Iteration 9, loss = 1.60679213\n",
            "Iteration 10, loss = 1.60513696\n",
            "Iteration 11, loss = 1.60392625\n",
            "Iteration 12, loss = 1.60267457\n",
            "Iteration 13, loss = 1.60188316\n",
            "Iteration 14, loss = 1.60123016\n",
            "Iteration 15, loss = 1.60096829\n",
            "Iteration 16, loss = 1.60043833\n",
            "Iteration 17, loss = 1.60020325\n",
            "Iteration 18, loss = 1.60016031\n",
            "Iteration 19, loss = 1.59984954\n",
            "Iteration 20, loss = 1.59968214\n",
            "Iteration 21, loss = 1.59962355\n",
            "Iteration 22, loss = 1.59945084\n",
            "Iteration 23, loss = 1.59927934\n",
            "Iteration 24, loss = 1.59918334\n",
            "Iteration 25, loss = 1.59906406\n",
            "Iteration 26, loss = 1.59900298\n",
            "Iteration 27, loss = 1.59893491\n",
            "Iteration 28, loss = 1.59880830\n",
            "Iteration 29, loss = 1.59872897\n",
            "Iteration 30, loss = 1.59871727\n",
            "Iteration 31, loss = 1.59859092\n",
            "Iteration 32, loss = 1.59848709\n",
            "Iteration 33, loss = 1.59846091\n",
            "Iteration 34, loss = 1.59850903\n",
            "Iteration 35, loss = 1.59838790\n",
            "Iteration 36, loss = 1.59833958\n",
            "Iteration 37, loss = 1.59830412\n",
            "Iteration 38, loss = 1.59842138\n",
            "Iteration 39, loss = 1.59830562\n",
            "Iteration 40, loss = 1.59842929\n",
            "Iteration 41, loss = 1.59827455\n",
            "Iteration 42, loss = 1.59816666\n",
            "Iteration 43, loss = 1.59815493\n",
            "Iteration 44, loss = 1.59815816\n",
            "Iteration 45, loss = 1.59813973\n",
            "Iteration 46, loss = 1.59811649\n",
            "Iteration 47, loss = 1.59806333\n",
            "Iteration 48, loss = 1.59815345\n",
            "Iteration 49, loss = 1.59803780\n",
            "Iteration 50, loss = 1.59788373\n",
            "Iteration 51, loss = 1.59783988\n",
            "Iteration 52, loss = 1.59782497\n",
            "Iteration 53, loss = 1.59778140\n",
            "Iteration 54, loss = 1.59774402\n",
            "Iteration 55, loss = 1.59763871\n",
            "Iteration 56, loss = 1.59768214\n",
            "Iteration 57, loss = 1.59751744\n",
            "Iteration 58, loss = 1.59751017\n",
            "Iteration 59, loss = 1.59740332\n",
            "Iteration 60, loss = 1.59732445\n",
            "Iteration 61, loss = 1.59730729\n",
            "Iteration 62, loss = 1.59710708\n",
            "Iteration 63, loss = 1.59705823\n",
            "Iteration 64, loss = 1.59689301\n",
            "Iteration 65, loss = 1.59676348\n",
            "Iteration 66, loss = 1.59660824\n",
            "Iteration 67, loss = 1.59645255\n",
            "Iteration 68, loss = 1.59624868\n",
            "Iteration 69, loss = 1.59594661\n",
            "Iteration 70, loss = 1.59569849\n",
            "Iteration 71, loss = 1.59551027\n",
            "Iteration 72, loss = 1.59518237\n",
            "Iteration 73, loss = 1.59479220\n",
            "Iteration 74, loss = 1.59429090\n",
            "Iteration 75, loss = 1.59377950\n",
            "Iteration 76, loss = 1.59333147\n",
            "Iteration 77, loss = 1.59262120\n",
            "Iteration 78, loss = 1.59200584\n",
            "Iteration 79, loss = 1.59116875\n",
            "Iteration 80, loss = 1.59027160\n",
            "Iteration 81, loss = 1.58918291\n",
            "Iteration 82, loss = 1.58820013\n",
            "Iteration 83, loss = 1.58686162\n",
            "Iteration 84, loss = 1.58543865\n",
            "Iteration 85, loss = 1.58383155\n",
            "Iteration 86, loss = 1.58233711\n",
            "Iteration 87, loss = 1.58004884\n",
            "Iteration 88, loss = 1.57817187\n",
            "Iteration 89, loss = 1.57562463\n",
            "Iteration 90, loss = 1.57290268\n",
            "Iteration 91, loss = 1.57009852\n",
            "Iteration 92, loss = 1.56698414\n",
            "Iteration 93, loss = 1.56355462\n",
            "Iteration 94, loss = 1.55996944\n",
            "Iteration 95, loss = 1.55640134\n",
            "Iteration 96, loss = 1.55170846\n",
            "Iteration 97, loss = 1.54737160\n",
            "Iteration 98, loss = 1.54269383\n",
            "Iteration 99, loss = 1.53751201\n",
            "Iteration 100, loss = 1.53220427\n",
            "Iteration 1, loss = 1.62713790\n",
            "Iteration 2, loss = 1.60953166\n",
            "Iteration 3, loss = 1.60308058\n",
            "Iteration 4, loss = 1.60476062\n",
            "Iteration 5, loss = 1.60218379\n",
            "Iteration 6, loss = 1.60226285\n",
            "Iteration 7, loss = 1.60028542\n",
            "Iteration 8, loss = 1.59986273\n",
            "Iteration 9, loss = 1.59890059\n",
            "Iteration 10, loss = 1.59733488\n",
            "Iteration 11, loss = 1.59506709\n",
            "Iteration 12, loss = 1.59349318\n",
            "Iteration 13, loss = 1.58666820\n",
            "Iteration 14, loss = 1.57997943\n",
            "Iteration 15, loss = 1.56643205\n",
            "Iteration 16, loss = 1.54808740\n",
            "Iteration 17, loss = 1.52961981\n",
            "Iteration 18, loss = 1.49956631\n",
            "Iteration 19, loss = 1.46776830\n",
            "Iteration 20, loss = 1.43130216\n",
            "Iteration 21, loss = 1.39515629\n",
            "Iteration 22, loss = 1.35616652\n",
            "Iteration 23, loss = 1.31866957\n",
            "Iteration 24, loss = 1.28505133\n",
            "Iteration 25, loss = 1.25252233\n",
            "Iteration 26, loss = 1.22707470\n",
            "Iteration 27, loss = 1.20460167\n",
            "Iteration 28, loss = 1.18657246\n",
            "Iteration 29, loss = 1.17238295\n",
            "Iteration 30, loss = 1.16232922\n",
            "Iteration 31, loss = 1.15203512\n",
            "Iteration 32, loss = 1.14505553\n",
            "Iteration 33, loss = 1.14001470\n",
            "Iteration 34, loss = 1.14046898\n",
            "Iteration 35, loss = 1.13322137\n",
            "Iteration 36, loss = 1.13071749\n",
            "Iteration 37, loss = 1.12559741\n",
            "Iteration 38, loss = 1.12871264\n",
            "Iteration 39, loss = 1.12626578\n",
            "Iteration 40, loss = 1.12067632\n",
            "Iteration 41, loss = 1.12139432\n",
            "Iteration 42, loss = 1.11556411\n",
            "Iteration 43, loss = 1.11445510\n",
            "Iteration 44, loss = 1.11387698\n",
            "Iteration 45, loss = 1.11077796\n",
            "Iteration 46, loss = 1.10994974\n",
            "Iteration 47, loss = 1.10797900\n",
            "Iteration 48, loss = 1.10910434\n",
            "Iteration 49, loss = 1.10924915\n",
            "Iteration 50, loss = 1.10570732\n",
            "Iteration 51, loss = 1.10479827\n",
            "Iteration 52, loss = 1.10359591\n",
            "Iteration 53, loss = 1.10328711\n",
            "Iteration 54, loss = 1.10465074\n",
            "Iteration 55, loss = 1.10391488\n",
            "Iteration 56, loss = 1.10399435\n",
            "Iteration 57, loss = 1.10142679\n",
            "Iteration 58, loss = 1.10433194\n",
            "Iteration 59, loss = 1.09954764\n",
            "Iteration 60, loss = 1.10272124\n",
            "Iteration 61, loss = 1.10321790\n",
            "Iteration 62, loss = 1.09902407\n",
            "Iteration 63, loss = 1.10144992\n",
            "Iteration 64, loss = 1.10240017\n",
            "Iteration 65, loss = 1.10217281\n",
            "Iteration 66, loss = 1.09966553\n",
            "Iteration 67, loss = 1.09853293\n",
            "Iteration 68, loss = 1.09802242\n",
            "Iteration 69, loss = 1.09785854\n",
            "Iteration 70, loss = 1.09724538\n",
            "Iteration 71, loss = 1.09859804\n",
            "Iteration 72, loss = 1.10613134\n",
            "Iteration 73, loss = 1.10523309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 1.10016380\n",
            "Iteration 75, loss = 1.09977430\n",
            "Iteration 76, loss = 1.10163921\n",
            "Iteration 77, loss = 1.09616130\n",
            "Iteration 78, loss = 1.09673297\n",
            "Iteration 79, loss = 1.09723809\n",
            "Iteration 80, loss = 1.09634682\n",
            "Iteration 81, loss = 1.09678148\n",
            "Iteration 82, loss = 1.10060395\n",
            "Iteration 83, loss = 1.09826531\n",
            "Iteration 84, loss = 1.09753792\n",
            "Iteration 85, loss = 1.10111692\n",
            "Iteration 86, loss = 1.09715169\n",
            "Iteration 87, loss = 1.09924727\n",
            "Iteration 88, loss = 1.10145134\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.66984116\n",
            "Iteration 2, loss = 1.66729422\n",
            "Iteration 3, loss = 1.66494825\n",
            "Iteration 4, loss = 1.66256631\n",
            "Iteration 5, loss = 1.66022686\n",
            "Iteration 6, loss = 1.65795638\n",
            "Iteration 7, loss = 1.65589285\n",
            "Iteration 8, loss = 1.65380545\n",
            "Iteration 9, loss = 1.65178571\n",
            "Iteration 10, loss = 1.64990968\n",
            "Iteration 11, loss = 1.64791212\n",
            "Iteration 12, loss = 1.64610356\n",
            "Iteration 13, loss = 1.64443513\n",
            "Iteration 14, loss = 1.64277372\n",
            "Iteration 15, loss = 1.64097928\n",
            "Iteration 16, loss = 1.63935850\n",
            "Iteration 17, loss = 1.63772025\n",
            "Iteration 18, loss = 1.63624793\n",
            "Iteration 19, loss = 1.63472609\n",
            "Iteration 20, loss = 1.63345872\n",
            "Iteration 21, loss = 1.63226738\n",
            "Iteration 22, loss = 1.63072488\n",
            "Iteration 23, loss = 1.62950353\n",
            "Iteration 24, loss = 1.62831123\n",
            "Iteration 25, loss = 1.62710956\n",
            "Iteration 26, loss = 1.62603569\n",
            "Iteration 27, loss = 1.62480456\n",
            "Iteration 28, loss = 1.62365875\n",
            "Iteration 29, loss = 1.62272852\n",
            "Iteration 30, loss = 1.62168332\n",
            "Iteration 31, loss = 1.62055800\n",
            "Iteration 32, loss = 1.61966150\n",
            "Iteration 33, loss = 1.61873041\n",
            "Iteration 34, loss = 1.61775294\n",
            "Iteration 35, loss = 1.61686684\n",
            "Iteration 36, loss = 1.61614748\n",
            "Iteration 37, loss = 1.61525100\n",
            "Iteration 38, loss = 1.61440784\n",
            "Iteration 39, loss = 1.61365910\n",
            "Iteration 40, loss = 1.61291304\n",
            "Iteration 41, loss = 1.61219346\n",
            "Iteration 42, loss = 1.61137887\n",
            "Iteration 43, loss = 1.61067502\n",
            "Iteration 44, loss = 1.61009204\n",
            "Iteration 45, loss = 1.60947929\n",
            "Iteration 46, loss = 1.60868507\n",
            "Iteration 47, loss = 1.60807439\n",
            "Iteration 48, loss = 1.60748468\n",
            "Iteration 49, loss = 1.60688593\n",
            "Iteration 50, loss = 1.60635316\n",
            "Iteration 51, loss = 1.60571921\n",
            "Iteration 52, loss = 1.60512200\n",
            "Iteration 53, loss = 1.60465058\n",
            "Iteration 54, loss = 1.60418638\n",
            "Iteration 55, loss = 1.60364817\n",
            "Iteration 56, loss = 1.60316818\n",
            "Iteration 57, loss = 1.60268077\n",
            "Iteration 58, loss = 1.60212481\n",
            "Iteration 59, loss = 1.60167584\n",
            "Iteration 60, loss = 1.60128789\n",
            "Iteration 61, loss = 1.60079922\n",
            "Iteration 62, loss = 1.60038280\n",
            "Iteration 63, loss = 1.59994804\n",
            "Iteration 64, loss = 1.59950373\n",
            "Iteration 65, loss = 1.59916225\n",
            "Iteration 66, loss = 1.59868538\n",
            "Iteration 67, loss = 1.59828729\n",
            "Iteration 68, loss = 1.59789757\n",
            "Iteration 69, loss = 1.59748700\n",
            "Iteration 70, loss = 1.59713841\n",
            "Iteration 71, loss = 1.59673392\n",
            "Iteration 72, loss = 1.59638883\n",
            "Iteration 73, loss = 1.59602796\n",
            "Iteration 74, loss = 1.59565581\n",
            "Iteration 75, loss = 1.59528638\n",
            "Iteration 76, loss = 1.59496062\n",
            "Iteration 77, loss = 1.59457350\n",
            "Iteration 78, loss = 1.59425540\n",
            "Iteration 79, loss = 1.59390197\n",
            "Iteration 80, loss = 1.59354043\n",
            "Iteration 81, loss = 1.59321244\n",
            "Iteration 82, loss = 1.59285442\n",
            "Iteration 83, loss = 1.59254216\n",
            "Iteration 84, loss = 1.59219609\n",
            "Iteration 85, loss = 1.59184968\n",
            "Iteration 86, loss = 1.59151268\n",
            "Iteration 87, loss = 1.59119357\n",
            "Iteration 88, loss = 1.59087310\n",
            "Iteration 89, loss = 1.59056405\n",
            "Iteration 90, loss = 1.59019399\n",
            "Iteration 91, loss = 1.58988508\n",
            "Iteration 92, loss = 1.58953569\n",
            "Iteration 93, loss = 1.58920931\n",
            "Iteration 94, loss = 1.58889350\n",
            "Iteration 95, loss = 1.58855645\n",
            "Iteration 96, loss = 1.58822391\n",
            "Iteration 97, loss = 1.58790182\n",
            "Iteration 98, loss = 1.58758707\n",
            "Iteration 99, loss = 1.58725831\n",
            "Iteration 100, loss = 1.58690566\n",
            "Iteration 1, loss = 1.66579902\n",
            "Iteration 2, loss = 1.64474536\n",
            "Iteration 3, loss = 1.63027578\n",
            "Iteration 4, loss = 1.61842771\n",
            "Iteration 5, loss = 1.61057596\n",
            "Iteration 6, loss = 1.60489441\n",
            "Iteration 7, loss = 1.60113089\n",
            "Iteration 8, loss = 1.59901686\n",
            "Iteration 9, loss = 1.59732462\n",
            "Iteration 10, loss = 1.59506762\n",
            "Iteration 11, loss = 1.59298407\n",
            "Iteration 12, loss = 1.59040877\n",
            "Iteration 13, loss = 1.58781832\n",
            "Iteration 14, loss = 1.58477696\n",
            "Iteration 15, loss = 1.58203223\n",
            "Iteration 16, loss = 1.57893588\n",
            "Iteration 17, loss = 1.57603749\n",
            "Iteration 18, loss = 1.57267045\n",
            "Iteration 19, loss = 1.56960618\n",
            "Iteration 20, loss = 1.56581961\n",
            "Iteration 21, loss = 1.56218396\n",
            "Iteration 22, loss = 1.55828873\n",
            "Iteration 23, loss = 1.55422228\n",
            "Iteration 24, loss = 1.54993001\n",
            "Iteration 25, loss = 1.54521503\n",
            "Iteration 26, loss = 1.54053722\n",
            "Iteration 27, loss = 1.53524066\n",
            "Iteration 28, loss = 1.53016083\n",
            "Iteration 29, loss = 1.52428248\n",
            "Iteration 30, loss = 1.51866031\n",
            "Iteration 31, loss = 1.51260986\n",
            "Iteration 32, loss = 1.50586397\n",
            "Iteration 33, loss = 1.49975544\n",
            "Iteration 34, loss = 1.49248807\n",
            "Iteration 35, loss = 1.48552571\n",
            "Iteration 36, loss = 1.47787553\n",
            "Iteration 37, loss = 1.47033226\n",
            "Iteration 38, loss = 1.46250692\n",
            "Iteration 39, loss = 1.45435212\n",
            "Iteration 40, loss = 1.44633036\n",
            "Iteration 41, loss = 1.43795144\n",
            "Iteration 42, loss = 1.42971024\n",
            "Iteration 43, loss = 1.42091195\n",
            "Iteration 44, loss = 1.41278389\n",
            "Iteration 45, loss = 1.40394529\n",
            "Iteration 46, loss = 1.39505314\n",
            "Iteration 47, loss = 1.38673918\n",
            "Iteration 48, loss = 1.37782690\n",
            "Iteration 49, loss = 1.36913336\n",
            "Iteration 50, loss = 1.36107883\n",
            "Iteration 51, loss = 1.35236525\n",
            "Iteration 52, loss = 1.34474878\n",
            "Iteration 53, loss = 1.33630833\n",
            "Iteration 54, loss = 1.32861805\n",
            "Iteration 55, loss = 1.32075764\n",
            "Iteration 56, loss = 1.31341203\n",
            "Iteration 57, loss = 1.30615166\n",
            "Iteration 58, loss = 1.29971803\n",
            "Iteration 59, loss = 1.29226475\n",
            "Iteration 60, loss = 1.28559768\n",
            "Iteration 61, loss = 1.27943724\n",
            "Iteration 62, loss = 1.27318515\n",
            "Iteration 63, loss = 1.26738181\n",
            "Iteration 64, loss = 1.26179483\n",
            "Iteration 65, loss = 1.25663830\n",
            "Iteration 66, loss = 1.25099167\n",
            "Iteration 67, loss = 1.24600554\n",
            "Iteration 68, loss = 1.24140623\n",
            "Iteration 69, loss = 1.23663628\n",
            "Iteration 70, loss = 1.23225143\n",
            "Iteration 71, loss = 1.22825966\n",
            "Iteration 72, loss = 1.22399991\n",
            "Iteration 73, loss = 1.22046322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 1.21673785\n",
            "Iteration 75, loss = 1.21286021\n",
            "Iteration 76, loss = 1.20945832\n",
            "Iteration 77, loss = 1.20652767\n",
            "Iteration 78, loss = 1.20315891\n",
            "Iteration 79, loss = 1.20049380\n",
            "Iteration 80, loss = 1.19789847\n",
            "Iteration 81, loss = 1.19443925\n",
            "Iteration 82, loss = 1.19187626\n",
            "Iteration 83, loss = 1.18923603\n",
            "Iteration 84, loss = 1.18719570\n",
            "Iteration 85, loss = 1.18454241\n",
            "Iteration 86, loss = 1.18226472\n",
            "Iteration 87, loss = 1.18004244\n",
            "Iteration 88, loss = 1.17809464\n",
            "Iteration 89, loss = 1.17616949\n",
            "Iteration 90, loss = 1.17403515\n",
            "Iteration 91, loss = 1.17239622\n",
            "Iteration 92, loss = 1.17085489\n",
            "Iteration 93, loss = 1.16861618\n",
            "Iteration 94, loss = 1.16687797\n",
            "Iteration 95, loss = 1.16535144\n",
            "Iteration 96, loss = 1.16365093\n",
            "Iteration 97, loss = 1.16228327\n",
            "Iteration 98, loss = 1.16065535\n",
            "Iteration 99, loss = 1.15917661\n",
            "Iteration 100, loss = 1.15775348\n",
            "Iteration 1, loss = 1.66526962\n",
            "Iteration 2, loss = 1.61240678\n",
            "Iteration 3, loss = 1.60290169\n",
            "Iteration 4, loss = 1.58448502\n",
            "Iteration 5, loss = 1.56739877\n",
            "Iteration 6, loss = 1.54226556\n",
            "Iteration 7, loss = 1.50597902\n",
            "Iteration 8, loss = 1.45832648\n",
            "Iteration 9, loss = 1.40521754\n",
            "Iteration 10, loss = 1.34357434\n",
            "Iteration 11, loss = 1.28735092\n",
            "Iteration 12, loss = 1.24037532\n",
            "Iteration 13, loss = 1.20534834\n",
            "Iteration 14, loss = 1.17311047\n",
            "Iteration 15, loss = 1.15620951\n",
            "Iteration 16, loss = 1.14171861\n",
            "Iteration 17, loss = 1.13606999\n",
            "Iteration 18, loss = 1.12886736\n",
            "Iteration 19, loss = 1.12309138\n",
            "Iteration 20, loss = 1.11513417\n",
            "Iteration 21, loss = 1.11686479\n",
            "Iteration 22, loss = 1.10723993\n",
            "Iteration 23, loss = 1.11017921\n",
            "Iteration 24, loss = 1.10440455\n",
            "Iteration 25, loss = 1.10152048\n",
            "Iteration 26, loss = 1.10628544\n",
            "Iteration 27, loss = 1.10045772\n",
            "Iteration 28, loss = 1.09864423\n",
            "Iteration 29, loss = 1.10379472\n",
            "Iteration 30, loss = 1.09551168\n",
            "Iteration 31, loss = 1.10458540\n",
            "Iteration 32, loss = 1.08887990\n",
            "Iteration 33, loss = 1.09401659\n",
            "Iteration 34, loss = 1.08550080\n",
            "Iteration 35, loss = 1.09391797\n",
            "Iteration 36, loss = 1.08966159\n",
            "Iteration 37, loss = 1.08302470\n",
            "Iteration 38, loss = 1.08065391\n",
            "Iteration 39, loss = 1.07488730\n",
            "Iteration 40, loss = 1.07216944\n",
            "Iteration 41, loss = 1.07290985\n",
            "Iteration 42, loss = 1.06453265\n",
            "Iteration 43, loss = 1.06113552\n",
            "Iteration 44, loss = 1.06776243\n",
            "Iteration 45, loss = 1.05209987\n",
            "Iteration 46, loss = 1.05342884\n",
            "Iteration 47, loss = 1.04431043\n",
            "Iteration 48, loss = 1.04301936\n",
            "Iteration 49, loss = 1.03861017\n",
            "Iteration 50, loss = 1.04610771\n",
            "Iteration 51, loss = 1.02084263\n",
            "Iteration 52, loss = 1.02915726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 1.02038918\n",
            "Iteration 54, loss = 1.01182549\n",
            "Iteration 55, loss = 1.01071281\n",
            "Iteration 56, loss = 1.01035761\n",
            "Iteration 57, loss = 1.01041222\n",
            "Iteration 58, loss = 0.99439983\n",
            "Iteration 59, loss = 0.99815728\n",
            "Iteration 60, loss = 0.98234246\n",
            "Iteration 61, loss = 0.99624666\n",
            "Iteration 62, loss = 0.97723076\n",
            "Iteration 63, loss = 0.99104642\n",
            "Iteration 64, loss = 0.97651340\n",
            "Iteration 65, loss = 0.99786426\n",
            "Iteration 66, loss = 0.98824945\n",
            "Iteration 67, loss = 0.98996152\n",
            "Iteration 68, loss = 0.95335339\n",
            "Iteration 69, loss = 0.96551609\n",
            "Iteration 70, loss = 0.95400964\n",
            "Iteration 71, loss = 0.93746456\n",
            "Iteration 72, loss = 0.93344328\n",
            "Iteration 73, loss = 0.92625999\n",
            "Iteration 74, loss = 0.92546407\n",
            "Iteration 75, loss = 0.92115823\n",
            "Iteration 76, loss = 0.92261693\n",
            "Iteration 77, loss = 0.91717971\n",
            "Iteration 78, loss = 0.91315702\n",
            "Iteration 79, loss = 0.90971001\n",
            "Iteration 80, loss = 0.94235077\n",
            "Iteration 81, loss = 0.96880423\n",
            "Iteration 82, loss = 0.94809732\n",
            "Iteration 83, loss = 0.91201243\n",
            "Iteration 84, loss = 0.88806673\n",
            "Iteration 85, loss = 0.91299845\n",
            "Iteration 86, loss = 0.92053648\n",
            "Iteration 87, loss = 0.94418505\n",
            "Iteration 88, loss = 0.97141643\n",
            "Iteration 89, loss = 0.93816231\n",
            "Iteration 90, loss = 0.93219584\n",
            "Iteration 91, loss = 0.92971818\n",
            "Iteration 92, loss = 0.91620625\n",
            "Iteration 93, loss = 0.91091819\n",
            "Iteration 94, loss = 0.86928697\n",
            "Iteration 95, loss = 0.87700442\n",
            "Iteration 96, loss = 0.88231822\n",
            "Iteration 97, loss = 0.86533792\n",
            "Iteration 98, loss = 0.86163551\n",
            "Iteration 99, loss = 0.86288872\n",
            "Iteration 100, loss = 0.85699699\n",
            "Iteration 1, loss = 1.66720283\n",
            "Iteration 2, loss = 1.66525385\n",
            "Iteration 3, loss = 1.66350129\n",
            "Iteration 4, loss = 1.66158670\n",
            "Iteration 5, loss = 1.65978347\n",
            "Iteration 6, loss = 1.65798914\n",
            "Iteration 7, loss = 1.65627811\n",
            "Iteration 8, loss = 1.65464320\n",
            "Iteration 9, loss = 1.65303663\n",
            "Iteration 10, loss = 1.65147026\n",
            "Iteration 11, loss = 1.64982568\n",
            "Iteration 12, loss = 1.64832594\n",
            "Iteration 13, loss = 1.64694382\n",
            "Iteration 14, loss = 1.64554114\n",
            "Iteration 15, loss = 1.64403649\n",
            "Iteration 16, loss = 1.64268296\n",
            "Iteration 17, loss = 1.64127198\n",
            "Iteration 18, loss = 1.64000633\n",
            "Iteration 19, loss = 1.63870601\n",
            "Iteration 20, loss = 1.63758026\n",
            "Iteration 21, loss = 1.63654900\n",
            "Iteration 22, loss = 1.63525012\n",
            "Iteration 23, loss = 1.63418453\n",
            "Iteration 24, loss = 1.63312836\n",
            "Iteration 25, loss = 1.63209686\n",
            "Iteration 26, loss = 1.63110691\n",
            "Iteration 27, loss = 1.63008278\n",
            "Iteration 28, loss = 1.62908514\n",
            "Iteration 29, loss = 1.62825128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 30, loss = 1.62734369\n",
            "Iteration 31, loss = 1.62630829\n",
            "Iteration 32, loss = 1.62555061\n",
            "Iteration 33, loss = 1.62475845\n",
            "Iteration 34, loss = 1.62387972\n",
            "Iteration 35, loss = 1.62304818\n",
            "Iteration 36, loss = 1.62246514\n",
            "Iteration 37, loss = 1.62165883\n",
            "Iteration 38, loss = 1.62091784\n",
            "Iteration 39, loss = 1.62023860\n",
            "Iteration 40, loss = 1.61958683\n",
            "Iteration 41, loss = 1.61894845\n",
            "Iteration 42, loss = 1.61824204\n",
            "Iteration 43, loss = 1.61762150\n",
            "Iteration 44, loss = 1.61709691\n",
            "Iteration 45, loss = 1.61655545\n",
            "Iteration 46, loss = 1.61583813\n",
            "Iteration 47, loss = 1.61529989\n",
            "Iteration 48, loss = 1.61479165\n",
            "Iteration 49, loss = 1.61426540\n",
            "Iteration 50, loss = 1.61375607\n",
            "Iteration 51, loss = 1.61322600\n",
            "Iteration 52, loss = 1.61271368\n",
            "Iteration 53, loss = 1.61229876\n",
            "Iteration 54, loss = 1.61188303\n",
            "Iteration 55, loss = 1.61140190\n",
            "Iteration 56, loss = 1.61097818\n",
            "Iteration 57, loss = 1.61055231\n",
            "Iteration 58, loss = 1.61008083\n",
            "Iteration 59, loss = 1.60968805\n",
            "Iteration 60, loss = 1.60934142\n",
            "Iteration 61, loss = 1.60893690\n",
            "Iteration 62, loss = 1.60856777\n",
            "Iteration 63, loss = 1.60822154\n",
            "Iteration 64, loss = 1.60781461\n",
            "Iteration 65, loss = 1.60756258\n",
            "Iteration 66, loss = 1.60713432\n",
            "Iteration 67, loss = 1.60684584\n",
            "Iteration 68, loss = 1.60652806\n",
            "Iteration 69, loss = 1.60618635\n",
            "Iteration 70, loss = 1.60589384\n",
            "Iteration 71, loss = 1.60559718\n",
            "Iteration 72, loss = 1.60532696\n",
            "Iteration 73, loss = 1.60504109\n",
            "Iteration 74, loss = 1.60477509\n",
            "Iteration 75, loss = 1.60447899\n",
            "Iteration 76, loss = 1.60428038\n",
            "Iteration 77, loss = 1.60397474\n",
            "Iteration 78, loss = 1.60376211\n",
            "Iteration 79, loss = 1.60352990\n",
            "Iteration 80, loss = 1.60318910\n",
            "Iteration 81, loss = 1.60305216\n",
            "Iteration 82, loss = 1.60280050\n",
            "Iteration 83, loss = 1.60260252\n",
            "Iteration 84, loss = 1.60239758\n",
            "Iteration 85, loss = 1.60215989\n",
            "Iteration 86, loss = 1.60195040\n",
            "Iteration 87, loss = 1.60175255\n",
            "Iteration 88, loss = 1.60161384\n",
            "Iteration 89, loss = 1.60143047\n",
            "Iteration 90, loss = 1.60119740\n",
            "Iteration 91, loss = 1.60101324\n",
            "Iteration 92, loss = 1.60084392\n",
            "Iteration 93, loss = 1.60067538\n",
            "Iteration 94, loss = 1.60054318\n",
            "Iteration 95, loss = 1.60034528\n",
            "Iteration 96, loss = 1.60017912\n",
            "Iteration 97, loss = 1.60004995\n",
            "Iteration 98, loss = 1.59991816\n",
            "Iteration 99, loss = 1.59977939\n",
            "Iteration 100, loss = 1.59958296\n",
            "Iteration 1, loss = 1.66364983\n",
            "Iteration 2, loss = 1.64647966\n",
            "Iteration 3, loss = 1.63411926\n",
            "Iteration 4, loss = 1.62282819\n",
            "Iteration 5, loss = 1.61526828\n",
            "Iteration 6, loss = 1.60952621\n",
            "Iteration 7, loss = 1.60512163\n",
            "Iteration 8, loss = 1.60267757\n",
            "Iteration 9, loss = 1.60110824\n",
            "Iteration 10, loss = 1.59911810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 1.59784019\n",
            "Iteration 12, loss = 1.59728695\n",
            "Iteration 13, loss = 1.59694770\n",
            "Iteration 14, loss = 1.59662451\n",
            "Iteration 15, loss = 1.59652112\n",
            "Iteration 16, loss = 1.59645127\n",
            "Iteration 17, loss = 1.59628578\n",
            "Iteration 18, loss = 1.59562697\n",
            "Iteration 19, loss = 1.59535688\n",
            "Iteration 20, loss = 1.59449368\n",
            "Iteration 21, loss = 1.59407764\n",
            "Iteration 22, loss = 1.59320017\n",
            "Iteration 23, loss = 1.59252342\n",
            "Iteration 24, loss = 1.59198415\n",
            "Iteration 25, loss = 1.59126137\n",
            "Iteration 26, loss = 1.59097655\n",
            "Iteration 27, loss = 1.59008307\n",
            "Iteration 28, loss = 1.58953649\n",
            "Iteration 29, loss = 1.58886393\n",
            "Iteration 30, loss = 1.58809191\n",
            "Iteration 31, loss = 1.58755214\n",
            "Iteration 32, loss = 1.58672381\n",
            "Iteration 33, loss = 1.58607272\n",
            "Iteration 34, loss = 1.58509201\n",
            "Iteration 35, loss = 1.58431976\n",
            "Iteration 36, loss = 1.58329235\n",
            "Iteration 37, loss = 1.58223382\n",
            "Iteration 38, loss = 1.58117484\n",
            "Iteration 39, loss = 1.58008516\n",
            "Iteration 40, loss = 1.57896754\n",
            "Iteration 41, loss = 1.57781218\n",
            "Iteration 42, loss = 1.57645876\n",
            "Iteration 43, loss = 1.57506990\n",
            "Iteration 44, loss = 1.57385800\n",
            "Iteration 45, loss = 1.57236765\n",
            "Iteration 46, loss = 1.57058911\n",
            "Iteration 47, loss = 1.56912788\n",
            "Iteration 48, loss = 1.56718321\n",
            "Iteration 49, loss = 1.56536836\n",
            "Iteration 50, loss = 1.56370084\n",
            "Iteration 51, loss = 1.56150183\n",
            "Iteration 52, loss = 1.55975728\n",
            "Iteration 53, loss = 1.55731637\n",
            "Iteration 54, loss = 1.55519801\n",
            "Iteration 55, loss = 1.55274307\n",
            "Iteration 56, loss = 1.55057101\n",
            "Iteration 57, loss = 1.54792415\n",
            "Iteration 58, loss = 1.54551998\n",
            "Iteration 59, loss = 1.54271000\n",
            "Iteration 60, loss = 1.53985888\n",
            "Iteration 61, loss = 1.53712784\n",
            "Iteration 62, loss = 1.53401315\n",
            "Iteration 63, loss = 1.53101932\n",
            "Iteration 64, loss = 1.52794732\n",
            "Iteration 65, loss = 1.52491592\n",
            "Iteration 66, loss = 1.52128931\n",
            "Iteration 67, loss = 1.51793047\n",
            "Iteration 68, loss = 1.51469651\n",
            "Iteration 69, loss = 1.51091008\n",
            "Iteration 70, loss = 1.50723445\n",
            "Iteration 71, loss = 1.50360257\n",
            "Iteration 72, loss = 1.49969534\n",
            "Iteration 73, loss = 1.49606761\n",
            "Iteration 74, loss = 1.49214126\n",
            "Iteration 75, loss = 1.48811120\n",
            "Iteration 76, loss = 1.48409789\n",
            "Iteration 77, loss = 1.48021780\n",
            "Iteration 78, loss = 1.47582826\n",
            "Iteration 79, loss = 1.47182319\n",
            "Iteration 80, loss = 1.46786272\n",
            "Iteration 81, loss = 1.46320174\n",
            "Iteration 82, loss = 1.45911494\n",
            "Iteration 83, loss = 1.45458141\n",
            "Iteration 84, loss = 1.45030698\n",
            "Iteration 85, loss = 1.44570271\n",
            "Iteration 86, loss = 1.44144727\n",
            "Iteration 87, loss = 1.43694872\n",
            "Iteration 88, loss = 1.43237961\n",
            "Iteration 89, loss = 1.42808936\n",
            "Iteration 90, loss = 1.42347450\n",
            "Iteration 91, loss = 1.41915936\n",
            "Iteration 92, loss = 1.41443931\n",
            "Iteration 93, loss = 1.40981851\n",
            "Iteration 94, loss = 1.40561998\n",
            "Iteration 95, loss = 1.40100388\n",
            "Iteration 96, loss = 1.39656129\n",
            "Iteration 97, loss = 1.39243756\n",
            "Iteration 98, loss = 1.38785225\n",
            "Iteration 99, loss = 1.38358350\n",
            "Iteration 100, loss = 1.37913859\n",
            "Iteration 1, loss = 1.65065798\n",
            "Iteration 2, loss = 1.60475850\n",
            "Iteration 3, loss = 1.61995760"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4, loss = 1.61151004\n",
            "Iteration 5, loss = 1.59306830\n",
            "Iteration 6, loss = 1.58703510\n",
            "Iteration 7, loss = 1.59085687\n",
            "Iteration 8, loss = 1.58466112\n",
            "Iteration 9, loss = 1.57383101\n",
            "Iteration 10, loss = 1.55817529\n",
            "Iteration 11, loss = 1.54268337\n",
            "Iteration 12, loss = 1.52352368\n",
            "Iteration 13, loss = 1.50135431\n",
            "Iteration 14, loss = 1.47132471\n",
            "Iteration 15, loss = 1.43682949\n",
            "Iteration 16, loss = 1.40188119\n",
            "Iteration 17, loss = 1.36721487\n",
            "Iteration 18, loss = 1.33368541\n",
            "Iteration 19, loss = 1.30366079\n",
            "Iteration 20, loss = 1.27246806\n",
            "Iteration 21, loss = 1.25079660\n",
            "Iteration 22, loss = 1.23087953\n",
            "Iteration 23, loss = 1.20952251\n",
            "Iteration 24, loss = 1.19656292\n",
            "Iteration 25, loss = 1.18432384\n",
            "Iteration 26, loss = 1.17358404\n",
            "Iteration 27, loss = 1.16376615\n",
            "Iteration 28, loss = 1.15756817\n",
            "Iteration 29, loss = 1.15313437\n",
            "Iteration 30, loss = 1.14661261\n",
            "Iteration 31, loss = 1.14441410\n",
            "Iteration 32, loss = 1.14096591\n",
            "Iteration 33, loss = 1.13642182\n",
            "Iteration 34, loss = 1.13274570\n",
            "Iteration 35, loss = 1.13130459\n",
            "Iteration 36, loss = 1.12938870\n",
            "Iteration 37, loss = 1.12508392\n",
            "Iteration 38, loss = 1.12309547\n",
            "Iteration 39, loss = 1.12050736\n",
            "Iteration 40, loss = 1.12052770\n",
            "Iteration 41, loss = 1.11850123\n",
            "Iteration 42, loss = 1.11612230\n",
            "Iteration 43, loss = 1.11633369\n",
            "Iteration 44, loss = 1.12407367\n",
            "Iteration 45, loss = 1.11503551\n",
            "Iteration 46, loss = 1.11931434\n",
            "Iteration 47, loss = 1.11352773\n",
            "Iteration 48, loss = 1.11501968\n",
            "Iteration 49, loss = 1.11512371\n",
            "Iteration 50, loss = 1.12344818\n",
            "Iteration 51, loss = 1.11298800\n",
            "Iteration 52, loss = 1.11271262\n",
            "Iteration 53, loss = 1.11574697\n",
            "Iteration 54, loss = 1.10928663\n",
            "Iteration 55, loss = 1.11046602\n",
            "Iteration 56, loss = 1.10871473\n",
            "Iteration 57, loss = 1.10794257\n",
            "Iteration 58, loss = 1.10773538\n",
            "Iteration 59, loss = 1.10944651\n",
            "Iteration 60, loss = 1.10840143\n",
            "Iteration 61, loss = 1.10665269\n",
            "Iteration 62, loss = 1.10522630\n",
            "Iteration 63, loss = 1.10539261\n",
            "Iteration 64, loss = 1.10509594\n",
            "Iteration 65, loss = 1.11048245\n",
            "Iteration 66, loss = 1.10451531\n",
            "Iteration 67, loss = 1.10576276\n",
            "Iteration 68, loss = 1.10497808\n",
            "Iteration 69, loss = 1.10300206\n",
            "Iteration 70, loss = 1.10511335\n",
            "Iteration 71, loss = 1.10260880\n",
            "Iteration 72, loss = 1.10347320\n",
            "Iteration 73, loss = 1.10290955\n",
            "Iteration 74, loss = 1.10598931\n",
            "Iteration 75, loss = 1.10372233\n",
            "Iteration 76, loss = 1.10510346\n",
            "Iteration 77, loss = 1.10554186\n",
            "Iteration 78, loss = 1.10584005\n",
            "Iteration 79, loss = 1.10661038\n",
            "Iteration 80, loss = 1.10923244\n",
            "Iteration 81, loss = 1.10146843\n",
            "Iteration 82, loss = 1.10457628\n",
            "Iteration 83, loss = 1.10066953\n",
            "Iteration 84, loss = 1.10049343\n",
            "Iteration 85, loss = 1.10025362\n",
            "Iteration 86, loss = 1.10077667\n",
            "Iteration 87, loss = 1.10122722\n",
            "Iteration 88, loss = 1.10138445\n",
            "Iteration 89, loss = 1.10238347\n",
            "Iteration 90, loss = 1.10036963\n",
            "Iteration 91, loss = 1.10112687\n",
            "Iteration 92, loss = 1.10155397\n",
            "Iteration 93, loss = 1.09892486\n",
            "Iteration 94, loss = 1.10225099\n",
            "Iteration 95, loss = 1.10026265\n",
            "Iteration 96, loss = 1.09808713\n",
            "Iteration 97, loss = 1.10139372\n",
            "Iteration 98, loss = 1.09817283\n",
            "Iteration 99, loss = 1.09821294\n",
            "Iteration 100, loss = 1.09852430\n",
            "Iteration 1, loss = 1.66771146\n",
            "Iteration 2, loss = 1.66533955\n",
            "Iteration 3, loss = 1.66315119\n",
            "Iteration 4, loss = 1.66090325\n",
            "Iteration 5, loss = 1.65870658\n",
            "Iteration 6, loss = 1.65656050\n",
            "Iteration 7, loss = 1.65459451\n",
            "Iteration 8, loss = 1.65261953\n",
            "Iteration 9, loss = 1.65069869\n",
            "Iteration 10, loss = 1.64890089\n",
            "Iteration 11, loss = 1.64698694\n",
            "Iteration 12, loss = 1.64524887\n",
            "Iteration 13, loss = 1.64364339\n",
            "Iteration 14, loss = 1.64203679\n",
            "Iteration 15, loss = 1.64030046\n",
            "Iteration 16, loss = 1.63872878\n",
            "Iteration 17, loss = 1.63713121\n",
            "Iteration 18, loss = 1.63569428\n",
            "Iteration 19, loss = 1.63420484\n",
            "Iteration 20, loss = 1.63295989\n",
            "Iteration 21, loss = 1.63178726\n",
            "Iteration 22, loss = 1.63027434\n",
            "Iteration 23, loss = 1.62906963\n",
            "Iteration 24, loss = 1.62789089\n",
            "Iteration 25, loss = 1.62670300\n",
            "Iteration 26, loss = 1.62563610\n",
            "Iteration 27, loss = 1.62441959\n",
            "Iteration 28, loss = 1.62328075\n",
            "Iteration 29, loss = 1.62235909\n",
            "Iteration 30, loss = 1.62131830\n",
            "Iteration 31, loss = 1.62020094\n",
            "Iteration 32, loss = 1.61930692\n",
            "Iteration 33, loss = 1.61837613\n",
            "Iteration 34, loss = 1.61740847\n",
            "Iteration 35, loss = 1.61652471\n",
            "Iteration 36, loss = 1.61580535\n",
            "Iteration 37, loss = 1.61491421\n",
            "Iteration 38, loss = 1.61407353\n",
            "Iteration 39, loss = 1.61332831\n",
            "Iteration 40, loss = 1.61258070\n",
            "Iteration 41, loss = 1.61186792\n",
            "Iteration 42, loss = 1.61105356\n",
            "Iteration 43, loss = 1.61035842\n",
            "Iteration 44, loss = 1.60976397\n",
            "Iteration 45, loss = 1.60916605\n",
            "Iteration 46, loss = 1.60838081\n",
            "Iteration 47, loss = 1.60777525\n",
            "Iteration 48, loss = 1.60718833\n",
            "Iteration 49, loss = 1.60659555\n",
            "Iteration 50, loss = 1.60607134\n",
            "Iteration 51, loss = 1.60544238\n",
            "Iteration 52, loss = 1.60484462\n",
            "Iteration 53, loss = 1.60438752\n",
            "Iteration 54, loss = 1.60393480\n",
            "Iteration 55, loss = 1.60340515\n",
            "Iteration 56, loss = 1.60293329\n",
            "Iteration 57, loss = 1.60245699\n",
            "Iteration 58, loss = 1.60191230\n",
            "Iteration 59, loss = 1.60147636\n",
            "Iteration 60, loss = 1.60110261\n",
            "Iteration 61, loss = 1.60062356"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 62, loss = 1.60022425\n",
            "Iteration 63, loss = 1.59980419\n",
            "Iteration 64, loss = 1.59937639\n",
            "Iteration 65, loss = 1.59903877\n",
            "Iteration 66, loss = 1.59858863\n",
            "Iteration 67, loss = 1.59819850\n",
            "Iteration 68, loss = 1.59783425\n",
            "Iteration 69, loss = 1.59743682\n",
            "Iteration 70, loss = 1.59710848\n",
            "Iteration 71, loss = 1.59671583\n",
            "Iteration 72, loss = 1.59639677\n",
            "Iteration 73, loss = 1.59605046\n",
            "Iteration 74, loss = 1.59570019\n",
            "Iteration 75, loss = 1.59534746\n",
            "Iteration 76, loss = 1.59504804\n",
            "Iteration 77, loss = 1.59467365\n",
            "Iteration 78, loss = 1.59438448\n",
            "Iteration 79, loss = 1.59404611\n",
            "Iteration 80, loss = 1.59370844\n",
            "Iteration 81, loss = 1.59340420\n",
            "Iteration 82, loss = 1.59306356\n",
            "Iteration 83, loss = 1.59277734\n",
            "Iteration 84, loss = 1.59245186\n",
            "Iteration 85, loss = 1.59213052\n",
            "Iteration 86, loss = 1.59181797\n",
            "Iteration 87, loss = 1.59152185\n",
            "Iteration 88, loss = 1.59122274\n",
            "Iteration 89, loss = 1.59093975\n",
            "Iteration 90, loss = 1.59058837\n",
            "Iteration 91, loss = 1.59030262\n",
            "Iteration 92, loss = 1.58997867\n",
            "Iteration 93, loss = 1.58967997\n",
            "Iteration 94, loss = 1.58938854\n",
            "Iteration 95, loss = 1.58907487\n",
            "Iteration 96, loss = 1.58876859\n",
            "Iteration 97, loss = 1.58846769\n",
            "Iteration 98, loss = 1.58817989\n",
            "Iteration 99, loss = 1.58787630\n",
            "Iteration 100, loss = 1.58754739\n",
            "Iteration 1, loss = 1.66388016\n",
            "Iteration 2, loss = 1.64396014\n",
            "Iteration 3, loss = 1.62985646\n",
            "Iteration 4, loss = 1.61800401\n",
            "Iteration 5, loss = 1.60989054\n",
            "Iteration 6, loss = 1.60414619\n",
            "Iteration 7, loss = 1.60048930\n",
            "Iteration 8, loss = 1.59844701\n",
            "Iteration 9, loss = 1.59695811\n",
            "Iteration 10, loss = 1.59519716\n",
            "Iteration 11, loss = 1.59332594\n",
            "Iteration 12, loss = 1.59095645\n",
            "Iteration 13, loss = 1.58849420\n",
            "Iteration 14, loss = 1.58560664\n",
            "Iteration 15, loss = 1.58305276\n",
            "Iteration 16, loss = 1.58017637\n",
            "Iteration 17, loss = 1.57746958\n",
            "Iteration 18, loss = 1.57426739\n",
            "Iteration 19, loss = 1.57139256\n",
            "Iteration 20, loss = 1.56768357\n",
            "Iteration 21, loss = 1.56414581\n",
            "Iteration 22, loss = 1.56038177\n",
            "Iteration 23, loss = 1.55649789\n",
            "Iteration 24, loss = 1.55243200\n",
            "Iteration 25, loss = 1.54792730\n",
            "Iteration 26, loss = 1.54354835\n",
            "Iteration 27, loss = 1.53844910\n",
            "Iteration 28, loss = 1.53354596\n",
            "Iteration 29, loss = 1.52785913\n",
            "Iteration 30, loss = 1.52244935\n",
            "Iteration 31, loss = 1.51655930\n",
            "Iteration 32, loss = 1.51008948\n",
            "Iteration 33, loss = 1.50421471\n",
            "Iteration 34, loss = 1.49717468\n",
            "Iteration 35, loss = 1.49036859\n",
            "Iteration 36, loss = 1.48292122\n",
            "Iteration 37, loss = 1.47556927\n",
            "Iteration 38, loss = 1.46791492\n",
            "Iteration 39, loss = 1.45995049"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 40, loss = 1.45205686\n",
            "Iteration 41, loss = 1.44378206\n",
            "Iteration 42, loss = 1.43565669\n",
            "Iteration 43, loss = 1.42693186\n",
            "Iteration 44, loss = 1.41881771\n",
            "Iteration 45, loss = 1.41000867\n",
            "Iteration 46, loss = 1.40116585\n",
            "Iteration 47, loss = 1.39278816\n",
            "Iteration 48, loss = 1.38385140\n",
            "Iteration 49, loss = 1.37509352\n",
            "Iteration 50, loss = 1.36692681\n",
            "Iteration 51, loss = 1.35810776\n",
            "Iteration 52, loss = 1.35028862\n",
            "Iteration 53, loss = 1.34169539\n",
            "Iteration 54, loss = 1.33386002\n",
            "Iteration 55, loss = 1.32580379\n",
            "Iteration 56, loss = 1.31819312\n",
            "Iteration 57, loss = 1.31071616\n",
            "Iteration 58, loss = 1.30406765\n",
            "Iteration 59, loss = 1.29639122\n",
            "Iteration 60, loss = 1.28949782\n",
            "Iteration 61, loss = 1.28310342\n",
            "Iteration 62, loss = 1.27662992\n",
            "Iteration 63, loss = 1.27059213\n",
            "Iteration 64, loss = 1.26481084\n",
            "Iteration 65, loss = 1.25941995\n",
            "Iteration 66, loss = 1.25361159\n",
            "Iteration 67, loss = 1.24842444\n",
            "Iteration 68, loss = 1.24367245\n",
            "Iteration 69, loss = 1.23874227\n",
            "Iteration 70, loss = 1.23421789\n",
            "Iteration 71, loss = 1.23009655\n",
            "Iteration 72, loss = 1.22570848\n",
            "Iteration 73, loss = 1.22201000\n",
            "Iteration 74, loss = 1.21822444\n",
            "Iteration 75, loss = 1.21422232\n",
            "Iteration 76, loss = 1.21074027\n",
            "Iteration 77, loss = 1.20773836\n",
            "Iteration 78, loss = 1.20430920\n",
            "Iteration 79, loss = 1.20151580\n",
            "Iteration 80, loss = 1.19876788\n",
            "Iteration 81, loss = 1.19539155\n",
            "Iteration 82, loss = 1.19276617\n",
            "Iteration 83, loss = 1.19006936\n",
            "Iteration 84, loss = 1.18800523\n",
            "Iteration 85, loss = 1.18532829\n",
            "Iteration 86, loss = 1.18302408\n",
            "Iteration 87, loss = 1.18071489\n",
            "Iteration 88, loss = 1.17876690\n",
            "Iteration 89, loss = 1.17681639\n",
            "Iteration 90, loss = 1.17465812\n",
            "Iteration 91, loss = 1.17297320\n",
            "Iteration 92, loss = 1.17145159\n",
            "Iteration 93, loss = 1.16922135\n",
            "Iteration 94, loss = 1.16743180\n",
            "Iteration 95, loss = 1.16590476\n",
            "Iteration 96, loss = 1.16418377\n",
            "Iteration 97, loss = 1.16275008\n",
            "Iteration 98, loss = 1.16110483\n",
            "Iteration 99, loss = 1.15967542\n",
            "Iteration 100, loss = 1.15820901\n",
            "Iteration 1, loss = 1.65950975\n",
            "Iteration 2, loss = 1.61265704\n",
            "Iteration 3, loss = 1.60227323\n",
            "Iteration 4, loss = 1.58489940\n",
            "Iteration 5, loss = 1.56760192\n",
            "Iteration 6, loss = 1.54072103\n",
            "Iteration 7, loss = 1.50300361\n",
            "Iteration 8, loss = 1.45423778\n",
            "Iteration 9, loss = 1.40183966"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 10, loss = 1.34003248\n",
            "Iteration 11, loss = 1.28440692\n",
            "Iteration 12, loss = 1.23800148\n",
            "Iteration 13, loss = 1.20440048\n",
            "Iteration 14, loss = 1.17234930\n",
            "Iteration 15, loss = 1.15656170\n",
            "Iteration 16, loss = 1.14214199\n",
            "Iteration 17, loss = 1.13774360\n",
            "Iteration 18, loss = 1.13017833\n",
            "Iteration 19, loss = 1.12266856\n",
            "Iteration 20, loss = 1.11750121\n",
            "Iteration 21, loss = 1.11638054\n",
            "Iteration 22, loss = 1.10811036\n",
            "Iteration 23, loss = 1.11050205\n",
            "Iteration 24, loss = 1.10389731\n",
            "Iteration 25, loss = 1.10270754\n",
            "Iteration 26, loss = 1.10603601\n",
            "Iteration 27, loss = 1.09870614\n",
            "Iteration 28, loss = 1.09909320\n",
            "Iteration 29, loss = 1.10151382\n",
            "Iteration 30, loss = 1.09205801\n",
            "Iteration 31, loss = 1.09995180\n",
            "Iteration 32, loss = 1.08551622\n",
            "Iteration 33, loss = 1.09001699\n",
            "Iteration 34, loss = 1.08295642\n",
            "Iteration 35, loss = 1.08826774\n",
            "Iteration 36, loss = 1.08384832\n",
            "Iteration 37, loss = 1.07685352\n",
            "Iteration 38, loss = 1.07458276\n",
            "Iteration 39, loss = 1.06923116\n",
            "Iteration 40, loss = 1.06598002\n",
            "Iteration 41, loss = 1.06418366\n",
            "Iteration 42, loss = 1.05878306\n",
            "Iteration 43, loss = 1.05473677\n",
            "Iteration 44, loss = 1.06746003\n",
            "Iteration 45, loss = 1.04339763\n",
            "Iteration 46, loss = 1.04933911\n",
            "Iteration 47, loss = 1.03742378\n",
            "Iteration 48, loss = 1.03611644\n",
            "Iteration 49, loss = 1.03197996\n",
            "Iteration 50, loss = 1.03878024\n",
            "Iteration 51, loss = 1.01156240\n",
            "Iteration 52, loss = 1.01960306\n",
            "Iteration 53, loss = 1.01099605\n",
            "Iteration 54, loss = 1.00185617\n",
            "Iteration 55, loss = 0.99890205\n",
            "Iteration 56, loss = 0.99816133\n",
            "Iteration 57, loss = 0.99703758\n",
            "Iteration 58, loss = 0.98378517\n",
            "Iteration 59, loss = 0.98404352\n",
            "Iteration 60, loss = 0.97196980\n",
            "Iteration 61, loss = 0.98656256\n",
            "Iteration 62, loss = 0.97199087\n",
            "Iteration 63, loss = 0.98708652\n",
            "Iteration 64, loss = 0.97263967\n",
            "Iteration 65, loss = 0.99402111\n",
            "Iteration 66, loss = 0.98070014\n",
            "Iteration 67, loss = 0.97510972\n",
            "Iteration 68, loss = 0.94107963\n",
            "Iteration 69, loss = 0.95028935\n",
            "Iteration 70, loss = 0.93299193\n",
            "Iteration 71, loss = 0.91917314\n",
            "Iteration 72, loss = 0.91672959\n",
            "Iteration 73, loss = 0.91265422\n",
            "Iteration 74, loss = 0.91157903\n",
            "Iteration 75, loss = 0.90605962\n",
            "Iteration 76, loss = 0.90424884\n",
            "Iteration 77, loss = 0.89998389\n",
            "Iteration 78, loss = 0.89627114\n",
            "Iteration 79, loss = 0.89949194\n",
            "Iteration 80, loss = 0.93509329\n",
            "Iteration 81, loss = 0.95415485\n",
            "Iteration 82, loss = 0.92922456\n",
            "Iteration 83, loss = 0.89459936\n",
            "Iteration 84, loss = 0.87307421\n",
            "Iteration 85, loss = 0.89866714\n",
            "Iteration 86, loss = 0.90470563\n",
            "Iteration 87, loss = 0.92790123\n",
            "Iteration 88, loss = 0.95297486\n",
            "Iteration 89, loss = 0.92772720\n",
            "Iteration 90, loss = 0.91716331\n",
            "Iteration 91, loss = 0.91318751\n",
            "Iteration 92, loss = 0.89886445\n",
            "Iteration 93, loss = 0.90322678\n",
            "Iteration 94, loss = 0.85883678\n",
            "Iteration 95, loss = 0.85660724\n",
            "Iteration 96, loss = 0.86534480\n",
            "Iteration 97, loss = 0.85288601\n",
            "Iteration 98, loss = 0.84725282\n",
            "Iteration 99, loss = 0.84549240\n",
            "Iteration 100, loss = 0.84127403\n",
            "Iteration 1, loss = 1.66722422\n",
            "Iteration 2, loss = 1.66544510\n",
            "Iteration 3, loss = 1.66378198\n",
            "Iteration 4, loss = 1.66196293\n",
            "Iteration 5, loss = 1.66025664\n",
            "Iteration 6, loss = 1.65847715\n",
            "Iteration 7, loss = 1.65674864\n",
            "Iteration 8, loss = 1.65509969\n",
            "Iteration 9, loss = 1.65344596\n",
            "Iteration 10, loss = 1.65184482\n",
            "Iteration 11, loss = 1.65036186\n",
            "Iteration 12, loss = 1.64916475\n",
            "Iteration 13, loss = 1.64813638\n",
            "Iteration 14, loss = 1.64707237\n",
            "Iteration 15, loss = 1.64593216\n",
            "Iteration 16, loss = 1.64491637\n",
            "Iteration 17, loss = 1.64383717\n",
            "Iteration 18, loss = 1.64286104\n",
            "Iteration 19, loss = 1.64186307\n",
            "Iteration 20, loss = 1.64098256\n",
            "Iteration 21, loss = 1.64016482\n",
            "Iteration 22, loss = 1.63915746\n",
            "Iteration 23, loss = 1.63830095\n",
            "Iteration 24, loss = 1.63744402\n",
            "Iteration 25, loss = 1.63661887\n",
            "Iteration 26, loss = 1.63579476\n",
            "Iteration 27, loss = 1.63496437\n",
            "Iteration 28, loss = 1.63414972\n",
            "Iteration 29, loss = 1.63342384\n",
            "Iteration 30, loss = 1.63265025\n",
            "Iteration 31, loss = 1.63178213\n",
            "Iteration 32, loss = 1.63112330\n",
            "Iteration 33, loss = 1.63041313\n",
            "Iteration 34, loss = 1.62963772\n",
            "Iteration 35, loss = 1.62890419\n",
            "Iteration 36, loss = 1.62832809\n",
            "Iteration 37, loss = 1.62759586\n",
            "Iteration 38, loss = 1.62689574\n",
            "Iteration 39, loss = 1.62623254\n",
            "Iteration 40, loss = 1.62558582\n",
            "Iteration 41, loss = 1.62491952\n",
            "Iteration 42, loss = 1.62421624\n",
            "Iteration 43, loss = 1.62353841\n",
            "Iteration 44, loss = 1.62298902\n",
            "Iteration 45, loss = 1.62236929\n",
            "Iteration 46, loss = 1.62163435\n",
            "Iteration 47, loss = 1.62102718\n",
            "Iteration 48, loss = 1.62046972\n",
            "Iteration 49, loss = 1.61987001\n",
            "Iteration 50, loss = 1.61929279\n",
            "Iteration 51, loss = 1.61868821\n",
            "Iteration 52, loss = 1.61808758\n",
            "Iteration 53, loss = 1.61754701\n",
            "Iteration 54, loss = 1.61701744\n",
            "Iteration 55, loss = 1.61642792\n",
            "Iteration 56, loss = 1.61589014\n",
            "Iteration 57, loss = 1.61533335\n",
            "Iteration 58, loss = 1.61474926\n",
            "Iteration 59, loss = 1.61419719\n",
            "Iteration 60, loss = 1.61371941\n",
            "Iteration 61, loss = 1.61316795\n",
            "Iteration 62, loss = 1.61264709\n",
            "Iteration 63, loss = 1.61212584\n",
            "Iteration 64, loss = 1.61158622\n",
            "Iteration 65, loss = 1.61114851\n",
            "Iteration 66, loss = 1.61056345\n",
            "Iteration 67, loss = 1.61007267\n",
            "Iteration 68, loss = 1.60957841\n",
            "Iteration 69, loss = 1.60904253\n",
            "Iteration 70, loss = 1.60857261\n",
            "Iteration 71, loss = 1.60806715\n",
            "Iteration 72, loss = 1.60759269\n",
            "Iteration 73, loss = 1.60708009\n",
            "Iteration 74, loss = 1.60661550\n",
            "Iteration 75, loss = 1.60610129\n",
            "Iteration 76, loss = 1.60565336\n",
            "Iteration 77, loss = 1.60513991\n",
            "Iteration 78, loss = 1.60468024\n",
            "Iteration 79, loss = 1.60418677\n",
            "Iteration 80, loss = 1.60364312\n",
            "Iteration 81, loss = 1.60322195\n",
            "Iteration 82, loss = 1.60272552\n",
            "Iteration 83, loss = 1.60226422\n",
            "Iteration 84, loss = 1.60177813\n",
            "Iteration 85, loss = 1.60127201\n",
            "Iteration 86, loss = 1.60079604\n",
            "Iteration 87, loss = 1.60031227\n",
            "Iteration 88, loss = 1.59984993\n",
            "Iteration 89, loss = 1.59939077\n",
            "Iteration 90, loss = 1.59885024\n",
            "Iteration 91, loss = 1.59834208\n",
            "Iteration 92, loss = 1.59787399\n",
            "Iteration 93, loss = 1.59738272\n",
            "Iteration 94, loss = 1.59693074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 95, loss = 1.59641058\n",
            "Iteration 96, loss = 1.59592306\n",
            "Iteration 97, loss = 1.59542484\n",
            "Iteration 98, loss = 1.59497482\n",
            "Iteration 99, loss = 1.59448958\n",
            "Iteration 100, loss = 1.59396229\n",
            "Iteration 1, loss = 1.66385840\n",
            "Iteration 2, loss = 1.64843702\n",
            "Iteration 3, loss = 1.63891443\n",
            "Iteration 4, loss = 1.62957142\n",
            "Iteration 5, loss = 1.62282353\n",
            "Iteration 6, loss = 1.61662436\n",
            "Iteration 7, loss = 1.61110830\n",
            "Iteration 8, loss = 1.60712418\n",
            "Iteration 9, loss = 1.60364166\n",
            "Iteration 10, loss = 1.60007743\n",
            "Iteration 11, loss = 1.59694102\n",
            "Iteration 12, loss = 1.59430911\n",
            "Iteration 13, loss = 1.59208234\n",
            "Iteration 14, loss = 1.58985859\n",
            "Iteration 15, loss = 1.58772882\n",
            "Iteration 16, loss = 1.58584912\n",
            "Iteration 17, loss = 1.58371187\n",
            "Iteration 18, loss = 1.58164872\n",
            "Iteration 19, loss = 1.57966326\n",
            "Iteration 20, loss = 1.57735008\n",
            "Iteration 21, loss = 1.57536017\n",
            "Iteration 22, loss = 1.57288917\n",
            "Iteration 23, loss = 1.57046305\n",
            "Iteration 24, loss = 1.56790820\n",
            "Iteration 25, loss = 1.56512418\n",
            "Iteration 26, loss = 1.56245001\n",
            "Iteration 27, loss = 1.55930020\n",
            "Iteration 28, loss = 1.55629061\n",
            "Iteration 29, loss = 1.55258930\n",
            "Iteration 30, loss = 1.54923708\n",
            "Iteration 31, loss = 1.54551883\n",
            "Iteration 32, loss = 1.54148228\n",
            "Iteration 33, loss = 1.53779337\n",
            "Iteration 34, loss = 1.53327747\n",
            "Iteration 35, loss = 1.52876328\n",
            "Iteration 36, loss = 1.52404283\n",
            "Iteration 37, loss = 1.51923667\n",
            "Iteration 38, loss = 1.51412976\n",
            "Iteration 39, loss = 1.50868320\n",
            "Iteration 40, loss = 1.50329217\n",
            "Iteration 41, loss = 1.49766845\n",
            "Iteration 42, loss = 1.49196530\n",
            "Iteration 43, loss = 1.48576016\n",
            "Iteration 44, loss = 1.47977214\n",
            "Iteration 45, loss = 1.47340578\n",
            "Iteration 46, loss = 1.46678155\n",
            "Iteration 47, loss = 1.46043592\n",
            "Iteration 48, loss = 1.45348965\n",
            "Iteration 49, loss = 1.44653139\n",
            "Iteration 50, loss = 1.43990104\n",
            "Iteration 51, loss = 1.43270564\n",
            "Iteration 52, loss = 1.42613173\n",
            "Iteration 53, loss = 1.41868903\n",
            "Iteration 54, loss = 1.41172714\n",
            "Iteration 55, loss = 1.40450028\n",
            "Iteration 56, loss = 1.39749621\n",
            "Iteration 57, loss = 1.39031458\n",
            "Iteration 58, loss = 1.38385191\n",
            "Iteration 59, loss = 1.37633484\n",
            "Iteration 60, loss = 1.36932149\n",
            "Iteration 61, loss = 1.36266315\n",
            "Iteration 62, loss = 1.35566574\n",
            "Iteration 63, loss = 1.34899563\n",
            "Iteration 64, loss = 1.34252883\n",
            "Iteration 65, loss = 1.33624025\n",
            "Iteration 66, loss = 1.32957843\n",
            "Iteration 67, loss = 1.32335005\n",
            "Iteration 68, loss = 1.31747660\n",
            "Iteration 69, loss = 1.31138587\n",
            "Iteration 70, loss = 1.30559483\n",
            "Iteration 71, loss = 1.30025477\n",
            "Iteration 72, loss = 1.29441602\n",
            "Iteration 73, loss = 1.28929090\n",
            "Iteration 74, loss = 1.28422340\n",
            "Iteration 75, loss = 1.27893104\n",
            "Iteration 76, loss = 1.27406681\n",
            "Iteration 77, loss = 1.26971088\n",
            "Iteration 78, loss = 1.26480608\n",
            "Iteration 79, loss = 1.26073114\n",
            "Iteration 80, loss = 1.25639950\n",
            "Iteration 81, loss = 1.25204190\n",
            "Iteration 82, loss = 1.24817285\n",
            "Iteration 83, loss = 1.24420460\n",
            "Iteration 84, loss = 1.24072544\n",
            "Iteration 85, loss = 1.23694060\n",
            "Iteration 86, loss = 1.23352782\n",
            "Iteration 87, loss = 1.23006163\n",
            "Iteration 88, loss = 1.22691315\n",
            "Iteration 89, loss = 1.22387782\n",
            "Iteration 90, loss = 1.22074174\n",
            "Iteration 91, loss = 1.21797877\n",
            "Iteration 92, loss = 1.21536421\n",
            "Iteration 93, loss = 1.21221793\n",
            "Iteration 94, loss = 1.20961356\n",
            "Iteration 95, loss = 1.20717294\n",
            "Iteration 96, loss = 1.20453364\n",
            "Iteration 97, loss = 1.20233500\n",
            "Iteration 98, loss = 1.19988503\n",
            "Iteration 99, loss = 1.19773140\n",
            "Iteration 100, loss = 1.19548500\n",
            "Iteration 1, loss = 1.64891081\n",
            "Iteration 2, loss = 1.59742625\n",
            "Iteration 3, loss = 1.58833093\n",
            "Iteration 4, loss = 1.57693278\n",
            "Iteration 5, loss = 1.56153264\n",
            "Iteration 6, loss = 1.53295197\n",
            "Iteration 7, loss = 1.50085059\n",
            "Iteration 8, loss = 1.45687308\n",
            "Iteration 9, loss = 1.40950297\n",
            "Iteration 10, loss = 1.36063941\n",
            "Iteration 11, loss = 1.31566762\n",
            "Iteration 12, loss = 1.27153645\n",
            "Iteration 13, loss = 1.23911058\n",
            "Iteration 14, loss = 1.20497521\n",
            "Iteration 15, loss = 1.18554136\n",
            "Iteration 16, loss = 1.16562207\n",
            "Iteration 17, loss = 1.15691347\n",
            "Iteration 18, loss = 1.14607343\n",
            "Iteration 19, loss = 1.13481406\n",
            "Iteration 20, loss = 1.12930883\n",
            "Iteration 21, loss = 1.12526841\n",
            "Iteration 22, loss = 1.12077270\n",
            "Iteration 23, loss = 1.11675341\n",
            "Iteration 24, loss = 1.11336363\n",
            "Iteration 25, loss = 1.11423812\n",
            "Iteration 26, loss = 1.11254736\n",
            "Iteration 27, loss = 1.10704876\n",
            "Iteration 28, loss = 1.10708180\n",
            "Iteration 29, loss = 1.10588670\n",
            "Iteration 30, loss = 1.10451026\n",
            "Iteration 31, loss = 1.10141451\n",
            "Iteration 32, loss = 1.09928526\n",
            "Iteration 33, loss = 1.09997980\n",
            "Iteration 34, loss = 1.09870973\n",
            "Iteration 35, loss = 1.09803451\n",
            "Iteration 36, loss = 1.09561680\n",
            "Iteration 37, loss = 1.09400423\n",
            "Iteration 38, loss = 1.09212573\n",
            "Iteration 39, loss = 1.09317059\n",
            "Iteration 40, loss = 1.09182624\n",
            "Iteration 41, loss = 1.09277082\n",
            "Iteration 42, loss = 1.09027198\n",
            "Iteration 43, loss = 1.09029154\n",
            "Iteration 44, loss = 1.10487296\n",
            "Iteration 45, loss = 1.08997010\n",
            "Iteration 46, loss = 1.09754041\n",
            "Iteration 47, loss = 1.08354093\n",
            "Iteration 48, loss = 1.09471122\n",
            "Iteration 49, loss = 1.08738395\n",
            "Iteration 50, loss = 1.09418504\n",
            "Iteration 51, loss = 1.08451720\n",
            "Iteration 52, loss = 1.09520364\n",
            "Iteration 53, loss = 1.08878437\n",
            "Iteration 54, loss = 1.08381535\n",
            "Iteration 55, loss = 1.08693364\n",
            "Iteration 56, loss = 1.07652207\n",
            "Iteration 57, loss = 1.08143529\n",
            "Iteration 58, loss = 1.08238579\n",
            "Iteration 59, loss = 1.07821891\n",
            "Iteration 60, loss = 1.08132340\n",
            "Iteration 61, loss = 1.07465605\n",
            "Iteration 62, loss = 1.07342449\n",
            "Iteration 63, loss = 1.07162869\n",
            "Iteration 64, loss = 1.07190746\n",
            "Iteration 65, loss = 1.07755293\n",
            "Iteration 66, loss = 1.06547175\n",
            "Iteration 67, loss = 1.07024167\n",
            "Iteration 68, loss = 1.06640049\n",
            "Iteration 69, loss = 1.06490728\n",
            "Iteration 70, loss = 1.06215550\n",
            "Iteration 71, loss = 1.06147413\n",
            "Iteration 72, loss = 1.06044685\n",
            "Iteration 73, loss = 1.06254854\n",
            "Iteration 74, loss = 1.06021676\n",
            "Iteration 75, loss = 1.05489693\n",
            "Iteration 76, loss = 1.05314617\n",
            "Iteration 77, loss = 1.05370802\n",
            "Iteration 78, loss = 1.04684842\n",
            "Iteration 79, loss = 1.05634802\n",
            "Iteration 80, loss = 1.05160683\n",
            "Iteration 81, loss = 1.05118486\n",
            "Iteration 82, loss = 1.05282088\n",
            "Iteration 83, loss = 1.04526846\n",
            "Iteration 84, loss = 1.04643911\n",
            "Iteration 85, loss = 1.03500783\n",
            "Iteration 86, loss = 1.03132541\n",
            "Iteration 87, loss = 1.03875099\n",
            "Iteration 88, loss = 1.03124524\n",
            "Iteration 89, loss = 1.03146975\n",
            "Iteration 90, loss = 1.01911060\n",
            "Iteration 91, loss = 1.01589770\n",
            "Iteration 92, loss = 1.01407620\n",
            "Iteration 93, loss = 1.00925106\n",
            "Iteration 94, loss = 1.01043141\n",
            "Iteration 95, loss = 1.00394712\n",
            "Iteration 96, loss = 1.00512265\n",
            "Iteration 97, loss = 1.00347228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 98, loss = 0.99897411\n",
            "Iteration 99, loss = 0.99342494\n",
            "Iteration 100, loss = 0.99234341\n",
            "Iteration 1, loss = 1.97938122\n",
            "Iteration 2, loss = 1.97242542\n",
            "Iteration 3, loss = 1.96539550\n",
            "Iteration 4, loss = 1.95857908\n",
            "Iteration 5, loss = 1.95192714\n",
            "Iteration 6, loss = 1.94524940\n",
            "Iteration 7, loss = 1.93858977\n",
            "Iteration 8, loss = 1.93214208\n",
            "Iteration 9, loss = 1.92565542\n",
            "Iteration 10, loss = 1.91953257\n",
            "Iteration 11, loss = 1.91309505\n",
            "Iteration 12, loss = 1.90712830\n",
            "Iteration 13, loss = 1.90107274\n",
            "Iteration 14, loss = 1.89507598\n",
            "Iteration 15, loss = 1.88916708\n",
            "Iteration 16, loss = 1.88344327\n",
            "Iteration 17, loss = 1.87766411\n",
            "Iteration 18, loss = 1.87213611\n",
            "Iteration 19, loss = 1.86665126\n",
            "Iteration 20, loss = 1.86120671\n",
            "Iteration 21, loss = 1.85570863\n",
            "Iteration 22, loss = 1.85064732\n",
            "Iteration 23, loss = 1.84553969\n",
            "Iteration 24, loss = 1.84040709\n",
            "Iteration 25, loss = 1.83531853\n",
            "Iteration 26, loss = 1.83051105\n",
            "Iteration 27, loss = 1.82556581\n",
            "Iteration 28, loss = 1.82091458\n",
            "Iteration 29, loss = 1.81628351\n",
            "Iteration 30, loss = 1.81149411\n",
            "Iteration 31, loss = 1.80719129\n",
            "Iteration 32, loss = 1.80260050\n",
            "Iteration 33, loss = 1.79827984\n",
            "Iteration 34, loss = 1.79398804\n",
            "Iteration 35, loss = 1.78970022\n",
            "Iteration 36, loss = 1.78558587\n",
            "Iteration 37, loss = 1.78166039\n",
            "Iteration 38, loss = 1.77753493\n",
            "Iteration 39, loss = 1.77353424\n",
            "Iteration 40, loss = 1.76985215\n",
            "Iteration 41, loss = 1.76591563\n",
            "Iteration 42, loss = 1.76229541\n",
            "Iteration 43, loss = 1.75841710\n",
            "Iteration 44, loss = 1.75493550\n",
            "Iteration 45, loss = 1.75156348\n",
            "Iteration 46, loss = 1.74783883\n",
            "Iteration 47, loss = 1.74440289\n",
            "Iteration 48, loss = 1.74113756\n",
            "Iteration 49, loss = 1.73767041\n",
            "Iteration 50, loss = 1.73470707\n",
            "Iteration 51, loss = 1.73139417\n",
            "Iteration 52, loss = 1.72827395\n",
            "Iteration 53, loss = 1.72516580\n",
            "Iteration 54, loss = 1.72206440\n",
            "Iteration 55, loss = 1.71923890\n",
            "Iteration 56, loss = 1.71614549\n",
            "Iteration 57, loss = 1.71340698\n",
            "Iteration 58, loss = 1.71056371\n",
            "Iteration 59, loss = 1.70786567\n",
            "Iteration 60, loss = 1.70521185\n",
            "Iteration 61, loss = 1.70242701\n",
            "Iteration 62, loss = 1.69970757\n",
            "Iteration 63, loss = 1.69735228\n",
            "Iteration 64, loss = 1.69471497\n",
            "Iteration 65, loss = 1.69224757\n",
            "Iteration 66, loss = 1.68975238\n",
            "Iteration 67, loss = 1.68740058\n",
            "Iteration 68, loss = 1.68506668\n",
            "Iteration 69, loss = 1.68274789\n",
            "Iteration 70, loss = 1.68045432\n",
            "Iteration 71, loss = 1.67832338\n",
            "Iteration 72, loss = 1.67605207\n",
            "Iteration 73, loss = 1.67390199\n",
            "Iteration 74, loss = 1.67190218\n",
            "Iteration 75, loss = 1.66981579\n",
            "Iteration 76, loss = 1.66787565\n",
            "Iteration 77, loss = 1.66580977\n",
            "Iteration 78, loss = 1.66383156\n",
            "Iteration 79, loss = 1.66189894\n",
            "Iteration 80, loss = 1.66003855\n",
            "Iteration 81, loss = 1.65831326\n",
            "Iteration 82, loss = 1.65648091\n",
            "Iteration 83, loss = 1.65464032\n",
            "Iteration 84, loss = 1.65295713\n",
            "Iteration 85, loss = 1.65127812\n",
            "Iteration 86, loss = 1.64954102\n",
            "Iteration 87, loss = 1.64784552\n",
            "Iteration 88, loss = 1.64629832\n",
            "Iteration 89, loss = 1.64480059\n",
            "Iteration 90, loss = 1.64329725\n",
            "Iteration 91, loss = 1.64174715\n",
            "Iteration 92, loss = 1.64032370\n",
            "Iteration 93, loss = 1.63876115\n",
            "Iteration 94, loss = 1.63734928\n",
            "Iteration 95, loss = 1.63595364\n",
            "Iteration 96, loss = 1.63467913\n",
            "Iteration 97, loss = 1.63323870\n",
            "Iteration 98, loss = 1.63195496\n",
            "Iteration 99, loss = 1.63070089\n",
            "Iteration 100, loss = 1.62941905\n",
            "Iteration 1, loss = 1.96515193\n",
            "Iteration 2, loss = 1.90065605\n",
            "Iteration 3, loss = 1.84208630\n",
            "Iteration 4, loss = 1.79243568\n",
            "Iteration 5, loss = 1.75052853\n",
            "Iteration 6, loss = 1.71414665\n",
            "Iteration 7, loss = 1.68349979\n",
            "Iteration 8, loss = 1.65958308\n",
            "Iteration 9, loss = 1.63894297\n",
            "Iteration 10, loss = 1.62497720\n",
            "Iteration 11, loss = 1.61121845\n",
            "Iteration 12, loss = 1.60377590\n",
            "Iteration 13, loss = 1.59772996\n",
            "Iteration 14, loss = 1.59272469\n",
            "Iteration 15, loss = 1.58960296\n",
            "Iteration 16, loss = 1.58745261\n",
            "Iteration 17, loss = 1.58532746\n",
            "Iteration 18, loss = 1.58378225\n",
            "Iteration 19, loss = 1.58203163\n",
            "Iteration 20, loss = 1.58067366\n",
            "Iteration 21, loss = 1.57925561\n",
            "Iteration 22, loss = 1.57752919\n",
            "Iteration 23, loss = 1.57590129\n",
            "Iteration 24, loss = 1.57440279\n",
            "Iteration 25, loss = 1.57284580\n",
            "Iteration 26, loss = 1.57149992\n",
            "Iteration 27, loss = 1.57014474\n",
            "Iteration 28, loss = 1.56856627\n",
            "Iteration 29, loss = 1.56694699\n",
            "Iteration 30, loss = 1.56558985\n",
            "Iteration 31, loss = 1.56373891\n",
            "Iteration 32, loss = 1.56219505\n",
            "Iteration 33, loss = 1.56034593\n",
            "Iteration 34, loss = 1.55829570\n",
            "Iteration 35, loss = 1.55639548\n",
            "Iteration 36, loss = 1.55424551\n",
            "Iteration 37, loss = 1.55208355\n",
            "Iteration 38, loss = 1.54983281\n",
            "Iteration 39, loss = 1.54754155\n",
            "Iteration 40, loss = 1.54534963\n",
            "Iteration 41, loss = 1.54272592\n",
            "Iteration 42, loss = 1.54026877\n",
            "Iteration 43, loss = 1.53761014\n",
            "Iteration 44, loss = 1.53479366\n",
            "Iteration 45, loss = 1.53202821\n",
            "Iteration 46, loss = 1.52890218\n",
            "Iteration 47, loss = 1.52581628\n",
            "Iteration 48, loss = 1.52262495\n",
            "Iteration 49, loss = 1.51935172\n",
            "Iteration 50, loss = 1.51574247\n",
            "Iteration 51, loss = 1.51212325\n",
            "Iteration 52, loss = 1.50854128\n",
            "Iteration 53, loss = 1.50450492\n",
            "Iteration 54, loss = 1.50058730\n",
            "Iteration 55, loss = 1.49651385\n",
            "Iteration 56, loss = 1.49228251\n",
            "Iteration 57, loss = 1.48772403\n",
            "Iteration 58, loss = 1.48337842\n",
            "Iteration 59, loss = 1.47868184\n",
            "Iteration 60, loss = 1.47397499\n",
            "Iteration 61, loss = 1.46902231\n",
            "Iteration 62, loss = 1.46398463\n",
            "Iteration 63, loss = 1.45905986\n",
            "Iteration 64, loss = 1.45365242\n",
            "Iteration 65, loss = 1.44840984\n",
            "Iteration 66, loss = 1.44293166\n",
            "Iteration 67, loss = 1.43752969\n",
            "Iteration 68, loss = 1.43187836\n",
            "Iteration 69, loss = 1.42615876\n",
            "Iteration 70, loss = 1.42064314\n",
            "Iteration 71, loss = 1.41481481\n",
            "Iteration 72, loss = 1.40891800\n",
            "Iteration 73, loss = 1.40295632\n",
            "Iteration 74, loss = 1.39713255\n",
            "Iteration 75, loss = 1.39131736\n",
            "Iteration 76, loss = 1.38568797\n",
            "Iteration 77, loss = 1.37939201\n",
            "Iteration 78, loss = 1.37358993\n",
            "Iteration 79, loss = 1.36776496\n",
            "Iteration 80, loss = 1.36193832\n",
            "Iteration 81, loss = 1.35631757\n",
            "Iteration 82, loss = 1.35053920\n",
            "Iteration 83, loss = 1.34468313\n",
            "Iteration 84, loss = 1.33931069\n",
            "Iteration 85, loss = 1.33374429\n",
            "Iteration 86, loss = 1.32820972\n",
            "Iteration 87, loss = 1.32292604\n",
            "Iteration 88, loss = 1.31762600\n",
            "Iteration 89, loss = 1.31257611\n",
            "Iteration 90, loss = 1.30755367\n",
            "Iteration 91, loss = 1.30252454\n",
            "Iteration 92, loss = 1.29778559\n",
            "Iteration 93, loss = 1.29289970\n",
            "Iteration 94, loss = 1.28827417\n",
            "Iteration 95, loss = 1.28364585\n",
            "Iteration 96, loss = 1.27945026\n",
            "Iteration 97, loss = 1.27497628\n",
            "Iteration 98, loss = 1.27080982\n",
            "Iteration 99, loss = 1.26684639\n",
            "Iteration 100, loss = 1.26270868\n",
            "Iteration 1, loss = 1.86165691\n",
            "Iteration 2, loss = 1.62189481\n",
            "Iteration 3, loss = 1.63026762\n",
            "Iteration 4, loss = 1.60284234\n",
            "Iteration 5, loss = 1.58237780\n",
            "Iteration 6, loss = 1.59057095\n",
            "Iteration 7, loss = 1.57894532\n",
            "Iteration 8, loss = 1.56849433\n",
            "Iteration 9, loss = 1.55166393\n",
            "Iteration 10, loss = 1.53939164\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 1.51549067\n",
            "Iteration 12, loss = 1.48938870\n",
            "Iteration 13, loss = 1.46400826\n",
            "Iteration 14, loss = 1.43257531\n",
            "Iteration 15, loss = 1.39310131\n",
            "Iteration 16, loss = 1.35297267\n",
            "Iteration 17, loss = 1.31366285\n",
            "Iteration 18, loss = 1.28086797\n",
            "Iteration 19, loss = 1.24957043\n",
            "Iteration 20, loss = 1.22281734\n",
            "Iteration 21, loss = 1.20066015\n",
            "Iteration 22, loss = 1.18317510\n",
            "Iteration 23, loss = 1.16875689\n",
            "Iteration 24, loss = 1.15643282\n",
            "Iteration 25, loss = 1.14756755\n",
            "Iteration 26, loss = 1.13888526\n",
            "Iteration 27, loss = 1.13503976\n",
            "Iteration 28, loss = 1.12874006\n",
            "Iteration 29, loss = 1.12514746\n",
            "Iteration 30, loss = 1.12172267\n",
            "Iteration 31, loss = 1.11867070\n",
            "Iteration 32, loss = 1.12046239\n",
            "Iteration 33, loss = 1.11269293\n",
            "Iteration 34, loss = 1.11381285\n",
            "Iteration 35, loss = 1.11467200\n",
            "Iteration 36, loss = 1.10992282\n",
            "Iteration 37, loss = 1.10964267\n",
            "Iteration 38, loss = 1.10771037\n",
            "Iteration 39, loss = 1.10489247\n",
            "Iteration 40, loss = 1.10513466\n",
            "Iteration 41, loss = 1.10375889\n",
            "Iteration 42, loss = 1.10383011\n",
            "Iteration 43, loss = 1.10616402\n",
            "Iteration 44, loss = 1.09776700\n",
            "Iteration 45, loss = 1.10456609\n",
            "Iteration 46, loss = 1.09907626\n",
            "Iteration 47, loss = 1.10117558\n",
            "Iteration 48, loss = 1.09537968\n",
            "Iteration 49, loss = 1.09740039\n",
            "Iteration 50, loss = 1.09723156\n",
            "Iteration 51, loss = 1.09174745\n",
            "Iteration 52, loss = 1.09199753\n",
            "Iteration 53, loss = 1.08958081\n",
            "Iteration 54, loss = 1.08709966\n",
            "Iteration 55, loss = 1.08872995\n",
            "Iteration 56, loss = 1.08880949\n",
            "Iteration 57, loss = 1.08156842\n",
            "Iteration 58, loss = 1.08061158\n",
            "Iteration 59, loss = 1.07734937\n",
            "Iteration 60, loss = 1.07692842\n",
            "Iteration 61, loss = 1.07065274\n",
            "Iteration 62, loss = 1.07136225\n",
            "Iteration 63, loss = 1.07497561\n",
            "Iteration 64, loss = 1.06455485\n",
            "Iteration 65, loss = 1.05987842\n",
            "Iteration 66, loss = 1.05835235\n",
            "Iteration 67, loss = 1.05287559\n",
            "Iteration 68, loss = 1.04876172\n",
            "Iteration 69, loss = 1.04535200\n",
            "Iteration 70, loss = 1.04703024\n",
            "Iteration 71, loss = 1.03624090\n",
            "Iteration 72, loss = 1.03747670\n",
            "Iteration 73, loss = 1.02890857\n",
            "Iteration 74, loss = 1.02331384\n",
            "Iteration 75, loss = 1.01995078\n",
            "Iteration 76, loss = 1.01550735\n",
            "Iteration 77, loss = 1.01036106\n",
            "Iteration 78, loss = 1.00591937\n",
            "Iteration 79, loss = 1.01993462\n",
            "Iteration 80, loss = 1.00577824\n",
            "Iteration 81, loss = 0.98595602\n",
            "Iteration 82, loss = 0.98967684\n",
            "Iteration 83, loss = 0.99579372\n",
            "Iteration 84, loss = 0.98987699\n",
            "Iteration 85, loss = 0.99004214\n",
            "Iteration 86, loss = 0.97696981\n",
            "Iteration 87, loss = 0.96819849\n",
            "Iteration 88, loss = 0.96698514\n",
            "Iteration 89, loss = 0.96551296\n",
            "Iteration 90, loss = 0.95075805\n",
            "Iteration 91, loss = 0.94654942\n",
            "Iteration 92, loss = 0.95036253\n",
            "Iteration 93, loss = 0.94186960\n",
            "Iteration 94, loss = 0.92905050\n",
            "Iteration 95, loss = 0.93929701\n",
            "Iteration 96, loss = 0.93297572\n",
            "Iteration 97, loss = 0.91793220\n",
            "Iteration 98, loss = 0.92194406\n",
            "Iteration 99, loss = 0.91466722\n",
            "Iteration 100, loss = 0.92398212\n",
            "Iteration 1, loss = 1.71944701\n",
            "Iteration 2, loss = 1.71702311\n",
            "Iteration 3, loss = 1.71458527\n",
            "Iteration 4, loss = 1.71219784\n",
            "Iteration 5, loss = 1.70993283\n",
            "Iteration 6, loss = 1.70768703\n",
            "Iteration 7, loss = 1.70540472\n",
            "Iteration 8, loss = 1.70317343\n",
            "Iteration 9, loss = 1.70100361\n",
            "Iteration 10, loss = 1.69899203\n",
            "Iteration 11, loss = 1.69678264\n",
            "Iteration 12, loss = 1.69483094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 13, loss = 1.69279777\n",
            "Iteration 14, loss = 1.69075543\n",
            "Iteration 15, loss = 1.68883049\n",
            "Iteration 16, loss = 1.68696406\n",
            "Iteration 17, loss = 1.68503262\n",
            "Iteration 18, loss = 1.68321411\n",
            "Iteration 19, loss = 1.68140661\n",
            "Iteration 20, loss = 1.67963631\n",
            "Iteration 21, loss = 1.67780897\n",
            "Iteration 22, loss = 1.67621044\n",
            "Iteration 23, loss = 1.67457432\n",
            "Iteration 24, loss = 1.67290731\n",
            "Iteration 25, loss = 1.67123238\n",
            "Iteration 26, loss = 1.66974500\n",
            "Iteration 27, loss = 1.66814633\n",
            "Iteration 28, loss = 1.66665062\n",
            "Iteration 29, loss = 1.66518232\n",
            "Iteration 30, loss = 1.66364756\n",
            "Iteration 31, loss = 1.66231294\n",
            "Iteration 32, loss = 1.66081916\n",
            "Iteration 33, loss = 1.65950412\n",
            "Iteration 34, loss = 1.65814659\n",
            "Iteration 35, loss = 1.65678377\n",
            "Iteration 36, loss = 1.65552517\n",
            "Iteration 37, loss = 1.65434490\n",
            "Iteration 38, loss = 1.65304619\n",
            "Iteration 39, loss = 1.65179528\n",
            "Iteration 40, loss = 1.65069635\n",
            "Iteration 41, loss = 1.64947182\n",
            "Iteration 42, loss = 1.64836615\n",
            "Iteration 43, loss = 1.64713299\n",
            "Iteration 44, loss = 1.64611162\n",
            "Iteration 45, loss = 1.64514964\n",
            "Iteration 46, loss = 1.64394197\n",
            "Iteration 47, loss = 1.64291364\n",
            "Iteration 48, loss = 1.64196033\n",
            "Iteration 49, loss = 1.64086378\n",
            "Iteration 50, loss = 1.64007031\n",
            "Iteration 51, loss = 1.63905376\n",
            "Iteration 52, loss = 1.63811965\n",
            "Iteration 53, loss = 1.63718501\n",
            "Iteration 54, loss = 1.63626520\n",
            "Iteration 55, loss = 1.63546223\n",
            "Iteration 56, loss = 1.63450417\n",
            "Iteration 57, loss = 1.63373787\n",
            "Iteration 58, loss = 1.63291329\n",
            "Iteration 59, loss = 1.63212743\n",
            "Iteration 60, loss = 1.63136722\n",
            "Iteration 61, loss = 1.63052955\n",
            "Iteration 62, loss = 1.62971718\n",
            "Iteration 63, loss = 1.62908293\n",
            "Iteration 64, loss = 1.62830481\n",
            "Iteration 65, loss = 1.62759081\n",
            "Iteration 66, loss = 1.62687086\n",
            "Iteration 67, loss = 1.62619996\n",
            "Iteration 68, loss = 1.62553856\n",
            "Iteration 69, loss = 1.62489556\n",
            "Iteration 70, loss = 1.62425817\n",
            "Iteration 71, loss = 1.62363756\n",
            "Iteration 72, loss = 1.62296857\n",
            "Iteration 73, loss = 1.62236523\n",
            "Iteration 74, loss = 1.62183816\n",
            "Iteration 75, loss = 1.62123486\n",
            "Iteration 76, loss = 1.62072070\n",
            "Iteration 77, loss = 1.62012700\n",
            "Iteration 78, loss = 1.61955670\n",
            "Iteration 79, loss = 1.61903714\n",
            "Iteration 80, loss = 1.61850165\n",
            "Iteration 81, loss = 1.61806005\n",
            "Iteration 82, loss = 1.61755710\n",
            "Iteration 83, loss = 1.61703360\n",
            "Iteration 84, loss = 1.61656194\n",
            "Iteration 85, loss = 1.61612427\n",
            "Iteration 86, loss = 1.61562047\n",
            "Iteration 87, loss = 1.61514644\n",
            "Iteration 88, loss = 1.61472627\n",
            "Iteration 89, loss = 1.61435205\n",
            "Iteration 90, loss = 1.61395883\n",
            "Iteration 91, loss = 1.61352301\n",
            "Iteration 92, loss = 1.61314444\n",
            "Iteration 93, loss = 1.61271701\n",
            "Iteration 94, loss = 1.61232379\n",
            "Iteration 95, loss = 1.61194861\n",
            "Iteration 96, loss = 1.61161867\n",
            "Iteration 97, loss = 1.61121019\n",
            "Iteration 98, loss = 1.61087676\n",
            "Iteration 99, loss = 1.61055398\n",
            "Iteration 100, loss = 1.61020175\n",
            "Iteration 1, loss = 1.71427256\n",
            "Iteration 2, loss = 1.69227035\n",
            "Iteration 3, loss = 1.67280931\n",
            "Iteration 4, loss = 1.65663639\n",
            "Iteration 5, loss = 1.64383259\n",
            "Iteration 6, loss = 1.63298339\n",
            "Iteration 7, loss = 1.62375571\n",
            "Iteration 8, loss = 1.61660344\n",
            "Iteration 9, loss = 1.61063266\n",
            "Iteration 10, loss = 1.60729229\n",
            "Iteration 11, loss = 1.60298985\n",
            "Iteration 12, loss = 1.60150848\n",
            "Iteration 13, loss = 1.60020513\n",
            "Iteration 14, loss = 1.59875618\n",
            "Iteration 15, loss = 1.59821278"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 16, loss = 1.59803212\n",
            "Iteration 17, loss = 1.59748044\n",
            "Iteration 18, loss = 1.59768108\n",
            "Iteration 19, loss = 1.59731617\n",
            "Iteration 20, loss = 1.59727996\n",
            "Iteration 21, loss = 1.59732697\n",
            "Iteration 22, loss = 1.59713753\n",
            "Iteration 23, loss = 1.59701905\n",
            "Iteration 24, loss = 1.59682519\n",
            "Iteration 25, loss = 1.59666983\n",
            "Iteration 26, loss = 1.59631406\n",
            "Iteration 27, loss = 1.59624972\n",
            "Iteration 28, loss = 1.59578126\n",
            "Iteration 29, loss = 1.59549833\n",
            "Iteration 30, loss = 1.59531962\n",
            "Iteration 31, loss = 1.59509822\n",
            "Iteration 32, loss = 1.59483799\n",
            "Iteration 33, loss = 1.59457766\n",
            "Iteration 34, loss = 1.59422224\n",
            "Iteration 35, loss = 1.59406652\n",
            "Iteration 36, loss = 1.59387035\n",
            "Iteration 37, loss = 1.59359063\n",
            "Iteration 38, loss = 1.59334029\n",
            "Iteration 39, loss = 1.59312504\n",
            "Iteration 40, loss = 1.59292144\n",
            "Iteration 41, loss = 1.59279640\n",
            "Iteration 42, loss = 1.59234653\n",
            "Iteration 43, loss = 1.59207402\n",
            "Iteration 44, loss = 1.59168402\n",
            "Iteration 45, loss = 1.59146481\n",
            "Iteration 46, loss = 1.59105307\n",
            "Iteration 47, loss = 1.59071919\n",
            "Iteration 48, loss = 1.59041254\n",
            "Iteration 49, loss = 1.59015913\n",
            "Iteration 50, loss = 1.58976871\n",
            "Iteration 51, loss = 1.58932010\n",
            "Iteration 52, loss = 1.58900486\n",
            "Iteration 53, loss = 1.58850558\n",
            "Iteration 54, loss = 1.58816704\n",
            "Iteration 55, loss = 1.58774604\n",
            "Iteration 56, loss = 1.58729182\n",
            "Iteration 57, loss = 1.58673687\n",
            "Iteration 58, loss = 1.58630992\n",
            "Iteration 59, loss = 1.58575658\n",
            "Iteration 60, loss = 1.58534497\n",
            "Iteration 61, loss = 1.58466381\n",
            "Iteration 62, loss = 1.58417744\n",
            "Iteration 63, loss = 1.58364873\n",
            "Iteration 64, loss = 1.58290842\n",
            "Iteration 65, loss = 1.58229040\n",
            "Iteration 66, loss = 1.58162409\n",
            "Iteration 67, loss = 1.58099732\n",
            "Iteration 68, loss = 1.58022460\n",
            "Iteration 69, loss = 1.57955296\n",
            "Iteration 70, loss = 1.57901577\n",
            "Iteration 71, loss = 1.57803903\n",
            "Iteration 72, loss = 1.57722767\n",
            "Iteration 73, loss = 1.57626214\n",
            "Iteration 74, loss = 1.57536848\n",
            "Iteration 75, loss = 1.57447670\n",
            "Iteration 76, loss = 1.57367723\n",
            "Iteration 77, loss = 1.57249503\n",
            "Iteration 78, loss = 1.57145328\n",
            "Iteration 79, loss = 1.57050794\n",
            "Iteration 80, loss = 1.56933608\n",
            "Iteration 81, loss = 1.56825989\n",
            "Iteration 82, loss = 1.56723448\n",
            "Iteration 83, loss = 1.56583053\n",
            "Iteration 84, loss = 1.56458231\n",
            "Iteration 85, loss = 1.56326970\n",
            "Iteration 86, loss = 1.56187889\n",
            "Iteration 87, loss = 1.56057376\n",
            "Iteration 88, loss = 1.55906868\n",
            "Iteration 89, loss = 1.55768117\n",
            "Iteration 90, loss = 1.55618524\n",
            "Iteration 91, loss = 1.55454834\n",
            "Iteration 92, loss = 1.55294871\n",
            "Iteration 93, loss = 1.55132968\n",
            "Iteration 94, loss = 1.54951703\n",
            "Iteration 95, loss = 1.54769835\n",
            "Iteration 96, loss = 1.54606426\n",
            "Iteration 97, loss = 1.54406717\n",
            "Iteration 98, loss = 1.54208523\n",
            "Iteration 99, loss = 1.54038499\n",
            "Iteration 100, loss = 1.53811740\n",
            "Iteration 1, loss = 1.67938262\n",
            "Iteration 2, loss = 1.61380674\n",
            "Iteration 3, loss = 1.61779733\n",
            "Iteration 4, loss = 1.62191927\n",
            "Iteration 5, loss = 1.60881297\n",
            "Iteration 6, loss = 1.60206438\n",
            "Iteration 7, loss = 1.59679910\n",
            "Iteration 8, loss = 1.59861618\n",
            "Iteration 9, loss = 1.59683266\n",
            "Iteration 10, loss = 1.59424196\n",
            "Iteration 11, loss = 1.58885622\n",
            "Iteration 12, loss = 1.58401786\n",
            "Iteration 13, loss = 1.58065240\n",
            "Iteration 14, loss = 1.57554907\n",
            "Iteration 15, loss = 1.57017662\n",
            "Iteration 16, loss = 1.56366959\n",
            "Iteration 17, loss = 1.55668846\n",
            "Iteration 18, loss = 1.54657336\n",
            "Iteration 19, loss = 1.53515118\n",
            "Iteration 20, loss = 1.52129423\n",
            "Iteration 21, loss = 1.50703997\n",
            "Iteration 22, loss = 1.48958518\n",
            "Iteration 23, loss = 1.46902270\n",
            "Iteration 24, loss = 1.44611203\n",
            "Iteration 25, loss = 1.42222710\n",
            "Iteration 26, loss = 1.39588334\n",
            "Iteration 27, loss = 1.37160596\n",
            "Iteration 28, loss = 1.34427656\n",
            "Iteration 29, loss = 1.31846499\n",
            "Iteration 30, loss = 1.29719632\n",
            "Iteration 31, loss = 1.27463332\n",
            "Iteration 32, loss = 1.25357554\n",
            "Iteration 33, loss = 1.23714440\n",
            "Iteration 34, loss = 1.22064419\n",
            "Iteration 35, loss = 1.20813995\n",
            "Iteration 36, loss = 1.19653631\n",
            "Iteration 37, loss = 1.18682184\n",
            "Iteration 38, loss = 1.17911222\n",
            "Iteration 39, loss = 1.17168751\n",
            "Iteration 40, loss = 1.16522441\n",
            "Iteration 41, loss = 1.16017613\n",
            "Iteration 42, loss = 1.15547906\n",
            "Iteration 43, loss = 1.15207354\n",
            "Iteration 44, loss = 1.14581149\n",
            "Iteration 45, loss = 1.14422980\n",
            "Iteration 46, loss = 1.14067446\n",
            "Iteration 47, loss = 1.13738025\n",
            "Iteration 48, loss = 1.13688571\n",
            "Iteration 49, loss = 1.13375591\n",
            "Iteration 50, loss = 1.13134160\n",
            "Iteration 51, loss = 1.13059846\n",
            "Iteration 52, loss = 1.12444901\n",
            "Iteration 53, loss = 1.12496570\n",
            "Iteration 54, loss = 1.12479312\n",
            "Iteration 55, loss = 1.12163714\n",
            "Iteration 56, loss = 1.12220679\n",
            "Iteration 57, loss = 1.11745213\n",
            "Iteration 58, loss = 1.11791149\n",
            "Iteration 59, loss = 1.11594368\n",
            "Iteration 60, loss = 1.11527942\n",
            "Iteration 61, loss = 1.11410578\n",
            "Iteration 62, loss = 1.11240294\n",
            "Iteration 63, loss = 1.11431709\n",
            "Iteration 64, loss = 1.11036723\n",
            "Iteration 65, loss = 1.10938387\n",
            "Iteration 66, loss = 1.10843836\n",
            "Iteration 67, loss = 1.10851766\n",
            "Iteration 68, loss = 1.10740496\n",
            "Iteration 69, loss = 1.10793363\n",
            "Iteration 70, loss = 1.10890745\n",
            "Iteration 71, loss = 1.10611202\n",
            "Iteration 72, loss = 1.10739455\n",
            "Iteration 73, loss = 1.10469290\n",
            "Iteration 74, loss = 1.10627250\n",
            "Iteration 75, loss = 1.10449796\n",
            "Iteration 76, loss = 1.10449665\n",
            "Iteration 77, loss = 1.10415928\n",
            "Iteration 78, loss = 1.10212055\n",
            "Iteration 79, loss = 1.10502601\n",
            "Iteration 80, loss = 1.10174875\n",
            "Iteration 81, loss = 1.10208042\n",
            "Iteration 82, loss = 1.10571820\n",
            "Iteration 83, loss = 1.10197058\n",
            "Iteration 84, loss = 1.09994478\n",
            "Iteration 85, loss = 1.10093875\n",
            "Iteration 86, loss = 1.09980750\n",
            "Iteration 87, loss = 1.09987968\n",
            "Iteration 88, loss = 1.09952711\n",
            "Iteration 89, loss = 1.09979938\n",
            "Iteration 90, loss = 1.09978018\n",
            "Iteration 91, loss = 1.09801407\n",
            "Iteration 92, loss = 1.09831323\n",
            "Iteration 93, loss = 1.09861069\n",
            "Iteration 94, loss = 1.09720311\n",
            "Iteration 95, loss = 1.09907063\n",
            "Iteration 96, loss = 1.09774491\n",
            "Iteration 97, loss = 1.09948091\n",
            "Iteration 98, loss = 1.09600678\n",
            "Iteration 99, loss = 1.09785918\n",
            "Iteration 100, loss = 1.09818790\n",
            "Iteration 1, loss = 1.95185448\n",
            "Iteration 2, loss = 1.94576507\n",
            "Iteration 3, loss = 1.93959743\n",
            "Iteration 4, loss = 1.93360422\n",
            "Iteration 5, loss = 1.92774847\n",
            "Iteration 6, loss = 1.92186019\n",
            "Iteration 7, loss = 1.91596498\n",
            "Iteration 8, loss = 1.91024508\n",
            "Iteration 9, loss = 1.90448830\n",
            "Iteration 10, loss = 1.89904127\n",
            "Iteration 11, loss = 1.89329946\n",
            "Iteration 12, loss = 1.88796729\n",
            "Iteration 13, loss = 1.88253909\n",
            "Iteration 14, loss = 1.87715186\n",
            "Iteration 15, loss = 1.87184104\n",
            "Iteration 16, loss = 1.86668571\n",
            "Iteration 17, loss = 1.86146015\n",
            "Iteration 18, loss = 1.85646278\n",
            "Iteration 19, loss = 1.85148303\n",
            "Iteration 20, loss = 1.84653882\n",
            "Iteration 21, loss = 1.84153501\n",
            "Iteration 22, loss = 1.83692076\n",
            "Iteration 23, loss = 1.83225830\n",
            "Iteration 24, loss = 1.82756501\n",
            "Iteration 25, loss = 1.82289934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 1.81849205\n",
            "Iteration 27, loss = 1.81394222\n",
            "Iteration 28, loss = 1.80966209\n",
            "Iteration 29, loss = 1.80539434\n",
            "Iteration 30, loss = 1.80097598\n",
            "Iteration 31, loss = 1.79700072\n",
            "Iteration 32, loss = 1.79275122\n",
            "Iteration 33, loss = 1.78875427\n",
            "Iteration 34, loss = 1.78477491\n",
            "Iteration 35, loss = 1.78079461\n",
            "Iteration 36, loss = 1.77697697\n",
            "Iteration 37, loss = 1.77333004\n",
            "Iteration 38, loss = 1.76949030\n",
            "Iteration 39, loss = 1.76576444\n",
            "Iteration 40, loss = 1.76233298\n",
            "Iteration 41, loss = 1.75866874\n",
            "Iteration 42, loss = 1.75528623\n",
            "Iteration 43, loss = 1.75166294\n",
            "Iteration 44, loss = 1.74841566\n",
            "Iteration 45, loss = 1.74527065\n",
            "Iteration 46, loss = 1.74178492\n",
            "Iteration 47, loss = 1.73857272\n",
            "Iteration 48, loss = 1.73552154\n",
            "Iteration 49, loss = 1.73227470\n",
            "Iteration 50, loss = 1.72951054\n",
            "Iteration 51, loss = 1.72640617\n",
            "Iteration 52, loss = 1.72348335\n",
            "Iteration 53, loss = 1.72057638\n",
            "Iteration 54, loss = 1.71767436\n",
            "Iteration 55, loss = 1.71503078\n",
            "Iteration 56, loss = 1.71212992\n",
            "Iteration 57, loss = 1.70957313\n",
            "Iteration 58, loss = 1.70691089\n",
            "Iteration 59, loss = 1.70438698\n",
            "Iteration 60, loss = 1.70190534\n",
            "Iteration 61, loss = 1.69929625\n",
            "Iteration 62, loss = 1.69675247\n",
            "Iteration 63, loss = 1.69455152\n",
            "Iteration 64, loss = 1.69208548\n",
            "Iteration 65, loss = 1.68977768\n",
            "Iteration 66, loss = 1.68744611\n",
            "Iteration 67, loss = 1.68524709\n",
            "Iteration 68, loss = 1.68306981\n",
            "Iteration 69, loss = 1.68090608\n",
            "Iteration 70, loss = 1.67876421\n",
            "Iteration 71, loss = 1.67677389\n",
            "Iteration 72, loss = 1.67465409\n",
            "Iteration 73, loss = 1.67265183\n",
            "Iteration 74, loss = 1.67078927\n",
            "Iteration 75, loss = 1.66884173\n",
            "Iteration 76, loss = 1.66703496\n",
            "Iteration 77, loss = 1.66511316\n",
            "Iteration 78, loss = 1.66326963\n",
            "Iteration 79, loss = 1.66147157\n",
            "Iteration 80, loss = 1.65973942\n",
            "Iteration 81, loss = 1.65813623\n",
            "Iteration 82, loss = 1.65643344\n",
            "Iteration 83, loss = 1.65472215\n",
            "Iteration 84, loss = 1.65315513\n",
            "Iteration 85, loss = 1.65159792\n",
            "Iteration 86, loss = 1.64998194\n",
            "Iteration 87, loss = 1.64840923\n",
            "Iteration 88, loss = 1.64697101\n",
            "Iteration 89, loss = 1.64558120\n",
            "Iteration 90, loss = 1.64418718\n",
            "Iteration 91, loss = 1.64274786\n",
            "Iteration 92, loss = 1.64142782\n",
            "Iteration 93, loss = 1.63997824\n",
            "Iteration 94, loss = 1.63866781\n",
            "Iteration 95, loss = 1.63737449\n",
            "Iteration 96, loss = 1.63619243\n",
            "Iteration 97, loss = 1.63485651\n",
            "Iteration 98, loss = 1.63366580\n",
            "Iteration 99, loss = 1.63250398\n",
            "Iteration 100, loss = 1.63131353\n",
            "Iteration 1, loss = 1.93932306\n",
            "Iteration 2, loss = 1.88217928\n",
            "Iteration 3, loss = 1.82940078\n",
            "Iteration 4, loss = 1.78395837\n",
            "Iteration 5, loss = 1.74520167\n",
            "Iteration 6, loss = 1.71130547\n",
            "Iteration 7, loss = 1.68249949\n",
            "Iteration 8, loss = 1.65986533\n",
            "Iteration 9, loss = 1.64039136\n",
            "Iteration 10, loss = 1.62715630\n",
            "Iteration 11, loss = 1.61408543\n",
            "Iteration 12, loss = 1.60690464\n",
            "Iteration 13, loss = 1.60097340\n",
            "Iteration 14, loss = 1.59602558\n",
            "Iteration 15, loss = 1.59283562\n",
            "Iteration 16, loss = 1.59058525\n",
            "Iteration 17, loss = 1.58841625\n",
            "Iteration 18, loss = 1.58687587\n",
            "Iteration 19, loss = 1.58523670\n",
            "Iteration 20, loss = 1.58402830\n",
            "Iteration 21, loss = 1.58280787\n",
            "Iteration 22, loss = 1.58142865\n",
            "Iteration 23, loss = 1.58012635\n",
            "Iteration 24, loss = 1.57890675\n",
            "Iteration 25, loss = 1.57759970\n",
            "Iteration 26, loss = 1.57654129\n",
            "Iteration 27, loss = 1.57537864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 1.57403412\n",
            "Iteration 29, loss = 1.57262163\n",
            "Iteration 30, loss = 1.57147414\n",
            "Iteration 31, loss = 1.56981475\n",
            "Iteration 32, loss = 1.56847761\n",
            "Iteration 33, loss = 1.56684735\n",
            "Iteration 34, loss = 1.56502934\n",
            "Iteration 35, loss = 1.56336412\n",
            "Iteration 36, loss = 1.56145106\n",
            "Iteration 37, loss = 1.55947278\n",
            "Iteration 38, loss = 1.55746131\n",
            "Iteration 39, loss = 1.55538669\n",
            "Iteration 40, loss = 1.55333337\n",
            "Iteration 41, loss = 1.55096474\n",
            "Iteration 42, loss = 1.54867640\n",
            "Iteration 43, loss = 1.54622210\n",
            "Iteration 44, loss = 1.54356546\n",
            "Iteration 45, loss = 1.54093751\n",
            "Iteration 46, loss = 1.53802523\n",
            "Iteration 47, loss = 1.53509992\n",
            "Iteration 48, loss = 1.53204966\n",
            "Iteration 49, loss = 1.52893133\n",
            "Iteration 50, loss = 1.52542922\n",
            "Iteration 51, loss = 1.52194390\n",
            "Iteration 52, loss = 1.51846537\n",
            "Iteration 53, loss = 1.51451491\n",
            "Iteration 54, loss = 1.51065673\n",
            "Iteration 55, loss = 1.50661929\n",
            "Iteration 56, loss = 1.50245721\n",
            "Iteration 57, loss = 1.49788583\n",
            "Iteration 58, loss = 1.49353730\n",
            "Iteration 59, loss = 1.48880728\n",
            "Iteration 60, loss = 1.48403640\n",
            "Iteration 61, loss = 1.47904139\n",
            "Iteration 62, loss = 1.47390586\n",
            "Iteration 63, loss = 1.46885692\n",
            "Iteration 64, loss = 1.46331802\n",
            "Iteration 65, loss = 1.45792121\n",
            "Iteration 66, loss = 1.45226190\n",
            "Iteration 67, loss = 1.44666524\n",
            "Iteration 68, loss = 1.44077089\n",
            "Iteration 69, loss = 1.43479844\n",
            "Iteration 70, loss = 1.42901386\n",
            "Iteration 71, loss = 1.42292248\n",
            "Iteration 72, loss = 1.41671268\n",
            "Iteration 73, loss = 1.41041248\n",
            "Iteration 74, loss = 1.40425415\n",
            "Iteration 75, loss = 1.39810823\n",
            "Iteration 76, loss = 1.39211993\n",
            "Iteration 77, loss = 1.38542433\n",
            "Iteration 78, loss = 1.37924951\n",
            "Iteration 79, loss = 1.37303240\n",
            "Iteration 80, loss = 1.36681678\n",
            "Iteration 81, loss = 1.36080505\n",
            "Iteration 82, loss = 1.35458051\n",
            "Iteration 83, loss = 1.34837264\n",
            "Iteration 84, loss = 1.34263111\n",
            "Iteration 85, loss = 1.33667300\n",
            "Iteration 86, loss = 1.33076670\n",
            "Iteration 87, loss = 1.32509392\n",
            "Iteration 88, loss = 1.31945480\n",
            "Iteration 89, loss = 1.31407913\n",
            "Iteration 90, loss = 1.30872985\n",
            "Iteration 91, loss = 1.30338007\n",
            "Iteration 92, loss = 1.29834192\n",
            "Iteration 93, loss = 1.29314622\n",
            "Iteration 94, loss = 1.28825873\n",
            "Iteration 95, loss = 1.28335298\n",
            "Iteration 96, loss = 1.27890151\n",
            "Iteration 97, loss = 1.27417850\n",
            "Iteration 98, loss = 1.26981829\n",
            "Iteration 99, loss = 1.26559597\n",
            "Iteration 100, loss = 1.26130865\n",
            "Iteration 1, loss = 1.84509345\n",
            "Iteration 2, loss = 1.62075502\n",
            "Iteration 3, loss = 1.62440091\n",
            "Iteration 4, loss = 1.61239431\n",
            "Iteration 5, loss = 1.59675507\n",
            "Iteration 6, loss = 1.60042392\n",
            "Iteration 7, loss = 1.58901362\n",
            "Iteration 8, loss = 1.58522255\n",
            "Iteration 9, loss = 1.57620146\n",
            "Iteration 10, loss = 1.56559528\n",
            "Iteration 11, loss = 1.54479561\n",
            "Iteration 12, loss = 1.52373380\n",
            "Iteration 13, loss = 1.50131588\n",
            "Iteration 14, loss = 1.47078722\n",
            "Iteration 15, loss = 1.43048869\n",
            "Iteration 16, loss = 1.38912494\n",
            "Iteration 17, loss = 1.34697501\n",
            "Iteration 18, loss = 1.30929751\n",
            "Iteration 19, loss = 1.27126109\n",
            "Iteration 20, loss = 1.24007854\n",
            "Iteration 21, loss = 1.21480508\n",
            "Iteration 22, loss = 1.19502827\n",
            "Iteration 23, loss = 1.17752976\n",
            "Iteration 24, loss = 1.16318678\n",
            "Iteration 25, loss = 1.15268611\n",
            "Iteration 26, loss = 1.14338608\n",
            "Iteration 27, loss = 1.13768651\n",
            "Iteration 28, loss = 1.13021841\n",
            "Iteration 29, loss = 1.12649590\n",
            "Iteration 30, loss = 1.12179378\n",
            "Iteration 31, loss = 1.11763060\n",
            "Iteration 32, loss = 1.11939858\n",
            "Iteration 33, loss = 1.11070099\n",
            "Iteration 34, loss = 1.11064685\n",
            "Iteration 35, loss = 1.11257745\n",
            "Iteration 36, loss = 1.10400032\n",
            "Iteration 37, loss = 1.10601469\n",
            "Iteration 38, loss = 1.10003730\n",
            "Iteration 39, loss = 1.10091048\n",
            "Iteration 40, loss = 1.09634845\n",
            "Iteration 41, loss = 1.09763775\n",
            "Iteration 42, loss = 1.09502347\n",
            "Iteration 43, loss = 1.09683930\n",
            "Iteration 44, loss = 1.08938341\n",
            "Iteration 45, loss = 1.09304089\n",
            "Iteration 46, loss = 1.08982175\n",
            "Iteration 47, loss = 1.08950335\n",
            "Iteration 48, loss = 1.08702918\n",
            "Iteration 49, loss = 1.08417211\n",
            "Iteration 50, loss = 1.08590966\n",
            "Iteration 51, loss = 1.07957086\n",
            "Iteration 52, loss = 1.07844285\n",
            "Iteration 53, loss = 1.07598302\n",
            "Iteration 54, loss = 1.07038761\n",
            "Iteration 55, loss = 1.07083351\n",
            "Iteration 56, loss = 1.07404307\n",
            "Iteration 57, loss = 1.06395806\n",
            "Iteration 58, loss = 1.06089178\n",
            "Iteration 59, loss = 1.05558276\n",
            "Iteration 60, loss = 1.05705595\n",
            "Iteration 61, loss = 1.05010801\n",
            "Iteration 62, loss = 1.04452533\n",
            "Iteration 63, loss = 1.04956794\n",
            "Iteration 64, loss = 1.03577965\n",
            "Iteration 65, loss = 1.03700025\n",
            "Iteration 66, loss = 1.02927439\n",
            "Iteration 67, loss = 1.02403900\n",
            "Iteration 68, loss = 1.01935017\n",
            "Iteration 69, loss = 1.01323080\n",
            "Iteration 70, loss = 1.01399626\n",
            "Iteration 71, loss = 1.00494804\n",
            "Iteration 72, loss = 1.00538270\n",
            "Iteration 73, loss = 0.99687681\n",
            "Iteration 74, loss = 0.99142638\n",
            "Iteration 75, loss = 0.98043267\n",
            "Iteration 76, loss = 0.98243073\n",
            "Iteration 77, loss = 0.97126737\n",
            "Iteration 78, loss = 0.96722946\n",
            "Iteration 79, loss = 0.97713365\n",
            "Iteration 80, loss = 0.98827581\n",
            "Iteration 81, loss = 0.97382202\n",
            "Iteration 82, loss = 0.95474390\n",
            "Iteration 83, loss = 0.94473702\n",
            "Iteration 84, loss = 0.93555038\n",
            "Iteration 85, loss = 0.93083773\n",
            "Iteration 86, loss = 0.92894422\n",
            "Iteration 87, loss = 0.92173567\n",
            "Iteration 88, loss = 0.91771385\n",
            "Iteration 89, loss = 0.91303627\n",
            "Iteration 90, loss = 0.91423807\n",
            "Iteration 91, loss = 0.90499504\n",
            "Iteration 92, loss = 0.90883390\n",
            "Iteration 93, loss = 0.89506889\n",
            "Iteration 94, loss = 0.88996560\n",
            "Iteration 95, loss = 0.91220710\n",
            "Iteration 96, loss = 0.89679221\n",
            "Iteration 97, loss = 0.90266262\n",
            "Iteration 98, loss = 0.87895260\n",
            "Iteration 99, loss = 0.92631387\n",
            "Iteration 100, loss = 0.88748745\n",
            "Iteration 1, loss = 1.97938122\n",
            "Iteration 2, loss = 1.97242542\n",
            "Iteration 3, loss = 1.96539550\n",
            "Iteration 4, loss = 1.95857908\n",
            "Iteration 5, loss = 1.95192714\n",
            "Iteration 6, loss = 1.94524940\n",
            "Iteration 7, loss = 1.93858977\n",
            "Iteration 8, loss = 1.93214208\n",
            "Iteration 9, loss = 1.92565542\n",
            "Iteration 10, loss = 1.91953257\n",
            "Iteration 11, loss = 1.91309505\n",
            "Iteration 12, loss = 1.90712830\n",
            "Iteration 13, loss = 1.90107274\n",
            "Iteration 14, loss = 1.89507598\n",
            "Iteration 15, loss = 1.88916708\n",
            "Iteration 16, loss = 1.88344327\n",
            "Iteration 17, loss = 1.87766411\n",
            "Iteration 18, loss = 1.87213611\n",
            "Iteration 19, loss = 1.86665126\n",
            "Iteration 20, loss = 1.86120671\n",
            "Iteration 21, loss = 1.85570863\n",
            "Iteration 22, loss = 1.85064732\n",
            "Iteration 23, loss = 1.84553969\n",
            "Iteration 24, loss = 1.84040709\n",
            "Iteration 25, loss = 1.83531853\n",
            "Iteration 26, loss = 1.83051105\n",
            "Iteration 27, loss = 1.82556581\n",
            "Iteration 28, loss = 1.82091458\n",
            "Iteration 29, loss = 1.81628351\n",
            "Iteration 30, loss = 1.81149411\n",
            "Iteration 31, loss = 1.80719129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 32, loss = 1.80260050\n",
            "Iteration 33, loss = 1.79827984\n",
            "Iteration 34, loss = 1.79398804\n",
            "Iteration 35, loss = 1.78970022\n",
            "Iteration 36, loss = 1.78558587\n",
            "Iteration 37, loss = 1.78166039\n",
            "Iteration 38, loss = 1.77753493\n",
            "Iteration 39, loss = 1.77353424\n",
            "Iteration 40, loss = 1.76985215\n",
            "Iteration 41, loss = 1.76591563\n",
            "Iteration 42, loss = 1.76229541\n",
            "Iteration 43, loss = 1.75841710\n",
            "Iteration 44, loss = 1.75493550\n",
            "Iteration 45, loss = 1.75156348\n",
            "Iteration 46, loss = 1.74783883\n",
            "Iteration 47, loss = 1.74440289\n",
            "Iteration 48, loss = 1.74113756\n",
            "Iteration 49, loss = 1.73767041\n",
            "Iteration 50, loss = 1.73470707\n",
            "Iteration 51, loss = 1.73139417\n",
            "Iteration 52, loss = 1.72827395\n",
            "Iteration 53, loss = 1.72516580\n",
            "Iteration 54, loss = 1.72206440\n",
            "Iteration 55, loss = 1.71923890\n",
            "Iteration 56, loss = 1.71614549\n",
            "Iteration 57, loss = 1.71340698\n",
            "Iteration 58, loss = 1.71056371\n",
            "Iteration 59, loss = 1.70786567\n",
            "Iteration 60, loss = 1.70521185\n",
            "Iteration 61, loss = 1.70242701\n",
            "Iteration 62, loss = 1.69970757\n",
            "Iteration 63, loss = 1.69735228\n",
            "Iteration 64, loss = 1.69471497\n",
            "Iteration 65, loss = 1.69224757\n",
            "Iteration 66, loss = 1.68975238\n",
            "Iteration 67, loss = 1.68740058\n",
            "Iteration 68, loss = 1.68506668\n",
            "Iteration 69, loss = 1.68274789\n",
            "Iteration 70, loss = 1.68045432\n",
            "Iteration 71, loss = 1.67832338\n",
            "Iteration 72, loss = 1.67605364\n",
            "Iteration 73, loss = 1.67390695\n",
            "Iteration 74, loss = 1.67191479\n",
            "Iteration 75, loss = 1.66983447\n",
            "Iteration 76, loss = 1.66790107\n",
            "Iteration 77, loss = 1.66584689\n",
            "Iteration 78, loss = 1.66388832\n",
            "Iteration 79, loss = 1.66197608\n",
            "Iteration 80, loss = 1.66014003\n",
            "Iteration 81, loss = 1.65845087\n",
            "Iteration 82, loss = 1.65665970\n",
            "Iteration 83, loss = 1.65486139\n",
            "Iteration 84, loss = 1.65322499\n",
            "Iteration 85, loss = 1.65160616\n",
            "Iteration 86, loss = 1.64994148\n",
            "Iteration 87, loss = 1.64832814\n",
            "Iteration 88, loss = 1.64685939\n",
            "Iteration 89, loss = 1.64544396\n",
            "Iteration 90, loss = 1.64403451\n",
            "Iteration 91, loss = 1.64257674\n",
            "Iteration 92, loss = 1.64125059\n",
            "Iteration 93, loss = 1.63979880\n",
            "Iteration 94, loss = 1.63849246\n",
            "Iteration 95, loss = 1.63721530\n",
            "Iteration 96, loss = 1.63604165\n",
            "Iteration 97, loss = 1.63472348\n",
            "Iteration 98, loss = 1.63355138\n",
            "Iteration 99, loss = 1.63240873\n",
            "Iteration 100, loss = 1.63123548\n",
            "Iteration 1, loss = 1.96515193\n",
            "Iteration 2, loss = 1.90065605\n",
            "Iteration 3, loss = 1.84208630\n",
            "Iteration 4, loss = 1.79243568\n",
            "Iteration 5, loss = 1.75052853\n",
            "Iteration 6, loss = 1.71414665\n",
            "Iteration 7, loss = 1.68350062\n",
            "Iteration 8, loss = 1.65992142\n",
            "Iteration 9, loss = 1.64061095\n",
            "Iteration 10, loss = 1.62871743\n",
            "Iteration 11, loss = 1.61734927\n",
            "Iteration 12, loss = 1.61114521\n",
            "Iteration 13, loss = 1.60561288\n",
            "Iteration 14, loss = 1.60069155\n",
            "Iteration 15, loss = 1.59725093\n",
            "Iteration 16, loss = 1.59470128\n",
            "Iteration 17, loss = 1.59227402\n",
            "Iteration 18, loss = 1.59050353\n",
            "Iteration 19, loss = 1.58871183\n",
            "Iteration 20, loss = 1.58738215\n",
            "Iteration 21, loss = 1.58604868\n",
            "Iteration 22, loss = 1.58470067\n",
            "Iteration 23, loss = 1.58344117\n",
            "Iteration 24, loss = 1.58227394\n",
            "Iteration 25, loss = 1.58104820\n",
            "Iteration 26, loss = 1.58020831\n",
            "Iteration 27, loss = 1.57925786\n",
            "Iteration 28, loss = 1.57810930\n",
            "Iteration 29, loss = 1.57699053\n",
            "Iteration 30, loss = 1.57614024\n",
            "Iteration 31, loss = 1.57485079\n",
            "Iteration 32, loss = 1.57380174\n",
            "Iteration 33, loss = 1.57260090\n",
            "Iteration 34, loss = 1.57119500\n",
            "Iteration 35, loss = 1.56993032\n",
            "Iteration 36, loss = 1.56850933\n",
            "Iteration 37, loss = 1.56705461\n",
            "Iteration 38, loss = 1.56555264\n",
            "Iteration 39, loss = 1.56404366\n",
            "Iteration 40, loss = 1.56260376\n",
            "Iteration 41, loss = 1.56092512\n",
            "Iteration 42, loss = 1.55930662\n",
            "Iteration 43, loss = 1.55759948\n",
            "Iteration 44, loss = 1.55574988\n",
            "Iteration 45, loss = 1.55398484\n",
            "Iteration 46, loss = 1.55198813\n",
            "Iteration 47, loss = 1.55002582\n",
            "Iteration 48, loss = 1.54799090\n",
            "Iteration 49, loss = 1.54593357\n",
            "Iteration 50, loss = 1.54361717\n",
            "Iteration 51, loss = 1.54132105\n",
            "Iteration 52, loss = 1.53906383\n",
            "Iteration 53, loss = 1.53646606\n",
            "Iteration 54, loss = 1.53398838\n",
            "Iteration 55, loss = 1.53137775\n",
            "Iteration 56, loss = 1.52868277\n",
            "Iteration 57, loss = 1.52570400\n",
            "Iteration 58, loss = 1.52292743\n",
            "Iteration 59, loss = 1.51984407\n",
            "Iteration 60, loss = 1.51677765\n",
            "Iteration 61, loss = 1.51350447\n",
            "Iteration 62, loss = 1.51017984\n",
            "Iteration 63, loss = 1.50689064\n",
            "Iteration 64, loss = 1.50322817\n",
            "Iteration 65, loss = 1.49966145\n",
            "Iteration 66, loss = 1.49589519\n",
            "Iteration 67, loss = 1.49216445\n",
            "Iteration 68, loss = 1.48816364\n",
            "Iteration 69, loss = 1.48410675\n",
            "Iteration 70, loss = 1.48018580\n",
            "Iteration 71, loss = 1.47588858\n",
            "Iteration 72, loss = 1.47153405\n",
            "Iteration 73, loss = 1.46703003\n",
            "Iteration 74, loss = 1.46258152\n",
            "Iteration 75, loss = 1.45810519\n",
            "Iteration 76, loss = 1.45367961\n",
            "Iteration 77, loss = 1.44862720\n",
            "Iteration 78, loss = 1.44387665\n",
            "Iteration 79, loss = 1.43909759\n",
            "Iteration 80, loss = 1.43417150\n",
            "Iteration 81, loss = 1.42935060\n",
            "Iteration 82, loss = 1.42425083\n",
            "Iteration 83, loss = 1.41907437\n",
            "Iteration 84, loss = 1.41418572\n",
            "Iteration 85, loss = 1.40899747\n",
            "Iteration 86, loss = 1.40382617\n",
            "Iteration 87, loss = 1.39871937\n",
            "Iteration 88, loss = 1.39351743\n",
            "Iteration 89, loss = 1.38854398\n",
            "Iteration 90, loss = 1.38339770\n",
            "Iteration 91, loss = 1.37820733\n",
            "Iteration 92, loss = 1.37318345\n",
            "Iteration 93, loss = 1.36795989\n",
            "Iteration 94, loss = 1.36291154\n",
            "Iteration 95, loss = 1.35773629\n",
            "Iteration 96, loss = 1.35300463\n",
            "Iteration 97, loss = 1.34785592\n",
            "Iteration 98, loss = 1.34298837\n",
            "Iteration 99, loss = 1.33825893\n",
            "Iteration 100, loss = 1.33338619\n",
            "Iteration 1, loss = 1.86165691\n",
            "Iteration 2, loss = 1.62481886\n",
            "Iteration 3, loss = 1.62224408\n",
            "Iteration 4, loss = 1.60451976\n",
            "Iteration 5, loss = 1.58389860\n",
            "Iteration 6, loss = 1.58516720\n",
            "Iteration 7, loss = 1.58304249\n",
            "Iteration 8, loss = 1.57345171\n",
            "Iteration 9, loss = 1.55220879\n",
            "Iteration 10, loss = 1.53329990\n",
            "Iteration 11, loss = 1.51362923\n",
            "Iteration 12, loss = 1.49409937\n",
            "Iteration 13, loss = 1.46990236\n",
            "Iteration 14, loss = 1.43932286\n",
            "Iteration 15, loss = 1.41150858\n",
            "Iteration 16, loss = 1.37973993\n",
            "Iteration 17, loss = 1.34841346\n",
            "Iteration 18, loss = 1.32279819\n",
            "Iteration 19, loss = 1.29717752\n",
            "Iteration 20, loss = 1.27540269\n",
            "Iteration 21, loss = 1.25522563\n",
            "Iteration 22, loss = 1.23924837\n",
            "Iteration 23, loss = 1.22337001\n",
            "Iteration 24, loss = 1.21093938\n",
            "Iteration 25, loss = 1.19942387\n",
            "Iteration 26, loss = 1.18939263\n",
            "Iteration 27, loss = 1.18230816\n",
            "Iteration 28, loss = 1.17385779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 29, loss = 1.16823319\n",
            "Iteration 30, loss = 1.16238580\n",
            "Iteration 31, loss = 1.15727423\n",
            "Iteration 32, loss = 1.15464656\n",
            "Iteration 33, loss = 1.14812664\n",
            "Iteration 34, loss = 1.14765738\n",
            "Iteration 35, loss = 1.14497633\n",
            "Iteration 36, loss = 1.14032852\n",
            "Iteration 37, loss = 1.13482113\n",
            "Iteration 38, loss = 1.13508744\n",
            "Iteration 39, loss = 1.12923188\n",
            "Iteration 40, loss = 1.12950408\n",
            "Iteration 41, loss = 1.12514717\n",
            "Iteration 42, loss = 1.12473382\n",
            "Iteration 43, loss = 1.12383331\n",
            "Iteration 44, loss = 1.11888723\n",
            "Iteration 45, loss = 1.11983087\n",
            "Iteration 46, loss = 1.11669050\n",
            "Iteration 47, loss = 1.11639165\n",
            "Iteration 48, loss = 1.11345294\n",
            "Iteration 49, loss = 1.11504360\n",
            "Iteration 50, loss = 1.11328279\n",
            "Iteration 51, loss = 1.10991975\n",
            "Iteration 52, loss = 1.11072638\n",
            "Iteration 53, loss = 1.10969540\n",
            "Iteration 54, loss = 1.11034145\n",
            "Iteration 55, loss = 1.10743625\n",
            "Iteration 56, loss = 1.10685819\n",
            "Iteration 57, loss = 1.10549469\n",
            "Iteration 58, loss = 1.10546849\n",
            "Iteration 59, loss = 1.10467253\n",
            "Iteration 60, loss = 1.10400789\n",
            "Iteration 61, loss = 1.10304148\n",
            "Iteration 62, loss = 1.10295886\n",
            "Iteration 63, loss = 1.10631271\n",
            "Iteration 64, loss = 1.10123368\n",
            "Iteration 65, loss = 1.10260601\n",
            "Iteration 66, loss = 1.10171815\n",
            "Iteration 67, loss = 1.10316391\n",
            "Iteration 68, loss = 1.10195466\n",
            "Iteration 69, loss = 1.10117048\n",
            "Iteration 70, loss = 1.10075240\n",
            "Iteration 71, loss = 1.10027750\n",
            "Iteration 72, loss = 1.09935451\n",
            "Iteration 73, loss = 1.09966244\n",
            "Iteration 74, loss = 1.09897244\n",
            "Iteration 75, loss = 1.09723474\n",
            "Iteration 76, loss = 1.09834150\n",
            "Iteration 77, loss = 1.09875948\n",
            "Iteration 78, loss = 1.09728000\n",
            "Iteration 79, loss = 1.09953105\n",
            "Iteration 80, loss = 1.09586617\n",
            "Iteration 81, loss = 1.09680534\n",
            "Iteration 82, loss = 1.09622456\n",
            "Iteration 83, loss = 1.09598082\n",
            "Iteration 84, loss = 1.09666645\n",
            "Iteration 85, loss = 1.09566019\n",
            "Iteration 86, loss = 1.09978244\n",
            "Iteration 87, loss = 1.09583651\n",
            "Iteration 88, loss = 1.09860127\n",
            "Iteration 89, loss = 1.09603949\n",
            "Iteration 90, loss = 1.09753917\n",
            "Iteration 91, loss = 1.09491346\n",
            "Iteration 92, loss = 1.09278721\n",
            "Iteration 93, loss = 1.09519459\n",
            "Iteration 94, loss = 1.09323228\n",
            "Iteration 95, loss = 1.09438070\n",
            "Iteration 96, loss = 1.09293458\n",
            "Iteration 97, loss = 1.09525376\n",
            "Iteration 98, loss = 1.09151975\n",
            "Iteration 99, loss = 1.09636255\n",
            "Iteration 100, loss = 1.09665213\n",
            "Iteration 1, loss = 1.83207868\n",
            "Iteration 2, loss = 1.82958339\n",
            "Iteration 3, loss = 1.82685519\n",
            "Iteration 4, loss = 1.82440663\n",
            "Iteration 5, loss = 1.82189308\n",
            "Iteration 6, loss = 1.81948021\n",
            "Iteration 7, loss = 1.81700344\n",
            "Iteration 8, loss = 1.81460129\n",
            "Iteration 9, loss = 1.81215098\n",
            "Iteration 10, loss = 1.80973544\n",
            "Iteration 11, loss = 1.80741582\n",
            "Iteration 12, loss = 1.80510060\n",
            "Iteration 13, loss = 1.80273627\n",
            "Iteration 14, loss = 1.80043655\n",
            "Iteration 15, loss = 1.79816418\n",
            "Iteration 16, loss = 1.79589300\n",
            "Iteration 17, loss = 1.79363155\n",
            "Iteration 18, loss = 1.79133767\n",
            "Iteration 19, loss = 1.78915827\n",
            "Iteration 20, loss = 1.78698353\n",
            "Iteration 21, loss = 1.78485051\n",
            "Iteration 22, loss = 1.78270841\n",
            "Iteration 23, loss = 1.78055391\n",
            "Iteration 24, loss = 1.77846396\n",
            "Iteration 25, loss = 1.77637541\n",
            "Iteration 26, loss = 1.77430749\n",
            "Iteration 27, loss = 1.77222212\n",
            "Iteration 28, loss = 1.77012898\n",
            "Iteration 29, loss = 1.76809520\n",
            "Iteration 30, loss = 1.76617815\n",
            "Iteration 31, loss = 1.76412363\n",
            "Iteration 32, loss = 1.76213481\n",
            "Iteration 33, loss = 1.76018324\n",
            "Iteration 34, loss = 1.75829813\n",
            "Iteration 35, loss = 1.75630392\n",
            "Iteration 36, loss = 1.75435309\n",
            "Iteration 37, loss = 1.75239444\n",
            "Iteration 38, loss = 1.75064686\n",
            "Iteration 39, loss = 1.74874038\n",
            "Iteration 40, loss = 1.74684124\n",
            "Iteration 41, loss = 1.74497584\n",
            "Iteration 42, loss = 1.74317474\n",
            "Iteration 43, loss = 1.74138270\n",
            "Iteration 44, loss = 1.73957610\n",
            "Iteration 45, loss = 1.73784916\n",
            "Iteration 46, loss = 1.73596966\n",
            "Iteration 47, loss = 1.73421583\n",
            "Iteration 48, loss = 1.73241444\n",
            "Iteration 49, loss = 1.73074274\n",
            "Iteration 50, loss = 1.72897117\n",
            "Iteration 51, loss = 1.72734801\n",
            "Iteration 52, loss = 1.72558874\n",
            "Iteration 53, loss = 1.72388156\n",
            "Iteration 54, loss = 1.72216710\n",
            "Iteration 55, loss = 1.72059296\n",
            "Iteration 56, loss = 1.71890739\n",
            "Iteration 57, loss = 1.71724479\n",
            "Iteration 58, loss = 1.71562793\n",
            "Iteration 59, loss = 1.71404236\n",
            "Iteration 60, loss = 1.71243404\n",
            "Iteration 61, loss = 1.71080100\n",
            "Iteration 62, loss = 1.70922723\n",
            "Iteration 63, loss = 1.70762908\n",
            "Iteration 64, loss = 1.70609691\n",
            "Iteration 65, loss = 1.70460671\n",
            "Iteration 66, loss = 1.70302094\n",
            "Iteration 67, loss = 1.70144889\n",
            "Iteration 68, loss = 1.70001717\n",
            "Iteration 69, loss = 1.69848722\n",
            "Iteration 70, loss = 1.69701613\n",
            "Iteration 71, loss = 1.69559731\n",
            "Iteration 72, loss = 1.69408463\n",
            "Iteration 73, loss = 1.69247677\n",
            "Iteration 74, loss = 1.69116245\n",
            "Iteration 75, loss = 1.68979808\n",
            "Iteration 76, loss = 1.68834226\n",
            "Iteration 77, loss = 1.68690693\n",
            "Iteration 78, loss = 1.68546797\n",
            "Iteration 79, loss = 1.68413841\n",
            "Iteration 80, loss = 1.68275424\n",
            "Iteration 81, loss = 1.68142085\n",
            "Iteration 82, loss = 1.68007974\n",
            "Iteration 83, loss = 1.67869620\n",
            "Iteration 84, loss = 1.67741499\n",
            "Iteration 85, loss = 1.67605689\n",
            "Iteration 86, loss = 1.67473378\n",
            "Iteration 87, loss = 1.67348650\n",
            "Iteration 88, loss = 1.67228613\n",
            "Iteration 89, loss = 1.67092669\n",
            "Iteration 90, loss = 1.66969923\n",
            "Iteration 91, loss = 1.66841940\n",
            "Iteration 92, loss = 1.66722187\n",
            "Iteration 93, loss = 1.66598853\n",
            "Iteration 94, loss = 1.66481340\n",
            "Iteration 95, loss = 1.66361633\n",
            "Iteration 96, loss = 1.66230963\n",
            "Iteration 97, loss = 1.66126743\n",
            "Iteration 98, loss = 1.66003633\n",
            "Iteration 99, loss = 1.65896690\n",
            "Iteration 100, loss = 1.65775458\n",
            "Iteration 1, loss = 1.82708092\n",
            "Iteration 2, loss = 1.80337809\n",
            "Iteration 3, loss = 1.77892329\n",
            "Iteration 4, loss = 1.75886000\n",
            "Iteration 5, loss = 1.73965387\n",
            "Iteration 6, loss = 1.72275647\n",
            "Iteration 7, loss = 1.70641883\n",
            "Iteration 8, loss = 1.69178834\n",
            "Iteration 9, loss = 1.67770115\n",
            "Iteration 10, loss = 1.66493995\n",
            "Iteration 11, loss = 1.65392636\n",
            "Iteration 12, loss = 1.64383584\n",
            "Iteration 13, loss = 1.63456516\n",
            "Iteration 14, loss = 1.62607949\n",
            "Iteration 15, loss = 1.61917702\n",
            "Iteration 16, loss = 1.61354135\n",
            "Iteration 17, loss = 1.60829671\n",
            "Iteration 18, loss = 1.60365798\n",
            "Iteration 19, loss = 1.60004412\n",
            "Iteration 20, loss = 1.59743494\n",
            "Iteration 21, loss = 1.59572539\n",
            "Iteration 22, loss = 1.59414351\n",
            "Iteration 23, loss = 1.59300309\n",
            "Iteration 24, loss = 1.59165172\n",
            "Iteration 25, loss = 1.59124903\n",
            "Iteration 26, loss = 1.59082964\n",
            "Iteration 27, loss = 1.59004730\n",
            "Iteration 28, loss = 1.58950988\n",
            "Iteration 29, loss = 1.58882218\n",
            "Iteration 30, loss = 1.58796359\n",
            "Iteration 31, loss = 1.58707203\n",
            "Iteration 32, loss = 1.58600960\n",
            "Iteration 33, loss = 1.58487265\n",
            "Iteration 34, loss = 1.58361919\n",
            "Iteration 35, loss = 1.58236977\n",
            "Iteration 36, loss = 1.58109738\n",
            "Iteration 37, loss = 1.57962126\n",
            "Iteration 38, loss = 1.57819274\n",
            "Iteration 39, loss = 1.57642924\n",
            "Iteration 40, loss = 1.57467174\n",
            "Iteration 41, loss = 1.57285943\n",
            "Iteration 42, loss = 1.57092175\n",
            "Iteration 43, loss = 1.56883801\n",
            "Iteration 44, loss = 1.56663477\n",
            "Iteration 45, loss = 1.56423711\n",
            "Iteration 46, loss = 1.56151654\n",
            "Iteration 47, loss = 1.55874169\n",
            "Iteration 48, loss = 1.55581379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 1.55266683\n",
            "Iteration 50, loss = 1.54944150\n",
            "Iteration 51, loss = 1.54588736\n",
            "Iteration 52, loss = 1.54205456\n",
            "Iteration 53, loss = 1.53800959\n",
            "Iteration 54, loss = 1.53381026\n",
            "Iteration 55, loss = 1.52946859\n",
            "Iteration 56, loss = 1.52497828\n",
            "Iteration 57, loss = 1.52010196\n",
            "Iteration 58, loss = 1.51500961\n",
            "Iteration 59, loss = 1.51024902\n",
            "Iteration 60, loss = 1.50509041\n",
            "Iteration 61, loss = 1.49914584\n",
            "Iteration 62, loss = 1.49337947\n",
            "Iteration 63, loss = 1.48744141\n",
            "Iteration 64, loss = 1.48161738\n",
            "Iteration 65, loss = 1.47541632\n",
            "Iteration 66, loss = 1.46886047\n",
            "Iteration 67, loss = 1.46224630\n",
            "Iteration 68, loss = 1.45572255\n",
            "Iteration 69, loss = 1.44892368\n",
            "Iteration 70, loss = 1.44203354\n",
            "Iteration 71, loss = 1.43515234\n",
            "Iteration 72, loss = 1.42803891\n",
            "Iteration 73, loss = 1.42107638\n",
            "Iteration 74, loss = 1.41374564\n",
            "Iteration 75, loss = 1.40647682\n",
            "Iteration 76, loss = 1.39943131\n",
            "Iteration 77, loss = 1.39204207\n",
            "Iteration 78, loss = 1.38471093\n",
            "Iteration 79, loss = 1.37759419\n",
            "Iteration 80, loss = 1.37068056\n",
            "Iteration 81, loss = 1.36336644\n",
            "Iteration 82, loss = 1.35687586\n",
            "Iteration 83, loss = 1.34995423\n",
            "Iteration 84, loss = 1.34272686\n",
            "Iteration 85, loss = 1.33620694\n",
            "Iteration 86, loss = 1.32970518\n",
            "Iteration 87, loss = 1.32320270\n",
            "Iteration 88, loss = 1.31718581\n",
            "Iteration 89, loss = 1.31075789\n",
            "Iteration 90, loss = 1.30500558\n",
            "Iteration 91, loss = 1.29896205\n",
            "Iteration 92, loss = 1.29316787\n",
            "Iteration 93, loss = 1.28755351\n",
            "Iteration 94, loss = 1.28217914\n",
            "Iteration 95, loss = 1.27696753\n",
            "Iteration 96, loss = 1.27189520\n",
            "Iteration 97, loss = 1.26702256\n",
            "Iteration 98, loss = 1.26210604\n",
            "Iteration 99, loss = 1.25774226\n",
            "Iteration 100, loss = 1.25309233\n",
            "Iteration 1, loss = 1.78837167\n",
            "Iteration 2, loss = 1.65871300\n",
            "Iteration 3, loss = 1.59967494\n",
            "Iteration 4, loss = 1.59737916\n",
            "Iteration 5, loss = 1.59447177\n",
            "Iteration 6, loss = 1.58078176\n",
            "Iteration 7, loss = 1.56851189\n",
            "Iteration 8, loss = 1.54536624\n",
            "Iteration 9, loss = 1.52192619\n",
            "Iteration 10, loss = 1.49233101\n",
            "Iteration 11, loss = 1.45741914\n",
            "Iteration 12, loss = 1.41254147\n",
            "Iteration 13, loss = 1.36916977\n",
            "Iteration 14, loss = 1.32077984\n",
            "Iteration 15, loss = 1.28374965\n",
            "Iteration 16, loss = 1.24380653\n",
            "Iteration 17, loss = 1.21221663\n",
            "Iteration 18, loss = 1.18940967\n",
            "Iteration 19, loss = 1.16816391\n",
            "Iteration 20, loss = 1.15610368\n",
            "Iteration 21, loss = 1.14406243\n",
            "Iteration 22, loss = 1.13788487\n",
            "Iteration 23, loss = 1.13204747\n",
            "Iteration 24, loss = 1.12560036\n",
            "Iteration 25, loss = 1.12382695\n",
            "Iteration 26, loss = 1.12120769\n",
            "Iteration 27, loss = 1.11698615\n",
            "Iteration 28, loss = 1.11857162\n",
            "Iteration 29, loss = 1.11569060\n",
            "Iteration 30, loss = 1.11619358\n",
            "Iteration 31, loss = 1.11408992\n",
            "Iteration 32, loss = 1.11251521\n",
            "Iteration 33, loss = 1.11311767\n",
            "Iteration 34, loss = 1.11075740\n",
            "Iteration 35, loss = 1.11312909\n",
            "Iteration 36, loss = 1.10855106\n",
            "Iteration 37, loss = 1.11462017\n",
            "Iteration 38, loss = 1.11000967\n",
            "Iteration 39, loss = 1.10878866\n",
            "Iteration 40, loss = 1.11223377\n",
            "Iteration 41, loss = 1.10678497\n",
            "Iteration 42, loss = 1.11083495\n",
            "Iteration 43, loss = 1.10655455\n",
            "Iteration 44, loss = 1.11045055\n",
            "Iteration 45, loss = 1.11704495\n",
            "Iteration 46, loss = 1.10761845\n",
            "Iteration 47, loss = 1.11096525\n",
            "Iteration 48, loss = 1.10613194\n",
            "Iteration 49, loss = 1.10930769\n",
            "Iteration 50, loss = 1.10649632\n",
            "Iteration 51, loss = 1.10828436\n",
            "Iteration 52, loss = 1.10668502\n",
            "Iteration 53, loss = 1.10634649\n",
            "Iteration 54, loss = 1.10579417\n",
            "Iteration 55, loss = 1.10538512\n",
            "Iteration 56, loss = 1.11566044\n",
            "Iteration 57, loss = 1.10410629\n",
            "Iteration 58, loss = 1.10824560\n",
            "Iteration 59, loss = 1.10431893\n",
            "Iteration 60, loss = 1.10853483\n",
            "Iteration 61, loss = 1.10505009\n",
            "Iteration 62, loss = 1.10299929\n",
            "Iteration 63, loss = 1.10740919\n",
            "Iteration 64, loss = 1.10259492\n",
            "Iteration 65, loss = 1.10533029\n",
            "Iteration 66, loss = 1.10841801\n",
            "Iteration 67, loss = 1.10393516\n",
            "Iteration 68, loss = 1.10085997\n",
            "Iteration 69, loss = 1.10095892\n",
            "Iteration 70, loss = 1.09987503\n",
            "Iteration 71, loss = 1.10030915\n",
            "Iteration 72, loss = 1.09990691\n",
            "Iteration 73, loss = 1.10403291\n",
            "Iteration 74, loss = 1.09836894\n",
            "Iteration 75, loss = 1.10560111\n",
            "Iteration 76, loss = 1.09893849\n",
            "Iteration 77, loss = 1.10233858\n",
            "Iteration 78, loss = 1.09582998\n",
            "Iteration 79, loss = 1.10197525\n",
            "Iteration 80, loss = 1.10028071\n",
            "Iteration 81, loss = 1.09884967\n",
            "Iteration 82, loss = 1.09654464\n",
            "Iteration 83, loss = 1.09939418\n",
            "Iteration 84, loss = 1.09537397\n",
            "Iteration 85, loss = 1.09269430\n",
            "Iteration 86, loss = 1.09281770\n",
            "Iteration 87, loss = 1.09205042\n",
            "Iteration 88, loss = 1.09188297\n",
            "Iteration 89, loss = 1.09011710\n",
            "Iteration 90, loss = 1.08950970\n",
            "Iteration 91, loss = 1.08714921\n",
            "Iteration 92, loss = 1.08680280\n",
            "Iteration 93, loss = 1.08843754\n",
            "Iteration 94, loss = 1.08451032\n",
            "Iteration 95, loss = 1.08316056\n",
            "Iteration 96, loss = 1.08879400\n",
            "Iteration 97, loss = 1.07943299\n",
            "Iteration 98, loss = 1.08311869\n",
            "Iteration 99, loss = 1.07689365\n",
            "Iteration 100, loss = 1.08479698\n",
            "Iteration 1, loss = 1.66649247\n",
            "Iteration 2, loss = 1.66542926\n",
            "Iteration 3, loss = 1.66403029\n",
            "Iteration 4, loss = 1.66291816\n",
            "Iteration 5, loss = 1.66175845\n",
            "Iteration 6, loss = 1.66069212\n",
            "Iteration 7, loss = 1.65955666\n",
            "Iteration 8, loss = 1.65845811\n",
            "Iteration 9, loss = 1.65729663\n",
            "Iteration 10, loss = 1.65619360\n",
            "Iteration 11, loss = 1.65515853\n",
            "Iteration 12, loss = 1.65410624\n",
            "Iteration 13, loss = 1.65305336\n",
            "Iteration 14, loss = 1.65197168\n",
            "Iteration 15, loss = 1.65095096\n",
            "Iteration 16, loss = 1.64996803\n",
            "Iteration 17, loss = 1.64897860\n",
            "Iteration 18, loss = 1.64793106\n",
            "Iteration 19, loss = 1.64696942\n",
            "Iteration 20, loss = 1.64600840\n",
            "Iteration 21, loss = 1.64512015\n",
            "Iteration 22, loss = 1.64422772\n",
            "Iteration 23, loss = 1.64325591\n",
            "Iteration 24, loss = 1.64234548\n",
            "Iteration 25, loss = 1.64147623\n",
            "Iteration 26, loss = 1.64063302\n",
            "Iteration 27, loss = 1.63976775\n",
            "Iteration 28, loss = 1.63887380\n",
            "Iteration 29, loss = 1.63804091\n",
            "Iteration 30, loss = 1.63729839\n",
            "Iteration 31, loss = 1.63645659\n",
            "Iteration 32, loss = 1.63564958\n",
            "Iteration 33, loss = 1.63490332\n",
            "Iteration 34, loss = 1.63417034\n",
            "Iteration 35, loss = 1.63339603\n",
            "Iteration 36, loss = 1.63258019\n",
            "Iteration 37, loss = 1.63186537\n",
            "Iteration 38, loss = 1.63122969\n",
            "Iteration 39, loss = 1.63054736\n",
            "Iteration 40, loss = 1.62979169\n",
            "Iteration 41, loss = 1.62909909\n",
            "Iteration 42, loss = 1.62846770\n",
            "Iteration 43, loss = 1.62786402\n",
            "Iteration 44, loss = 1.62719978\n",
            "Iteration 45, loss = 1.62663862\n",
            "Iteration 46, loss = 1.62594477\n",
            "Iteration 47, loss = 1.62532511\n",
            "Iteration 48, loss = 1.62471963\n",
            "Iteration 49, loss = 1.62416001\n",
            "Iteration 50, loss = 1.62353885\n",
            "Iteration 51, loss = 1.62309324\n",
            "Iteration 52, loss = 1.62246891\n",
            "Iteration 53, loss = 1.62193677\n",
            "Iteration 54, loss = 1.62134843\n",
            "Iteration 55, loss = 1.62088874\n",
            "Iteration 56, loss = 1.62036421\n",
            "Iteration 57, loss = 1.61983662\n",
            "Iteration 58, loss = 1.61935800\n",
            "Iteration 59, loss = 1.61888918\n",
            "Iteration 60, loss = 1.61843613\n",
            "Iteration 61, loss = 1.61791313\n",
            "Iteration 62, loss = 1.61747629\n",
            "Iteration 63, loss = 1.61700145\n",
            "Iteration 64, loss = 1.61654147\n",
            "Iteration 65, loss = 1.61615443"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 66, loss = 1.61568650\n",
            "Iteration 67, loss = 1.61525418\n",
            "Iteration 68, loss = 1.61487669\n",
            "Iteration 69, loss = 1.61448519\n",
            "Iteration 70, loss = 1.61407828\n",
            "Iteration 71, loss = 1.61373267\n",
            "Iteration 72, loss = 1.61329702\n",
            "Iteration 73, loss = 1.61284755\n",
            "Iteration 74, loss = 1.61256463\n",
            "Iteration 75, loss = 1.61224900\n",
            "Iteration 76, loss = 1.61185355\n",
            "Iteration 77, loss = 1.61150080\n",
            "Iteration 78, loss = 1.61114598\n",
            "Iteration 79, loss = 1.61083996\n",
            "Iteration 80, loss = 1.61048454\n",
            "Iteration 81, loss = 1.61020875\n",
            "Iteration 82, loss = 1.60987813\n",
            "Iteration 83, loss = 1.60955870\n",
            "Iteration 84, loss = 1.60928249\n",
            "Iteration 85, loss = 1.60895163\n",
            "Iteration 86, loss = 1.60866028\n",
            "Iteration 87, loss = 1.60840433\n",
            "Iteration 88, loss = 1.60819078\n",
            "Iteration 89, loss = 1.60783428\n",
            "Iteration 90, loss = 1.60760350\n",
            "Iteration 91, loss = 1.60730999\n",
            "Iteration 92, loss = 1.60706385\n",
            "Iteration 93, loss = 1.60683160\n",
            "Iteration 94, loss = 1.60658598\n",
            "Iteration 95, loss = 1.60634868\n",
            "Iteration 96, loss = 1.60605640\n",
            "Iteration 97, loss = 1.60590688\n",
            "Iteration 98, loss = 1.60562390\n",
            "Iteration 99, loss = 1.60548386\n",
            "Iteration 100, loss = 1.60521229\n",
            "Iteration 1, loss = 1.66365470\n",
            "Iteration 2, loss = 1.65393066\n",
            "Iteration 3, loss = 1.64199036\n",
            "Iteration 4, loss = 1.63404341\n",
            "Iteration 5, loss = 1.62667726\n",
            "Iteration 6, loss = 1.62099847\n",
            "Iteration 7, loss = 1.61572238\n",
            "Iteration 8, loss = 1.61134881\n",
            "Iteration 9, loss = 1.60770192\n",
            "Iteration 10, loss = 1.60440859\n",
            "Iteration 11, loss = 1.60257473\n",
            "Iteration 12, loss = 1.60104992\n",
            "Iteration 13, loss = 1.59986722\n",
            "Iteration 14, loss = 1.59873851\n",
            "Iteration 15, loss = 1.59837368\n",
            "Iteration 16, loss = 1.59839390\n",
            "Iteration 17, loss = 1.59802564\n",
            "Iteration 18, loss = 1.59795514\n",
            "Iteration 19, loss = 1.59777806\n",
            "Iteration 20, loss = 1.59780940\n",
            "Iteration 21, loss = 1.59773871\n",
            "Iteration 22, loss = 1.59763299\n",
            "Iteration 23, loss = 1.59741724\n",
            "Iteration 24, loss = 1.59724081\n",
            "Iteration 25, loss = 1.59687804\n",
            "Iteration 26, loss = 1.59659535\n",
            "Iteration 27, loss = 1.59627706\n",
            "Iteration 28, loss = 1.59597901\n",
            "Iteration 29, loss = 1.59570743\n",
            "Iteration 30, loss = 1.59538364\n",
            "Iteration 31, loss = 1.59512330\n",
            "Iteration 32, loss = 1.59483043\n",
            "Iteration 33, loss = 1.59454635\n",
            "Iteration 34, loss = 1.59425852\n",
            "Iteration 35, loss = 1.59392300\n",
            "Iteration 36, loss = 1.59359793\n",
            "Iteration 37, loss = 1.59330844\n",
            "Iteration 38, loss = 1.59294358\n",
            "Iteration 39, loss = 1.59240654\n",
            "Iteration 40, loss = 1.59187071\n",
            "Iteration 41, loss = 1.59134102\n",
            "Iteration 42, loss = 1.59080255\n",
            "Iteration 43, loss = 1.59028914\n",
            "Iteration 44, loss = 1.58983378\n",
            "Iteration 45, loss = 1.58917250\n",
            "Iteration 46, loss = 1.58842571\n",
            "Iteration 47, loss = 1.58772970\n",
            "Iteration 48, loss = 1.58704592\n",
            "Iteration 49, loss = 1.58624534\n",
            "Iteration 50, loss = 1.58560229\n",
            "Iteration 51, loss = 1.58467951\n",
            "Iteration 52, loss = 1.58374337\n",
            "Iteration 53, loss = 1.58275135\n",
            "Iteration 54, loss = 1.58185396\n",
            "Iteration 55, loss = 1.58074305\n",
            "Iteration 56, loss = 1.57980325\n",
            "Iteration 57, loss = 1.57856991\n",
            "Iteration 58, loss = 1.57734656\n",
            "Iteration 59, loss = 1.57634148\n",
            "Iteration 60, loss = 1.57517880\n",
            "Iteration 61, loss = 1.57366322\n",
            "Iteration 62, loss = 1.57229591\n",
            "Iteration 63, loss = 1.57094514\n",
            "Iteration 64, loss = 1.56946565\n",
            "Iteration 65, loss = 1.56795837\n",
            "Iteration 66, loss = 1.56634144\n",
            "Iteration 67, loss = 1.56473485\n",
            "Iteration 68, loss = 1.56304455\n",
            "Iteration 69, loss = 1.56135667\n",
            "Iteration 70, loss = 1.55949546\n",
            "Iteration 71, loss = 1.55773997\n",
            "Iteration 72, loss = 1.55581259\n",
            "Iteration 73, loss = 1.55390043\n",
            "Iteration 74, loss = 1.55169423\n",
            "Iteration 75, loss = 1.54960914\n",
            "Iteration 76, loss = 1.54746748\n",
            "Iteration 77, loss = 1.54515233\n",
            "Iteration 78, loss = 1.54288517\n",
            "Iteration 79, loss = 1.54046118\n",
            "Iteration 80, loss = 1.53816153\n",
            "Iteration 81, loss = 1.53555117\n",
            "Iteration 82, loss = 1.53325550\n",
            "Iteration 83, loss = 1.53064833\n",
            "Iteration 84, loss = 1.52777764\n",
            "Iteration 85, loss = 1.52514655\n",
            "Iteration 86, loss = 1.52247298\n",
            "Iteration 87, loss = 1.51961772\n",
            "Iteration 88, loss = 1.51705578\n",
            "Iteration 89, loss = 1.51388750\n",
            "Iteration 90, loss = 1.51111911\n",
            "Iteration 91, loss = 1.50800369\n",
            "Iteration 92, loss = 1.50497750\n",
            "Iteration 93, loss = 1.50188340\n",
            "Iteration 94, loss = 1.49875921\n",
            "Iteration 95, loss = 1.49560217\n",
            "Iteration 96, loss = 1.49234372\n",
            "Iteration 97, loss = 1.48926691\n",
            "Iteration 98, loss = 1.48588798\n",
            "Iteration 99, loss = 1.48269520\n",
            "Iteration 100, loss = 1.47923735\n",
            "Iteration 1, loss = 1.64319825\n",
            "Iteration 2, loss = 1.63295226\n",
            "Iteration 3, loss = 1.61303858\n",
            "Iteration 4, loss = 1.61012344\n",
            "Iteration 5, loss = 1.60876534\n",
            "Iteration 6, loss = 1.60219278\n",
            "Iteration 7, loss = 1.59740255\n",
            "Iteration 8, loss = 1.59892020\n",
            "Iteration 9, loss = 1.59570169\n",
            "Iteration 10, loss = 1.59123134\n",
            "Iteration 11, loss = 1.58599939\n",
            "Iteration 12, loss = 1.58086632\n",
            "Iteration 13, loss = 1.57586723\n",
            "Iteration 14, loss = 1.56789714\n",
            "Iteration 15, loss = 1.55651915\n",
            "Iteration 16, loss = 1.54301826\n",
            "Iteration 17, loss = 1.52696029\n",
            "Iteration 18, loss = 1.50804814\n",
            "Iteration 19, loss = 1.48507732\n",
            "Iteration 20, loss = 1.46180966\n",
            "Iteration 21, loss = 1.43689392\n",
            "Iteration 22, loss = 1.40826611\n",
            "Iteration 23, loss = 1.38003685\n",
            "Iteration 24, loss = 1.35225847\n",
            "Iteration 25, loss = 1.32400225\n",
            "Iteration 26, loss = 1.29879844\n",
            "Iteration 27, loss = 1.27486815\n",
            "Iteration 28, loss = 1.25342056\n",
            "Iteration 29, loss = 1.23476770\n",
            "Iteration 30, loss = 1.21934732\n",
            "Iteration 31, loss = 1.20362711\n",
            "Iteration 32, loss = 1.19132557\n",
            "Iteration 33, loss = 1.18085073\n",
            "Iteration 34, loss = 1.17293193\n",
            "Iteration 35, loss = 1.16479807\n",
            "Iteration 36, loss = 1.15832477\n",
            "Iteration 37, loss = 1.15443818\n",
            "Iteration 38, loss = 1.14900850\n",
            "Iteration 39, loss = 1.14397160\n",
            "Iteration 40, loss = 1.14079306\n",
            "Iteration 41, loss = 1.13681369\n",
            "Iteration 42, loss = 1.13340675\n",
            "Iteration 43, loss = 1.13241288\n",
            "Iteration 44, loss = 1.12863739\n",
            "Iteration 45, loss = 1.12881731\n",
            "Iteration 46, loss = 1.12443275\n",
            "Iteration 47, loss = 1.12229552\n",
            "Iteration 48, loss = 1.12120814\n",
            "Iteration 49, loss = 1.11912855\n",
            "Iteration 50, loss = 1.11899636\n",
            "Iteration 51, loss = 1.11737695\n",
            "Iteration 52, loss = 1.11596477\n",
            "Iteration 53, loss = 1.11454726\n",
            "Iteration 54, loss = 1.11378307\n",
            "Iteration 55, loss = 1.11243740\n",
            "Iteration 56, loss = 1.11532313\n",
            "Iteration 57, loss = 1.10971098\n",
            "Iteration 58, loss = 1.10973759\n",
            "Iteration 59, loss = 1.10969758\n",
            "Iteration 60, loss = 1.10950056\n",
            "Iteration 61, loss = 1.10834678\n",
            "Iteration 62, loss = 1.10670452\n",
            "Iteration 63, loss = 1.10716708\n",
            "Iteration 64, loss = 1.10720458\n",
            "Iteration 65, loss = 1.10517676\n",
            "Iteration 66, loss = 1.10532673\n",
            "Iteration 67, loss = 1.10462880\n",
            "Iteration 68, loss = 1.10492118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 69, loss = 1.10417211\n",
            "Iteration 70, loss = 1.10222212\n",
            "Iteration 71, loss = 1.10279623\n",
            "Iteration 72, loss = 1.10350889\n",
            "Iteration 73, loss = 1.10528371\n",
            "Iteration 74, loss = 1.10266438\n",
            "Iteration 75, loss = 1.10418198\n",
            "Iteration 76, loss = 1.10229855\n",
            "Iteration 77, loss = 1.10170735\n",
            "Iteration 78, loss = 1.10232067\n",
            "Iteration 79, loss = 1.10111579\n",
            "Iteration 80, loss = 1.10309258\n",
            "Iteration 81, loss = 1.09943640\n",
            "Iteration 82, loss = 1.09964436\n",
            "Iteration 83, loss = 1.09937837\n",
            "Iteration 84, loss = 1.09981730\n",
            "Iteration 85, loss = 1.09954444\n",
            "Iteration 86, loss = 1.09802865\n",
            "Iteration 87, loss = 1.09807504\n",
            "Iteration 88, loss = 1.09936632\n",
            "Iteration 89, loss = 1.09770817\n",
            "Iteration 90, loss = 1.09743062\n",
            "Iteration 91, loss = 1.09619278\n",
            "Iteration 92, loss = 1.09733423\n",
            "Iteration 93, loss = 1.09736511\n",
            "Iteration 94, loss = 1.09631946\n",
            "Iteration 95, loss = 1.09494476\n",
            "Iteration 96, loss = 1.09900237\n",
            "Iteration 97, loss = 1.09524752\n",
            "Iteration 98, loss = 1.09591807\n",
            "Iteration 99, loss = 1.09642705\n",
            "Iteration 100, loss = 1.10015826\n",
            "Iteration 1, loss = 1.82967997\n",
            "Iteration 2, loss = 1.82732525\n",
            "Iteration 3, loss = 1.82474181\n",
            "Iteration 4, loss = 1.82242843\n",
            "Iteration 5, loss = 1.82004768\n",
            "Iteration 6, loss = 1.81775864\n",
            "Iteration 7, loss = 1.81540644\n",
            "Iteration 8, loss = 1.81311973\n",
            "Iteration 9, loss = 1.81078816\n",
            "Iteration 10, loss = 1.80848773\n",
            "Iteration 11, loss = 1.80627344\n",
            "Iteration 12, loss = 1.80406335\n",
            "Iteration 13, loss = 1.80180622\n",
            "Iteration 14, loss = 1.79960381\n",
            "Iteration 15, loss = 1.79742738\n",
            "Iteration 16, loss = 1.79525059\n",
            "Iteration 17, loss = 1.79308313\n",
            "Iteration 18, loss = 1.79087978\n",
            "Iteration 19, loss = 1.78878680\n",
            "Iteration 20, loss = 1.78669653\n",
            "Iteration 21, loss = 1.78464554\n",
            "Iteration 22, loss = 1.78258661\n",
            "Iteration 23, loss = 1.78051204\n",
            "Iteration 24, loss = 1.77849889\n",
            "Iteration 25, loss = 1.77648819\n",
            "Iteration 26, loss = 1.77449729\n",
            "Iteration 27, loss = 1.77248902\n",
            "Iteration 28, loss = 1.77047260\n",
            "Iteration 29, loss = 1.76851384\n",
            "Iteration 30, loss = 1.76666718\n",
            "Iteration 31, loss = 1.76468957\n",
            "Iteration 32, loss = 1.76277445\n",
            "Iteration 33, loss = 1.76089656\n",
            "Iteration 34, loss = 1.75908205\n",
            "Iteration 35, loss = 1.75716568\n",
            "Iteration 36, loss = 1.75528821\n",
            "Iteration 37, loss = 1.75340787\n",
            "Iteration 38, loss = 1.75172586\n",
            "Iteration 39, loss = 1.74989816\n",
            "Iteration 40, loss = 1.74807589\n",
            "Iteration 41, loss = 1.74628697\n",
            "Iteration 42, loss = 1.74456230\n",
            "Iteration 43, loss = 1.74284661\n",
            "Iteration 44, loss = 1.74111629\n",
            "Iteration 45, loss = 1.73946592\n",
            "Iteration 46, loss = 1.73767309\n",
            "Iteration 47, loss = 1.73599748\n",
            "Iteration 48, loss = 1.73428074\n",
            "Iteration 49, loss = 1.73268623\n",
            "Iteration 50, loss = 1.73099859\n",
            "Iteration 51, loss = 1.72945568\n",
            "Iteration 52, loss = 1.72778388\n",
            "Iteration 53, loss = 1.72616503\n",
            "Iteration 54, loss = 1.72453602\n",
            "Iteration 55, loss = 1.72304430\n",
            "Iteration 56, loss = 1.72144657\n",
            "Iteration 57, loss = 1.71987594\n",
            "Iteration 58, loss = 1.71834788\n",
            "Iteration 59, loss = 1.71685308\n",
            "Iteration 60, loss = 1.71533956\n",
            "Iteration 61, loss = 1.71379538\n",
            "Iteration 62, loss = 1.71231309\n",
            "Iteration 63, loss = 1.71080973\n",
            "Iteration 64, loss = 1.70936945\n",
            "Iteration 65, loss = 1.70797149\n",
            "Iteration 66, loss = 1.70648339\n",
            "Iteration 67, loss = 1.70501163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 68, loss = 1.70366965\n",
            "Iteration 69, loss = 1.70224043\n",
            "Iteration 70, loss = 1.70086369\n",
            "Iteration 71, loss = 1.69953668\n",
            "Iteration 72, loss = 1.69812070\n",
            "Iteration 73, loss = 1.69662979\n",
            "Iteration 74, loss = 1.69540300\n",
            "Iteration 75, loss = 1.69412961\n",
            "Iteration 76, loss = 1.69277476\n",
            "Iteration 77, loss = 1.69143995\n",
            "Iteration 78, loss = 1.69009953\n",
            "Iteration 79, loss = 1.68886543\n",
            "Iteration 80, loss = 1.68757711\n",
            "Iteration 81, loss = 1.68634077\n",
            "Iteration 82, loss = 1.68509756\n",
            "Iteration 83, loss = 1.68381535\n",
            "Iteration 84, loss = 1.68262312\n",
            "Iteration 85, loss = 1.68136706\n",
            "Iteration 86, loss = 1.68014085\n",
            "Iteration 87, loss = 1.67898684\n",
            "Iteration 88, loss = 1.67787495\n",
            "Iteration 89, loss = 1.67661353\n",
            "Iteration 90, loss = 1.67548148\n",
            "Iteration 91, loss = 1.67429640\n",
            "Iteration 92, loss = 1.67318333\n",
            "Iteration 93, loss = 1.67204669\n",
            "Iteration 94, loss = 1.67095675\n",
            "Iteration 95, loss = 1.66984666\n",
            "Iteration 96, loss = 1.66864326\n",
            "Iteration 97, loss = 1.66767354\n",
            "Iteration 98, loss = 1.66653567\n",
            "Iteration 99, loss = 1.66554043\n",
            "Iteration 100, loss = 1.66442268\n",
            "Iteration 1, loss = 1.82496350\n",
            "Iteration 2, loss = 1.80245008\n",
            "Iteration 3, loss = 1.77897274\n",
            "Iteration 4, loss = 1.75963827\n",
            "Iteration 5, loss = 1.74107379\n",
            "Iteration 6, loss = 1.72477445\n",
            "Iteration 7, loss = 1.70909009\n",
            "Iteration 8, loss = 1.69512507\n",
            "Iteration 9, loss = 1.68172449\n",
            "Iteration 10, loss = 1.66965582\n",
            "Iteration 11, loss = 1.65924041\n",
            "Iteration 12, loss = 1.64964458\n",
            "Iteration 13, loss = 1.64078374\n",
            "Iteration 14, loss = 1.63262222\n",
            "Iteration 15, loss = 1.62582476\n",
            "Iteration 16, loss = 1.62009466\n",
            "Iteration 17, loss = 1.61472211\n",
            "Iteration 18, loss = 1.60981697\n",
            "Iteration 19, loss = 1.60585006\n",
            "Iteration 20, loss = 1.60270143\n",
            "Iteration 21, loss = 1.60036412\n",
            "Iteration 22, loss = 1.59817679\n",
            "Iteration 23, loss = 1.59637742\n",
            "Iteration 24, loss = 1.59455779\n",
            "Iteration 25, loss = 1.59365426\n",
            "Iteration 26, loss = 1.59287866\n",
            "Iteration 27, loss = 1.59187611\n",
            "Iteration 28, loss = 1.59124499\n",
            "Iteration 29, loss = 1.59061852\n",
            "Iteration 30, loss = 1.59013024\n",
            "Iteration 31, loss = 1.58940631\n",
            "Iteration 32, loss = 1.58859974\n",
            "Iteration 33, loss = 1.58776522\n",
            "Iteration 34, loss = 1.58679352\n",
            "Iteration 35, loss = 1.58567935\n",
            "Iteration 36, loss = 1.58449092\n",
            "Iteration 37, loss = 1.58310532\n",
            "Iteration 38, loss = 1.58168268\n",
            "Iteration 39, loss = 1.58001624\n",
            "Iteration 40, loss = 1.57832290\n",
            "Iteration 41, loss = 1.57654032\n",
            "Iteration 42, loss = 1.57466882\n",
            "Iteration 43, loss = 1.57261214\n",
            "Iteration 44, loss = 1.57042083\n",
            "Iteration 45, loss = 1.56807929\n",
            "Iteration 46, loss = 1.56557295\n",
            "Iteration 47, loss = 1.56291706\n",
            "Iteration 48, loss = 1.56011033\n",
            "Iteration 49, loss = 1.55711097\n",
            "Iteration 50, loss = 1.55406890\n",
            "Iteration 51, loss = 1.55067475\n",
            "Iteration 52, loss = 1.54707596\n",
            "Iteration 53, loss = 1.54332224\n",
            "Iteration 54, loss = 1.53946892\n",
            "Iteration 55, loss = 1.53545097\n",
            "Iteration 56, loss = 1.53141523\n",
            "Iteration 57, loss = 1.52697191\n",
            "Iteration 58, loss = 1.52240700\n",
            "Iteration 59, loss = 1.51817043\n",
            "Iteration 60, loss = 1.51359546\n",
            "Iteration 61, loss = 1.50841285\n",
            "Iteration 62, loss = 1.50343632\n",
            "Iteration 63, loss = 1.49836420\n",
            "Iteration 64, loss = 1.49331557\n",
            "Iteration 65, loss = 1.48804057\n",
            "Iteration 66, loss = 1.48252277\n",
            "Iteration 67, loss = 1.47698400\n",
            "Iteration 68, loss = 1.47153730\n",
            "Iteration 69, loss = 1.46590595\n",
            "Iteration 70, loss = 1.46021755\n",
            "Iteration 71, loss = 1.45455436\n",
            "Iteration 72, loss = 1.44875851\n",
            "Iteration 73, loss = 1.44301385\n",
            "Iteration 74, loss = 1.43704801\n",
            "Iteration 75, loss = 1.43112906\n",
            "Iteration 76, loss = 1.42538792\n",
            "Iteration 77, loss = 1.41934301\n",
            "Iteration 78, loss = 1.41339462\n",
            "Iteration 79, loss = 1.40752264\n",
            "Iteration 80, loss = 1.40184117\n",
            "Iteration 81, loss = 1.39579723\n",
            "Iteration 82, loss = 1.39041043\n",
            "Iteration 83, loss = 1.38463044\n",
            "Iteration 84, loss = 1.37854733\n",
            "Iteration 85, loss = 1.37304727\n",
            "Iteration 86, loss = 1.36751347\n",
            "Iteration 87, loss = 1.36191043\n",
            "Iteration 88, loss = 1.35667472\n",
            "Iteration 89, loss = 1.35103260\n",
            "Iteration 90, loss = 1.34594085\n",
            "Iteration 91, loss = 1.34052118\n",
            "Iteration 92, loss = 1.33525492\n",
            "Iteration 93, loss = 1.33011230\n",
            "Iteration 94, loss = 1.32512900\n",
            "Iteration 95, loss = 1.32019897\n",
            "Iteration 96, loss = 1.31531422\n",
            "Iteration 97, loss = 1.31064050\n",
            "Iteration 98, loss = 1.30584018\n",
            "Iteration 99, loss = 1.30142936\n",
            "Iteration 100, loss = 1.29675911\n",
            "Iteration 1, loss = 1.78765164\n",
            "Iteration 2, loss = 1.66062589\n",
            "Iteration 3, loss = 1.60066525\n",
            "Iteration 4, loss = 1.59925615\n",
            "Iteration 5, loss = 1.60322690\n",
            "Iteration 6, loss = 1.59413083\n",
            "Iteration 7, loss = 1.58303428\n",
            "Iteration 8, loss = 1.57322670\n",
            "Iteration 9, loss = 1.55379089\n",
            "Iteration 10, loss = 1.53316384\n",
            "Iteration 11, loss = 1.50558781\n",
            "Iteration 12, loss = 1.46109807\n",
            "Iteration 13, loss = 1.41359969\n",
            "Iteration 14, loss = 1.36707854\n",
            "Iteration 15, loss = 1.32455820\n",
            "Iteration 16, loss = 1.28519412\n",
            "Iteration 17, loss = 1.24867069\n",
            "Iteration 18, loss = 1.21935850\n",
            "Iteration 19, loss = 1.19490563\n",
            "Iteration 20, loss = 1.17578455\n",
            "Iteration 21, loss = 1.16015785\n",
            "Iteration 22, loss = 1.14770586\n",
            "Iteration 23, loss = 1.13687875\n",
            "Iteration 24, loss = 1.12982629\n",
            "Iteration 25, loss = 1.12402723\n",
            "Iteration 26, loss = 1.12307104\n",
            "Iteration 27, loss = 1.11589136\n",
            "Iteration 28, loss = 1.11859210\n",
            "Iteration 29, loss = 1.11504573\n",
            "Iteration 30, loss = 1.11466855\n",
            "Iteration 31, loss = 1.11402783\n",
            "Iteration 32, loss = 1.10964112\n",
            "Iteration 33, loss = 1.11149321\n",
            "Iteration 34, loss = 1.10661375\n",
            "Iteration 35, loss = 1.10952494\n",
            "Iteration 36, loss = 1.10357513\n",
            "Iteration 37, loss = 1.11112705\n",
            "Iteration 38, loss = 1.10320544\n",
            "Iteration 39, loss = 1.10241269\n",
            "Iteration 40, loss = 1.10333752\n",
            "Iteration 41, loss = 1.10005770\n",
            "Iteration 42, loss = 1.10134778\n",
            "Iteration 43, loss = 1.09938280\n",
            "Iteration 44, loss = 1.10100310\n",
            "Iteration 45, loss = 1.10633180\n",
            "Iteration 46, loss = 1.09816187\n",
            "Iteration 47, loss = 1.09914520\n",
            "Iteration 48, loss = 1.09586818\n",
            "Iteration 49, loss = 1.09750735\n",
            "Iteration 50, loss = 1.09603592\n",
            "Iteration 51, loss = 1.09640478\n",
            "Iteration 52, loss = 1.09460929\n",
            "Iteration 53, loss = 1.09407134\n",
            "Iteration 54, loss = 1.09253610\n",
            "Iteration 55, loss = 1.09185810\n",
            "Iteration 56, loss = 1.10117659\n",
            "Iteration 57, loss = 1.08914084\n",
            "Iteration 58, loss = 1.09201093\n",
            "Iteration 59, loss = 1.08796373\n",
            "Iteration 60, loss = 1.09149641\n",
            "Iteration 61, loss = 1.08780563\n",
            "Iteration 62, loss = 1.08466292\n",
            "Iteration 63, loss = 1.08578658\n",
            "Iteration 64, loss = 1.08178841\n",
            "Iteration 65, loss = 1.08110270\n",
            "Iteration 66, loss = 1.08344437\n",
            "Iteration 67, loss = 1.07658559\n",
            "Iteration 68, loss = 1.07354441\n",
            "Iteration 69, loss = 1.07254074\n",
            "Iteration 70, loss = 1.06970675\n",
            "Iteration 71, loss = 1.06887304\n",
            "Iteration 72, loss = 1.06404758\n",
            "Iteration 73, loss = 1.06718217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 1.05774753\n",
            "Iteration 75, loss = 1.06148475\n",
            "Iteration 76, loss = 1.05439782\n",
            "Iteration 77, loss = 1.06239970\n",
            "Iteration 78, loss = 1.04923744\n",
            "Iteration 79, loss = 1.05127237\n",
            "Iteration 80, loss = 1.04465943\n",
            "Iteration 81, loss = 1.03806360\n",
            "Iteration 82, loss = 1.03076126\n",
            "Iteration 83, loss = 1.03092316\n",
            "Iteration 84, loss = 1.02105737\n",
            "Iteration 85, loss = 1.01915983\n",
            "Iteration 86, loss = 1.01698259\n",
            "Iteration 87, loss = 1.00696859\n",
            "Iteration 88, loss = 1.00285214\n",
            "Iteration 89, loss = 0.99628031\n",
            "Iteration 90, loss = 0.99201851\n",
            "Iteration 91, loss = 0.98613803\n",
            "Iteration 92, loss = 0.98005439\n",
            "Iteration 93, loss = 0.98532643\n",
            "Iteration 94, loss = 0.98060183\n",
            "Iteration 95, loss = 0.96988680\n",
            "Iteration 96, loss = 0.96361403\n",
            "Iteration 97, loss = 0.95177416\n",
            "Iteration 98, loss = 0.94988180\n",
            "Iteration 99, loss = 0.94647961\n",
            "Iteration 100, loss = 0.94653325\n",
            "Iteration 1, loss = 1.79698929\n",
            "Iteration 2, loss = 1.79552089\n",
            "Iteration 3, loss = 1.79388635\n",
            "Iteration 4, loss = 1.79245331\n",
            "Iteration 5, loss = 1.79095216\n",
            "Iteration 6, loss = 1.78948833\n",
            "Iteration 7, loss = 1.78797197\n",
            "Iteration 8, loss = 1.78647265\n",
            "Iteration 9, loss = 1.78495855\n",
            "Iteration 10, loss = 1.78345544\n",
            "Iteration 11, loss = 1.78197877\n",
            "Iteration 12, loss = 1.78050957\n",
            "Iteration 13, loss = 1.77901758\n",
            "Iteration 14, loss = 1.77750643\n",
            "Iteration 15, loss = 1.77601652\n",
            "Iteration 16, loss = 1.77451159\n",
            "Iteration 17, loss = 1.77301687\n",
            "Iteration 18, loss = 1.77145602\n",
            "Iteration 19, loss = 1.76998401\n",
            "Iteration 20, loss = 1.76850049\n",
            "Iteration 21, loss = 1.76702107\n",
            "Iteration 22, loss = 1.76554809\n",
            "Iteration 23, loss = 1.76402641\n",
            "Iteration 24, loss = 1.76253936\n",
            "Iteration 25, loss = 1.76105412\n",
            "Iteration 26, loss = 1.75955323\n",
            "Iteration 27, loss = 1.75804923\n",
            "Iteration 28, loss = 1.75652448\n",
            "Iteration 29, loss = 1.75500775\n",
            "Iteration 30, loss = 1.75358783\n",
            "Iteration 31, loss = 1.75204321\n",
            "Iteration 32, loss = 1.75054564\n",
            "Iteration 33, loss = 1.74906834\n",
            "Iteration 34, loss = 1.74761738\n",
            "Iteration 35, loss = 1.74607858\n",
            "Iteration 36, loss = 1.74455551\n",
            "Iteration 37, loss = 1.74302849\n",
            "Iteration 38, loss = 1.74167093\n",
            "Iteration 39, loss = 1.74015767\n",
            "Iteration 40, loss = 1.73862811\n",
            "Iteration 41, loss = 1.73713487\n",
            "Iteration 42, loss = 1.73567989\n",
            "Iteration 43, loss = 1.73423814\n",
            "Iteration 44, loss = 1.73276770\n",
            "Iteration 45, loss = 1.73135191\n",
            "Iteration 46, loss = 1.72979031\n",
            "Iteration 47, loss = 1.72833893\n",
            "Iteration 48, loss = 1.72684211\n",
            "Iteration 49, loss = 1.72544410\n",
            "Iteration 50, loss = 1.72395891\n",
            "Iteration 51, loss = 1.72259569\n",
            "Iteration 52, loss = 1.72109222\n",
            "Iteration 53, loss = 1.71964334\n",
            "Iteration 54, loss = 1.71818044\n",
            "Iteration 55, loss = 1.71682712\n",
            "Iteration 56, loss = 1.71538510\n",
            "Iteration 57, loss = 1.71394058\n",
            "Iteration 58, loss = 1.71253584\n",
            "Iteration 59, loss = 1.71113847\n",
            "Iteration 60, loss = 1.70973894\n",
            "Iteration 61, loss = 1.70831226\n",
            "Iteration 62, loss = 1.70693928\n",
            "Iteration 63, loss = 1.70553352\n",
            "Iteration 64, loss = 1.70417126\n",
            "Iteration 65, loss = 1.70284310\n",
            "Iteration 66, loss = 1.70143914\n",
            "Iteration 67, loss = 1.70003461\n",
            "Iteration 68, loss = 1.69875689\n",
            "Iteration 69, loss = 1.69738298\n",
            "Iteration 70, loss = 1.69606449\n",
            "Iteration 71, loss = 1.69478593\n",
            "Iteration 72, loss = 1.69343186\n",
            "Iteration 73, loss = 1.69196468\n",
            "Iteration 74, loss = 1.69077731\n",
            "Iteration 75, loss = 1.68954625\n",
            "Iteration 76, loss = 1.68821814\n",
            "Iteration 77, loss = 1.68691780\n",
            "Iteration 78, loss = 1.68560839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 79, loss = 1.68438956\n",
            "Iteration 80, loss = 1.68312821\n",
            "Iteration 81, loss = 1.68190821\n",
            "Iteration 82, loss = 1.68067532\n",
            "Iteration 83, loss = 1.67939918\n",
            "Iteration 84, loss = 1.67823452\n",
            "Iteration 85, loss = 1.67698053\n",
            "Iteration 86, loss = 1.67577138\n",
            "Iteration 87, loss = 1.67461365\n",
            "Iteration 88, loss = 1.67350157\n",
            "Iteration 89, loss = 1.67225819\n",
            "Iteration 90, loss = 1.67111317\n",
            "Iteration 91, loss = 1.66993176\n",
            "Iteration 92, loss = 1.66883241\n",
            "Iteration 93, loss = 1.66769110\n",
            "Iteration 94, loss = 1.66659930\n",
            "Iteration 95, loss = 1.66549368\n",
            "Iteration 96, loss = 1.66428007\n",
            "Iteration 97, loss = 1.66330554\n",
            "Iteration 98, loss = 1.66216936\n",
            "Iteration 99, loss = 1.66118520\n",
            "Iteration 100, loss = 1.66004759\n",
            "Iteration 1, loss = 1.79420527\n",
            "Iteration 2, loss = 1.77965031\n",
            "Iteration 3, loss = 1.76332715\n",
            "Iteration 4, loss = 1.74935047\n",
            "Iteration 5, loss = 1.73485525\n",
            "Iteration 6, loss = 1.72113190\n",
            "Iteration 7, loss = 1.70715627\n",
            "Iteration 8, loss = 1.69387284\n",
            "Iteration 9, loss = 1.68091818\n",
            "Iteration 10, loss = 1.66887937\n",
            "Iteration 11, loss = 1.65812870\n",
            "Iteration 12, loss = 1.64826878\n",
            "Iteration 13, loss = 1.63919704\n",
            "Iteration 14, loss = 1.63072984\n",
            "Iteration 15, loss = 1.62377898\n",
            "Iteration 16, loss = 1.61799485\n",
            "Iteration 17, loss = 1.61268444\n",
            "Iteration 18, loss = 1.60788427\n",
            "Iteration 19, loss = 1.60417230\n",
            "Iteration 20, loss = 1.60136872\n",
            "Iteration 21, loss = 1.59944752\n",
            "Iteration 22, loss = 1.59766371\n",
            "Iteration 23, loss = 1.59630833\n",
            "Iteration 24, loss = 1.59480528\n",
            "Iteration 25, loss = 1.59433368\n",
            "Iteration 26, loss = 1.59391434\n",
            "Iteration 27, loss = 1.59323192\n",
            "Iteration 28, loss = 1.59291416\n",
            "Iteration 29, loss = 1.59255150\n",
            "Iteration 30, loss = 1.59215995\n",
            "Iteration 31, loss = 1.59173917\n",
            "Iteration 32, loss = 1.59119457\n",
            "Iteration 33, loss = 1.59065729\n",
            "Iteration 34, loss = 1.58997344\n",
            "Iteration 35, loss = 1.58935620\n",
            "Iteration 36, loss = 1.58871617\n",
            "Iteration 37, loss = 1.58800699\n",
            "Iteration 38, loss = 1.58729939\n",
            "Iteration 39, loss = 1.58636535\n",
            "Iteration 40, loss = 1.58555980\n",
            "Iteration 41, loss = 1.58476802\n",
            "Iteration 42, loss = 1.58390874\n",
            "Iteration 43, loss = 1.58303019\n",
            "Iteration 44, loss = 1.58217279\n",
            "Iteration 45, loss = 1.58114804\n",
            "Iteration 46, loss = 1.58001028\n",
            "Iteration 47, loss = 1.57890593\n",
            "Iteration 48, loss = 1.57778697\n",
            "Iteration 49, loss = 1.57651288\n",
            "Iteration 50, loss = 1.57533077\n",
            "Iteration 51, loss = 1.57392762\n",
            "Iteration 52, loss = 1.57240875\n",
            "Iteration 53, loss = 1.57079626\n",
            "Iteration 54, loss = 1.56921011\n",
            "Iteration 55, loss = 1.56746278\n",
            "Iteration 56, loss = 1.56578957\n",
            "Iteration 57, loss = 1.56380717\n",
            "Iteration 58, loss = 1.56180156\n",
            "Iteration 59, loss = 1.55989434\n",
            "Iteration 60, loss = 1.55792422\n",
            "Iteration 61, loss = 1.55544524\n",
            "Iteration 62, loss = 1.55306312\n",
            "Iteration 63, loss = 1.55057693\n",
            "Iteration 64, loss = 1.54813136\n",
            "Iteration 65, loss = 1.54544677\n",
            "Iteration 66, loss = 1.54257709\n",
            "Iteration 67, loss = 1.53963506\n",
            "Iteration 68, loss = 1.53662621\n",
            "Iteration 69, loss = 1.53348009\n",
            "Iteration 70, loss = 1.53014391\n",
            "Iteration 71, loss = 1.52677884\n",
            "Iteration 72, loss = 1.52322412\n",
            "Iteration 73, loss = 1.51966117\n",
            "Iteration 74, loss = 1.51568112\n",
            "Iteration 75, loss = 1.51172427\n",
            "Iteration 76, loss = 1.50773792\n",
            "Iteration 77, loss = 1.50344329\n",
            "Iteration 78, loss = 1.49902468\n",
            "Iteration 79, loss = 1.49461857\n",
            "Iteration 80, loss = 1.49019707\n",
            "Iteration 81, loss = 1.48533143\n",
            "Iteration 82, loss = 1.48091899\n",
            "Iteration 83, loss = 1.47603256\n",
            "Iteration 84, loss = 1.47085578\n",
            "Iteration 85, loss = 1.46601079\n",
            "Iteration 86, loss = 1.46106103\n",
            "Iteration 87, loss = 1.45607128\n",
            "Iteration 88, loss = 1.45136993\n",
            "Iteration 89, loss = 1.44634480\n",
            "Iteration 90, loss = 1.44162616\n",
            "Iteration 91, loss = 1.43683968\n",
            "Iteration 92, loss = 1.43228520\n",
            "Iteration 93, loss = 1.42760758\n",
            "Iteration 94, loss = 1.42318985\n",
            "Iteration 95, loss = 1.41886974\n",
            "Iteration 96, loss = 1.41433292\n",
            "Iteration 97, loss = 1.41018261\n",
            "Iteration 98, loss = 1.40589366\n",
            "Iteration 99, loss = 1.40185891\n",
            "Iteration 100, loss = 1.39756469\n",
            "Iteration 1, loss = 1.77017798\n",
            "Iteration 2, loss = 1.66155287\n",
            "Iteration 3, loss = 1.59642115\n",
            "Iteration 4, loss = 1.59855054\n",
            "Iteration 5, loss = 1.59921779\n",
            "Iteration 6, loss = 1.58282138\n",
            "Iteration 7, loss = 1.56907427\n",
            "Iteration 8, loss = 1.55848506\n",
            "Iteration 9, loss = 1.53449107\n",
            "Iteration 10, loss = 1.51357017\n",
            "Iteration 11, loss = 1.49690488\n",
            "Iteration 12, loss = 1.47216635\n",
            "Iteration 13, loss = 1.44947250\n",
            "Iteration 14, loss = 1.42297667\n",
            "Iteration 15, loss = 1.39485024\n",
            "Iteration 16, loss = 1.36693556\n",
            "Iteration 17, loss = 1.33718287\n",
            "Iteration 18, loss = 1.31025784\n",
            "Iteration 19, loss = 1.28353345\n",
            "Iteration 20, loss = 1.26096909\n",
            "Iteration 21, loss = 1.23883307\n",
            "Iteration 22, loss = 1.22275700\n",
            "Iteration 23, loss = 1.20627575\n",
            "Iteration 24, loss = 1.19283741\n",
            "Iteration 25, loss = 1.18073754\n",
            "Iteration 26, loss = 1.17194844\n",
            "Iteration 27, loss = 1.16346406\n",
            "Iteration 28, loss = 1.15639326\n",
            "Iteration 29, loss = 1.15119745\n",
            "Iteration 30, loss = 1.14684945\n",
            "Iteration 31, loss = 1.14361532\n",
            "Iteration 32, loss = 1.13895375\n",
            "Iteration 33, loss = 1.13643664\n",
            "Iteration 34, loss = 1.13370230\n",
            "Iteration 35, loss = 1.13206863\n",
            "Iteration 36, loss = 1.12958211\n",
            "Iteration 37, loss = 1.13021782\n",
            "Iteration 38, loss = 1.12788383\n",
            "Iteration 39, loss = 1.12499556\n",
            "Iteration 40, loss = 1.12489990\n",
            "Iteration 41, loss = 1.12371091\n",
            "Iteration 42, loss = 1.12148647\n",
            "Iteration 43, loss = 1.12264862\n",
            "Iteration 44, loss = 1.11877724\n",
            "Iteration 45, loss = 1.12156270\n",
            "Iteration 46, loss = 1.11688014\n",
            "Iteration 47, loss = 1.11523120\n",
            "Iteration 48, loss = 1.11557745\n",
            "Iteration 49, loss = 1.11280223\n",
            "Iteration 50, loss = 1.11394543\n",
            "Iteration 51, loss = 1.11211978\n",
            "Iteration 52, loss = 1.11148101\n",
            "Iteration 53, loss = 1.10986695\n",
            "Iteration 54, loss = 1.10984357\n",
            "Iteration 55, loss = 1.10838564\n",
            "Iteration 56, loss = 1.11097909\n",
            "Iteration 57, loss = 1.10540452\n",
            "Iteration 58, loss = 1.10587324\n",
            "Iteration 59, loss = 1.10626867\n",
            "Iteration 60, loss = 1.10663227\n",
            "Iteration 61, loss = 1.10441563\n",
            "Iteration 62, loss = 1.10349855\n",
            "Iteration 63, loss = 1.10505208\n",
            "Iteration 64, loss = 1.10317019\n",
            "Iteration 65, loss = 1.10319998\n",
            "Iteration 66, loss = 1.10393996\n",
            "Iteration 67, loss = 1.10303582\n",
            "Iteration 68, loss = 1.10381990\n",
            "Iteration 69, loss = 1.10249110\n",
            "Iteration 70, loss = 1.10129813\n",
            "Iteration 71, loss = 1.10165455\n",
            "Iteration 72, loss = 1.10189074\n",
            "Iteration 73, loss = 1.10339122\n",
            "Iteration 74, loss = 1.10094585\n",
            "Iteration 75, loss = 1.10396650\n",
            "Iteration 76, loss = 1.10062576\n",
            "Iteration 77, loss = 1.10245693\n",
            "Iteration 78, loss = 1.09993008\n",
            "Iteration 79, loss = 1.10410217\n",
            "Iteration 80, loss = 1.10416888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 81, loss = 1.10102132\n",
            "Iteration 82, loss = 1.10186480\n",
            "Iteration 83, loss = 1.10083798\n",
            "Iteration 84, loss = 1.09852888\n",
            "Iteration 85, loss = 1.09899902\n",
            "Iteration 86, loss = 1.10391279\n",
            "Iteration 87, loss = 1.09905705\n",
            "Iteration 88, loss = 1.10231665\n",
            "Iteration 89, loss = 1.09931961\n",
            "Iteration 90, loss = 1.09797037\n",
            "Iteration 91, loss = 1.09759867\n",
            "Iteration 92, loss = 1.09839112\n",
            "Iteration 93, loss = 1.09900051\n",
            "Iteration 94, loss = 1.09763705\n",
            "Iteration 95, loss = 1.09761167\n",
            "Iteration 96, loss = 1.10369960\n",
            "Iteration 97, loss = 1.09790639\n",
            "Iteration 98, loss = 1.09965767\n",
            "Iteration 99, loss = 1.09918299\n",
            "Iteration 100, loss = 1.10698422\n",
            "Iteration 1, loss = 1.64909058\n",
            "Iteration 2, loss = 1.64814146\n",
            "Iteration 3, loss = 1.64713141\n",
            "Iteration 4, loss = 1.64632350\n",
            "Iteration 5, loss = 1.64542974\n",
            "Iteration 6, loss = 1.64446114\n",
            "Iteration 7, loss = 1.64371848\n",
            "Iteration 8, loss = 1.64281922\n",
            "Iteration 9, loss = 1.64199581\n",
            "Iteration 10, loss = 1.64109456\n",
            "Iteration 11, loss = 1.64026541\n",
            "Iteration 12, loss = 1.63961464\n",
            "Iteration 13, loss = 1.63878054\n",
            "Iteration 14, loss = 1.63799951\n",
            "Iteration 15, loss = 1.63720683\n",
            "Iteration 16, loss = 1.63648878\n",
            "Iteration 17, loss = 1.63577456\n",
            "Iteration 18, loss = 1.63508709\n",
            "Iteration 19, loss = 1.63433094\n",
            "Iteration 20, loss = 1.63358134\n",
            "Iteration 21, loss = 1.63299160\n",
            "Iteration 22, loss = 1.63227058\n",
            "Iteration 23, loss = 1.63168365\n",
            "Iteration 24, loss = 1.63095186\n",
            "Iteration 25, loss = 1.63034060\n",
            "Iteration 26, loss = 1.62977871\n",
            "Iteration 27, loss = 1.62910790\n",
            "Iteration 28, loss = 1.62848374\n",
            "Iteration 29, loss = 1.62788926\n",
            "Iteration 30, loss = 1.62728677\n",
            "Iteration 31, loss = 1.62673782\n",
            "Iteration 32, loss = 1.62613179\n",
            "Iteration 33, loss = 1.62556760\n",
            "Iteration 34, loss = 1.62501769\n",
            "Iteration 35, loss = 1.62453929\n",
            "Iteration 36, loss = 1.62396110\n",
            "Iteration 37, loss = 1.62341297\n",
            "Iteration 38, loss = 1.62280438\n",
            "Iteration 39, loss = 1.62238704\n",
            "Iteration 40, loss = 1.62181080\n",
            "Iteration 41, loss = 1.62126151\n",
            "Iteration 42, loss = 1.62082854\n",
            "Iteration 43, loss = 1.62029523\n",
            "Iteration 44, loss = 1.61979599\n",
            "Iteration 45, loss = 1.61927685\n",
            "Iteration 46, loss = 1.61872996\n",
            "Iteration 47, loss = 1.61837907\n",
            "Iteration 48, loss = 1.61784361\n",
            "Iteration 49, loss = 1.61737209\n",
            "Iteration 50, loss = 1.61685497\n",
            "Iteration 51, loss = 1.61644730\n",
            "Iteration 52, loss = 1.61595564\n",
            "Iteration 53, loss = 1.61555827\n",
            "Iteration 54, loss = 1.61509575\n",
            "Iteration 55, loss = 1.61461700\n",
            "Iteration 56, loss = 1.61416717\n",
            "Iteration 57, loss = 1.61375447\n",
            "Iteration 58, loss = 1.61329290\n",
            "Iteration 59, loss = 1.61280289\n",
            "Iteration 60, loss = 1.61242331\n",
            "Iteration 61, loss = 1.61194902\n",
            "Iteration 62, loss = 1.61158345\n",
            "Iteration 63, loss = 1.61111783\n",
            "Iteration 64, loss = 1.61069432\n",
            "Iteration 65, loss = 1.61036427\n",
            "Iteration 66, loss = 1.60988975\n",
            "Iteration 67, loss = 1.60947405\n",
            "Iteration 68, loss = 1.60909160\n",
            "Iteration 69, loss = 1.60868437\n",
            "Iteration 70, loss = 1.60827433\n",
            "Iteration 71, loss = 1.60784849\n",
            "Iteration 72, loss = 1.60748190\n",
            "Iteration 73, loss = 1.60710832\n",
            "Iteration 74, loss = 1.60669652\n",
            "Iteration 75, loss = 1.60632462\n",
            "Iteration 76, loss = 1.60592941\n",
            "Iteration 77, loss = 1.60559722\n",
            "Iteration 78, loss = 1.60516999\n",
            "Iteration 79, loss = 1.60482860\n",
            "Iteration 80, loss = 1.60447804\n",
            "Iteration 81, loss = 1.60407902\n",
            "Iteration 82, loss = 1.60371131\n",
            "Iteration 83, loss = 1.60339419\n",
            "Iteration 84, loss = 1.60300559\n",
            "Iteration 85, loss = 1.60267892\n",
            "Iteration 86, loss = 1.60234726\n",
            "Iteration 87, loss = 1.60196732\n",
            "Iteration 88, loss = 1.60157087\n",
            "Iteration 89, loss = 1.60128445\n",
            "Iteration 90, loss = 1.60094217\n",
            "Iteration 91, loss = 1.60059750\n",
            "Iteration 92, loss = 1.60026441\n",
            "Iteration 93, loss = 1.59992319\n",
            "Iteration 94, loss = 1.59961134\n",
            "Iteration 95, loss = 1.59928286\n",
            "Iteration 96, loss = 1.59895731\n",
            "Iteration 97, loss = 1.59864476\n",
            "Iteration 98, loss = 1.59832985\n",
            "Iteration 99, loss = 1.59800427\n",
            "Iteration 100, loss = 1.59768849\n",
            "Iteration 1, loss = 1.64781045\n",
            "Iteration 2, loss = 1.63914437\n",
            "Iteration 3, loss = 1.63100478\n",
            "Iteration 4, loss = 1.62548118\n",
            "Iteration 5, loss = 1.61990786\n",
            "Iteration 6, loss = 1.61441582\n",
            "Iteration 7, loss = 1.61110720\n",
            "Iteration 8, loss = 1.60699765\n",
            "Iteration 9, loss = 1.60372765\n",
            "Iteration 10, loss = 1.60041192\n",
            "Iteration 11, loss = 1.59744910\n",
            "Iteration 12, loss = 1.59554605\n",
            "Iteration 13, loss = 1.59305387\n",
            "Iteration 14, loss = 1.59096530\n",
            "Iteration 15, loss = 1.58886562\n",
            "Iteration 16, loss = 1.58693308\n",
            "Iteration 17, loss = 1.58496613\n",
            "Iteration 18, loss = 1.58356406\n",
            "Iteration 19, loss = 1.58168858\n",
            "Iteration 20, loss = 1.57978891\n",
            "Iteration 21, loss = 1.57766083\n",
            "Iteration 22, loss = 1.57594707\n",
            "Iteration 23, loss = 1.57385398\n",
            "Iteration 24, loss = 1.57194187\n",
            "Iteration 25, loss = 1.56972615\n",
            "Iteration 26, loss = 1.56749881\n",
            "Iteration 27, loss = 1.56511223\n",
            "Iteration 28, loss = 1.56277391\n",
            "Iteration 29, loss = 1.56002216\n",
            "Iteration 30, loss = 1.55753652\n",
            "Iteration 31, loss = 1.55448080\n",
            "Iteration 32, loss = 1.55154135\n",
            "Iteration 33, loss = 1.54844428\n",
            "Iteration 34, loss = 1.54523582\n",
            "Iteration 35, loss = 1.54212882\n",
            "Iteration 36, loss = 1.53853235\n",
            "Iteration 37, loss = 1.53514105\n",
            "Iteration 38, loss = 1.53129557\n",
            "Iteration 39, loss = 1.52755763\n",
            "Iteration 40, loss = 1.52352698\n",
            "Iteration 41, loss = 1.51941593\n",
            "Iteration 42, loss = 1.51539528\n",
            "Iteration 43, loss = 1.51115677\n",
            "Iteration 44, loss = 1.50673079\n",
            "Iteration 45, loss = 1.50228806\n",
            "Iteration 46, loss = 1.49802532\n",
            "Iteration 47, loss = 1.49330184\n",
            "Iteration 48, loss = 1.48831813\n",
            "Iteration 49, loss = 1.48349293\n",
            "Iteration 50, loss = 1.47861795\n",
            "Iteration 51, loss = 1.47359615\n",
            "Iteration 52, loss = 1.46870234\n",
            "Iteration 53, loss = 1.46356398\n",
            "Iteration 54, loss = 1.45839944\n",
            "Iteration 55, loss = 1.45307818\n",
            "Iteration 56, loss = 1.44821057\n",
            "Iteration 57, loss = 1.44244845\n",
            "Iteration 58, loss = 1.43714820\n",
            "Iteration 59, loss = 1.43183281\n",
            "Iteration 60, loss = 1.42644371\n",
            "Iteration 61, loss = 1.42103346\n",
            "Iteration 62, loss = 1.41567327\n",
            "Iteration 63, loss = 1.41025910\n",
            "Iteration 64, loss = 1.40486666\n",
            "Iteration 65, loss = 1.39962212\n",
            "Iteration 66, loss = 1.39436216\n",
            "Iteration 67, loss = 1.38886530\n",
            "Iteration 68, loss = 1.38368210\n",
            "Iteration 69, loss = 1.37842994\n",
            "Iteration 70, loss = 1.37321955\n",
            "Iteration 71, loss = 1.36807490\n",
            "Iteration 72, loss = 1.36311999\n",
            "Iteration 73, loss = 1.35805350\n",
            "Iteration 74, loss = 1.35326493\n",
            "Iteration 75, loss = 1.34813934\n",
            "Iteration 76, loss = 1.34335129\n",
            "Iteration 77, loss = 1.33861604\n",
            "Iteration 78, loss = 1.33403153\n",
            "Iteration 79, loss = 1.32958175\n",
            "Iteration 80, loss = 1.32495296\n",
            "Iteration 81, loss = 1.32066636\n",
            "Iteration 82, loss = 1.31623455\n",
            "Iteration 83, loss = 1.31205243\n",
            "Iteration 84, loss = 1.30781885\n",
            "Iteration 85, loss = 1.30395273\n",
            "Iteration 86, loss = 1.29985060\n",
            "Iteration 87, loss = 1.29614594\n",
            "Iteration 88, loss = 1.29215846\n",
            "Iteration 89, loss = 1.28847783\n",
            "Iteration 90, loss = 1.28480622\n",
            "Iteration 91, loss = 1.28127027\n",
            "Iteration 92, loss = 1.27780932\n",
            "Iteration 93, loss = 1.27461492\n",
            "Iteration 94, loss = 1.27115621\n",
            "Iteration 95, loss = 1.26791118\n",
            "Iteration 96, loss = 1.26488935\n",
            "Iteration 97, loss = 1.26178911\n",
            "Iteration 98, loss = 1.25884656\n",
            "Iteration 99, loss = 1.25595141\n",
            "Iteration 100, loss = 1.25322897\n",
            "Iteration 1, loss = 1.64255422\n",
            "Iteration 2, loss = 1.60734046\n",
            "Iteration 3, loss = 1.59267862\n",
            "Iteration 4, loss = 1.58618543\n",
            "Iteration 5, loss = 1.57535856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 1.55794736\n",
            "Iteration 7, loss = 1.53133740\n",
            "Iteration 8, loss = 1.50448207\n",
            "Iteration 9, loss = 1.47204523\n",
            "Iteration 10, loss = 1.43663407\n",
            "Iteration 11, loss = 1.39826006\n",
            "Iteration 12, loss = 1.35710395\n",
            "Iteration 13, loss = 1.32051404\n",
            "Iteration 14, loss = 1.28184384\n",
            "Iteration 15, loss = 1.25843965\n",
            "Iteration 16, loss = 1.22848562\n",
            "Iteration 17, loss = 1.21419089\n",
            "Iteration 18, loss = 1.19223595\n",
            "Iteration 19, loss = 1.18341241\n",
            "Iteration 20, loss = 1.16753771\n",
            "Iteration 21, loss = 1.16281843\n",
            "Iteration 22, loss = 1.15393260\n",
            "Iteration 23, loss = 1.14701970\n",
            "Iteration 24, loss = 1.14175328\n",
            "Iteration 25, loss = 1.13588271\n",
            "Iteration 26, loss = 1.13548890\n",
            "Iteration 27, loss = 1.13467500\n",
            "Iteration 28, loss = 1.13123382\n",
            "Iteration 29, loss = 1.12724013\n",
            "Iteration 30, loss = 1.12777789\n",
            "Iteration 31, loss = 1.12417287\n",
            "Iteration 32, loss = 1.11975741\n",
            "Iteration 33, loss = 1.11805879\n",
            "Iteration 34, loss = 1.11819899\n",
            "Iteration 35, loss = 1.11962755\n",
            "Iteration 36, loss = 1.11525810\n",
            "Iteration 37, loss = 1.11972817\n",
            "Iteration 38, loss = 1.11549393\n",
            "Iteration 39, loss = 1.11789949\n",
            "Iteration 40, loss = 1.11446537\n",
            "Iteration 41, loss = 1.11421572\n",
            "Iteration 42, loss = 1.11328128\n",
            "Iteration 43, loss = 1.11334516\n",
            "Iteration 44, loss = 1.11435952\n",
            "Iteration 45, loss = 1.11215668\n",
            "Iteration 46, loss = 1.11660215\n",
            "Iteration 47, loss = 1.12269818\n",
            "Iteration 48, loss = 1.11323963\n",
            "Iteration 49, loss = 1.11512466\n",
            "Iteration 50, loss = 1.11323595\n",
            "Iteration 51, loss = 1.11308614\n",
            "Iteration 52, loss = 1.11188121\n",
            "Iteration 53, loss = 1.11158058\n",
            "Iteration 54, loss = 1.11573212\n",
            "Iteration 55, loss = 1.11346996\n",
            "Iteration 56, loss = 1.11475347\n",
            "Iteration 57, loss = 1.11697154\n",
            "Iteration 58, loss = 1.11017198\n",
            "Iteration 59, loss = 1.11243380\n",
            "Iteration 60, loss = 1.11155512\n",
            "Iteration 61, loss = 1.11020863\n",
            "Iteration 62, loss = 1.11177677\n",
            "Iteration 63, loss = 1.10903499\n",
            "Iteration 64, loss = 1.11428137\n",
            "Iteration 65, loss = 1.11284840\n",
            "Iteration 66, loss = 1.11225085\n",
            "Iteration 67, loss = 1.10937680\n",
            "Iteration 68, loss = 1.11060652\n",
            "Iteration 69, loss = 1.10957109\n",
            "Iteration 70, loss = 1.11029312\n",
            "Iteration 71, loss = 1.10938309\n",
            "Iteration 72, loss = 1.11091091\n",
            "Iteration 73, loss = 1.10819628\n",
            "Iteration 74, loss = 1.11429534\n",
            "Iteration 75, loss = 1.11028539\n",
            "Iteration 76, loss = 1.11183624\n",
            "Iteration 77, loss = 1.10835126\n",
            "Iteration 78, loss = 1.11074436\n",
            "Iteration 79, loss = 1.10858363\n",
            "Iteration 80, loss = 1.10804180\n",
            "Iteration 81, loss = 1.11004912\n",
            "Iteration 82, loss = 1.10761223\n",
            "Iteration 83, loss = 1.10927116\n",
            "Iteration 84, loss = 1.10754533\n",
            "Iteration 85, loss = 1.10813261\n",
            "Iteration 86, loss = 1.10840757\n",
            "Iteration 87, loss = 1.10928591\n",
            "Iteration 88, loss = 1.10843127\n",
            "Iteration 89, loss = 1.10880302\n",
            "Iteration 90, loss = 1.10856224\n",
            "Iteration 91, loss = 1.10770258\n",
            "Iteration 92, loss = 1.10974669\n",
            "Iteration 93, loss = 1.10628129\n",
            "Iteration 94, loss = 1.10658149\n",
            "Iteration 95, loss = 1.10609706\n",
            "Iteration 96, loss = 1.10754188\n",
            "Iteration 97, loss = 1.10831233\n",
            "Iteration 98, loss = 1.10549717\n",
            "Iteration 99, loss = 1.10673262\n",
            "Iteration 100, loss = 1.10552758\n",
            "Iteration 1, loss = 1.63857807\n",
            "Iteration 2, loss = 1.63800304\n",
            "Iteration 3, loss = 1.63739962\n",
            "Iteration 4, loss = 1.63691810\n",
            "Iteration 5, loss = 1.63639666\n",
            "Iteration 6, loss = 1.63584002\n",
            "Iteration 7, loss = 1.63538400\n",
            "Iteration 8, loss = 1.63484234\n",
            "Iteration 9, loss = 1.63435469\n",
            "Iteration 10, loss = 1.63379253\n",
            "Iteration 11, loss = 1.63328735\n",
            "Iteration 12, loss = 1.63289022\n",
            "Iteration 13, loss = 1.63237478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14, loss = 1.63188240\n",
            "Iteration 15, loss = 1.63138981\n",
            "Iteration 16, loss = 1.63093129\n",
            "Iteration 17, loss = 1.63048154\n",
            "Iteration 18, loss = 1.63005147\n",
            "Iteration 19, loss = 1.62955563\n",
            "Iteration 20, loss = 1.62906907\n",
            "Iteration 21, loss = 1.62869569\n",
            "Iteration 22, loss = 1.62822084\n",
            "Iteration 23, loss = 1.62784156\n",
            "Iteration 24, loss = 1.62735818\n",
            "Iteration 25, loss = 1.62696122\n",
            "Iteration 26, loss = 1.62658910\n",
            "Iteration 27, loss = 1.62615633\n",
            "Iteration 28, loss = 1.62573154\n",
            "Iteration 29, loss = 1.62534105\n",
            "Iteration 30, loss = 1.62494752\n",
            "Iteration 31, loss = 1.62458274\n",
            "Iteration 32, loss = 1.62417056\n",
            "Iteration 33, loss = 1.62379502\n",
            "Iteration 34, loss = 1.62343058\n",
            "Iteration 35, loss = 1.62311848\n",
            "Iteration 36, loss = 1.62273318\n",
            "Iteration 37, loss = 1.62237055\n",
            "Iteration 38, loss = 1.62195796\n",
            "Iteration 39, loss = 1.62168828\n",
            "Iteration 40, loss = 1.62129940\n",
            "Iteration 41, loss = 1.62094749\n",
            "Iteration 42, loss = 1.62066061\n",
            "Iteration 43, loss = 1.62030478\n",
            "Iteration 44, loss = 1.61997584\n",
            "Iteration 45, loss = 1.61963699\n",
            "Iteration 46, loss = 1.61930081\n",
            "Iteration 47, loss = 1.61905488\n",
            "Iteration 48, loss = 1.61871228\n",
            "Iteration 49, loss = 1.61840651\n",
            "Iteration 50, loss = 1.61806725\n",
            "Iteration 51, loss = 1.61781574\n",
            "Iteration 52, loss = 1.61749008\n",
            "Iteration 53, loss = 1.61724190\n",
            "Iteration 54, loss = 1.61694929\n",
            "Iteration 55, loss = 1.61665333\n",
            "Iteration 56, loss = 1.61637843\n",
            "Iteration 57, loss = 1.61610447\n",
            "Iteration 58, loss = 1.61581824\n",
            "Iteration 59, loss = 1.61550887\n",
            "Iteration 60, loss = 1.61527701\n",
            "Iteration 61, loss = 1.61499799\n",
            "Iteration 62, loss = 1.61476478\n",
            "Iteration 63, loss = 1.61448896\n",
            "Iteration 64, loss = 1.61423079\n",
            "Iteration 65, loss = 1.61402952\n",
            "Iteration 66, loss = 1.61375316\n",
            "Iteration 67, loss = 1.61349205\n",
            "Iteration 68, loss = 1.61327952\n",
            "Iteration 69, loss = 1.61303707\n",
            "Iteration 70, loss = 1.61280029\n",
            "Iteration 71, loss = 1.61255340\n",
            "Iteration 72, loss = 1.61231042\n",
            "Iteration 73, loss = 1.61210737\n",
            "Iteration 74, loss = 1.61186641\n",
            "Iteration 75, loss = 1.61165934\n",
            "Iteration 76, loss = 1.61143222\n",
            "Iteration 77, loss = 1.61124734\n",
            "Iteration 78, loss = 1.61099267\n",
            "Iteration 79, loss = 1.61081724\n",
            "Iteration 80, loss = 1.61062907\n",
            "Iteration 81, loss = 1.61039274\n",
            "Iteration 82, loss = 1.61019245\n",
            "Iteration 83, loss = 1.61001607\n",
            "Iteration 84, loss = 1.60979032\n",
            "Iteration 85, loss = 1.60962535\n",
            "Iteration 86, loss = 1.60945111\n",
            "Iteration 87, loss = 1.60925553\n",
            "Iteration 88, loss = 1.60902460\n",
            "Iteration 89, loss = 1.60886297\n",
            "Iteration 90, loss = 1.60869808\n",
            "Iteration 91, loss = 1.60851405\n",
            "Iteration 92, loss = 1.60834116\n",
            "Iteration 93, loss = 1.60816018\n",
            "Iteration 94, loss = 1.60799799\n",
            "Iteration 95, loss = 1.60784173\n",
            "Iteration 96, loss = 1.60766763\n",
            "Iteration 97, loss = 1.60751379\n",
            "Iteration 98, loss = 1.60735406\n",
            "Iteration 99, loss = 1.60718720\n",
            "Iteration 100, loss = 1.60703706\n",
            "Iteration 1, loss = 1.63835625\n",
            "Iteration 2, loss = 1.63288731\n",
            "Iteration 3, loss = 1.62751935\n",
            "Iteration 4, loss = 1.62374659\n",
            "Iteration 5, loss = 1.61996229\n",
            "Iteration 6, loss = 1.61622469\n",
            "Iteration 7, loss = 1.61371590\n",
            "Iteration 8, loss = 1.61078918\n",
            "Iteration 9, loss = 1.60863463\n",
            "Iteration 10, loss = 1.60628456\n",
            "Iteration 11, loss = 1.60448523\n",
            "Iteration 12, loss = 1.60366642\n",
            "Iteration 13, loss = 1.60223690\n",
            "Iteration 14, loss = 1.60117056\n",
            "Iteration 15, loss = 1.60032932\n",
            "Iteration 16, loss = 1.59964772\n",
            "Iteration 17, loss = 1.59925244\n",
            "Iteration 18, loss = 1.59907393\n",
            "Iteration 19, loss = 1.59858600\n",
            "Iteration 20, loss = 1.59818481\n",
            "Iteration 21, loss = 1.59812518\n",
            "Iteration 22, loss = 1.59787653\n",
            "Iteration 23, loss = 1.59785727\n",
            "Iteration 24, loss = 1.59766143\n",
            "Iteration 25, loss = 1.59762582\n",
            "Iteration 26, loss = 1.59760317\n",
            "Iteration 27, loss = 1.59750024\n",
            "Iteration 28, loss = 1.59743458\n",
            "Iteration 29, loss = 1.59726417\n",
            "Iteration 30, loss = 1.59724852\n",
            "Iteration 31, loss = 1.59708637\n",
            "Iteration 32, loss = 1.59692759\n",
            "Iteration 33, loss = 1.59677787\n",
            "Iteration 34, loss = 1.59664033\n",
            "Iteration 35, loss = 1.59653966\n",
            "Iteration 36, loss = 1.59637035\n",
            "Iteration 37, loss = 1.59630577\n",
            "Iteration 38, loss = 1.59602619\n",
            "Iteration 39, loss = 1.59587146\n",
            "Iteration 40, loss = 1.59567398\n",
            "Iteration 41, loss = 1.59546578\n",
            "Iteration 42, loss = 1.59525369\n",
            "Iteration 43, loss = 1.59502178\n",
            "Iteration 44, loss = 1.59478819\n",
            "Iteration 45, loss = 1.59455856\n",
            "Iteration 46, loss = 1.59452497\n",
            "Iteration 47, loss = 1.59422165\n",
            "Iteration 48, loss = 1.59376659\n",
            "Iteration 49, loss = 1.59351100\n",
            "Iteration 50, loss = 1.59322718\n",
            "Iteration 51, loss = 1.59287948\n",
            "Iteration 52, loss = 1.59257276\n",
            "Iteration 53, loss = 1.59223689\n",
            "Iteration 54, loss = 1.59190873\n",
            "Iteration 55, loss = 1.59155139\n",
            "Iteration 56, loss = 1.59121464\n",
            "Iteration 57, loss = 1.59067684\n",
            "Iteration 58, loss = 1.59022248\n",
            "Iteration 59, loss = 1.58981290\n",
            "Iteration 60, loss = 1.58933016\n",
            "Iteration 61, loss = 1.58885722\n",
            "Iteration 62, loss = 1.58831652\n",
            "Iteration 63, loss = 1.58777335\n",
            "Iteration 64, loss = 1.58723778\n",
            "Iteration 65, loss = 1.58666805\n",
            "Iteration 66, loss = 1.58609638\n",
            "Iteration 67, loss = 1.58536765\n",
            "Iteration 68, loss = 1.58483077\n",
            "Iteration 69, loss = 1.58407992\n",
            "Iteration 70, loss = 1.58338075\n",
            "Iteration 71, loss = 1.58268062\n",
            "Iteration 72, loss = 1.58193383\n",
            "Iteration 73, loss = 1.58109713\n",
            "Iteration 74, loss = 1.58029621\n",
            "Iteration 75, loss = 1.57939082\n",
            "Iteration 76, loss = 1.57855055\n",
            "Iteration 77, loss = 1.57761986\n",
            "Iteration 78, loss = 1.57676714\n",
            "Iteration 79, loss = 1.57580439\n",
            "Iteration 80, loss = 1.57481523\n",
            "Iteration 81, loss = 1.57374335\n",
            "Iteration 82, loss = 1.57265914\n",
            "Iteration 83, loss = 1.57156758\n",
            "Iteration 84, loss = 1.57044034\n",
            "Iteration 85, loss = 1.56930321\n",
            "Iteration 86, loss = 1.56807823\n",
            "Iteration 87, loss = 1.56696922\n",
            "Iteration 88, loss = 1.56563199\n",
            "Iteration 89, loss = 1.56432517\n",
            "Iteration 90, loss = 1.56295253\n",
            "Iteration 91, loss = 1.56156768\n",
            "Iteration 92, loss = 1.56011736\n",
            "Iteration 93, loss = 1.55874583\n",
            "Iteration 94, loss = 1.55722981\n",
            "Iteration 95, loss = 1.55567593\n",
            "Iteration 96, loss = 1.55411486\n",
            "Iteration 97, loss = 1.55253612\n",
            "Iteration 98, loss = 1.55092636\n",
            "Iteration 99, loss = 1.54925461\n",
            "Iteration 100, loss = 1.54760101\n",
            "Iteration 1, loss = 1.64119046\n",
            "Iteration 2, loss = 1.60807532\n",
            "Iteration 3, loss = 1.59950976\n",
            "Iteration 4, loss = 1.60528926\n",
            "Iteration 5, loss = 1.60536502\n",
            "Iteration 6, loss = 1.60396124\n",
            "Iteration 7, loss = 1.59677096\n",
            "Iteration 8, loss = 1.59400932\n",
            "Iteration 9, loss = 1.59255113\n",
            "Iteration 10, loss = 1.59086386\n",
            "Iteration 11, loss = 1.58772395\n",
            "Iteration 12, loss = 1.58282639\n",
            "Iteration 13, loss = 1.57489672\n",
            "Iteration 14, loss = 1.56584983\n",
            "Iteration 15, loss = 1.55675309\n",
            "Iteration 16, loss = 1.54431848\n",
            "Iteration 17, loss = 1.53133845\n",
            "Iteration 18, loss = 1.51801550\n",
            "Iteration 19, loss = 1.50039191\n",
            "Iteration 20, loss = 1.48233512\n",
            "Iteration 21, loss = 1.46334808\n",
            "Iteration 22, loss = 1.44289862\n",
            "Iteration 23, loss = 1.42157301\n",
            "Iteration 24, loss = 1.39946634\n",
            "Iteration 25, loss = 1.37729291\n",
            "Iteration 26, loss = 1.35631267\n",
            "Iteration 27, loss = 1.33649187\n",
            "Iteration 28, loss = 1.31716502\n",
            "Iteration 29, loss = 1.29865479\n",
            "Iteration 30, loss = 1.28261391\n",
            "Iteration 31, loss = 1.26642055\n",
            "Iteration 32, loss = 1.25239610\n",
            "Iteration 33, loss = 1.24039110\n",
            "Iteration 34, loss = 1.22938961\n",
            "Iteration 35, loss = 1.22060041\n",
            "Iteration 36, loss = 1.21015522\n",
            "Iteration 37, loss = 1.20219085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 38, loss = 1.19572898\n",
            "Iteration 39, loss = 1.18948575\n",
            "Iteration 40, loss = 1.18335521\n",
            "Iteration 41, loss = 1.17778486\n",
            "Iteration 42, loss = 1.17286695\n",
            "Iteration 43, loss = 1.16835640\n",
            "Iteration 44, loss = 1.16465404\n",
            "Iteration 45, loss = 1.16038364\n",
            "Iteration 46, loss = 1.15984637\n",
            "Iteration 47, loss = 1.15691338\n",
            "Iteration 48, loss = 1.15154954\n",
            "Iteration 49, loss = 1.14953706\n",
            "Iteration 50, loss = 1.14774270\n",
            "Iteration 51, loss = 1.14459094\n",
            "Iteration 52, loss = 1.14238951\n",
            "Iteration 53, loss = 1.14040589\n",
            "Iteration 54, loss = 1.13942893\n",
            "Iteration 55, loss = 1.13780220\n",
            "Iteration 56, loss = 1.13662859\n",
            "Iteration 57, loss = 1.13526897\n",
            "Iteration 58, loss = 1.13195249\n",
            "Iteration 59, loss = 1.13200701\n",
            "Iteration 60, loss = 1.13071806\n",
            "Iteration 61, loss = 1.12886415\n",
            "Iteration 62, loss = 1.12788126\n",
            "Iteration 63, loss = 1.12597266\n",
            "Iteration 64, loss = 1.12703346\n",
            "Iteration 65, loss = 1.12556602\n",
            "Iteration 66, loss = 1.12299319\n",
            "Iteration 67, loss = 1.12231697\n",
            "Iteration 68, loss = 1.12227512\n",
            "Iteration 69, loss = 1.12086540\n",
            "Iteration 70, loss = 1.12195915\n",
            "Iteration 71, loss = 1.11995363\n",
            "Iteration 72, loss = 1.11958953\n",
            "Iteration 73, loss = 1.11930234\n",
            "Iteration 74, loss = 1.11846937\n",
            "Iteration 75, loss = 1.11705676\n",
            "Iteration 76, loss = 1.11624044\n",
            "Iteration 77, loss = 1.11502631\n",
            "Iteration 78, loss = 1.11575673\n",
            "Iteration 79, loss = 1.11480775\n",
            "Iteration 80, loss = 1.11518721\n",
            "Iteration 81, loss = 1.11286895\n",
            "Iteration 82, loss = 1.11236514\n",
            "Iteration 83, loss = 1.11306819\n",
            "Iteration 84, loss = 1.11280508\n",
            "Iteration 85, loss = 1.11095005\n",
            "Iteration 86, loss = 1.11077575\n",
            "Iteration 87, loss = 1.11142969\n",
            "Iteration 88, loss = 1.11094067\n",
            "Iteration 89, loss = 1.11003587\n",
            "Iteration 90, loss = 1.10930940\n",
            "Iteration 91, loss = 1.10887152\n",
            "Iteration 92, loss = 1.10860067\n",
            "Iteration 93, loss = 1.10759170\n",
            "Iteration 94, loss = 1.10728009\n",
            "Iteration 95, loss = 1.10747844\n",
            "Iteration 96, loss = 1.10780812\n",
            "Iteration 97, loss = 1.10656861\n",
            "Iteration 98, loss = 1.10582628\n",
            "Iteration 99, loss = 1.10683654\n",
            "Iteration 100, loss = 1.10653845\n",
            "Iteration 1, loss = 1.64907749\n",
            "Iteration 2, loss = 1.64813124\n",
            "Iteration 3, loss = 1.64712354\n",
            "Iteration 4, loss = 1.64631744\n",
            "Iteration 5, loss = 1.64542535\n",
            "Iteration 6, loss = 1.64445820\n",
            "Iteration 7, loss = 1.64371649\n",
            "Iteration 8, loss = 1.64281806\n",
            "Iteration 9, loss = 1.64199507\n",
            "Iteration 10, loss = 1.64109431\n",
            "Iteration 11, loss = 1.64026523\n",
            "Iteration 12, loss = 1.63961434\n",
            "Iteration 13, loss = 1.63878009\n",
            "Iteration 14, loss = 1.63799878\n",
            "Iteration 15, loss = 1.63720572\n",
            "Iteration 16, loss = 1.63648715\n",
            "Iteration 17, loss = 1.63577237\n",
            "Iteration 18, loss = 1.63508431\n",
            "Iteration 19, loss = 1.63432748\n",
            "Iteration 20, loss = 1.63357713\n",
            "Iteration 21, loss = 1.63298689\n",
            "Iteration 22, loss = 1.63226510\n",
            "Iteration 23, loss = 1.63167765\n",
            "Iteration 24, loss = 1.63094519\n",
            "Iteration 25, loss = 1.63033346\n",
            "Iteration 26, loss = 1.62977117\n",
            "Iteration 27, loss = 1.62909992\n",
            "Iteration 28, loss = 1.62847537\n",
            "Iteration 29, loss = 1.62788070\n",
            "Iteration 30, loss = 1.62727801\n",
            "Iteration 31, loss = 1.62672906\n",
            "Iteration 32, loss = 1.62612301\n",
            "Iteration 33, loss = 1.62555898\n",
            "Iteration 34, loss = 1.62500930\n",
            "Iteration 35, loss = 1.62453123\n",
            "Iteration 36, loss = 1.62395351\n",
            "Iteration 37, loss = 1.62340598\n",
            "Iteration 38, loss = 1.62279813\n",
            "Iteration 39, loss = 1.62238147\n",
            "Iteration 40, loss = 1.62180629\n",
            "Iteration 41, loss = 1.62125803\n",
            "Iteration 42, loss = 1.62082606\n",
            "Iteration 43, loss = 1.62029418\n",
            "Iteration 44, loss = 1.61979641\n",
            "Iteration 45, loss = 1.61927892\n",
            "Iteration 46, loss = 1.61873376\n",
            "Iteration 47, loss = 1.61838449\n",
            "Iteration 48, loss = 1.61785097\n",
            "Iteration 49, loss = 1.61738160\n",
            "Iteration 50, loss = 1.61686690\n",
            "Iteration 51, loss = 1.61646123\n",
            "Iteration 52, loss = 1.61597253\n",
            "Iteration 53, loss = 1.61557738\n",
            "Iteration 54, loss = 1.61511764\n",
            "Iteration 55, loss = 1.61464181\n",
            "Iteration 56, loss = 1.61419540\n",
            "Iteration 57, loss = 1.61378549\n",
            "Iteration 58, loss = 1.61332747\n",
            "Iteration 59, loss = 1.61284143\n",
            "Iteration 60, loss = 1.61246517\n",
            "Iteration 61, loss = 1.61199466\n",
            "Iteration 62, loss = 1.61163273\n",
            "Iteration 63, loss = 1.61117129\n",
            "Iteration 64, loss = 1.61075219\n",
            "Iteration 65, loss = 1.61042537\n",
            "Iteration 66, loss = 1.60995632\n",
            "Iteration 67, loss = 1.60954515\n",
            "Iteration 68, loss = 1.60916657\n",
            "Iteration 69, loss = 1.60876451\n",
            "Iteration 70, loss = 1.60835927\n",
            "Iteration 71, loss = 1.60793893\n",
            "Iteration 72, loss = 1.60757792\n",
            "Iteration 73, loss = 1.60720866\n",
            "Iteration 74, loss = 1.60680315\n",
            "Iteration 75, loss = 1.60643608\n",
            "Iteration 76, loss = 1.60604664\n",
            "Iteration 77, loss = 1.60571938\n",
            "Iteration 78, loss = 1.60529918\n",
            "Iteration 79, loss = 1.60496309\n",
            "Iteration 80, loss = 1.60461779\n",
            "Iteration 81, loss = 1.60422589\n",
            "Iteration 82, loss = 1.60386391\n",
            "Iteration 83, loss = 1.60355275\n",
            "Iteration 84, loss = 1.60317129\n",
            "Iteration 85, loss = 1.60284971\n",
            "Iteration 86, loss = 1.60252425\n",
            "Iteration 87, loss = 1.60215156\n",
            "Iteration 88, loss = 1.60176290\n",
            "Iteration 89, loss = 1.60148248\n",
            "Iteration 90, loss = 1.60114654\n",
            "Iteration 91, loss = 1.60080881\n",
            "Iteration 92, loss = 1.60048234\n",
            "Iteration 93, loss = 1.60014879\n",
            "Iteration 94, loss = 1.59984306\n",
            "Iteration 95, loss = 1.59952118\n",
            "Iteration 96, loss = 1.59920295\n",
            "Iteration 97, loss = 1.59889721\n",
            "Iteration 98, loss = 1.59858937\n",
            "Iteration 99, loss = 1.59827173\n",
            "Iteration 100, loss = 1.59796295\n",
            "Iteration 1, loss = 1.64779961\n",
            "Iteration 2, loss = 1.63914481\n",
            "Iteration 3, loss = 1.63100153\n",
            "Iteration 4, loss = 1.62547698\n",
            "Iteration 5, loss = 1.61991167\n",
            "Iteration 6, loss = 1.61444046\n",
            "Iteration 7, loss = 1.61116523\n",
            "Iteration 8, loss = 1.60710368\n",
            "Iteration 9, loss = 1.60388323\n",
            "Iteration 10, loss = 1.60064166\n",
            "Iteration 11, loss = 1.59774794\n",
            "Iteration 12, loss = 1.59588190\n",
            "Iteration 13, loss = 1.59345889\n",
            "Iteration 14, loss = 1.59143651\n",
            "Iteration 15, loss = 1.58940683\n",
            "Iteration 16, loss = 1.58752853\n",
            "Iteration 17, loss = 1.58561619\n",
            "Iteration 18, loss = 1.58427416\n",
            "Iteration 19, loss = 1.58244968\n",
            "Iteration 20, loss = 1.58060415\n",
            "Iteration 21, loss = 1.57852723\n",
            "Iteration 22, loss = 1.57686429\n",
            "Iteration 23, loss = 1.57481356\n",
            "Iteration 24, loss = 1.57295392\n",
            "Iteration 25, loss = 1.57078159\n",
            "Iteration 26, loss = 1.56859108\n",
            "Iteration 27, loss = 1.56625693\n",
            "Iteration 28, loss = 1.56396562\n",
            "Iteration 29, loss = 1.56124354\n",
            "Iteration 30, loss = 1.55880434\n",
            "Iteration 31, loss = 1.55576840\n",
            "Iteration 32, loss = 1.55285849\n",
            "Iteration 33, loss = 1.54978191\n",
            "Iteration 34, loss = 1.54659407\n",
            "Iteration 35, loss = 1.54348950\n",
            "Iteration 36, loss = 1.53991319\n",
            "Iteration 37, loss = 1.53653304\n",
            "Iteration 38, loss = 1.53270288\n",
            "Iteration 39, loss = 1.52897525\n",
            "Iteration 40, loss = 1.52495970\n",
            "Iteration 41, loss = 1.52085751\n",
            "Iteration 42, loss = 1.51685380\n",
            "Iteration 43, loss = 1.51262890\n",
            "Iteration 44, loss = 1.50822110\n",
            "Iteration 45, loss = 1.50379713\n",
            "Iteration 46, loss = 1.49954208\n",
            "Iteration 47, loss = 1.49486348\n",
            "Iteration 48, loss = 1.48991664\n",
            "Iteration 49, loss = 1.48512586\n",
            "Iteration 50, loss = 1.48029223\n",
            "Iteration 51, loss = 1.47533017\n",
            "Iteration 52, loss = 1.47048809\n",
            "Iteration 53, loss = 1.46542202\n",
            "Iteration 54, loss = 1.46033511\n",
            "Iteration 55, loss = 1.45509895\n",
            "Iteration 56, loss = 1.45033172\n",
            "Iteration 57, loss = 1.44468238\n",
            "Iteration 58, loss = 1.43949159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 59, loss = 1.43428916\n",
            "Iteration 60, loss = 1.42903721\n",
            "Iteration 61, loss = 1.42376553\n",
            "Iteration 62, loss = 1.41855516\n",
            "Iteration 63, loss = 1.41329515\n",
            "Iteration 64, loss = 1.40806973\n",
            "Iteration 65, loss = 1.40297749\n",
            "Iteration 66, loss = 1.39789973\n",
            "Iteration 67, loss = 1.39258731\n",
            "Iteration 68, loss = 1.38756810\n",
            "Iteration 69, loss = 1.38250614\n",
            "Iteration 70, loss = 1.37748321\n",
            "Iteration 71, loss = 1.37252345\n",
            "Iteration 72, loss = 1.36773921\n",
            "Iteration 73, loss = 1.36286955\n",
            "Iteration 74, loss = 1.35825804\n",
            "Iteration 75, loss = 1.35332150\n",
            "Iteration 76, loss = 1.34871180\n",
            "Iteration 77, loss = 1.34415673\n",
            "Iteration 78, loss = 1.33973569\n",
            "Iteration 79, loss = 1.33545216\n",
            "Iteration 80, loss = 1.33099531\n",
            "Iteration 81, loss = 1.32686232\n",
            "Iteration 82, loss = 1.32258083\n",
            "Iteration 83, loss = 1.31855227\n",
            "Iteration 84, loss = 1.31445264\n",
            "Iteration 85, loss = 1.31071877\n",
            "Iteration 86, loss = 1.30674717\n",
            "Iteration 87, loss = 1.30316782\n",
            "Iteration 88, loss = 1.29927956\n",
            "Iteration 89, loss = 1.29570817\n",
            "Iteration 90, loss = 1.29213863\n",
            "Iteration 91, loss = 1.28869265\n",
            "Iteration 92, loss = 1.28532524\n",
            "Iteration 93, loss = 1.28219432\n",
            "Iteration 94, loss = 1.27881595\n",
            "Iteration 95, loss = 1.27564244\n",
            "Iteration 96, loss = 1.27267187\n",
            "Iteration 97, loss = 1.26961394\n",
            "Iteration 98, loss = 1.26673929\n",
            "Iteration 99, loss = 1.26387665\n",
            "Iteration 100, loss = 1.26120754\n",
            "Iteration 1, loss = 1.64253983\n",
            "Iteration 2, loss = 1.60749587\n",
            "Iteration 3, loss = 1.59334342\n",
            "Iteration 4, loss = 1.58766605\n",
            "Iteration 5, loss = 1.57781623\n",
            "Iteration 6, loss = 1.56130202\n",
            "Iteration 7, loss = 1.53550918\n",
            "Iteration 8, loss = 1.50826982\n",
            "Iteration 9, loss = 1.47600099\n",
            "Iteration 10, loss = 1.44095520\n",
            "Iteration 11, loss = 1.40325183\n",
            "Iteration 12, loss = 1.36387557\n",
            "Iteration 13, loss = 1.32750200\n",
            "Iteration 14, loss = 1.29057061\n",
            "Iteration 15, loss = 1.26640550\n",
            "Iteration 16, loss = 1.23730942\n",
            "Iteration 17, loss = 1.22179490\n",
            "Iteration 18, loss = 1.19984936\n",
            "Iteration 19, loss = 1.19102494\n",
            "Iteration 20, loss = 1.17371094\n",
            "Iteration 21, loss = 1.16942570\n",
            "Iteration 22, loss = 1.15882378\n",
            "Iteration 23, loss = 1.15318408\n",
            "Iteration 24, loss = 1.14580119\n",
            "Iteration 25, loss = 1.14052572\n",
            "Iteration 26, loss = 1.13830197\n",
            "Iteration 27, loss = 1.13718922\n",
            "Iteration 28, loss = 1.13337101\n",
            "Iteration 29, loss = 1.13013323\n",
            "Iteration 30, loss = 1.12964327\n",
            "Iteration 31, loss = 1.12532208\n",
            "Iteration 32, loss = 1.12080478\n",
            "Iteration 33, loss = 1.11866617\n",
            "Iteration 34, loss = 1.11843064\n",
            "Iteration 35, loss = 1.11873154\n",
            "Iteration 36, loss = 1.11476822\n",
            "Iteration 37, loss = 1.11783292\n",
            "Iteration 38, loss = 1.11355829\n",
            "Iteration 39, loss = 1.11585981\n",
            "Iteration 40, loss = 1.11191310\n",
            "Iteration 41, loss = 1.11172426\n",
            "Iteration 42, loss = 1.11046158\n",
            "Iteration 43, loss = 1.11014841\n",
            "Iteration 44, loss = 1.11035511\n",
            "Iteration 45, loss = 1.10850460\n",
            "Iteration 46, loss = 1.11214431\n",
            "Iteration 47, loss = 1.11781823\n",
            "Iteration 48, loss = 1.10876288\n",
            "Iteration 49, loss = 1.11074316\n",
            "Iteration 50, loss = 1.10849333\n",
            "Iteration 51, loss = 1.10827793\n",
            "Iteration 52, loss = 1.10701617\n",
            "Iteration 53, loss = 1.10631899\n",
            "Iteration 54, loss = 1.11000343\n",
            "Iteration 55, loss = 1.10874372\n",
            "Iteration 56, loss = 1.10748169\n",
            "Iteration 57, loss = 1.10928965\n",
            "Iteration 58, loss = 1.10379565\n",
            "Iteration 59, loss = 1.10539460\n",
            "Iteration 60, loss = 1.10472547\n",
            "Iteration 61, loss = 1.10394022\n",
            "Iteration 62, loss = 1.10434246\n",
            "Iteration 63, loss = 1.10253340\n",
            "Iteration 64, loss = 1.10718888\n",
            "Iteration 65, loss = 1.10562106\n",
            "Iteration 66, loss = 1.10493301\n",
            "Iteration 67, loss = 1.10251951\n",
            "Iteration 68, loss = 1.10302340\n",
            "Iteration 69, loss = 1.10228224\n",
            "Iteration 70, loss = 1.10337926\n",
            "Iteration 71, loss = 1.10197554\n",
            "Iteration 72, loss = 1.10322469\n",
            "Iteration 73, loss = 1.10099021\n",
            "Iteration 74, loss = 1.10549762\n",
            "Iteration 75, loss = 1.10245228\n",
            "Iteration 76, loss = 1.10320385\n",
            "Iteration 77, loss = 1.10089401\n",
            "Iteration 78, loss = 1.10189631\n",
            "Iteration 79, loss = 1.10143817\n",
            "Iteration 80, loss = 1.10009487\n",
            "Iteration 81, loss = 1.10156957\n",
            "Iteration 82, loss = 1.10035060\n",
            "Iteration 83, loss = 1.10082865\n",
            "Iteration 84, loss = 1.10014237\n",
            "Iteration 85, loss = 1.09976951\n",
            "Iteration 86, loss = 1.10027676\n",
            "Iteration 87, loss = 1.10054491\n",
            "Iteration 88, loss = 1.10087550\n",
            "Iteration 89, loss = 1.10016628\n",
            "Iteration 90, loss = 1.10094280\n",
            "Iteration 91, loss = 1.09980270\n",
            "Iteration 92, loss = 1.10150541\n",
            "Iteration 93, loss = 1.09808010\n",
            "Iteration 94, loss = 1.09855624\n",
            "Iteration 95, loss = 1.09791666\n",
            "Iteration 96, loss = 1.09926008\n",
            "Iteration 97, loss = 1.09860323\n",
            "Iteration 98, loss = 1.09734887\n",
            "Iteration 99, loss = 1.09801272\n",
            "Iteration 100, loss = 1.09775084\n",
            "Iteration 1, loss = 1.64909058\n",
            "Iteration 2, loss = 1.64814146\n",
            "Iteration 3, loss = 1.64713141\n",
            "Iteration 4, loss = 1.64632350\n",
            "Iteration 5, loss = 1.64542974\n",
            "Iteration 6, loss = 1.64446114\n",
            "Iteration 7, loss = 1.64371848\n",
            "Iteration 8, loss = 1.64281922\n",
            "Iteration 9, loss = 1.64199581\n",
            "Iteration 10, loss = 1.64109456\n",
            "Iteration 11, loss = 1.64026541\n",
            "Iteration 12, loss = 1.63961464\n",
            "Iteration 13, loss = 1.63878054\n",
            "Iteration 14, loss = 1.63799951\n",
            "Iteration 15, loss = 1.63720683\n",
            "Iteration 16, loss = 1.63648878\n",
            "Iteration 17, loss = 1.63577456\n",
            "Iteration 18, loss = 1.63508709\n",
            "Iteration 19, loss = 1.63433188\n",
            "Iteration 20, loss = 1.63358687\n",
            "Iteration 21, loss = 1.63300506\n",
            "Iteration 22, loss = 1.63235284\n",
            "Iteration 23, loss = 1.63183879\n",
            "Iteration 24, loss = 1.63124577\n",
            "Iteration 25, loss = 1.63076903\n",
            "Iteration 26, loss = 1.63042988\n",
            "Iteration 27, loss = 1.63001299\n",
            "Iteration 28, loss = 1.62969807\n",
            "Iteration 29, loss = 1.62935570\n",
            "Iteration 30, loss = 1.62901780\n",
            "Iteration 31, loss = 1.62862303\n",
            "Iteration 32, loss = 1.62824045\n",
            "Iteration 33, loss = 1.62786155\n",
            "Iteration 34, loss = 1.62749146\n",
            "Iteration 35, loss = 1.62712616\n",
            "Iteration 36, loss = 1.62676277\n",
            "Iteration 37, loss = 1.62644832\n",
            "Iteration 38, loss = 1.62606469\n",
            "Iteration 39, loss = 1.62576437\n",
            "Iteration 40, loss = 1.62541471\n",
            "Iteration 41, loss = 1.62503142\n",
            "Iteration 42, loss = 1.62472628\n",
            "Iteration 43, loss = 1.62439477\n",
            "Iteration 44, loss = 1.62405789\n",
            "Iteration 45, loss = 1.62370782\n",
            "Iteration 46, loss = 1.62333070\n",
            "Iteration 47, loss = 1.62309764\n",
            "Iteration 48, loss = 1.62271087\n",
            "Iteration 49, loss = 1.62239109\n",
            "Iteration 50, loss = 1.62204154\n",
            "Iteration 51, loss = 1.62174040\n",
            "Iteration 52, loss = 1.62142244\n",
            "Iteration 53, loss = 1.62112682\n",
            "Iteration 54, loss = 1.62080193\n",
            "Iteration 55, loss = 1.62045390\n",
            "Iteration 56, loss = 1.62015432\n",
            "Iteration 57, loss = 1.61984188\n",
            "Iteration 58, loss = 1.61951653\n",
            "Iteration 59, loss = 1.61917373\n",
            "Iteration 60, loss = 1.61889461\n",
            "Iteration 61, loss = 1.61854149\n",
            "Iteration 62, loss = 1.61827591\n",
            "Iteration 63, loss = 1.61793474\n",
            "Iteration 64, loss = 1.61762345\n",
            "Iteration 65, loss = 1.61735754\n",
            "Iteration 66, loss = 1.61702477\n",
            "Iteration 67, loss = 1.61670896\n",
            "Iteration 68, loss = 1.61640062\n",
            "Iteration 69, loss = 1.61610226\n",
            "Iteration 70, loss = 1.61578104\n",
            "Iteration 71, loss = 1.61546103\n",
            "Iteration 72, loss = 1.61518599\n",
            "Iteration 73, loss = 1.61487652\n",
            "Iteration 74, loss = 1.61456858\n",
            "Iteration 75, loss = 1.61426218\n",
            "Iteration 76, loss = 1.61394793\n",
            "Iteration 77, loss = 1.61367251\n",
            "Iteration 78, loss = 1.61334996\n",
            "Iteration 79, loss = 1.61306222\n",
            "Iteration 80, loss = 1.61275994\n",
            "Iteration 81, loss = 1.61245231\n",
            "Iteration 82, loss = 1.61213872\n",
            "Iteration 83, loss = 1.61187138\n",
            "Iteration 84, loss = 1.61154891\n",
            "Iteration 85, loss = 1.61125843\n",
            "Iteration 86, loss = 1.61097941\n",
            "Iteration 87, loss = 1.61066215\n",
            "Iteration 88, loss = 1.61033156\n",
            "Iteration 89, loss = 1.61007508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 90, loss = 1.60976998\n",
            "Iteration 91, loss = 1.60946764\n",
            "Iteration 92, loss = 1.60917019\n",
            "Iteration 93, loss = 1.60887155\n",
            "Iteration 94, loss = 1.60858408\n",
            "Iteration 95, loss = 1.60827956\n",
            "Iteration 96, loss = 1.60798557\n",
            "Iteration 97, loss = 1.60769554\n",
            "Iteration 98, loss = 1.60740273\n",
            "Iteration 99, loss = 1.60710377\n",
            "Iteration 100, loss = 1.60680583\n",
            "Iteration 1, loss = 1.64781045\n",
            "Iteration 2, loss = 1.63914437\n",
            "Iteration 3, loss = 1.63130236\n",
            "Iteration 4, loss = 1.62870073\n",
            "Iteration 5, loss = 1.62610620\n",
            "Iteration 6, loss = 1.62270228\n",
            "Iteration 7, loss = 1.62087862\n",
            "Iteration 8, loss = 1.61786135\n",
            "Iteration 9, loss = 1.61423649\n",
            "Iteration 10, loss = 1.61047928\n",
            "Iteration 11, loss = 1.60860428\n",
            "Iteration 12, loss = 1.60690428\n",
            "Iteration 13, loss = 1.60440082\n",
            "Iteration 14, loss = 1.60166111\n",
            "Iteration 15, loss = 1.59879720\n",
            "Iteration 16, loss = 1.59669953\n",
            "Iteration 17, loss = 1.59494527\n",
            "Iteration 18, loss = 1.59291156\n",
            "Iteration 19, loss = 1.59028249\n",
            "Iteration 20, loss = 1.58796437\n",
            "Iteration 21, loss = 1.58568235\n",
            "Iteration 22, loss = 1.58334226\n",
            "Iteration 23, loss = 1.58098097\n",
            "Iteration 24, loss = 1.57834369\n",
            "Iteration 25, loss = 1.57590643\n",
            "Iteration 26, loss = 1.57352045\n",
            "Iteration 27, loss = 1.57068808\n",
            "Iteration 28, loss = 1.56809364\n",
            "Iteration 29, loss = 1.56498445\n",
            "Iteration 30, loss = 1.56235750\n",
            "Iteration 31, loss = 1.55909522\n",
            "Iteration 32, loss = 1.55588307\n",
            "Iteration 33, loss = 1.55253114\n",
            "Iteration 34, loss = 1.54912020\n",
            "Iteration 35, loss = 1.54578205\n",
            "Iteration 36, loss = 1.54200889\n",
            "Iteration 37, loss = 1.53839935\n",
            "Iteration 38, loss = 1.53435173\n",
            "Iteration 39, loss = 1.53041149\n",
            "Iteration 40, loss = 1.52620299\n",
            "Iteration 41, loss = 1.52186680\n",
            "Iteration 42, loss = 1.51763355\n",
            "Iteration 43, loss = 1.51321116\n",
            "Iteration 44, loss = 1.50858787\n",
            "Iteration 45, loss = 1.50391046\n",
            "Iteration 46, loss = 1.49936474\n",
            "Iteration 47, loss = 1.49449352\n",
            "Iteration 48, loss = 1.48936048\n",
            "Iteration 49, loss = 1.48432715\n",
            "Iteration 50, loss = 1.47925695\n",
            "Iteration 51, loss = 1.47404473\n",
            "Iteration 52, loss = 1.46900157\n",
            "Iteration 53, loss = 1.46370827\n",
            "Iteration 54, loss = 1.45839378\n",
            "Iteration 55, loss = 1.45298384\n",
            "Iteration 56, loss = 1.44799015\n",
            "Iteration 57, loss = 1.44215488\n",
            "Iteration 58, loss = 1.43685471\n",
            "Iteration 59, loss = 1.43147525\n",
            "Iteration 60, loss = 1.42610919\n",
            "Iteration 61, loss = 1.42069992\n",
            "Iteration 62, loss = 1.41538046\n",
            "Iteration 63, loss = 1.41003488\n",
            "Iteration 64, loss = 1.40473367\n",
            "Iteration 65, loss = 1.39946439\n",
            "Iteration 66, loss = 1.39441401\n",
            "Iteration 67, loss = 1.38905703\n",
            "Iteration 68, loss = 1.38394198\n",
            "Iteration 69, loss = 1.37897553\n",
            "Iteration 70, loss = 1.37393214\n",
            "Iteration 71, loss = 1.36898538\n",
            "Iteration 72, loss = 1.36423072\n",
            "Iteration 73, loss = 1.35941025\n",
            "Iteration 74, loss = 1.35482961\n",
            "Iteration 75, loss = 1.35001116\n",
            "Iteration 76, loss = 1.34548501\n",
            "Iteration 77, loss = 1.34101434\n",
            "Iteration 78, loss = 1.33668366\n",
            "Iteration 79, loss = 1.33253540\n",
            "Iteration 80, loss = 1.32821937\n",
            "Iteration 81, loss = 1.32418589\n",
            "Iteration 82, loss = 1.32011039\n",
            "Iteration 83, loss = 1.31619898\n",
            "Iteration 84, loss = 1.31227065\n",
            "Iteration 85, loss = 1.30865987\n",
            "Iteration 86, loss = 1.30490359\n",
            "Iteration 87, loss = 1.30154318\n",
            "Iteration 88, loss = 1.29777066\n",
            "Iteration 89, loss = 1.29440491\n",
            "Iteration 90, loss = 1.29099192\n",
            "Iteration 91, loss = 1.28772990\n",
            "Iteration 92, loss = 1.28460760\n",
            "Iteration 93, loss = 1.28160137\n",
            "Iteration 94, loss = 1.27845082\n",
            "Iteration 95, loss = 1.27541914\n",
            "Iteration 96, loss = 1.27261422\n",
            "Iteration 97, loss = 1.26970254\n",
            "Iteration 98, loss = 1.26706091\n",
            "Iteration 99, loss = 1.26434243\n",
            "Iteration 100, loss = 1.26183481\n",
            "Iteration 1, loss = 1.64159668\n",
            "Iteration 2, loss = 1.61419011\n",
            "Iteration 3, loss = 1.60374708\n",
            "Iteration 4, loss = 1.60353252\n",
            "Iteration 5, loss = 1.60295820\n",
            "Iteration 6, loss = 1.60573630\n",
            "Iteration 7, loss = 1.60369660\n",
            "Iteration 8, loss = 1.60250281\n",
            "Iteration 9, loss = 1.60137521\n",
            "Iteration 10, loss = 1.59989753\n",
            "Iteration 11, loss = 1.60026768\n",
            "Iteration 12, loss = 1.60058463\n",
            "Iteration 13, loss = 1.60066189\n",
            "Iteration 14, loss = 1.60029113\n",
            "Iteration 15, loss = 1.59995244\n",
            "Iteration 16, loss = 1.59951095\n",
            "Iteration 17, loss = 1.59999717\n",
            "Iteration 18, loss = 1.60029157\n",
            "Iteration 19, loss = 1.60008359\n",
            "Iteration 20, loss = 1.59983084\n",
            "Iteration 21, loss = 1.59981092\n",
            "Iteration 22, loss = 1.59925204\n",
            "Iteration 23, loss = 1.59962461\n",
            "Iteration 24, loss = 1.59954026\n",
            "Iteration 25, loss = 1.59939076\n",
            "Iteration 26, loss = 1.59945336\n",
            "Iteration 27, loss = 1.59937995\n",
            "Iteration 28, loss = 1.59978807\n",
            "Iteration 29, loss = 1.59927690\n",
            "Iteration 30, loss = 1.59968927\n",
            "Iteration 31, loss = 1.59951501\n",
            "Iteration 32, loss = 1.59902608\n",
            "Iteration 33, loss = 1.59901486\n",
            "Iteration 34, loss = 1.59916878\n",
            "Iteration 35, loss = 1.59992387\n",
            "Iteration 36, loss = 1.59975836\n",
            "Iteration 37, loss = 1.60000523\n",
            "Iteration 38, loss = 1.59949725\n",
            "Iteration 39, loss = 1.59938824\n",
            "Iteration 40, loss = 1.59951540\n",
            "Iteration 41, loss = 1.59965223\n",
            "Iteration 42, loss = 1.59995940\n",
            "Iteration 43, loss = 1.59924854\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "acuracias = explorar_MLP()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tabela_resultados = pd.DataFrame(melhores_resultados.values())\n",
        "# Ordenar a tabela por acurácia\n",
        "tabela_ordenada = tabela_resultados.sort_values(by=\"accuracy\", ascending=False)\n",
        "print(\"\\nTabela ordenada por acurácia:\")\n",
        "print(tabela_ordenada)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OEImco1IURR",
        "outputId": "f11896f3-a930-4ddd-e5d4-8d8d442ba915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tabela ordenada por acurácia:\n",
            "     neurons activation  alpha   accuracy\n",
            "30        23       tanh   0.10  75.301205\n",
            "26        24       tanh   0.10  74.096386\n",
            "66        14       tanh   0.10  74.096386\n",
            "64        14   identity   0.10  73.493976\n",
            "74        12       tanh   0.10  73.493976\n",
            "..       ...        ...    ...        ...\n",
            "116        1   identity   0.10  51.807229\n",
            "17        26   logistic   0.10  51.204819\n",
            "33        22   logistic   0.10  50.602410\n",
            "115        2       relu   0.10  48.795181\n",
            "119        1       relu   0.01  45.180723\n",
            "\n",
            "[120 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "wAe3RwiDg18Z",
        "outputId": "d95c7fc2-d5e3-4ce6-c873-241b30418ee3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Transformar os resultados em um DataFrame para facilitar o plot\n",
        "resultados_df = pd.DataFrame([\n",
        "    {'neurons': k[0], 'activation': k[1], 'alpha': k[2], 'accuracy': v}\n",
        "    for k, v in acuracias.items()\n",
        "])\n",
        "\n",
        "# Configurar estilo do Seaborn\n",
        "seaborn.set(style=\"whitegrid\")\n",
        "\n",
        "# Gráfico simplificado: Acurácia por número de neurônios\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Criar um gráfico de linhas para cada função de ativação\n",
        "for act in resultados_df[\"activation\"].unique():\n",
        "    dados_filtrados = resultados_df[resultados_df[\"activation\"] == act]\n",
        "    medias = dados_filtrados.groupby(\"neurons\")[\"accuracy\"].mean()\n",
        "    plt.plot(medias.index, medias.values, marker=\"o\", label=f\"Ativação: {act}\")\n",
        "\n",
        "# Configurações do gráfico\n",
        "plt.title(\"Acurácia Média vs Número de Neurônios (Por Função de Ativação)\", fontsize=14)\n",
        "plt.xlabel(\"Número de Neurônios\", fontsize=12)\n",
        "plt.ylabel(\"Acurácia\", fontsize=12)\n",
        "plt.gca().invert_xaxis()  # Inverter o eixo X para mostrar de maior para menor\n",
        "plt.legend(title=\"Função de Ativação\", fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlsvftgEdJYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "outputId": "8d96b807-e9f1-406a-f468-a3da8d34d7e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Gráfico: Comparação de acurácia por taxa de aprendizado inicial (Alpha)\n",
        "plt.figure(figsize=(10, 6))\n",
        "seaborn.boxplot(data=resultados_df, x=\"alpha\", y=\"accuracy\", hue=\"activation\", palette=\"coolwarm\")\n",
        "plt.title(\"Distribuição da Acurácia por Alpha e Função de Ativação\")\n",
        "plt.xlabel(\"Alpha\")\n",
        "plt.ylabel(\"Acurácia\")\n",
        "plt.legend(title=\"Função de Ativação\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1MjeOfVMRMP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNA+Q80UjFgXlA+lFYXHkP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}